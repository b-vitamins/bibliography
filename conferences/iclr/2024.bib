@comment{@inproceedings{gowda2024conserveupdaterevise,
  author       = {Shruthi Gowda and Bahram Zonooz and Elahe Arani},
  title        = {Conserve-Update-Revise to Cure Generalization and Robustness Trade-off in Adversarial Training},
  booktitle    = {The Twelfth International Conference on Learning Representations},
  year         = {2024},
  publisher    = {OpenReview.net},
  url          = {https://openreview.net/forum?id=6IjN7oxjXt},
  pdf          = {https://openreview.net/pdf?id=6IjN7oxjXt},
  abstract     = {Adversarial training improves the robustness of neural networks against adversarial attacks, albeit at the expense of the trade-off between standard and robust generalization. To unveil the underlying factors driving this phenomenon, we examine the layer-wise learning capabilities of neural networks during the transition from a standard to an adversarial setting. Our analysis reveals that during adversarial training, the learning in the lower layers is limited due to the upper layers' fast adaptation to adversarial perturbations, resulting in a seesaw effect that inhibits the model from learning robust and generalizable features. To address this limitation, we introduce Conserve-Update-Revise ({CUR}), a training framework that enables robust and generalizable feature learning in adversarially trained models. {CUR} divides adversarial training into three distinct phases: conserve to preserve critical features, update to selectively enhance robustness, and revise to improve generalization. Extensive experiments across multiple datasets demonstrate {CUR}'s effectiveness in mitigating the robustness-generalization trade-off while simultaneously reducing robust overfitting.},
  note         = {Accepted as poster presentation}
  keywords     = {Adversarial training, Adversarial Robustness, Generalization, Robustness, Robust overfitting, Selective training},
}}

@inproceedings{hansen2024tdmpc2,
  abstract = {{TD-MPC} is a model-based reinforcement learning ({RL}) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present {TD-MPC2}: a series of improvements upon the {TD-MPC} algorithm. We demonstrate that {TD-MPC2} improves significantly over baselines across 104 online {RL} tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317{M} parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large {TD-MPC2} agents.},
  author = {Nicklas Hansen and Hao Su and Xiaolong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hansen2024tdmpc2.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Oxh5CstDJU},
  publisher = {OpenReview.net},
  title = {{TD-MPC2}: {S}calable, {R}obust {W}orld {M}odels for {C}ontinuous {C}ontrol},
  url = {https://openreview.net/forum?id=Oxh5CstDJU},
  year = {2024}
}

@inproceedings{shen2024labela,
  abstract = {Machine unlearning aims to remove information derived from forgotten data while preserving that of the remaining dataset in a well-trained model. With the increasing emphasis on data privacy, several approaches to machine unlearning have emerged. However, these methods typically rely on complete supervision throughout the unlearning process. Unfortunately, obtaining such supervision, whether for the forgetting or remaining data, can be impractical due to the substantial cost associated with annotating real-world datasets. In this work, we propose a supervision-free unlearning approach that operates without the need for labels during the unlearning process. We introduce a variational approach to approximate the distribution of representations for the remaining data. Leveraging this approximation, we adapt the original model to eliminate information from the forgotten data at the representation level. To further address the issue of lacking supervision information, we introduce a contrastive loss to facilitate the matching of representations between the remaining data and those of the original model, thus preserving predictive performance. Experimental results across various unlearning tasks demonstrate the effectiveness of our proposed method without using any labels, achieving comparable performance to state-of-the-art methods that rely on full supervision information.},
  author = {Shaofei Shen and Chenhao Zhang and Yawen Zhao and Alina Bialkowski and Weitong Chen and Miao Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024labela.pdf:pdf},
  note = {DBLP last modified: 2025-06-16},
  pdf = {https://openreview.net/pdf?id=SIZWiya7FE},
  publisher = {OpenReview.net},
  title = {Label-{A}gnostic {F}orgetting: {A} {S}upervision-{F}ree {U}nlearning in {D}eep {M}odels},
  url = {https://openreview.net/forum?id=SIZWiya7FE},
  year = {2024}
}

@inproceedings{yan2024neural,
  abstract = {Multi-agent path finding ({MAPF}) is the combinatorial problem of planning optimal collision-avoiding paths for multiple agents, with application to robotics, logistics, and transportation. Though many recent learning-based works have focused on large-scale combinatorial problems by guiding their decomposition into sequences of smaller subproblems, the combined spatiotemporal and time-restricted nature of {MAPF} poses a particular challenge for learning-based guidance of iterative approaches like large neighborhood search ({LNS}), which is already a state-of-the-art approach for {MAPF} even without learning. We address this challenge of neural-guided {LNS} for {MAPF} by designing an architecture which interleaves convolution and attention to efficiently represent {MAPF} subproblems, enabling practical guidance of {LNS} in benchmark settings. We demonstrate the speedup of our method over existing state-of-the-art {LNS}-based methods for {MAPF} as well as the robustness of our method to unseen settings. Our proposed method expands the horizon of effective deep learning-guided {LNS} methods into multi-path planning problems, and our proposed representation may be more broadly applicable for representing path-wise interactions.},
  author = {Zhongxia Yan and Cathy Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2NpAw2QJBY},
  publisher = {OpenReview.net},
  title = {Neural {N}eighborhood {S}earch for {M}ulti-agent {P}ath {F}inding},
  url = {https://openreview.net/forum?id=2NpAw2QJBY},
  year = {2024}
}

@inproceedings{chen2024less,
  abstract = {Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. In this work, we model the image attribution problem as a submodular subset selection problem. We propose a novel submodular function that can discover accurate small interpretation regions. To the best of our knowledge, this is the first attempt to model attribution as a submodular optimization problem. Furthermore, we impose four constraints that consider the effectiveness, confidence, consistency, and collaboration scores of sub-regions to select interpretable regions. This paper provides theoretical analysis to substantiate that our formulation is indeed submodular. Extensive experiments and comparisons with existing attribution algorithms demonstrate the superiority of our proposed method.},
  author = {Ruoyu Chen and Hua Zhang and Siyuan Liang and Jingzhi Li and Xiaochun Cao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024less.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=jKTUlxo5zy},
  publisher = {OpenReview.net},
  title = {Less is {M}ore: {F}ewer {I}nterpretable {R}egion via {S}ubmodular {S}ubset {S}election},
  url = {https://openreview.net/forum?id=jKTUlxo5zy},
  year = {2024}
}

@inproceedings{tian2024fairseg,
  abstract = {Advances in medical imaging and machine learning have facilitated the development of robust segmentation models for various clinical applications. However, these models often exhibit biases across different demographic groups, which can lead to disparities in medical diagnosis and treatment. Currently, no fairness datasets are available for medical image segmentation, which limits the development and evaluation of fair medical {AI} algorithms. To address this gap, we propose {Harvard-FairSeg}, the first fairness dataset for medical segmentation with 10,000 subject samples. We utilize the segment anything model ({SAM}) to generate initial segmentation masks and employ a fair error-bound scaling approach to reweight the loss function, using the upper error-bound in each identity group. We evaluate our approach across multiple segmentation metrics and propose equity-scaled segmentation performance metrics for fair comparisons. Through comprehensive experiments, we demonstrate that our fair error-bound scaling approach either has superior or comparable fairness performance to the state-of-the-art fairness learning models.},
  author = {Yu Tian and Min Shi and Yan Luo and Ava Kouhana and Tobias Elze and Mengyu Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024fairseg.pdf:pdf},
  note = {DBLP last modified: 2025-06-10},
  pdf = {https://openreview.net/pdf?id=qNrJJZAKI3},
  publisher = {OpenReview.net},
  title = {{FairSeg}: {A} {L}arge-{S}cale {M}edical {I}mage {S}egmentation {D}ataset for {F}airness {L}earning {U}sing {S}egment {A}nything {M}odel with {F}air {E}rror-{B}ound {S}caling},
  url = {https://openreview.net/forum?id=qNrJJZAKI3},
  year = {2024}
}

@inproceedings{nie2024towards,
  abstract = {Category-specific models are provenly valuable methods in {3D} single object tracking ({SOT}) regardless of {S}iamese or motion-centric paradigms. However, such over-specialized model designs incur redundant parameters, thus limiting the broader applicability of {3D} {SOT} task. This paper first introduces unified models that can simultaneously track objects across all categories using a single network with shared model parameters. Specifically, we propose to explicitly encode distinct attributes associated to different object categories, enabling the model to adapt to cross-category data. We develop {A}da{F}ormer, a novel point set representation learning network that adaptively encodes the dynamically varying shape and size information from cross-category data in a unified manner. Additionally, we incorporate the size and shape prior derived from the known template targets into the model's inputs and learning objective, facilitating the learning of unified representation. Equipped with such designs, we construct two category-unified models {S}iam{CUT} and {M}o{CUT}. Extensive experiments demonstrate that our models exhibit strong generalization and training stability. Furthermore, our category-unified models outperform the category-specific counterparts by a significant margin.},
  author = {Jiahao Nie and Zhiwei He and Xudong Lv and Xueyi Zhou and Dong-Kyu Chae and Fei Xie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/nie2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QlqdXrzzD1},
  publisher = {OpenReview.net},
  title = {Towards {C}ategory {U}nification of {3D} {S}ingle {O}bject {T}racking on {P}oint {C}louds},
  url = {https://openreview.net/forum?id=QlqdXrzzD1},
  year = {2024}
}

@inproceedings{mao2024neural,
  abstract = {We tackle the problem of {3D} reconstruction of dynamic scenes from multi-view videos. Previous dynamic scene reconstruction works either attempt to model the motion of {3D} points in space, or assume knowledge of the object priors and only track the deformation parameters, which limits them to handle a single articulated object or require depth maps as input. By contrast, we propose to directly estimate the change of the {S}igned {D}istance {F}unction ({SDF}), namely {SDF} flow, of the dynamic scene. We show that the {SDF} flow captures the evolution of the scene surface and derives the mathematical relation between the {SDF} flow and the scene flow, which allows us to compute the scene flow analytically from the {SDF} flow. Given multi-view images, we train a neural network to learn the {SDF} and the {SDF} flow of the dynamic scene. Our experiments on real-world multi-view video datasets show that our method can reconstruct dynamic scenes with higher quality compared to the state-of-the-art methods.},
  author = {Wei Mao and Richard Hartley and Mathieu Salzmann and Miaomiao Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/mao2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rzF0R6GOd4},
  publisher = {OpenReview.net},
  title = {Neural {SDF} {F}low for {3D} {R}econstruction of {D}ynamic {S}cenes},
  url = {https://openreview.net/forum?id=rzF0R6GOd4},
  year = {2024}
}

@inproceedings{ge2024incontext,
  abstract = {We propose the In-context Autoencoder ({ICAE}), leveraging the power of a large language model ({LLM}) to compress a long context into short compact memory slots that can be directly conditioned on by the {LLM} for various purposes. {ICAE} has two modules: a learnable encoder adapted with {LoRA} from an {LLM} for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target {LLM} that can condition on the memory slots for various purposes. {ICAE} is first pretrained using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context; Then, it is fine-tuned on instruction data for producing desirable responses to various prompts. Experiments demonstrate that our lightweight {ICAE}, introducing fewer than 1\% additional parameters, effectively achieves 4{X} context compression based on {L}lama, offering advantages in both improved latency and {GPU} memory cost during inference.},
  author = {Tao Ge and Jing Hu and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ge2024incontext.pdf:pdf},
  note = {DBLP last modified: 2025-05-19},
  pdf = {https://openreview.net/pdf?id=uREj4ZuGJE},
  publisher = {OpenReview.net},
  title = {In-context {A}utoencoder for {C}ontext {C}ompression in a {L}arge {L}anguage {M}odel},
  url = {https://openreview.net/forum?id=uREj4ZuGJE},
  year = {2024}
}

@inproceedings{barnes2024massively,
  abstract = {Inverse reinforcement learning ({IRL}) offers a powerful framework for learning humans' latent preferences in route recommendation. Yet no approach has successfully addressed planetary-scale problems with hundreds of millions of states and demonstration trajectories. This paper introduces techniques that enable {IRL} to scale massively. We revisit classic algorithms in this space and introduce a new {IRL} algorithm called {R}eceding {H}orizon {I}nverse {P}lanning ({RHIP}) that provides fine-grained control over performance trade-offs. The techniques we introduce are based on graph compression, spatial parallelization, and improved initialization conditions inspired by a connection to eigenvector algorithms. We test our approach on a real-world dataset of de-identified Google Maps driving trajectories. Our final {RHIP} policy achieves a 16--24\% relative improvement in global route match rate, i.e., the percentage of de-identified travelled routes that exactly match the suggested route. To the best of our knowledge, this represents the largest instance of {IRL} in a real world setting to date.},
  author = {Matt Barnes and Matthew Abueg and Oliver F. Lange and Matt Deeds and Jason Trader and Denali Molitor and Markus Wulfmeier and Shawn O'Banion},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/barnes2024massively.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=z3L59iGALM},
  publisher = {OpenReview.net},
  title = {Massively {S}calable {I}nverse {R}einforcement {L}earning in {G}oogle {M}aps},
  url = {https://openreview.net/forum?id=z3L59iGALM},
  year = {2024}
}

@inproceedings{zhao2024consciousnessi,
  abstract = {We propose {S}kipper, a model-based reinforcement learning framework utilizing spatio-temporal abstractions to generalize better in novel situations. {S}kipper automatically decomposes the given task into smaller, more manageable subtasks, and thus enables sparse decision-making and focused computation on the relevant parts of the environment. The decomposition relies on the extraction of an abstracted proxy problem represented as a directed graph, in which vertices correspond to checkpoints and edges correspond to skills. Learning to reach a checkpoint is a spatial abstraction, and the autonomous decomposition into skills represents a temporal abstraction. We establish where our approach is expected to be helpful and show that it achieves better generalization performance compared to some existing state-of-the-art hierarchical planning methods.},
  author = {Mingde Zhao and Safa Alver and Harm van Seijen and Romain Laroche and Doina Precup and Yoshua Bengio},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024consciousnessi.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eo9dHwtTFt},
  publisher = {OpenReview.net},
  title = {Consciousness-{I}nspired {S}patio-{T}emporal {A}bstractions for {B}etter {G}eneralization in {R}einforcement {L}earning},
  url = {https://openreview.net/forum?id=eo9dHwtTFt},
  year = {2024}
}

@inproceedings{laurent2024feature,
  abstract = {We formalize and study a phenomenon called feature collapse that makes precise the intuitive idea that entities playing a similar role in a learning task receive similar representations. As feature collapse requires a notion of task, we leverage a synthetic task in which a learner must classify sentences constituted of L tokens. We start by showing experimentally that feature collapse goes hand in hand with generalization. We then prove that, in the large sample limit, distinct tokens that play identical roles in the task receive identical local feature representations in the first layer of the network. This analysis shows that a neural network trained on this task provably learns interpretable and meaningful representations in its first layer.},
  author = {Thomas Laurent and James von Brecht and Xavier Bresson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/laurent2024feature.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gctmyMiPHH},
  publisher = {OpenReview.net},
  title = {Feature Collapse},
  url = {https://openreview.net/forum?id=gctmyMiPHH},
  year = {2024}
}

@inproceedings{ye2024leaveoneout,
  abstract = {We introduce an analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability ({LOOD}). This is key to measuring data memorization and information leakage as well as the influence of training data points in machine learning.},
  author = {Jiayuan Ye and Anastasia Borovykh and Soufiane Hayou and Reza Shokri},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024leaveoneout.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9RNfX0ah0K},
  publisher = {OpenReview.net},
  title = {Leave-one-out Distinguishability in Machine Learning},
  url = {https://openreview.net/forum?id=9RNfX0ah0K},
  year = {2024}
}

@inproceedings{wu2024lemur,
  abstract = {The demonstrated code-understanding capability of {LLMs} raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of {LLMs} and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.},
  author = {Haoze Wu and Clark W. Barrett and Nina Narodytska},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024lemur.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Q3YaCghZNt},
  publisher = {OpenReview.net},
  title = {Lemur: Integrating Large Language Models in Automated Program Verification},
  url = {https://openreview.net/forum?id=Q3YaCghZNt},
  year = {2024}
}

@inproceedings{fan2024neuronenhanced,
  abstract = {Neural networks have shown promising performance in collaborative filtering and matrix completion but the theoretical analysis is limited and there is still room for improvement in terms of the accuracy of recovering missing values. This paper presents a neuron-enhanced autoencoder matrix completion ({AEMC-NE}) method and applies it to collaborative filtering.},
  author = {Jicong Fan and Rui Chen and Zhao Zhang and Chris Ding},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fan2024neuronenhanced.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kPrxk6tUcg},
  publisher = {OpenReview.net},
  title = {Neuron-Enhanced {AutoEncoder} Matrix Completion and Collaborative Filtering: Theory and Practice},
  url = {https://openreview.net/forum?id=kPrxk6tUcg},
  year = {2024}
}

@inproceedings{zhou2024differentially,
  abstract = {We consider cross-silo federated linear contextual bandit ({LCB}) problem under differential privacy, where multiple silos interact with their respective local users and communicate via a central server to realize collaboration without sacrificing each user's privacy.},
  author = {Xingyu Zhou and Sayak Ray Chowdhury},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024differentially.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cuAxSHcsSX},
  publisher = {OpenReview.net},
  title = {On Differentially Private Federated Linear Contextual Bandits},
  url = {https://openreview.net/forum?id=cuAxSHcsSX},
  year = {2024}
}

@inproceedings{zhao2024pseudogeneralized,
  abstract = {Rendering scenes observed in a monocular video from novel viewpoints is a challenging problem. For static scenes the community has studied both scene-specific optimization techniques, which optimize on every test scene, and generalized techniques, which only run a deep net forward pass on a test scene. In contrast, for dynamic scenes, scene-specific optimization techniques exist, but, to our best knowledge, there is currently no generalized method for dynamic novel view synthesis from a given monocular video. To answer whether generalized dynamic novel view synthesis from monocular videos is possible today, we establish an analysis framework based on existing techniques and work toward the generalized approach.},
  author = {Xiaoming Zhao and Alex Colburn and Fangchang Ma and Miguel √Ångel Bautista and Joshua M. Susskind and Alexander G. Schwing},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024pseudogeneralized.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=QuVlUn4T2G},
  publisher = {OpenReview.net},
  title = {Pseudo-Generalized Dynamic View Synthesis from a Video},
  url = {https://openreview.net/forum?id=QuVlUn4T2G},
  year = {2024}
}

@inproceedings{yan2024rethinking,
  abstract = {With the arrival of the Noisy Intermediate-Scale Quantum ({NISQ}) era, Variational Quantum Algorithms ({VQAs}) have emerged as popular approaches to obtain possible quantum advantage in the relatively near future. In particular, how to effectively incorporate the common symmetries in physical systems as hard constraints in {VQAs} remains a critical and open question. In this paper, we revisit the Hamming Weight ({HW}) preserving ansatz and establish the links from ansatz to various symmetries and constraints, which both enlarges the usage of {HW} preserving ansatz and provides a coherent solution for constrained {VQAs}.},
  author = {Ge Yan and Hongxu Chen and Kaisen Pan and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=SL7djdVpde},
  publisher = {OpenReview.net},
  title = {Rethinking the symmetry-preserving circuits for constrained variational quantum algorithms},
  url = {https://openreview.net/forum?id=SL7djdVpde},
  year = {2024}
}

@inproceedings{gao2024magicdrive,
  abstract = {Recent advancements in diffusion models have significantly enhanced the data synthesis with {2D} control. Yet, precise {3D} control in street view generation, crucial for {3D} perception tasks, remains elusive. Specifically, utilizing Bird's-Eye View ({BEV}) as the primary condition often leads to challenges in geometry control (e.g., height), affecting the representation of object shapes, occlusion patterns, and road surface elevations, all of which are essential to perception data synthesis, especially for {3D} object detection tasks. In this paper, we introduce {MagicDrive}, a novel street view generation framework, offering diverse {3D} geometry controls including camera poses, road maps, and {3D} bounding boxes, together with textual descriptions, achieved through tailored encoding strategies. Besides, our design incorporates a cross-view attention module, ensuring consistency across multiple camera views.},
  author = {Ruiyuan Gao and Kai Chen and Enze Xie and Lanqing Hong and Zhenguo Li and Dit-Yan Yeung and Qiang Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024magicdrive.pdf:pdf},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=sBQwvucduK},
  publisher = {OpenReview.net},
  title = {{MagicDrive}: Street View Generation with Diverse {3D} Geometry Control},
  url = {https://openreview.net/forum?id=sBQwvucduK},
  year = {2024}
}

@inproceedings{lin2024differentially,
  abstract = {Generating differentially private ({DP}) synthetic data that closely resembles the original private data is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate {DP} Synthetic Data via {APIs} ({DPSDA}), where we treat foundation models as blackboxes and only utilize their inference {APIs}. Such {API}-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of {API}-based apps. These approaches can also leverage the power of large foundation models which are only accessible via their inference {APIs}. However, this comes with greater challenges due to strictly more restrictive model access and the need to protect privacy from the {API} provider. In this paper, we present a new framework called Private Evolution ({PE}) to solve this problem and show its initial promise on synthetic images.},
  author = {Zinan Lin and Sivakanth Gopi and Janardhan Kulkarni and Harsha Nori and Sergey Yekhanin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024differentially.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=YEhQs8POIo},
  publisher = {OpenReview.net},
  title = {Differentially Private Synthetic Data via Foundation Model {APIs} 1: Images},
  url = {https://openreview.net/forum?id=YEhQs8POIo},
  year = {2024}
}

@inproceedings{shen2024vdetr,
  abstract = {We introduce a highly performant {3D} object detector for point clouds using the {DETR} framework. The prior attempts all end up with suboptimal results because they fail to learn accurate inductive biases from the limited scale of training data. In particular, the queries often attend to points that are far away from the target objects, violating the locality principle in object detection. To address the limitation, we introduce a novel {3D} Vertex Relative Position Encoding ({3DV-RPE}) method which computes position encoding for each point based on its relative position to the {3D} boxes predicted by the queries in each decoder layer, thus providing clear information to guide the model to focus on points near the objects, in accordance with the principle of locality.},
  author = {Yichao Shen and Zigang Geng and Yuhui Yuan and Yutong Lin and Ze Liu and Chunyu Wang and Han Hu and Nanning Zheng and Baining Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024vdetr.pdf:pdf},
  note = {DBLP last modified: 2025-05-20},
  pdf = {https://openreview.net/pdf?id=NDkpxG94sF},
  publisher = {OpenReview.net},
  title = {{V-DETR}: {DETR} with Vertex Relative Position Encoding for {3D} Object Detection},
  url = {https://openreview.net/forum?id=NDkpxG94sF},
  year = {2024}
}

@inproceedings{zhu2024learning,
  abstract = {The paper introduces the MoleculAR Conformer Ensemble Learning (MARCEL) benchmark to evaluate learning on molecular conformer ensembles. It addresses how existing molecular representation learning methods often overlook molecular flexibility. The benchmark includes four datasets covering diverse molecule and reaction-level properties, extending beyond typical drug-like molecule studies.},
  author = {Yanqiao Zhu and Jeehyun Hwang and Keir Adams and Zhen Liu and Bozhao Nan and Brock Stenfors and Yuanqi Du and Jatin Chauhan and Olaf Wiest and Olexandr Isayev and Connor W. Coley and Yizhou Sun and Wei Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024learning.pdf:pdf},
  keywords = {conformer ensembles, geometric learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NSDszJ2uIV},
  publisher = {OpenReview.net},
  title = {Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks},
  url = {https://openreview.net/forum?id=NSDszJ2uIV},
  year = {2024}
}

@inproceedings{cho2024davidsonian,
  abstract = {Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers.},
  author = {Jaemin Cho and Yushi Hu and Jason Michael Baldridge and Roopal Garg and Peter Anderson and Ranjay Krishna and Mohit Bansal and Jordi Pont-Tuset and Su Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cho2024davidsonian.pdf:pdf},
  keywords = {text-to-image generation, text-to-image evaluation, Davidsonian semantics, large language models, scene graphs, visual question answering, question generation, benchmark},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ITq4ZRUT4a},
  publisher = {OpenReview.net},
  title = {Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-to-Image Generation},
  url = {https://openreview.net/forum?id=ITq4ZRUT4a},
  year = {2024}
}

@inproceedings{cui2024learning,
  abstract = {Recent reinforcement learning (RL) methods have achieved success in various domains. However, multi-agent RL (MARL) remains a challenge in terms of decentralization, partial observability and scalability to many agents. Meanwhile, collective behavior requires resolution of the aforementioned challenges, and remains of importance to many state-of-the-art applications such as active matter physics, self-organizing systems, opinion dynamics, and biological or robotic swarms.},
  author = {Kai Cui and Sascha H. Hauck and Christian Fabian and Heinz Koeppl},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cui2024learning.pdf:pdf},
  keywords = {Mean Field Control, Multi-Agent Reinforcement Learning, Partial Observability, Collective Behavior},
  note = {DBLP last modified: 2024-09-27},
  pdf = {https://openreview.net/pdf?id=99tKiMVJhY},
  publisher = {OpenReview.net},
  title = {Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior},
  url = {https://openreview.net/forum?id=99tKiMVJhY},
  year = {2024}
}

@inproceedings{cardoso2024monte,
  abstract = {Ill-posed linear inverse problems arise frequently in various applications, from computational photography to medical imaging. A recent line of research exploits Bayesian inference with informative priors to handle the ill-posedness of such problems. Amongst such priors, score-based generative models (SGM) have recently been successfully applied to several different inverse problems. In this study, we exploit the particular structure of the prior defined by the SGM to define a sequence of intermediate linear inverse problems. As the noise level decreases, the posteriors of these inverse problems get closer to the target posterior of the original inverse problem. To sample from this sequence of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods.},
  author = {Gabriel Cardoso and Yazid Janati el idrissi and Sylvain Le Corff and Eric Moulines},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cardoso2024monte.pdf:pdf},
  keywords = {Monte Carlo, Denoising Diffusion model, Score-based generative models, Sequential Monte Carlo, Bayesian Inverse Problems, Generative Models},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=nHESwXvxWK},
  publisher = {OpenReview.net},
  title = {Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems},
  url = {https://openreview.net/forum?id=nHESwXvxWK},
  year = {2024}
}

@inproceedings{wang2024lightweight,
  abstract = {The paper addresses the challenge of diverse client participation statistics in federated learning (FL). The authors propose a new algorithm called FedAU that improves FedAvg by adaptively weighting the client updates based on online estimates of the optimal weights without knowing the statistics of client participation.},
  author = {Shiqiang Wang and Mingyue Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024lightweight.pdf:pdf},
  keywords = {Federated learning, Partial client participation, Adaptation, Aggregation weights},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ZKEuFKfCKA},
  publisher = {OpenReview.net},
  title = {A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging},
  url = {https://openreview.net/forum?id=ZKEuFKfCKA},
  year = {2024}
}

@inproceedings{zhou2024dnabert2,
  abstract = {The paper discusses developing an efficient genome foundation model by replacing traditional k-mer tokenization with Byte Pair Encoding (BPE). The authors introduce DNABERT-2, which aims to overcome computational inefficiencies in genome language modeling. They also propose the Genome Understanding Evaluation (GUE) benchmark, which includes 36 datasets across 9 tasks. DNABERT-2 achieves comparable performance to state-of-the-art models with 21x fewer parameters and reduces GPU pre-training time by approximately 92x.},
  author = {Zhihan Zhou and Yanrong Ji and Weijian Li and Pratik Dutta and Ramana V. Davuluri and Han Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024dnabert2.pdf:pdf},
  keywords = {DNA, Genome, Language Model, Foundation Model, Benchmark},
  note = {DBLP last modified: 2025-06-02},
  pdf = {https://openreview.net/pdf?id=oMLQB4EZE1},
  publisher = {OpenReview.net},
  title = {DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genomes},
  url = {https://openreview.net/forum?id=oMLQB4EZE1},
  year = {2024}
}

@inproceedings{meng2024representation,
  abstract = {Masked Language Modeling (MLM) has been one of the most prominent approaches for pretraining bidirectional text encoders due to its simplicity and effectiveness. One notable concern about MLM is that the special [MASK] symbol causes a discrepancy between pretraining data and downstream data as it is present only in pretraining but not in fine-tuning. The authors demonstrate that MLM pretraining allocates some model dimensions exclusively for representing [MASK] tokens, resulting in a representation deficiency for real tokens. They propose MAE-LM, which pretrains the Masked Autoencoder architecture with MLM where [MASK] tokens are excluded from the encoder.},
  author = {Yu Meng and Jitin Krishnan and Sinong Wang and Qifan Wang and Yuning Mao and Han Fang and Marjan Ghazvininejad and Jiawei Han and Luke Zettlemoyer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/meng2024representation.pdf:pdf},
  keywords = {Masked Language Modeling},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=b3l0piOrGU},
  publisher = {OpenReview.net},
  title = {Representation Deficiency in Masked Language Modeling},
  url = {https://openreview.net/forum?id=b3l0piOrGU},
  year = {2024}
}

@inproceedings{jiang2024megatts,
  abstract = {Zero-shot text-to-speech (TTS) aims to synthesize voices with unseen speech prompts, which significantly reduces the data and computation requirements for voice cloning by skipping the fine-tuning process. Previous works limited by single-sentence prompts and prosodic information highly coupled with timbre are addressed through an acoustic autoencoder separating prosody and timbre, multi-reference timbre encoder, and prosody latent language model (P-LLM).},
  author = {Ziyue Jiang and Jinglin Liu and Yi Ren and Jinzheng He and Zhenhui Ye and Shengpeng Ji and Qian Yang and Chen Zhang and Pengfei Wei and Chunfeng Wang and Xiang Yin and Zejun MA and Zhou Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024megatts.pdf:pdf},
  keywords = {Prompting Mechanisms, Zero-Shot Text-to-Speech, Multi-Sentence Prompts},
  note = {DBLP last modified: 2025-05-27},
  pdf = {https://openreview.net/pdf?id=mvMI3N4AvD},
  publisher = {OpenReview.net},
  title = {Mega-TTS 2: Boosting Prompting Mechanisms for Zero-Shot Speech Synthesis},
  url = {https://openreview.net/forum?id=mvMI3N4AvD},
  year = {2024}
}

@inproceedings{zhang2024cameras,
  abstract = {Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparsely sampled views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays.},
  author = {Jason Y. Zhang and Amy Lin and Moneish Kumar and Tzu-Hsuan Yang and Deva Ramanan and Shubham Tulsiani},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024cameras.pdf:pdf},
  keywords = {3D Computer Vision, Pose Estimation, Diffusion},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=EanCFCwAjM},
  publisher = {OpenReview.net},
  title = {Cameras as Rays: Pose Estimation via Ray Diffusion},
  url = {https://openreview.net/forum?id=EanCFCwAjM},
  year = {2024}
}

@inproceedings{gong2024listen,
  abstract = {Most existing audio models map sounds to predefined labels. Humans can analyze sounds beyond simple classification. This paper proposes a new audio foundation model called LTU and creates the OpenAQA-5M dataset with 1.9M closed-ended and 3.7M open-ended audio tuples. LTU demonstrates emerging audio reasoning and comprehension abilities.},
  author = {Yuan Gong and Hongyin Luo and Alexander H. Liu and Leonid Karlinsky and James R. Glass},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gong2024listen.pdf:pdf},
  keywords = {audio processing, large language model},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=nBZBPXdJlC},
  publisher = {OpenReview.net},
  title = {Listen, Think, and Understand},
  url = {https://openreview.net/forum?id=nBZBPXdJlC},
  year = {2024}
}

@inproceedings{cai2024fast,
  abstract = {We study the sparse phase retrieval problem, which seeks to recover a sparse signal from a limited set of magnitude-only measurements. In contrast to prevalent sparse phase retrieval algorithms that primarily use first-order methods, we propose an innovative second-order algorithm that employs a Newton-type method with hard thresholding.},
  author = {Jian-Feng Cai and Yu Long and Ruixue Wen and Jiaxi Ying},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cai2024fast.pdf:pdf},
  keywords = {Sparse Phase Retrieval, Quadratic Convergence, Newton-type Method, Hard Thresholding, Nonconvex Optimization},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BlkxbI6vzl},
  publisher = {OpenReview.net},
  title = {A Fast and Provable Algorithm for Sparse Phase Retrieval},
  url = {https://openreview.net/forum?id=BlkxbI6vzl},
  year = {2024}
}

@inproceedings{pham2024let,
  abstract = {Discussion and debate among Large Language Models ({LLM}s) have gained considerable attention due to their potential to enhance the reasoning ability of {LLM}s. Although natural language is an obvious choice for communication due to {LLM}'s language understanding capability, the token sampling step needed when generating natural language poses a potential risk of information loss, which may negatively affect the performance of multi-agent debate. In this work, we introduce a communication protocol that allows models to exchange information directly through their embeddings, thereby avoiding potential information loss from token sampling.},
  author = {Chau Pham and Boyi Liu and Yingxiang Yang and Zhengyu Chen and Tianyi Liu and Jianbo Yuan and Bryan A. Plummer and Zhaoran Wang and Hongxia Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pham2024let.pdf:pdf},
  keywords = {multiagent debate, large language models, inter-model communication, embedding representation},
  note = {DBLP last modified: 2025-02-06},
  pdf = {https://openreview.net/pdf?id=sehRvaIPQQ},
  publisher = {OpenReview.net},
  title = {Let Models Speak Ciphers: Multiagent Debate through Embeddings},
  url = {https://openreview.net/forum?id=sehRvaIPQQ},
  year = {2024}
}

@inproceedings{li2024chain,
  abstract = {Generating a sequence of intermediate steps, a.k.a., a chain of thought ({CoT}), is a highly effective method to improve the accuracy of large language models ({LLM}s) on arithmetics and symbolic reasoning tasks. However, the mechanism behind {CoT} remains unclear. This work provides a theoretical understanding of the power of {CoT} for decoder-only transformers through the lens of expressiveness.},
  author = {Zhiyuan Li and Hong Liu and Denny Zhou and Tengyu Ma},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024chain.pdf:pdf},
  keywords = {Chain of thought, language modeling, circuit complexity, deep learning theory},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3EWTEy9MTM},
  publisher = {OpenReview.net},
  title = {Chain of Thought Empowers Transformers to Solve Inherently Serial Problems},
  url = {https://openreview.net/forum?id=3EWTEy9MTM},
  year = {2024}
}

@inproceedings{li2024mend,
  abstract = {Large Language models ({LLM}s) have demonstrated impressive in-context learning ({ICL}) capabilities, where a {LLM} makes predictions for a given test input together with a few input-output pairs (demonstrations). The paper presents Meta Demonstration Distillation ({MEND}) to address computational challenges in demonstration processing. {MEND} leverages the {LLM}'s generative ability to create synthetic demonstrations that capture essential information, addressing the computational challenge of processing many examples in the context window.},
  author = {Yichuan Li and Xiyao Ma and Sixing Lu and Kyumin Lee and Xiaohu Liu and Chenlei Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024mend.pdf:pdf},
  keywords = {in-context learning, language modeling, data distillation, knowledge distillation},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2Y5kBPtU0o},
  publisher = {OpenReview.net},
  title = {{MEND}: Meta Demonstration Distillation for Efficient and Effective In-Context Learning},
  url = {https://openreview.net/forum?id=2Y5kBPtU0o},
  year = {2024}
}

@inproceedings{bhatt2024crossq,
  abstract = {Sample efficiency is a crucial problem in deep reinforcement learning. Recent algorithms, such as {REDQ} and {DroQ}, found a way to improve the sample efficiency by increasing the update-to-data ({UTD}) ratio to 20 gradient update steps on the critic per environment sample. However, this comes at the expense of a greatly increased computational cost. To reduce this computational burden, we introduce {CrossQ}: A lightweight algorithm for continuous control tasks that makes careful use of Batch Normalization and removes target networks to surpass the current state-of-the-art in sample efficiency while maintaining a low {UTD} ratio of 1.},
  author = {Aditya Bhatt and Daniel Palenicek and Boris Belousov and Max Argus and Artemij Amiranashvili and Thomas Brox and Jan Peters},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bhatt2024crossq.pdf:pdf},
  keywords = {deep reinforcement learning, batch normalization, sample efficiency, continuous control},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=PczQtTsTIX},
  publisher = {OpenReview.net},
  title = {{CrossQ}: Batch Normalization in Deep Reinforcement Learning for Greater Sample Efficiency and Simplicity},
  url = {https://openreview.net/forum?id=PczQtTsTIX},
  year = {2024}
}

@inproceedings{vargas2024transport,
  abstract = {We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of the Controlled Monte Carlo Diffusion sampler ({CMCD}) for Bayesian computation, a score-based annealing technique that crucially adapts both forward and backward dynamics in a diffusion model. Additionally, we clarify the relationship between the {EM}-algorithm and iterative proportional fitting ({IPF}) for {S}chr{\"{o}}dinger bridges, deriving a regularised objective that bypasses the iterative bottleneck of standard {IPF}-updates.},
  author = {Francisco Vargas and Shreyas Padhy and Denis Blessing and Nikolas N{\"{u}}sken},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/vargas2024transport.pdf:pdf},
  keywords = {optimal transport, variational inference, diffusion models, Monte Carlo methods, Schr{\"{o}}dinger bridges},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PP1rudnxiW},
  publisher = {OpenReview.net},
  title = {Transport meets Variational Inference: Controlled Monte Carlo Diffusions},
  url = {https://openreview.net/forum?id=PP1rudnxiW},
  year = {2024}
}

@inproceedings{wang2024fusing,
  abstract = {Training {AI} models that generalize across tasks and domains has long been among the open problems driving {AI} research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts ({FoE}) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning.},
  author = {Hongyi Wang and Felipe Maia Polo and Yuekai Sun and Souvik Kundu and Eric P. Xing and Mikhail Yurochkin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024fusing.pdf:pdf},
  keywords = {model fusion, ensemble methods, domain adaptation, foundation models},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PhMrGCMIRL},
  publisher = {OpenReview.net},
  title = {Fusing Models with Complementary Expertise},
  url = {https://openreview.net/forum?id=PhMrGCMIRL},
  year = {2024}
}

@inproceedings{bianchi2024safetytuned,
  abstract = {Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3\% safety examples (a few hundred demonstrations) when fine-tuning a model like {LLaMA} can substantially improve its safety.},
  author = {Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul R{\"{o}}ttger and Dan Jurafsky and Tatsunori Hashimoto and James Zou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bianchi2024safetytuned.pdf:pdf},
  keywords = {language model safety, instruction tuning, AI alignment, harmfulness},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gT5hALch9z},
  publisher = {OpenReview.net},
  title = {Safety-Tuned {LLaMA}s: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
  url = {https://openreview.net/forum?id=gT5hALch9z},
  year = {2024}
}

@inproceedings{severo2024unreasonable,
  abstract = {We show how perceptual embeddings of the visual system can be constructed at inference-time with no training data or deep neural network features. Our perceptual embeddings are solutions to a weighted least squares ({WLS}) problem, defined at the pixel-level, and solved at inference-time, that can capture global and local image characteristics. The distance in embedding space is used to define a perceptual similarity metric which we call {LASI}: Linear Autoregressive Similarity Index. Experiments on full-reference image quality assessment datasets show {LASI} performs competitively with learned deep feature based methods like {LPIPS} and {PIM}, at a similar computational cost to hand-crafted methods such as {MS-SSIM}.},
  author = {Daniel Severo and Lucas Theis and Johannes Ball{\'{e}}},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/severo2024unreasonable.pdf:pdf},
  keywords = {perceptual metrics, image quality assessment, linear prediction, visual perception},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=e4FG5PJ9uC},
  publisher = {OpenReview.net},
  title = {The Unreasonable Effectiveness of Linear Prediction as a Perceptual Metric},
  url = {https://openreview.net/forum?id=e4FG5PJ9uC},
  year = {2024}
}

@inproceedings{jiang2024efficient3dim,
  abstract = {The task of novel view synthesis aims to generate unseen perspectives of an object or scene from a limited set of input images. Nevertheless, synthesizing novel views from a single image remains a significant challenge. Previous approaches tackle this problem by adopting mesh prediction, multi-plane image construction, or more advanced techniques such as neural radiance fields. Recently, a pre-trained diffusion model that is specifically designed for 2D image synthesis has demonstrated its capability in producing photorealistic novel views, if sufficiently optimized with a 3D finetuning task. However, training such a powerful diffusion model requires a vast volume of training data and model parameters, resulting in a notoriously long time and high computational costs. We introduce Efficient-{3DiM}, a simple but effective framework to learn a single-image novel-view synthesizer, and several pragmatic strategies to reduce the training overhead to a manageable scale.},
  author = {Yifan Jiang and Hao Tang and Jen-Hao Rick Chang and Liangchen Song and Zhangyang Wang and Liangliang Cao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024efficient3dim.pdf:pdf},
  keywords = {novel view synthesis, diffusion models, 3D generation, single image, computational efficiency},
  note = {DBLP last modified: 2024-11-21},
  pdf = {https://openreview.net/pdf?id=3eFMnZ3N4J},
  publisher = {OpenReview.net},
  title = {Efficient-{3DiM}: Learning a Generalizable Single-image Novel-view Synthesizer in One Day},
  url = {https://openreview.net/forum?id=3eFMnZ3N4J},
  year = {2024}
}

@inproceedings{dai2024periodicity,
  abstract = {Convolutional neural network (CNN)-based and Transformer-based methods have recently made significant strides in time series forecasting, which excel at modeling local temporal variations or capturing long-term dependencies. However, real-world time series usually contain intricate temporal patterns, thus making it challenging for existing methods that mainly focus on temporal variations modeling from the 1D time series directly. We propose a novel Periodicity Decoupling Framework (PDF) for long-term series forecasting by capturing 2D temporal variation modeling.},
  author = {Tao Dai and Beiliang Wu and Peiyuan Liu and Naiqi Li and Jigang Bao and Yong Jiang and Shu-Tao Xia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dai2024periodicity.pdf:pdf},
  keywords = {Long-term time series forecasting, Transformer, CNN},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dp27P5HBBt},
  publisher = {OpenReview.net},
  title = {Periodicity Decoupling Framework for Long-term Series Forecasting},
  url = {https://openreview.net/forum?id=dp27P5HBBt},
  year = {2024}
}

@inproceedings{xue2024sampleefficient,
  abstract = {Quality-Diversity (QD) algorithms, as a subset of evolutionary algorithms, have emerged as a powerful optimization paradigm with the aim of generating a set of high-quality and diverse solutions. Although QD has demonstrated competitive performance in reinforcement learning, its low sample efficiency remains a significant impediment for real-world applications. We propose a Cooperative Coevolution QD (CCQD) framework that decomposes policy networks into representation and decision layers, achieving approximately 200\% improvement in sample efficiency.},
  author = {Ke Xue and Ren-Jian Wang and Pengyi Li and Dong Li and Jianye Hao and Chao Qian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xue2024sampleefficient.pdf:pdf},
  keywords = {Quality-Diversity, Reinforcement Learning, Evolutionary Algorithms},
  note = {DBLP last modified: 2024-07-30, ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=JDud6zbpFv},
  publisher = {OpenReview.net},
  title = {Sample-Efficient Quality-Diversity by Cooperative Coevolution},
  url = {https://openreview.net/forum?id=JDud6zbpFv},
  year = {2024}
}

@inproceedings{huang2024humanity,
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable capabilities across diverse domains like clinical medicine, legal consultation, and education. We propose ``PsychoBench'', a framework for evaluating psychological aspects of LLMs, comprising thirteen scales from clinical psychology categorized into personality traits, interpersonal relationships, motivational tests, and emotional abilities. We studied five models: text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b, using a jailbreak approach to test LLMs' intrinsic natures.},
  author = {Jen-tse Huang and Wenxuan Wang and Eric John Li and Man Ho Lam and Shujie Ren and Youliang Yuan and Wenxiang Jiao and Zhaopeng Tu and Michael R. Lyu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024humanity.pdf:pdf},
  keywords = {LLM, Benchmark, Evaluation, Psychometrics},
  note = {DBLP last modified: 2024-08-08, ICLR 2024 Oral},
  pdf = {https://openreview.net/pdf?id=H3UayAQWoE},
  publisher = {OpenReview.net},
  title = {On the Humanity of Conversational {AI}: Evaluating the Psychological Portrayal of {LLMs}},
  url = {https://openreview.net/forum?id=H3UayAQWoE},
  year = {2024}
}

@inproceedings{he2024candidate,
  abstract = {This paper introduces a novel approach to partial-label learning (PLL), focusing on a ``data-centric perspective'' to address challenges when candidate label sets become too large. We propose a new task called Candidate Label Set Pruning (CLSP), which aims to filter out potential false candidate labels without additional training. The method examines the inconsistency between representation and label spaces, potentially improving performance of deep partial-label learning methods.},
  author = {Shuo He and Chaojie Wang and Guowu Yang and Lei Feng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024candidate.pdf:pdf},
  keywords = {partial label learning, label disambiguation, candidate label set pruning},
  note = {DBLP last modified: 2024-07-30, ICLR 2024 Oral},
  pdf = {https://openreview.net/pdf?id=Fk5IzauJ7F},
  publisher = {OpenReview.net},
  title = {Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning},
  url = {https://openreview.net/forum?id=Fk5IzauJ7F},
  year = {2024}
}

@inproceedings{xu2024demystifying,
  abstract = {We argue that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. We introduce MetaCLIP, a method that takes a raw data pool and metadata to create a balanced subset. MetaCLIP with 400M image-text pairs outperforms original CLIP, achieves 70.8\% accuracy on zero-shot ImageNet classification, scaling to 1B data reaches 72.4\% accuracy, and ViT-H model achieves 80.5\% accuracy.},
  author = {Hu Xu and Saining Xie and Xiaoqing Ellen Tan and Po-Yao Huang and Russell Howes and Vasu Sharma and Shang-Wen Li and Gargi Ghosh and Luke Zettlemoyer and Christoph Feichtenhofer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024demystifying.pdf:pdf},
  keywords = {multi-modal pretraining, CLIP, image, text},
  note = {DBLP last modified: 2024-07-29, ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=5BCFlnfE1g},
  publisher = {OpenReview.net},
  title = {Demystifying {CLIP} Data},
  url = {https://openreview.net/forum?id=5BCFlnfE1g},
  year = {2024}
}

@inproceedings{kim2024generalized,
  abstract = {Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. We develop a neural sorting network with error-free differentiable swap functions, aiming to improve sorting capabilities for complex, high-dimensional inputs.},
  author = {Jungtaek Kim and Jeongbeen Yoon and Minsu Cho},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024generalized.pdf:pdf},
  keywords = {Sorting networks, Neural sorting networks, Differentiable swap functions},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RLSWbk9kPw},
  publisher = {OpenReview.net},
  title = {Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions},
  url = {https://openreview.net/forum?id=RLSWbk9kPw},
  year = {2024}
}

@inproceedings{galkin2024towards,
  abstract = {Foundation models in language and vision can generalize across inputs, but knowledge graphs (KGs) have non-overlapping entity and relation vocabularies. We present ULTRA, an approach for learning universal and transferable graph representations. ULTRA builds relational representations conditioned on interactions. Experiments on 57 KGs showed zero-shot inductive inference performance often matches or exceeds baselines.},
  author = {Mikhail Galkin and Xinyu Yuan and Hesham Mostafa and Jian Tang and Zhaocheng Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/galkin2024towards.pdf:pdf},
  keywords = {graph neural networks, foundation model, knowledge graph, link prediction, knowledge graph reasoning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jVEoydFOl9},
  publisher = {OpenReview.net},
  title = {Towards Foundation Models for Knowledge Graph Reasoning},
  url = {https://openreview.net/forum?id=jVEoydFOl9},
  year = {2024}
}

@inproceedings{geng2024neural,
  abstract = {We study macro motion analysis, where macro motion refers to the collection of all visually observable motions in a dynamic scene. Traditional filtering-based methods on motion analysis typically focus only on local and tiny motions, yet fail to represent large motions or 3D scenes. Recent dynamic neural representations can faithfully represent motions using correspondences, but they cannot be directly used for motion analysis. We propose Phase-based neural polynomial Gabor fields (Phase-PGF) to analyze and represent macro motions in dynamic scenes.},
  author = {Chen Geng and Hong-Xing Yu and Sida Peng and Xiaowei Zhou and Jiajun Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/geng2024neural.pdf:pdf},
  keywords = {Scene Representation, Video Analysis, Motion Analysis, Neural Rendering, 3D Vision},
  note = {DBLP last modified: 2025-01-28},
  pdf = {https://openreview.net/pdf?id=dTlKCQuuxP},
  publisher = {OpenReview.net},
  title = {Neural Polynomial {Gabor} Fields for Macro Motion Analysis},
  url = {https://openreview.net/forum?id=dTlKCQuuxP},
  year = {2024}
}

@inproceedings{wu2024qbench,
  abstract = {We present Q-Bench, a benchmark to evaluate Multi-modality Large Language Models (MLLMs) on three key areas: low-level visual perception (using LLVisionQA dataset), low-level visual description (using LLDescribe dataset), and visual quality assessment. We aim to encourage the research community to delve deeper to discover and enhance these untapped potentials of MLLMs.},
  author = {Haoning Wu and Zicheng Zhang and Erli Zhang and Chaofeng Chen and Liang Liao and Annan Wang and Chunyi Li and Wenxiu Sun and Qiong Yan and Guangtao Zhai and Weisi Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024qbench.pdf:pdf},
  keywords = {Benchmark, Vision-Language, Large Language Models, Low-level Vision, Image Quality Assessment},
  note = {DBLP last modified: 2024-07-29, ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=0V5TVt9bk0},
  publisher = {OpenReview.net},
  title = {Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision},
  url = {https://openreview.net/forum?id=0V5TVt9bk0},
  year = {2024}
}

@inproceedings{lin2024multigranularity,
  abstract = {We address the challenge of learning correspondence in long-term video-language models. The key problem is the ``multi-granularity noisy correspondence (MNC)'' which includes clip-caption and frame-word misalignments. We propose ``NOise Robust Temporal Optimal traNsport (Norton)'' to address these alignment issues using an optimal transport framework. Our method employs video-paragraph and clip-caption contrastive losses, uses an alignable prompt bucket to filter irrelevant clips and captions, and incorporates a soft-maximum operator to identify crucial words and key frames.},
  author = {Yijie Lin and Jie Zhang and Zhenyu Huang and Jia Liu and Zujie Wen and Xi Peng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024multigranularity.pdf:pdf},
  keywords = {Video-language pre-training, Noisy correspondence},
  note = {DBLP last modified: 2024-07-29, ICLR 2024 Oral},
  pdf = {https://openreview.net/pdf?id=9Cu8MRmhq2},
  publisher = {OpenReview.net},
  title = {Multi-granularity Correspondence Learning from Long-term Noisy Videos},
  url = {https://openreview.net/forum?id=9Cu8MRmhq2},
  year = {2024}
}

@inproceedings{jiang2024one,
  abstract = {While backpropagation (BP) is the mainstream approach for gradient computation in neural network training, its heavy reliance on the chain rule of differentiation constrains the designing flexibility of network architecture and training pipelines. We avoid the recursive computation in BP and develop a unified likelihood ratio (ULR) method for gradient estimation with only one forward propagation.},
  author = {Jinyang Jiang and Zeliang Zhang and Chenliang Xu and Zhaofei Yu and Yijie Peng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024one.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=ALGFFPXWSi},
  publisher = {OpenReview.net},
  title = {One Forward is Enough for Neural Network Training via Likelihood Ratio Method},
  url = {https://openreview.net/forum?id=ALGFFPXWSi},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{li2024image,
  abstract = {Generative adversarial networks (GANs) have made great success in image inpainting yet still have difficulties tackling large missing regions. In contrast, iterative probabilistic algorithms, such as autoregressive and denoising diffusion models, have to be deployed with massive computing resources for decent effect. To achieve high-quality results with low computational cost, we present a novel pixel spread model (PSM) that iteratively employs decoupled probabilistic modeling, combining the optimization efficiency of GANs with the prediction tractability of probabilistic models. As a result, our model selectively spreads informative pixels throughout the image in a few iterations, largely enhancing the completion quality and efficiency. On multiple benchmarks, we achieve new state-of-the-art performance.},
  author = {Wenbo Li and Xin Yu and Kun Zhou and Yibing Song and Zhe Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024image.pdf:pdf},
  month = {5},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=rUf9G9k2im},
  publisher = {OpenReview.net},
  title = {Image Inpainting via Iteratively Decoupled Probabilistic Modeling},
  url = {https://openreview.net/forum?id=rUf9G9k2im},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{chen2024hardness,
  abstract = {Constrained cooperative multi-agent reinforcement learning (MARL) is an emerging learning framework that proves strong duality fails to hold in constrained cooperative MARL by revealing a nonconvex quadratic type constraint on the occupation measure. This work shows that convergence of primal-dual algorithms is hindered by a nonzero duality gap and proposes a decentralized primal approach to address the fundamental limitations of constrained cooperative MARL.},
  author = {Ziyi Chen and Yi Zhou and Heng Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024hardness.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=wFWuX1Fhtj},
  publisher = {OpenReview.net},
  title = {On the Hardness of Constrained Cooperative Multi-Agent Reinforcement Learning},
  url = {https://openreview.net/forum?id=wFWuX1Fhtj},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{wang2024mint,
  abstract = {To solve complex tasks, large language models (LLMs) often require multiple rounds of interactions with the user, sometimes assisted by external tools. This paper introduces MINT benchmark to evaluate LLMs across multi-turn interactions, tool usage, and natural language feedback. The benchmark provides a reproducible framework where LLMs access tools by executing Python code and receive user feedback simulated by GPT-4. Key findings show that LLMs generally benefit from tools (1-8\% performance gain per turn) and language feedback (2-17\% performance improvement), while supervised instruction-finetuning and reinforcement learning often hurt multi-turn capabilities.},
  author = {Xingyao Wang and Zihan Wang and Jiateng Liu and Yangyi Chen and Lifan Yuan and Hao Peng and Heng Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024mint.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=jp3gWrMuIZ},
  publisher = {OpenReview.net},
  title = {{MINT}: Evaluating {LLMs} in Multi-turn Interaction with Tools and Language Feedback},
  url = {https://openreview.net/forum?id=jp3gWrMuIZ},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{laurent2024symmetryaware,
  abstract = {This paper presents one of the first large-scale explorations of the posterior distribution of deep Bayesian Neural Networks (BNNs), expanding to real-world vision tasks and architectures. The research investigates optimal posterior approximation approaches, analyzes the connection between posterior quality and uncertainty quantification, and explores weight-space symmetries as a critical aspect for understanding posteriors. The authors examine permutation and scaling symmetries that tend to obfuscate the Bayesian posterior and explore the relationship between scaling symmetries and L2 regularization, releasing the first large-scale checkpoint dataset with thousands of real-world models.},
  author = {Olivier Laurent and Emanuel Aldea and Gianni Franchi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/laurent2024symmetryaware.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=FOSBQuXgAq},
  publisher = {OpenReview.net},
  title = {A Symmetry-Aware Exploration of {Bayesian} Neural Network Posteriors},
  url = {https://openreview.net/forum?id=FOSBQuXgAq},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{luo2024universal,
  abstract = {This paper presents a universal motion representation that encompasses a comprehensive range of motor skills for physics-based humanoid control. The approach learns a motion imitator capable of imitating all human motions from a large, unstructured dataset and creates a motion representation by distilling skills directly from the imitator using an encoder-decoder structure with a variational information bottleneck. The method learns a prior conditioned on proprioception to improve model expressiveness and enables generation of long, stable, and diverse human motions. The effectiveness is demonstrated through generative tasks like striking and terrain traversal, motion tracking using VR controllers, and hierarchical reinforcement learning applications.},
  author = {Zhengyi Luo and Jinkun Cao and Josh Merel and Alexander Winkler and Jing Huang and Kris M. Kitani and Weipeng Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/luo2024universal.pdf:pdf},
  month = {5},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=OrOd8PxOO2},
  publisher = {OpenReview.net},
  title = {Universal Humanoid Motion Representations for Physics-Based Control},
  url = {https://openreview.net/forum?id=OrOd8PxOO2},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{zhu2024latent,
  abstract = {This paper introduces latent intuitive physics, a transfer learning framework for physics simulation that can infer hidden properties of fluids from a single 3D video and simulate the observed fluid in novel scenes. The key insight is to use latent features drawn from a learnable prior distribution conditioned on underlying particle states to capture invisible and complex physical properties. The approach trains a parametrized prior learner given visual observations to approximate the visual posterior of inverse graphics, using a learned neural renderer to obtain particle states. The converged prior learner is embedded in a probabilistic physics engine, enabling novel simulations on unseen geometries, boundaries, and dynamics without knowledge of true physical parameters.},
  author = {Xiangming Zhu and Huayu Deng and Haochen Yuan and Yunbo Wang and Xiaokang Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024latent.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=WZu4gUGN13},
  publisher = {OpenReview.net},
  title = {Latent Intuitive Physics: Learning to Transfer Hidden Physics from A {3D} Video},
  url = {https://openreview.net/forum?id=WZu4gUGN13},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{xiao2024dreamclean,
  abstract = {Image restoration poses substantial challenges when people are blind to the type of degradation the images suffer, which is usually the case in real-day scenarios. Current research heavily relies on prior knowledge of the restoration type, either explicitly through rules or implicitly through availability of degraded-clean image pairs. This paper introduces DreamClean, a training-free method that needs no degradation prior knowledge but yields high-fidelity and generality towards various types of image degradation. DreamClean embeds the degraded image back to the latent of pre-trained diffusion models and re-samples it through a carefully designed diffusion process using novel Variance Preservation Sampling (VPS) technique. The method handles various degradation types simultaneously and provides superior experimental performance over various challenging tasks.},
  author = {Jie Xiao and Ruili Feng and Han Zhang and Zhiheng Liu and Zhantao Yang and Yurui Zhu and Xueyang Fu and Kai Zhu and Yu Liu and Zheng-Jun Zha},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024dreamclean.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=6ALuy19mPa},
  publisher = {OpenReview.net},
  title = {{DreamClean}: Restoring Clean Image Using Deep Diffusion Prior},
  url = {https://openreview.net/forum?id=6ALuy19mPa},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{lu2024learning,
  abstract = {Out-of-distribution (OOD) detection aims to detect testing samples far away from the in-distribution (ID) training data, crucial for safe deployment of machine learning models. This paper proposes PrototypicAl Learning with a Mixture of prototypes (PALM) to improve OOD detection by modeling each class with multiple prototypes to capture data diversity, addressing limitations in existing approaches that use oversimplified data assumptions. The method automatically identifies and dynamically updates prototypes, assigns samples using reciprocal neighbor soft assignment weights, and optimizes maximum likelihood estimation and contrastive losses. PALM achieves state-of-the-art average AUROC performance of 93.82 on the challenging CIFAR-100 benchmark.},
  author = {Haodong Lu and Dong Gong and Shuo Wang and Jason Xue and Lina Yao and Kristen Moore},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lu2024learning.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=uNkKaD3MCs},
  publisher = {OpenReview.net},
  title = {Learning with Mixture of Prototypes for Out-of-Distribution Detection},
  url = {https://openreview.net/forum?id=uNkKaD3MCs},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{jin2024llms,
  abstract = {This paper introduces DVDet, a novel open-vocabulary object detection approach that enhances region-text alignment by leveraging both vision language models (VLMs) and large language models (LLMs). Unlike existing open-vocabulary detectors that only align region embeddings with categorical labels, DVDet incorporates fine-grained descriptors of object parts using conditional context prompts to transform regional embeddings into image-like representations. The method employs LLMs as an interactive and implicit knowledge repository to mine and refine textual descriptors, enabling more precise region-text alignment. DVDet outperforms state-of-the-art methods consistently by large margins across multiple large-scale benchmarks.},
  author = {Sheng Jin and Xueying Jiang and Jiaxing Huang and Lewei Lu and Shijian Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024llms.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=usrChqw6yK},
  publisher = {OpenReview.net},
  title = {{LLMs} Meet {VLMs}: Boost Open Vocabulary Object Detection with Fine-grained Descriptors},
  url = {https://openreview.net/forum?id=usrChqw6yK},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{yu2024amortizedperiod,
  abstract = {Periodic patterns are a fundamental characteristic of time series in natural world, with significant implications for a range of disciplines, from economics to cloud systems. However, the current literature on periodicity detection faces two key challenges: limited robustness in real-world scenarios and a lack of memory to leverage previously observed time series to accelerate and improve inference on new data.},
  author = {Hang Yu and Cong Liao and Ruolan Liu and Jianguo Li and Hu Yun and Xinzhe Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024amortizedperiod.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=psEswR8Jz4},
  publisher = {OpenReview.net},
  title = {{AmortizedPeriod}: Attention-based Amortized Inference for Periodicity Identification},
  url = {https://openreview.net/forum?id=psEswR8Jz4},
  year = {2024}
}

@inproceedings{chen2024llcp,
  abstract = {Current approaches to Video Question Answering (VideoQA) primarily focus on cross-modality matching, which is limited by the requirement for extensive data annotations and the insufficient capacity for causal reasoning (e.g. attributing accidents).},
  author = {Guangyi Chen and Yuke Li and Xiao Liu and Zijian Li and Eman Al Suradi and Donglai Wei and Kun Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024llcp.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=Cu5wJa5LGO},
  publisher = {OpenReview.net},
  title = {{LLCP}: Learning Latent Causal Processes for Reasoning-based Video Question Answer},
  url = {https://openreview.net/forum?id=Cu5wJa5LGO},
  year = {2024}
}

@inproceedings{li2024optimal,
  abstract = {We study the problem of residual error estimation for matrix and vector norms using a linear sketch. Such estimates can be used, for example, to quickly assess how useful a more expensive low-rank approximation computation will be.},
  author = {Yi Li and Honghao Lin and David Woodruff},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024optimal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RsJwmWvE6Q},
  publisher = {OpenReview.net},
  title = {Optimal Sketching for Residual Error Estimation for Matrix and Vector Norms},
  url = {https://openreview.net/forum?id=RsJwmWvE6Q},
  year = {2024}
}

@inproceedings{ding2024causallm,
  abstract = {Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which in-context samples can all attend to each other, compared to causal language models (causalLM).},
  author = {Nan Ding and Tomer Levinboim and Jialin Wu and Sebastian Goodman and Radu Soricut},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ding2024causallm.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=guRNebwZBb},
  publisher = {OpenReview.net},
  title = {{CausalLM} is not optimal for in-context learning},
  url = {https://openreview.net/forum?id=guRNebwZBb},
  year = {2024}
}

@inproceedings{liu2024nfgtransformer,
  abstract = {Normal-form games (NFGs) are the fundamental model of strategic interaction. We study their representation using neural networks. We describe the inherent equivariance of NFGs --- any permutation of strategies describes an equivalent game --- as well as the challenges this poses for representation learning.},
  author = {Siqi Liu and Luke Marris and Georgios Piliouras and Ian Gemp and Nicolas Heess},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024nfgtransformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4YESQqIys7},
  publisher = {OpenReview.net},
  title = {{NfgTransformer}: Equivariant Representation Learning for Normal-form Games},
  url = {https://openreview.net/forum?id=4YESQqIys7},
  year = {2024}
}

@inproceedings{xiao2024hebbian,
  abstract = {Neuromorphic computing with spiking neural networks is promising for energy-efficient AI applications. The paper addresses catastrophic forgetting by developing a method using lateral connections and Hebbian learning to protect neural network knowledge.},
  author = {Mingqing Xiao and Qingyan Meng and Zongpeng Zhang and Di He and Zhouchen Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024hebbian.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MeB86edZ1P},
  publisher = {OpenReview.net},
  title = {Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks},
  url = {https://openreview.net/forum?id=MeB86edZ1P},
  year = {2024}
}

@inproceedings{zhou2024archlock,
  abstract = {Deep neural network models are vulnerable to exploitation by attackers attempting to transfer them to other tasks. The paper proposes a novel neural architecture search algorithm to reduce model transferability at the architecture level.},
  author = {Tong Zhou and Shaolei Ren and Xiaolin Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024archlock.pdf:pdf},
  note = {DBLP last modified: 2024-11-08},
  pdf = {https://openreview.net/pdf?id=e2YOVTenU9},
  publisher = {OpenReview.net},
  title = {{ArchLock}: Locking {DNN} Transferability at the Architecture Level with a Zero-Cost Binary Predictor},
  url = {https://openreview.net/forum?id=e2YOVTenU9},
  year = {2024}
}

@inproceedings{he2024multisize,
  abstract = {While dataset condensation effectively enhances training efficiency, its application in on-device scenarios brings unique challenges. The method compresses N condensation processes into a single condensation process to obtain datasets with multiple sizes.},
  author = {Yang He and Lingao Xiao and Joey Tianyi Zhou and Ivor Tsang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024multisize.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; Oral Presentation},
  pdf = {https://openreview.net/pdf?id=FVhmnvqnsI},
  publisher = {OpenReview.net},
  title = {Multisize Dataset Condensation},
  url = {https://openreview.net/forum?id=FVhmnvqnsI},
  year = {2024}
}

@inproceedings{guo2024animatediff,
  abstract = {The paper presents AnimateDiff, a practical framework for animating personalized T2I models without requiring model-specific tuning that introduces a plug-and-play motion module trained to learn motion priors from real-world videos.},
  author = {Yuwei Guo and Ceyuan Yang and Anyi Rao and Zhengyang Liang and Yaohui Wang and Yu Qiao and Maneesh Agrawala and Dahua Lin and Bo Dai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024animatediff.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; Spotlight},
  pdf = {https://openreview.net/pdf?id=Fx2SbBgcte},
  publisher = {OpenReview.net},
  title = {{AnimateDiff}: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning},
  url = {https://openreview.net/forum?id=Fx2SbBgcte},
  year = {2024}
}

@inproceedings{he2024dataindependent,
  abstract = {Hierarchical vision transformers (ViTs) have two key advantages: linear computational complexity via local self-attention and hierarchical feature maps through patch merging. The paper introduces a Data-independent Module-Aware Pruning method (DIMAP) to address limitations in existing pruning techniques.},
  author = {Yang He and Joey Tianyi Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024dataindependent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7Ol6foUi1G},
  publisher = {OpenReview.net},
  title = {Data-independent Module-aware Pruning for Hierarchical Vision Transformers},
  url = {https://openreview.net/forum?id=7Ol6foUi1G},
  year = {2024}
}

@inproceedings{zhao2024provably,
  abstract = {We study risk-sensitive Reinforcement Learning (RL), where we aim to maximize the Conditional Value at Risk (CVaR) with a fixed risk tolerance $\tau$. Prior theoretical work studying risk-sensitive RL focuses on the tabular Markov Decision Processes (MDPs) setting. To extend CVaR RL to settings where state space is large, function approximation must be deployed. We study CVaR RL in low-rank MDPs with nonlinear function approximation. Low-rank MDPs assume the underlying transition kernel admits a low-rank decomposition, but unlike prior linear models, low-rank MDPs do not assume the feature or state-action representation is known. We propose a novel Upper Confidence Bound (UCB) bonus-driven algorithm to carefully balance the interplay between exploration, exploitation, and representation learning in CVaR RL. We prove that our algorithm achieves a sample complexity of $\tilde{O}\left(\frac{H^7 A^2 d^4}{\tau^2 \epsilon^2}\right)$ to yield an $\epsilon$-optimal CVaR, where $H$ is the length of each episode, $A$ is the capacity of action space, and $d$ is the dimension of representations. Computational-wise, we design a novel discretized Least-Squares Value Iteration (LSVI) algorithm for the CVaR objective as the planning oracle and show that we can find the near-optimal policy in a polynomial running time with a Maximum Likelihood Estimation oracle. To our knowledge, this is the first provably efficient CVaR RL algorithm in low-rank MDPs.},
  author = {Yulai Zhao and Wenhao Zhan and Xiaoyan Hu and Ho-fung Leung and Farzan Farnia and Wen Sun and Jason D. Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024provably.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9x6yrFAPnx},
  publisher = {OpenReview.net},
  title = {Provably Efficient {CVaR} {RL} in Low-rank {MDPs}},
  url = {https://openreview.net/forum?id=9x6yrFAPnx},
  year = {2024}
}

@inproceedings{liu2024statistical,
  abstract = {Improving the alignment of language models with human preferences remains an active research challenge. Recently, offline methods such as Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO) have emerged as attractive alternatives, offering improvements in stability and scalability while maintaining competitive performance. SLiC refines its loss function using sequence pairs sampled from a supervised fine-tuned (SFT) policy, while DPO directly optimizes language models based on preference data, foregoing the need for a separate reward model. However, the maximum likelihood estimator (MLE) of the target optimal policy requires labeled preference pairs sampled from that policy. The absence of a reward model in DPO constrains its ability to sample preference pairs from the optimal policy. To address this limitation, we propose Statistical Rejection Sampling Optimization (RSO), which uses rejection sampling to collect preference data from the optimal policy. Our approach improves policy estimation over DPO and SLiC by ensuring that preference pairs are sampled from the estimated optimal policy rather than from unknown or suboptimal distributions.},
  author = {Tianqi Liu and Yao Zhao and Rishabh Joshi and Misha Khalman and Mohammad Saleh and Peter J. Liu and Jialu Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024statistical.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xbjSwwrQOe},
  publisher = {OpenReview.net},
  title = {Statistical Rejection Sampling Improves Preference Optimization},
  url = {https://openreview.net/forum?id=xbjSwwrQOe},
  year = {2024}
}

@inproceedings{wang2024chainoftable,
  abstract = {Table-based reasoning with large language models (LLMs) is a promising direction to tackle many table understanding tasks, such as table-based question answering and fact verification. Compared with generic reasoning, table-based reasoning requires the extraction of underlying semantics from both free-form questions and semi-structured tabular data. Chain-of-Thought and its similar approaches incorporate the reasoning chain in the form of textual context, but it is still an open question how to effectively leverage tabular data in the reasoning chain. We propose the Chain-of-Table framework, where tabular data is explicitly used in the reasoning chain as a proxy for intermediate thoughts. Specifically, we guide LLMs using in-context learning to iteratively generate operations and update the table to represent a tabular reasoning chain. LLMs can therefore dynamically plan the next operation based on the results of the previous ones. This continuous evolution of the table forms a chain, showing the reasoning process for a given tabular problem. The chain carries structured information of the intermediate results, enabling more accurate and reliable predictions. Chain-of-Table achieves new state-of-the-art performance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM choices.},
  author = {Zilong Wang and Hao Zhang and Chun-Liang Li and Julian Martin Eisenschlos and Vincent Perot and Zifeng Wang and Lesly Miculicich and Yasuhisa Fujii and Jingbo Shang and Chen-Yu Lee and Tomas Pfister},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024chainoftable.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4L0xnS4GQM},
  publisher = {OpenReview.net},
  title = {Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding},
  url = {https://openreview.net/forum?id=4L0xnS4GQM},
  year = {2024}
}

@inproceedings{deng2024plugandplay,
  abstract = {Proactive dialogues serve as a practical yet challenging dialogue problem in the era of large language models (LLMs), where the dialogue policy planning is the key to improving the proactivity of LLMs. Most existing studies enable the dialogue policy planning of LLMs using various prompting schemes or iteratively enhance this capability in handling the given case with verbal AI feedback. However, these approaches are either bounded by the policy planning capability of the frozen LLMs or hard to be transferred to new cases. In this work, we introduce a new dialogue policy planning paradigm to strategize LLMs for proactive dialogue problems with a tunable language model plug-in as a plug-and-play dialogue policy planner, named PPDPP. Specifically, we develop a novel training framework to facilitate supervised fine-tuning over available human-annotated data as well as reinforcement learning from goal-oriented AI feedback with dynamic interaction data collected by the LLM-based self-play simulation. The self-play paradigm prompts two LLMs to perform role-playing conversation that simulates the dynamic environment of multi-turn interactions between the dialogue agent and the real user. A third LLM acts as the reward model to provide goal-oriented verbal feedback for reinforcement learning. Extensive experiments across three different applications of proactive dialogues demonstrate that PPDPP consistently and substantially outperforms existing approaches.},
  author = {Yang Deng and Wenxuan Zhang and Wai Lam and See-Kiong Ng and Tat-Seng Chua},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/deng2024plugandplay.pdf:pdf},
  note = {DBLP last modified: 2025-02-24},
  pdf = {https://openreview.net/pdf?id=MCNqgUFTHI},
  publisher = {OpenReview.net},
  title = {Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents},
  url = {https://openreview.net/forum?id=MCNqgUFTHI},
  year = {2024}
}

@inproceedings{zhang2024dynamic,
  abstract = {The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSNT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSNT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSNT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSNT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSNT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70\% sparsity with LLaMA-7B.},
  author = {Yuxin Zhang and Lirui Zhao and Mingbao Lin and Yunyun Sun and Yiwu Yao and Xingjia Han and Jared Tanner and Shiwei Liu and Rongrong Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024dynamic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1ndDmZdT4g},
  publisher = {OpenReview.net},
  title = {Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse {LLMs}},
  url = {https://openreview.net/forum?id=1ndDmZdT4g},
  year = {2024}
}

@inproceedings{baron20242dimensional,
  abstract = {A central objective in computer vision is to design models with appropriate 2-D inductive bias. Desiderata for 2-D inductive bias include two-dimensional position awareness, dynamic spatial locality, and translation and permutation invariance. To address these goals, we leverage an expressive variation of the multidimensional State Space Model (SSM). Our approach introduces efficient parameterization, accelerated computation, and a suitable normalization scheme. Empirically, we observe that incorporating our layer at the beginning of each transformer block of Vision Transformers (ViT), as well significantly enhances performance for multiple ViT backbones and across datasets. The new layer is effective even with a negligible amount of additional parameters and inference time. Ablation studies and visualizations demonstrate that the layer has a strong 2-D inductive bias. For example, vision transformers equipped with our layer exhibit effective performance even without positional encoding.},
  author = {Ethan Baron and Itamar Zimerman and Lior Wolf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/baron20242dimensional.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BGkqypmGvm},
  publisher = {OpenReview.net},
  title = {A 2-Dimensional State Space Layer for Spatial Inductive Bias},
  url = {https://openreview.net/forum?id=BGkqypmGvm},
  year = {2024}
}

@inproceedings{chen2024robust,
  abstract = {Similarity-based representation learning has shown impressive capabilities in both supervised (e.g., metric learning) and unsupervised (e.g., contrastive learning) scenarios. Existing approaches effectively constrained the representation difference by a margin to fit similarity supervision, but can hardly restrict the variation of representation difference, sometimes leading to overfitting results where the clusters are disordered by drastically changed differences. In this work, we propose a novel difference alignment regularization to encourage all representation differences between inter-class instances to be as close as possible, so that the learning algorithm can produce consistent differences to distinguish data points from each other. We construct a new cross-total-variation (CTV) norm to measure the divergence among representation differences, and convert it into an equivalent stochastic form for easy optimization. The regularizer is integrated into the empirical loss for difference-aligned similarity learning (DASL), shrinking the hypothesis space and alleviating overfitting. Theoretically, we prove that our regularizer tightens the error bound of the traditional similarity learning. Empirically, experiments demonstrate the superiority of DASL over existing approaches in both supervised metric learning and unsupervised contrastive learning tasks.},
  author = {Shuo Chen and Gang Niu and Chen Gong and Okan Koc and Jian Yang and Masashi Sugiyama},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024robust.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=K9V7ugVuUz},
  publisher = {OpenReview.net},
  title = {Robust Similarity Learning with Difference Alignment Regularization},
  url = {https://openreview.net/forum?id=K9V7ugVuUz},
  year = {2024}
}

@inproceedings{li2024neuralsymbolic,
  abstract = {Current learning models often struggle with human-like systematic generalization, particularly in learning compositional rules from limited data and extrapolating them to novel combinations. We introduce the Neural-Symbolic Recursive Machine (NSR), whose core is a Grounded Symbol System (GSS), allowing for the emergence of combinatorial syntax and semantics directly from training data. The NSR employs a modular design that integrates neural perception, syntactic parsing, and semantic reasoning. These components are synergistically trained through a novel deduction-abduction algorithm. Our findings demonstrate that NSR's design, imbued with the inductive biases of equivariance and compositionality, grants it the expressiveness to adeptly handle diverse sequence-to-sequence tasks and achieve unparalleled systematic generalization.},
  author = {Qing Li and Yixin Zhu and Yitao Liang and Ying Nian Wu and Song-Chun Zhu and Siyuan Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024neuralsymbolic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FWJAmwE0xH},
  publisher = {OpenReview.net},
  title = {Neural-Symbolic Recursive Machine for Systematic Generalization},
  url = {https://openreview.net/forum?id=FWJAmwE0xH},
  year = {2024}
}

@inproceedings{liu2024towards,
  abstract = {Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. This work proposes the first data poisoning framework attacking FRL. Existing attacks fall short in FRL due to notably different fairness goals and model architectures. We propose a new MI maximization attack goal and reveal its connection to existing fairness notion such as demographic parity. Our attack outperforms baselines by a large margin and raises an alert of the vulnerability of existing FRL methods.},
  author = {Tianci Liu and Haoyu Wang and Feijie Wu and Hengtong Zhang and Pan Li and Lu Su and Jing Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-23},
  pdf = {https://openreview.net/pdf?id=YLJs4mKJCF},
  publisher = {OpenReview.net},
  title = {Towards Poisoning Fair Representations},
  url = {https://openreview.net/forum?id=YLJs4mKJCF},
  year = {2024}
}

@inproceedings{wong2024sign2gpt,
  abstract = {Automatic Sign Language Translation requires the integration of both computer vision and natural language processing to effectively bridge the communication gap between sign and spoken languages. However, the deficiency in large-scale training data to support sign language translation means we need to leverage resources from spoken language. We introduce, Sign2GPT, a novel framework for sign language translation that utilizes large-scale pretrained vision and language models via lightweight adapters for gloss-free sign language translation. The lightweight adapters are crucial for sign language translation, due to the constraints imposed by limited dataset sizes and the computational requirements when training with long sign videos. We introduce an end-to-end gloss-free sign language model, Sign2GPT, designed for sign language translation, leveraging a frozen GPT language model. We propose a novel pseudo-gloss pretraining strategy, utilizing automatically extracted pseudo-glosses from sentences to pretrain the sign encoder. Sign2GPT demonstrates significant performance improvements over previous gloss-free sign language translation approaches, offering a promising pathway for adapting frozen language and vision models to the domain of sign language translation.},
  author = {Ryan Wong and Necati Cihan Camg{\"o}z and Richard Bowden},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wong2024sign2gpt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LqaEEs3UxU},
  publisher = {OpenReview.net},
  title = {{Sign2GPT}: Leveraging Large Language Models for Gloss-Free Sign Language Translation},
  url = {https://openreview.net/forum?id=LqaEEs3UxU},
  year = {2024}
}

@inproceedings{he2024recombiner,
  abstract = {COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.},
  author = {Jiajun He and Gergely Flamich and Zongyu Guo and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/he2024recombiner.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=VkWbxFrCC8},
  publisher = {OpenReview.net},
  title = {{RECOMBINER}: Robust and Enhanced Compression with {B}ayesian Implicit Neural Representations},
  url = {https://openreview.net/forum?id=VkWbxFrCC8},
  year = {2024}
}

@inproceedings{wang2024enabling,
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in open-ended text generation tasks. However, the inherent open-ended nature of these tasks implies that there is always room for improvement in the quality of model responses. To address this challenge, various approaches have been proposed to enhance the performance of LLMs. There has been a growing focus on enabling LLMs to self-improve their response quality, thereby reducing the reliance on extensive human annotation efforts for collecting diverse and high-quality training data. Recently, prompting-based methods have been widely explored among self-improvement methods owing to their effectiveness, efficiency, and convenience. However, those methods usually require explicitly and thoroughly written rubrics as inputs to LLMs. It is expensive and challenging to manually derive and provide all necessary rubrics with a real-world complex goal for improvement (e.g., being more helpfulness and less harmful). To this end, we propose an imPlicit self-ImprovemenT (PIT) framework that implicitly learns the improvement goal from human preference data. PIT only requires preference data that are used to train reward models with no extra human efforts. Specifically, we reformulate the training objective of reinforcement learning from human feedback (RLHF) -- instead of maximizing response quality for a given input, we maximize the quality gap of the response conditioned on a reference response. In this way, PIT is implicitly trained with the improvement goal of better aligning with human preferences. Experiments on two real-world datasets and one synthetic dataset show that our method significantly outperforms prompting-based methods.},
  author = {Ziqi Wang and Le Hou and Tianjian Lu and Yuexin Wu and Yunxuan Li and Hongkun Yu and Heng Ji},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/wang2024enabling.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=2tVHNRZuCs},
  publisher = {OpenReview.net},
  title = {Enabling Language Models to Implicitly Learn Self-Improvement},
  url = {https://openreview.net/forum?id=2tVHNRZuCs},
  year = {2024}
}

@inproceedings{dang2024beyond,
  abstract = {The posterior collapse phenomenon in variational autoencoder (VAE), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAE preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAE performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAE. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAE: conditional VAE and hierarchical VAE. Specifically, via a non-trivial theoretical analysis of linear conditional VAE and hierarchical VAE with two levels of latent, we prove that the cause of posterior collapses in these models includes the correlation between the input and output of the conditional VAE and the effect of learnable encoder variance in the hierarchical VAE. We empirically validate our theoretical findings for linear conditional and hierarchical VAE and demonstrate that these results are also predictive for non-linear cases with extensive experiments.},
  author = {Hien Dang and Tho Tran Huu and Tan Minh Nguyen and Nhat Ho},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/dang2024beyond.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=4zZFGliCl9},
  publisher = {OpenReview.net},
  title = {Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in Conditional and Hierarchical Variational Autoencoders},
  url = {https://openreview.net/forum?id=4zZFGliCl9},
  year = {2024}
}

@inproceedings{elsayed2024addressing,
  abstract = {Deep representation learning methods struggle with continual learning, suffering from both catastrophic forgetting of useful units and loss of plasticity, often due to rigid and unuseful units. While many methods address these two issues separately, only a few currently deal with both simultaneously. In this paper, we introduce Utility-based Perturbed Gradient Descent (UPGD) as a novel approach for the continual learning of representations. UPGD combines gradient updates with perturbations, where it applies smaller modifications to more useful units, protecting them from forgetting, and larger modifications to less useful units, rejuvenating their plasticity. We use a challenging streaming learning setup where continual learning problems have hundreds of non-stationarities and unknown task boundaries. We show that many existing methods suffer from at least one of the issues, predominantly manifested by their decreasing accuracy over tasks. On the other hand, UPGD continues to improve performance and surpasses or is competitive with all methods in all problems. Finally, in extended reinforcement learning experiments with PPO, we show that while Adam exhibits a performance drop after initial learning, UPGD avoids it by addressing both continual learning issues.},
  author = {Mohamed Elsayed and A. Rupam Mahmood},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/elsayed2024addressing.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=sKPzAXoylB},
  publisher = {OpenReview.net},
  title = {Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning},
  url = {https://openreview.net/forum?id=sKPzAXoylB},
  year = {2024}
}

@inproceedings{zhang2024accelerating,
  abstract = {A popular approach to sample a diffusion-based generative model is to solve an ordinary differential equation (ODE). In existing samplers, the coefficients of the ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes (including EDM, DDIM, and DPM-Solver) by optimizing certain coefficients via improved integration approximation (IIA). We propose to minimize, for each time step, a mean squared error (MSE) function with respect to the selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps, which in principle provides a more accurate integration approximation in predicting the next diffusion state. The proposed IIA technique does not require any change of a pre-trained model, and only introduces a very small computational overhead for solving a number of quadratic optimization problems. Extensive experiments show that considerably better FID scores can be achieved by using IIA-EDM, IIA-DDIM, and IIA-DPM-Solver than the original counterparts when the neural function evaluation (NFE) is small (i.e., less than 25).},
  author = {Guoqiang Zhang and Kenta Niwa and W. Bastiaan Kleijn},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/zhang2024accelerating.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=ktJAF3lxbi},
  publisher = {OpenReview.net},
  title = {On Accelerating Diffusion-Based Sampling Processes via Improved Integration Approximation},
  url = {https://openreview.net/forum?id=ktJAF3lxbi},
  year = {2024}
}

@inproceedings{leroy2024winwin,
  abstract = {Transformers have become the standard in state-of-the-art vision architectures, achieving impressive performance on both image-level and dense pixelwise tasks. However, training vision transformers for high-resolution pixelwise tasks has a prohibitive cost. Typical solutions boil down to hierarchical architectures, fast and approximate attention, or training on low-resolution crops. This latter solution does not constrain architectural choices, but it leads to a clear performance drop when testing at resolutions significantly higher than that used for training, thus requiring ad-hoc and slow post-processing schemes. In this paper, we propose a novel strategy for efficient training and inference of high-resolution vision transformers. The key principle is to mask out most of the high-resolution inputs during training, keeping only N random windows. This allows the model to learn local interactions between tokens inside each window, and global interactions between tokens from different windows. As a result, the model can directly process the high-resolution input at test time without any special trick. We show that this strategy is effective when using relative positional embedding such as rotary embeddings. It is 4 times faster to train than a full-resolution network, and it is straightforward to use at test time compared to existing approaches. We apply this strategy to three dense prediction tasks with high-resolution data. First, we show on the task of semantic segmentation that a simple setting with 2 windows performs best, hence the name of our method: Win-Win. Second, we confirm this result on the task of monocular depth prediction. Third, we further extend it to the binocular task of optical flow, reaching state-of-the-art performance on the Spring benchmark that contains Full-HD images with an order of magnitude faster inference than the best competitor.},
  author = {Vincent Leroy and J{\'e}r{\^o}me Revaud and Thomas Lucas and Philippe Weinzaepfel},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/leroy2024winwin.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=N23A4ybMJr},
  publisher = {OpenReview.net},
  title = {Win-Win: Training High-Resolution Vision Transformers from Two Windows},
  url = {https://openreview.net/forum?id=N23A4ybMJr},
  year = {2024}
}

@inproceedings{xiong2024node2ket,
  abstract = {Network embedding (NE) is a prominent technique for network analysis where the nodes are represented as vectorized embeddings in a continuous space. Existing works tend to resort to the low-dimensional embedding space for efficiency and less risk of over-fitting. In this paper, we explore a new NE paradigm whose embedding dimension goes exponentially high w.r.t. the number of parameters, yet being very efficient and effective. Specifically, the node embeddings are represented as product states that lie in a super high-dimensional (e.g. $2^{32}$-dim) quantum Hilbert space, with a carefully designed optimization approach to guarantee the robustness to work in different scenarios. In the experiments, we show diverse virtues of our methods, including but not limited to: the overwhelming performance on downstream tasks against conventional low-dimensional NE baselines with the similar amount of computing resources; the super high efficiency for a fixed low embedding dimension (e.g. 512) with less than 1/200 memory usage; and the robustness when equipped with different objectives and sampling strategies as a fundamental tool for future NE research.},
  author = {Hao Xiong and Yehui Tang and Yunlin He and Wei Tan and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/xiong2024node2ket.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=lROh08eK6n},
  publisher = {OpenReview.net},
  title = {Node2ket: Efficient High-Dimensional Network Embedding in Quantum {H}ilbert Space},
  url = {https://openreview.net/forum?id=lROh08eK6n},
  year = {2024}
}

@inproceedings{liu2024repobench,
  abstract = {Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.},
  author = {Tianyang Liu and Canwen Xu and Julian J. McAuley},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/liu2024repobench.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=pPjZIOuQuF},
  publisher = {OpenReview.net},
  title = {{RepoBench}: Benchmarking Repository-Level Code Auto-Completion Systems},
  url = {https://openreview.net/forum?id=pPjZIOuQuF},
  year = {2024}
}

@inproceedings{chen2024going,
  abstract = {The behavior of neural networks still remains opaque, and a recently widely noted phenomenon is that networks often achieve similar performance when initialized with different random parameters. This phenomenon has attracted significant attention in measuring the similarity between features learned by distinct networks. However, feature similarity could be vague in describing the same feature since equivalent features hardly exist. In this paper, we expand the concept of equivalent feature and provide the definition of what we call functionally equivalent features. These features produce equivalent output under certain transformations. Using this definition, we aim to derive a more intrinsic metric for the so-called feature complexity regarding the redundancy of features learned by a neural network at each layer. We offer a formal interpretation of our approach through the lens of category theory, a well-developed area in mathematics. To quantify the feature complexity, we further propose an efficient algorithm named Iterative Feature Merging. Our experimental results validate our ideas and theories from various perspectives. We empirically demonstrate that the functionally equivalence widely exists among different features learned by the same neural network and we could reduce the number of parameters of the network without affecting the performance. The IFM shows great potential as a data-agnostic model prune method. We have also drawn several interesting empirical findings regarding the defined feature complexity.},
  author = {Yiting Chen and Zhanpeng Zhou and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/chen2024going.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=4bSQ3lsfEV},
  publisher = {OpenReview.net},
  title = {Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory},
  url = {https://openreview.net/forum?id=4bSQ3lsfEV},
  year = {2024}
}

@inproceedings{wang2024freereg,
  abstract = {Matching cross-modality features between images and point clouds is a fundamental problem for image-to-point cloud registration. However, due to the modality difference between images and points, it is difficult to learn robust and discriminative cross-modality features by existing metric learning methods for feature matching. Instead of applying metric learning on cross-modality data, we propose to unify the modality between images and point clouds by pretrained large-scale models first, and then establish robust correspondence within the same modality. We show that the intermediate features, called diffusion features, extracted by depth-to-image diffusion models are semantically consistent between images and point clouds, which enables the building of coarse but robust cross-modality correspondences. We further extract geometric features on depth maps produced by the monocular depth estimator. By matching such geometric features, we significantly improve the accuracy of the coarse correspondences produced by diffusion features. Extensive experiments demonstrate that without any task-specific training, direct utilization of both features produces accurate image-to-point cloud registration. On three public indoor and outdoor benchmarks, the proposed method averagely achieves a 20.6 percent improvement in Inlier Ratio, a three-fold higher Inlier Number, and a 48.6 percent improvement in Registration Recall than existing state-of-the-arts.},
  author = {Haiping Wang and Yuan Liu and Bing Wang and Yujing Sun and Zhen Dong and Wenping Wang and Bisheng Yang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/wang2024freereg.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=BPb5AhT2Vf},
  publisher = {OpenReview.net},
  title = {{FreeReg}: Image-to-Point Cloud Registration Leveraging Pretrained Diffusion Models and Monocular Depth Estimators},
  url = {https://openreview.net/forum?id=BPb5AhT2Vf},
  year = {2024}
}

@inproceedings{yu2024textto3d,
  abstract = {The paper explores text-to-{3D} generation using Classifier Score Distillation ({CSD}), which reveals that guidance alone can be effective for {3D} generation tasks. The method uses an implicit classification model for generation and demonstrates superior results across shape generation, texture synthesis, and shape editing. The authors highlight that their method provides new insights for understanding existing techniques in text-to-{3D} generation.},
  author = {Xin Yu and Yuan-Chen Guo and Yangguang Li and Ding Liang and Song-Hai Zhang and Xiaojuan Qi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024textto3d.pdf:pdf},
  keywords = {{3D} modeling, generative models},
  note = {{DBLP} last modified: 2025-05-05},
  pdf = {https://openreview.net/pdf?id=ktG8Tun1Cy},
  publisher = {OpenReview.net},
  title = {Text-to-{3D} with Classifier Score Distillation},
  url = {https://openreview.net/forum?id=ktG8Tun1Cy},
  year = {2024}
}

@inproceedings{guo2024how,
  abstract = {Explores in-context learning ({ICL}) capabilities of transformers by constructing synthetic learning problems with compositional structure. Shows transformers can implement learning algorithms with mild depth and size, revealing mechanisms like copying behaviors and representation selection. Transformers can learn in-context on function classes with a representation, with empirical mechanisms that align well with the efficient theoretical constructions.},
  author = {Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024how.pdf:pdf},
  keywords = {in-context learning, transformers, representation learning, learning theory, mechanistic understanding},
  note = {{DBLP} last modified: 2024-08-13},
  pdf = {https://openreview.net/pdf?id=ikwEDva1JZ},
  publisher = {OpenReview.net},
  title = {How Do Transformers Learn In-Context Beyond Simple Functions? {A} Case Study on Learning with Representations},
  url = {https://openreview.net/forum?id=ikwEDva1JZ},
  year = {2024}
}

@inproceedings{lee2024entropy,
  abstract = {We first address the limitations of relying solely on entropy as a confidence metric for test-time adaptation ({TTA}). Focuses on test-time adaptation for fine-tuning neural networks, challenges existing entropy-based confidence metrics, and introduces {``}Destroy Your Object{''} ({DeYO}) method. Uses {``}Pseudo-Label Probability Difference{''} ({PLPD}) as a new confidence metric and aims to improve adaptation by prioritizing samples with shape-based information.},
  author = {Jonghyun Lee and Dahuin Jung and Saehyung Lee and Junsung Park and Juhyeon Shin and Uiwon Hwang and Sungroh Yoon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024entropy.pdf:pdf},
  keywords = {Test-time adaptation, Robustness},
  note = {{DBLP} last modified: 2024-07-29, Spotlight},
  pdf = {https://openreview.net/pdf?id=9w3iw8wDuE},
  publisher = {OpenReview.net},
  title = {Entropy is not Enough for Test-Time Adaptation: From the Perspective of Disentangled Factors},
  url = {https://openreview.net/forum?id=9w3iw8wDuE},
  year = {2024}
}

@inproceedings{lee2024llmcxr,
  abstract = {We present a state-of-the-art multimodal {LLM} for chest {X}-ray understanding and generation. Focuses on vision-language alignment in Large Language Models ({LLMs}) and aims to improve medical image analysis by allowing visual and language features to interact more freely. Develops method for instruction-tuning text-only {LLM} for medical image tasks and demonstrates better image-text alignment while being smaller than previous models.},
  author = {Suhyeon Lee and Won Jun Kim and Jinho Chang and Jong Chul Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024llmcxr.pdf:pdf},
  keywords = {large language model, multimodal, medical imaging, chest {X}-ray, bidirectional, instruction-tuning},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BqHaLnans2},
  publisher = {OpenReview.net},
  title = {{LLM-CXR}: Instruction-Finetuned {LLM} for {CXR} Image Understanding and Generation},
  url = {https://openreview.net/forum?id=BqHaLnans2},
  year = {2024}
}

@inproceedings{lan2024towards,
  abstract = {Identifies two types of {``}bad positive pairs{''} in time series contrastive learning and introduces Dynamic Bad Pair Mining ({DBPM}) algorithm. Uses memory module to track training behaviors of positive pairs and aims to mitigate negative impacts on representation learning. A lightweight {``}plug-in{''} algorithm to enhance existing time series contrastive learning methods by identifying and suppressing unhelpful positive pairs.},
  author = {Xiang Lan and Hanshu Yan and Shenda Hong and Mengling Feng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lan2024towards.pdf:pdf},
  keywords = {Time Series Contrastive Learning, Healthcare, Self-Supervised Representation Learning},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=K2c04ulKXn},
  publisher = {OpenReview.net},
  title = {Towards Enhancing Time Series Contrastive Learning: {A} Dynamic Bad Pair Mining Approach},
  url = {https://openreview.net/forum?id=K2c04ulKXn},
  year = {2024}
}

@inproceedings{hong2024harmonizing,
  abstract = {We propose a novel method for subpopulation imbalanced learning, which succeeds in discovering and balancing the latent structures in training data. The paper addresses the challenge of machine learning algorithms struggling with skewed data distributions, particularly when minority classes are critical. The authors propose a method called Scatter and {HarmonizE} ({SHE}) that aims to discover and balance hidden subpopulations in class-balanced datasets.},
  author = {Feng Hong and Jiangchao Yao and Yueming Lyu and Zhihan Zhou and Ivor W. Tsang and Ya Zhang and Yanfeng Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024harmonizing.pdf:pdf},
  keywords = {imbalanced learning, subpopulation imbalance},
  note = {{DBLP} last modified: 2024-11-14},
  pdf = {https://openreview.net/pdf?id=3GurO0kRue},
  publisher = {OpenReview.net},
  title = {On Harmonizing Implicit Subpopulations},
  url = {https://openreview.net/forum?id=3GurO0kRue},
  year = {2024}
}

@inproceedings{zhong2024seeking,
  abstract = {Parametric knowledge are transferable between large language models across varying scales. {LLMs} encode knowledge within their parameters through pre-training and investigates knowledge transfer from larger to smaller models. Uses sensitivity-based techniques to extract and align knowledge-specific parameters and employs {LoRA} module to inject extracted knowledge into smaller models. Validated across four benchmarks.},
  author = {Ming Zhong and Chenxin An and Weizhu Chen and Jiawei Han and Pengcheng He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhong2024seeking.pdf:pdf},
  keywords = {Parametric Knowledge Transfer, Large Language Model},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mIEHIcHGOo},
  publisher = {OpenReview.net},
  title = {Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective},
  url = {https://openreview.net/forum?id=mIEHIcHGOo},
  year = {2024}
}

@inproceedings{huang2024adversarial,
  abstract = {The paper addresses the security threats to deep neural networks from backdoor attacks, which are achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger, but when the trigger is complex and invisible, defenders cannot reproduce it successfully and the {DNN} model will not be repaired. The authors propose Adversarial Feature Map Pruning for Backdoor ({FMP}) to mitigate backdoor from the {DNN}.},
  author = {Dong Huang and Qingwen Bu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024adversarial.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IOEEDkla96},
  publisher = {OpenReview.net},
  title = {Adversarial Feature Map Pruning for Backdoor},
  url = {https://openreview.net/forum?id=IOEEDkla96},
  year = {2024}
}

@inproceedings{wang2024negatively,
  abstract = {This paper proposes a regularised ensemble reinforcement learning approach with policy regularisation theorems to train generators that generates diverse and promising game levels in real-time. Deep reinforcement learning has recently been successfully applied to online procedural content generation in which a policy determines promising game-level segments. However, existing methods can hardly discover diverse level patterns, while the lack of diversity makes the gameplay boring. This paper proposes an ensemble reinforcement learning approach that uses multiple negatively correlated sub-policies to generate different alternative level segments, and stochastically selects one of them following a selector model. A novel policy regularisation technique is integrated into the approach to diversify the generated alternatives.},
  author = {Ziqi Wang and Chengpeng Hu and Jialin Liu and Xin Yao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024negatively.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iAW2EQXfwb},
  publisher = {OpenReview.net},
  title = {Negatively Correlated Ensemble Reinforcement Learning for Online Diverse Game Level Generation},
  url = {https://openreview.net/forum?id=iAW2EQXfwb},
  year = {2024}
}

@inproceedings{yang2024gencorres,
  abstract = {This paper introduces {GenCorres}, a novel unsupervised joint shape matching ({JSM}) approach. Our key idea is to learn a mesh generator to fit an unorganized deformable shape collection while constraining deformations between adjacent synthetic shapes to preserve geometric structures such as local rigidity and local conformality. {GenCorres} performs {JSM} among a synthetic shape collection whose size is much bigger than the input shapes and fully leverages the data-driven power of {JSM}. {GenCorres} unifies consistent shape matching and pairwise matching by enforcing deformation priors between adjacent synthetic shapes. The generator provides a concise encoding of consistent shape correspondences.},
  author = {Haitao Yang and Xiangru Huang and Bo Sun and Chandrajit L. Bajaj and Qixing Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024gencorres.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dGH4kHFKFj},
  publisher = {OpenReview.net},
  title = {{GenCorres}: Consistent Shape Matching via Coupled Implicit-Explicit Shape Generative Models},
  url = {https://openreview.net/forum?id=dGH4kHFKFj},
  year = {2024}
}

@inproceedings{pham2024circumventing,
  abstract = {Text-to-image generative models can produce photo-realistic images for an extremely broad range of concepts, and recent works have shown that these models memorize and can regurgitate a wide variety of copyrighted, violent, and otherwise problematic content. Various methods have been proposed in order to 'erase' sensitive concepts from text-to-image models. In this work, we examine seven recently proposed concept erasure methods, and show that targeted concepts are not fully excised from any of these methods. We demonstrate that problematic content can be reconstructed through a variety of techniques, even when the space of possible user inputs is significantly restricted.},
  address = {Vienna, Austria},
  author = {Minh Pham and Kelly O. Marshall and Niv Cohen and Govind Mittal and Chinmay Hegde},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pham2024circumventing.pdf:pdf},
  keywords = {model editing, diffusion model, concept erasure},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=ag3o2T51Ht},
  publisher = {OpenReview.net},
  title = {Circumventing Concept Erasure Methods For Text-To-Image Generative Models},
  url = {https://openreview.net/forum?id=ag3o2T51Ht},
  year = {2024}
}

@inproceedings{lu2024temporal,
  abstract = {The ability to predict how well models generalize to future data is crucial for machine learning applications. While recent works have made significant progress in generalization estimation for traditional machine learning settings, few have addressed the challenges of evolving graphs. In this work, we establish a theoretical lower bound, proving that representation distortion inevitably occurs over time in evolving graphs. We introduce SMART, an adaptive feature extractor that uses self-supervised graph reconstruction to maintain robust representations as graphs evolve. Our experiments demonstrate SMART's outstanding generalization estimation capabilities on four real-world evolving graphs.},
  address = {Vienna, Austria},
  author = {Bin Lu and Tingyan Ma and Xiaoying Gan and Xinbing Wang and Yunqiang Zhu and Chenghu Zhou and Shiyu Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lu2024temporal.pdf:pdf},
  keywords = {generalization estimation, evolving graph, graph neural network},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=HFtrXBfNru},
  publisher = {OpenReview.net},
  title = {Temporal Generalization Estimation in Evolving Graphs},
  url = {https://openreview.net/forum?id=HFtrXBfNru},
  year = {2024}
}

@inproceedings{michal2024prediction,
  abstract = {Class-incremental learning (CIL) is a particularly challenging variant of continual learning, where the goal is to learn to discriminate between all classes presented in an incremental fashion. Existing approaches often suffer from excessive forgetting and imbalance of the scores assigned to classes that have not been seen together during training. In this study, we introduce a novel approach, Prediction Error-based Classification (PEC), which differs from traditional discriminative and generative classification paradigms. PEC computes a class score by measuring the prediction error of a model trained to replicate the outputs of a frozen random neural network on data from that class. The method can be interpreted as approximating a classification rule based on Gaussian Process posterior variance. PEC offers several practical advantages, including sample efficiency, ease of tuning, and effectiveness even when data are presented one class at a time. Our empirical results show that PEC performs strongly in single-pass-through-data CIL, outperforming other rehearsal-free baselines in all cases and rehearsal-based methods with moderate replay buffer size in most cases across multiple benchmarks.},
  address = {Vienna, Austria},
  author = {Micha{\l} Zaj{\c{a}}c and Tinne Tuytelaars and Gido M. van de Ven},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/michal2024prediction.pdf:pdf},
  keywords = {continual learning, class-incremental learning},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=DJZDgMOLXQ},
  publisher = {OpenReview.net},
  title = {Prediction Error-based Classification for Class-Incremental Learning},
  url = {https://openreview.net/forum?id=DJZDgMOLXQ},
  year = {2024}
}

@inproceedings{li2024towards,
  abstract = {Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to $\ell_2$-accurate estimates of the (Stein) score functions. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results characterize how $\ell_2$ score estimation errors affect the quality of the data generation process. In contrast to prior works, our theory is developed based on an elementary yet versatile non-asymptotic approach without resorting to toolboxes for SDEs and ODEs.},
  address = {Vienna, Austria},
  author = {Gen Li and Yuting Wei and Yuxin Chen and Yuejie Chi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024towards.pdf:pdf},
  keywords = {diffusion models, score-based generative modeling, non-asymptotic theory, reverse SDE, probability flow ODE, denoising diffusion probabilistic model},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=4VGEeER6W9},
  publisher = {OpenReview.net},
  title = {Towards Non-Asymptotic Convergence for Diffusion-Based Generative Models},
  url = {https://openreview.net/forum?id=4VGEeER6W9},
  year = {2024}
}

@inproceedings{jin2024timellm,
  abstract = {Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.},
  address = {Vienna, Austria},
  author = {Ming Jin and Shiyu Wang and Lintao Ma and Zhixuan Chu and James Y. Zhang and Xiaoming Shi and Pin-Yu Chen and Yuxuan Liang and Yuan-Fang Li and Shirui Pan and Qingsong Wen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024timellm.pdf:pdf},
  keywords = {time series forecasting, large language models, model reprogramming},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=Unb5CVPtae},
  publisher = {OpenReview.net},
  title = {Time-{LLM}: Time Series Forecasting by Reprogramming Large Language Models},
  url = {https://openreview.net/forum?id=Unb5CVPtae},
  year = {2024}
}

@inproceedings{fu2024mofdiff,
  abstract = {Metal-organic frameworks (MOFs) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry. Their modular nature has enabled the use of template-based methods to generate hypothetical MOFs by combining molecular building blocks in accordance with known network topologies. However, the ability of these methods to identify top-performing MOFs is often hindered by the limited diversity of the resulting chemical space. In this work, we propose MOFDiff: a coarse-grained (CG) diffusion model that generates CG MOF structures through a denoising diffusion process over the coordinates and identities of the building blocks. The all-atom MOF structure is then determined through a novel assembly algorithm. We evaluate MOFDiff on the validity and novelty of the sampled structures, as well as the gas adsorption performance of structures optimized for CO2 capture applications.},
  address = {Vienna, Austria},
  author = {Xiang Fu and Tian Xie and Andrew S. Rosen and Tommi S. Jaakkola and Jake Smith},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024mofdiff.pdf:pdf},
  keywords = {materials design, diffusion model, metal-organic framework, carbon capture, generative model, AI for Science},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=0VBsoluxR2},
  publisher = {OpenReview.net},
  title = {{MOFDiff}: Coarse-grained Diffusion for Metal-Organic Framework Design},
  url = {https://openreview.net/forum?id=0VBsoluxR2},
  year = {2024}
}

@inproceedings{qiao2024understanding,
  abstract = {Despite the rapid development of machine learning algorithms for domain generalization (DG), there is no clear empirical evidence that the existing DG algorithms outperform the classic empirical risk minimization (ERM) across standard benchmarks. To better understand this phenomenon, we investigate whether there are benefits of DG algorithms over ERM through the lens of label noise. Specifically, our finite-sample analysis reveals that label noise exacerbates the effect of spurious correlations for ERM, undermining generalization. Conversely, we illustrate that DG algorithms exhibit implicit label-noise robustness during finite-sample training even when spurious correlation is present. Such desirable property helps mitigate spurious correlations and improve generalization in synthetic experiments. However, additional comprehensive experiments on real-world benchmark datasets indicate that label-noise robustness does not necessarily translate to better performance compared to ERM. We conjecture that the failure mode of ERM arising from spurious correlations may be less pronounced in practice.},
  address = {Vienna, Austria},
  author = {Rui Qiao and Bryan Kian Hsiang Low},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiao2024understanding.pdf:pdf},
  keywords = {out-of-distribution generalization, distribution shifts, spurious correlation, noise robustness},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=I2mIxuXA72},
  publisher = {OpenReview.net},
  title = {Understanding Domain Generalization: A Noise Robustness Perspective},
  url = {https://openreview.net/forum?id=I2mIxuXA72},
  year = {2024}
}

@inproceedings{zhang2024when,
  abstract = {While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follows a power-based multiplicative joint scaling law between finetuning data size and each other scaling factor; 2) LLM finetuning benefits more from LLM model scaling than pretraining data scaling, and PET parameter scaling is generally ineffective; and 3) the optimal finetuning method is highly task- and finetuning data-dependent. We hope our findings could shed light on understanding, selecting and developing LLM finetuning methods.},
  address = {Vienna, Austria},
  author = {Biao Zhang and Zhongtao Liu and Colin Cherry and Orhan Firat},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024when.pdf:pdf},
  keywords = {LLM finetuning, scaling laws, full-model finetuning, parameter efficient tuning, machine translation, multilingual summarization},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=5HCnKDeTws},
  publisher = {OpenReview.net},
  title = {When Scaling Meets {LLM} Finetuning: The Effect of Data, Model and Finetuning Method},
  url = {https://openreview.net/forum?id=5HCnKDeTws},
  year = {2024}
}

@inproceedings{li2024finetuning,
  abstract = {Recent advancements in Multimodal Large Language Models (MLLMs) have been utilizing Visual Prompt Generators (VPGs) to convert visual features into tokens that LLMs can recognize. This is achieved by training the VPGs on millions of image-caption pairs, where the VPG-generated tokens of images are fed into a frozen LLM to generate the corresponding captions. However, this image-captioning based training objective inherently biases the VPG to concentrate solely on the primary visual contents sufficient for caption generation, often neglecting other visual details. This shortcoming results in MLLMs' underperformance in comprehending demonstrative instructions consisting of multiple, interleaved, and multimodal instructions that demonstrate the required context to complete a task. The paper proposes VPG-C (Visual Prompt Generator Complete) to address these limitations. Further, we propose a synthetic discriminative training strategy to fine-tune VPG-C, eliminating the need for supervised demonstrative instructions. As for evaluation, we build DEMON, a comprehensive benchmark for demonstrative instruction understanding. Synthetically trained with the proposed strategy, VPG-C achieves significantly stronger zero-shot performance across all tasks of DEMON. Further evaluation on the MME and OwlEval benchmarks also demonstrate the superiority of VPG-C.},
  address = {Vienna, Austria},
  author = {Juncheng Li and Kaihang Pan and Zhiqi Ge and Minghe Gao and Wei Ji and Wenqiao Zhang and Tat-Seng Chua and Siliang Tang and Hanwang Zhang and Yueting Zhuang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024finetuning.pdf:pdf},
  keywords = {multimodal large language models, demonstrative instruction},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=BXY6fe7q31},
  publisher = {OpenReview.net},
  title = {Fine-tuning Multimodal {LLMs} to Follow Zero-shot Demonstrative Instructions},
  url = {https://openreview.net/forum?id=BXY6fe7q31},
  year = {2024}
}

@inproceedings{yang2024vqgraph,
  abstract = {GNN-to-MLP distillation aims to utilize knowledge distillation (KD) to learn computationally-efficient multi-layer perceptron (student MLP) on graph data by mimicking the output representations of teacher GNN. Existing methods mainly make the MLP to mimic the GNN predictions over a few class labels. However, the class space may not be expressive enough for covering numerous diverse local graph structures, thus limiting the performance of knowledge transfer from GNN to MLP. To address this issue, we propose to learn a new powerful graph representation space by directly labeling nodes' diverse local structures for GNN-to-MLP distillation. Specifically, we propose a variant of VQ-VAE to learn a structure-aware tokenizer on graph data that can encode each node's local substructure as a discrete code. The discrete codes constitute a codebook as a new graph representation space that is able to identify different local graph structures of nodes with the corresponding code indices. Then, based on the learned codebook, we propose a new distillation target, namely soft code assignments, to directly transfer the structural knowledge of each node from GNN to MLP. The resulting framework VQGraph achieves new state-of-the-art performance on GNN-to-MLP distillation in both transductive and inductive settings across seven graph datasets. We show that VQGraph with better performance infers faster than GNNs by 828√ó, and also achieves accuracy improvement over GNNs and stand-alone MLPs by 3.90% and 28.05% on average, respectively.},
  address = {Vienna, Austria},
  author = {Ling Yang and Ye Tian and Minkai Xu and Zhongyi Liu and Shenda Hong and Wei Qu and Wentao Zhang and Bin Cui and Muhan Zhang and Jure Leskovec},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024vqgraph.pdf:pdf},
  keywords = {graph knowledge distillation, efficient graph learning},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=h6Tz85BqRI},
  publisher = {OpenReview.net},
  title = {{VQGraph}: Rethinking Graph Representation Space for Bridging {GNNs} and {MLPs}},
  url = {https://openreview.net/forum?id=h6Tz85BqRI},
  year = {2024}
}

@inproceedings{chapman2024unconstrained,
  abstract = {The Canonical Correlation Analysis (CCA) family of methods is foundational in multiview learning. Regularised linear CCA methods can be seen to generalise Partial Least Squares (PLS) and be unified with a Generalized Eigenvalue Problem (GEP) framework. However, classical algorithms for these linear methods are computationally infeasible for large-scale data, and extensions to Deep CCA show great promise, but current training procedures are slow and complicated. In this paper, we propose a novel unconstrained objective that characterizes the top subspace of GEPs. We develop a family of fast algorithms for stochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying stochastic gradient descent (SGD) to the corresponding CCA objectives.},
  author = {James Chapman and Lennie Wells and Ana Lawry Aguila},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chapman2024unconstrained.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PHLVmV88Zy},
  publisher = {OpenReview.net},
  title = {Unconstrained Stochastic {CCA}: Unifying Multiview and Self-Supervised Learning},
  url = {https://openreview.net/forum?id=PHLVmV88Zy},
  year = {2024}
}

@inproceedings{zhao2024testtime,
  abstract = {One fascinating aspect of pre-trained vision-language models (VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation (TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs and tend to be stuck in incorrect model predictions. In this work, we propose a novel approach for TTA where a CLIP model is adopted as the reward model and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. We call this approach Reinforcement Learning with CLIP Feedback (RLCF). RLCF is highly flexible and universal, which can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning.},
  author = {Shuai Zhao and Xiaohan Wang and Linchao Zhu and Yi Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024testtime.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=kIP0duasBb},
  publisher = {OpenReview.net},
  title = {Test-Time Adaptation with {CLIP} Reward for Zero-Shot Generalization in Vision-Language Models},
  url = {https://openreview.net/forum?id=kIP0duasBb},
  year = {2024}
}

@inproceedings{yang2024crossmodal,
  abstract = {Conditional diffusion models have exhibited superior performance in high-fidelity text-guided visual generation and editing. Nevertheless, prevailing text-guided visual diffusion models primarily focus on incorporating text-visual relationships exclusively into the reverse process, often disregarding their relevance in the forward process. This inconsistency leads to suboptimal cross-modal conditional modeling. We propose ContextDiff, a novel cross-modal contextualized diffusion model that harnesses cross-modal context to facilitate the learning capacity of diffusion models for text-guided visual generation. Our approach propagates context to all timesteps in the diffusion processes to adapt their trajectories, thereby facilitating cross-modal conditional modeling. We generalize our approach to both DDPMs and DDIMs with theoretical derivations. We conduct experiments on two challenging tasks: text-to-image generation and text-to-video editing, where ContextDiff achieves new state-of-the-art performance.},
  author = {Ling Yang and Zhilong Zhang and Zhaochen Yu and Jingwei Liu and Minkai Xu and Stefano Ermon and Bin Cui},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024crossmodal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nFMS6wF2xq},
  publisher = {OpenReview.net},
  title = {Cross-Modal Contextualized Diffusion Models for Text-Guided Visual Generation and Editing},
  url = {https://openreview.net/forum?id=nFMS6wF2xq},
  year = {2024}
}

@inproceedings{gao2024enhancing,
  abstract = {Existing game AI research mainly focuses on enhancing agents' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, in MOBA games, agents may win by performing well in team fights, but this can involve killstealing behaviors that negatively impact the experience of human teammates. In this work, we propose a human-centered modeling scheme for collaborative agents and introduce the Reinforcement Learning from Human Gain (RLHG) approach. We model the experience of humans as the goals they expect to achieve during the task, and expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents' original abilities. The RLHG approach introduces a baseline, which corresponds to the extent to which humans primitively achieve their goals, and encourages agents to learn behaviors that can effectively enhance humans in achieving their goals better.},
  author = {Yiming Gao and Feiyu Liu and Liang Wang and Dehua Zheng and Zhenjie Lian and Weixuan Wang and Wenjin Yang and Siqin Li and Xianliang Wang and Wenhui Chen and Jing Dai and Qiang Fu and Wei Yang and Lanxiao Huang and Wei Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024enhancing.pdf:pdf},
  note = {DBLP last modified: 2025-01-16},
  pdf = {https://openreview.net/pdf?id=BqEvdOS1Hs},
  publisher = {OpenReview.net},
  title = {Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain},
  url = {https://openreview.net/forum?id=BqEvdOS1Hs},
  year = {2024}
}

@inproceedings{huang2024towards,
  abstract = {Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-sourced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. To address this challenge, we propose GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most appropriate set of tensors in training. Experiment results over multiple open-sourced LLM models and abstractive summarization datasets show that, compared to fine-tuning the whole LLM model, GreenTrainer can save up to 64% FLOPs in fine-tuning without any noticeable model accuracy loss.},
  author = {Kai Huang and Hanyun Yin and Heng Huang and Wei Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Vja3ecieXY},
  publisher = {OpenReview.net},
  title = {Towards Green {AI} in Fine-tuning Large Language Models via Adaptive Backpropagation},
  url = {https://openreview.net/forum?id=Vja3ecieXY},
  year = {2024}
}

@inproceedings{huang2024safedreamer,
  abstract = {The deployment of Reinforcement Learning ({RL}) in real-world applications is constrained by its failure to satisfy safety criteria. Existing Safe Reinforcement Learning ({SafeRL}) methods, which rely on cost functions to enforce safety, often fail to achieve zero-cost performance in complex scenarios, especially vision-only tasks. These limitations are primarily due to model inaccuracies and inadequate sample efficiency. The integration of the world model has proven effective in mitigating these shortcomings. In this work, we introduce {SafeDreamer}, a novel algorithm incorporating Lagrangian-based methods into world model planning processes within the superior Dreamer framework. Our method achieves nearly zero-cost performance on various tasks, spanning low-dimensional and vision-only input, within the Safety-Gymnasium benchmark, showcasing its efficacy in balancing performance and safety in {RL} tasks.},
  author = {Weidong Huang and Jiaming Ji and Chunhe Xia and Borong Zhang and Yaodong Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024safedreamer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=tsE5HLYtYg},
  publisher = {OpenReview.net},
  title = {{SafeDreamer}: Safe Reinforcement Learning with World Models},
  url = {https://openreview.net/forum?id=tsE5HLYtYg},
  year = {2024}
}

@inproceedings{xu2024retrieval,
  abstract = {Extending the context window of large language models ({LLMs}) is getting popular recently, while the solution of augmenting {LLMs} with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained {LLMs}, i.e., a proprietary 43B {GPT} and Llama2-70B. Perhaps surprisingly, we find that {LLM} with 4K context window using simple retrieval-augmentation at generation can achieve comparable performance to finetuned {LLM} with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the performance of {LLMs} regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms {GPT}-3.5-turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks.},
  author = {Peng Xu and Wei Ping and Xianchao Wu and Lawrence McAfee and Chen Zhu and Zihan Liu and Sandeep Subramanian and Evelina Bakhturina and Mohammad Shoeybi and Bryan Catanzaro},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024retrieval.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=xw5nxFWMlo},
  publisher = {OpenReview.net},
  title = {Retrieval meets Long Context Large Language Models},
  url = {https://openreview.net/forum?id=xw5nxFWMlo},
  year = {2024}
}

@inproceedings{wu2024you,
  abstract = {As one of the privacy threats to machine learning models, the membership inference attack ({MIA}) tries to infer whether a given sample is in the original training set of a victim model by analyzing its outputs. Recent studies only use the predicted hard labels to achieve impressive membership inference accuracy. However, such label-only {MIA} approach requires very high query budgets to evaluate the distance of the target sample from the victim model's decision boundary. We propose {YOQO}, a novel label-only attack to overcome the above limitation. {YOQO} aims at identifying a special area (called improvement area) around the target sample and crafting a query sample, whose hard label from the victim model can reliably reflect the target sample's membership. {YOQO} can successfully reduce the query budget from more than 1,000 times to only {ONCE}. Experiments demonstrate that {YOQO} is not only as effective as {SOTA} attack methods, but also performs comparably or even more robustly against many sophisticated defenses.},
  author = {Yutong Wu and Han Qiu and Shangwei Guo and Jiwei Li and Tianwei Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024you.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7WsivwyHrS},
  publisher = {OpenReview.net},
  title = {You Only Query Once: An Efficient Label-Only Membership Inference Attack},
  url = {https://openreview.net/forum?id=7WsivwyHrS},
  year = {2024}
}

@inproceedings{huang2024large,
  abstract = {Large Language Models ({LLMs}) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within {LLMs}, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an {LLM} attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that {LLMs} struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.},
  author = {Jie Huang and Xinyun Chen and Swaroop Mishra and Huaixiu Steven Zheng and Adams Wei Yu and Xinying Song and Denny Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024large.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IkmD3fKBPQ},
  publisher = {OpenReview.net},
  title = {Large Language Models Cannot Self-Correct Reasoning Yet},
  url = {https://openreview.net/forum?id=IkmD3fKBPQ},
  year = {2024}
}

@inproceedings{yang2024objectaware,
  abstract = {By comparing the original and target prompts, we can obtain numerous editing pairs, each comprising an object and its corresponding editing target. To allow editability while maintaining fidelity to the input image, existing editing methods typically involve a fixed number of inversion steps that project the whole input image to its noisier latent representation, followed by a denoising process guided by the target prompt. However, we find that the optimal number of inversion steps for achieving ideal editing results varies significantly among different editing pairs, owing to varying editing difficulties. Therefore, the current literature, which relies on a fixed number of inversion steps, produces sub-optimal generation quality, especially when handling multiple editing pairs in a natural image. To this end, we propose a new image editing paradigm, dubbed Object-aware Inversion and Reassembly ({OIR}), to enable object-level fine-grained editing. Specifically, we design a new search metric, which determines the optimal inversion steps for each editing pair, by jointly considering the editability of the target and the fidelity of the non-editing region. We use our search metric to find the optimal inversion step for each editing pair when editing an image. We then edit these editing pairs separately to avoid concept mismatch. Subsequently, we propose an additional reassembly step to seamlessly integrate the respective editing results and the non-editing region to obtain the final edited image.},
  author = {Zhen Yang and Ganggui Ding and Wen Wang and Hao Chen and Bohan Zhuang and Chunhua Shen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024objectaware.pdf:pdf},
  note = {DBLP last modified: 2024-12-05},
  pdf = {https://openreview.net/pdf?id=dpcVXiMlcv},
  publisher = {OpenReview.net},
  title = {Object-Aware Inversion and Reassembly for Image Editing},
  url = {https://openreview.net/forum?id=dpcVXiMlcv},
  year = {2024}
}

@inproceedings{kumar2024genz,
  abstract = {Language model ({LM}) prompting---a popular paradigm for solving {NLP} tasks---has been shown to be susceptible to miscalibration and brittleness to slight prompt variations, caused by its discriminative prompting approach, i.e., predicting the label given the input. To address these issues, we propose {Gen-Z}---a generative prompting framework for zero-shot text classification. {GEN-Z} is generative, as it measures the {LM} likelihood of input text, conditioned on natural language descriptions of labels. The framework is multivariate, as label descriptions allow us to seamlessly integrate additional contextual information about the labels to improve task performance. On various standard classification benchmarks, with six open-source {LM} families, we show that zero-shot classification with simple contextualization of the data source of the evaluation set consistently outperforms both zero-shot and few-shot baselines while improving robustness to prompt variations. Further, our approach enables personalizing classification in a zero-shot manner by incorporating author, subject, or reader information in the label descriptions.},
  author = {Sachin Kumar and Chan Young Park and Yulia Tsvetkov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kumar2024genz.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rkplYfqUr0},
  publisher = {OpenReview.net},
  title = {{Gen-Z}: {G}enerative {Z}ero-{S}hot {T}ext {C}lassification with {C}ontextualized {L}abel {D}escriptions},
  url = {https://openreview.net/forum?id=rkplYfqUr0},
  year = {2024}
}

@inproceedings{yang2024towards,
  abstract = {Offline reinforcement learning ({RL}) presents a promising approach for learning reinforced policies from offline datasets without the need for costly or unsafe interactions with the environment. However, datasets collected by humans in real-world environments are often noisy and may even be maliciously corrupted, which can significantly degrade the performance of offline {RL}. In this work, we first investigate the performance of current offline {RL} algorithms under comprehensive data corruption, including states, actions, rewards, and dynamics. Our extensive experiments reveal that implicit {Q}-learning ({IQL}) demonstrates remarkable resilience to data corruption among various offline {RL} algorithms. Furthermore, we conduct both empirical and theoretical analyses to understand {IQL}'s robust performance, identifying its supervised policy learning scheme as the key factor. Despite its relative robustness, {IQL} still suffers from heavy-tail targets of {Q} functions under dynamics corruption. To tackle this challenge, we draw inspiration from robust statistics to employ the {Huber} loss to handle the heavy-tailedness and utilize quantile estimators to balance penalization for corrupted data and learning stability. By incorporating these simple yet effective modifications into {IQL}, we propose a more robust offline {RL} approach named Robust {IQL} ({RIQL}). Extensive experiments demonstrate that {RIQL} exhibits highly robust performance when subjected to diverse data corruption scenarios.},
  author = {Rui Yang and Han Zhong and Jiawei Xu and Amy Zhang and Chongjie Zhang and Lei Han and Tong Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024towards.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=5hAMmCU0bK},
  publisher = {OpenReview.net},
  title = {Towards Robust Offline Reinforcement Learning under Diverse Data Corruption},
  url = {https://openreview.net/forum?id=5hAMmCU0bK},
  year = {2024}
}

@inproceedings{cheng2024fusion,
  abstract = {Multi-sensor fusion ({MSF}) is widely used in autonomous vehicles ({AVs}) for perception, particularly for {3D} object detection with camera and {LiDAR} sensors. The purpose of fusion is to capitalize on the advantages of each modality while minimizing its weaknesses. Advanced deep neural network ({DNN})-based fusion techniques have demonstrated the exceptional and industry-leading performance. Due to the redundant information in multiple modalities, {MSF} is also recognized as a general defence strategy against adversarial attacks. In this paper, we attack fusion models from the camera modality that is considered to be of lesser importance in fusion but is more affordable for attackers. We argue that the weakest link of fusion models depends on their most vulnerable modality and propose an attack framework that targets advanced camera-{LiDAR} fusion-based {3D} object detection models through camera-only adversarial attacks. Our approach employs a two-stage optimization-based strategy that first thoroughly evaluates vulnerable image areas under adversarial attacks, and then applies dedicated attack strategies for different fusion models to generate deployable patches. The evaluations with six advanced camera-{LiDAR} fusion models and one camera-only model indicate that our attacks successfully compromise all of them. Our approach can either decrease the mean average precision ({mAP}) of detection performance from 0.824 to 0.353 or degrade the detection score of a target object from 0.728 to 0.156, demonstrating the efficacy of our proposed attack framework. Code is available.},
  author = {Zhiyuan Cheng and Hongjun Choi and Shiwei Feng and James Chenhao Liang and Guanhong Tao and Dongfang Liu and Michael Zuzak and Xiangyu Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024fusion.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3VD4PNEt5q},
  publisher = {OpenReview.net},
  title = {Fusion Is Not Enough: Single Modal Attacks on Fusion Models for {3D} Object Detection},
  url = {https://openreview.net/forum?id=3VD4PNEt5q},
  year = {2024}
}

@inproceedings{wang2024biobridge,
  abstract = {Foundation models ({FMs}) learn from large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, {FMs} developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation, we present {BioBridge}, a parameter-efficient learning framework, to bridge independently trained unimodal {FMs} to establish multimodal behavior. {BioBridge} achieves it by utilizing Knowledge Graphs ({KG}) to learn transformations between one unimodal {FM} and another without fine-tuning any underlying unimodal {FMs}. Our results demonstrate that {BioBridge} can beat the best baseline {KG} embedding methods (on average by \~{} 76.3\%) in cross-modal retrieval tasks. We also identify {BioBridge} demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relations. Additionally, we also show that {BioBridge} presents itself as a general-purpose retriever that can aid biomedical multimodal question answering as well as enhance the guided generation of novel drugs.},
  author = {Zifeng Wang and Zichen Wang and Balasubramaniam Srinivasan and Vassilis N. Ioannidis and Huzefa Rangwala and Rishita Anubhai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024biobridge.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-12-06},
  pdf = {https://openreview.net/pdf?id=jJCeMiwHdH},
  publisher = {OpenReview.net},
  title = {{BioBridge}: Bridging Biomedical Foundation Models via Knowledge Graphs},
  url = {https://openreview.net/forum?id=jJCeMiwHdH},
  year = {2024}
}

@inproceedings{deng2024multilingual,
  abstract = {While large language models ({LLMs}) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the 'jailbreak' problem, wherein malicious instructions can manipulate {LLMs} to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with {LLMs}, they have primarily focused on English. In this work, we reveal the multilingual jailbreak challenges that exist in {LLMs} and investigate potential ways to mitigate such challenges. Specifically, we first investigate two scenarios of multilingual jailbreak: (1) replicating harmful content by translating unsafe English instructions into low-resource languages, and (2) unsafe {LLM} usage by triggering {LLMs} to respond to harmful content with multilingual prompts. We discover that low-resource languages suffer from the lack of safety alignment and have around 3\times higher rates of unsafe responses. More concerningly, we find that multilingual prompts can be used to successfully bypass the safety alignment of {LLMs}, increasing the unsafe response rates to 80.92\% for {ChatGPT} and 40.71\% for {GPT-4}. To mitigate such challenges, we propose a Self-Defense framework that leverages multilingual safety training data to enhance cross-lingual safety alignment. Experiment results on two safety benchmarks with human annotations show that our approach significantly improves multilingual safety alignment for both low-resource languages and multilingual settings while maintaining model performance.},
  author = {Yue Deng and Wenxuan Zhang and Sinno Jialin Pan and Lidong Bing},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/deng2024multilingual.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-24},
  pdf = {https://openreview.net/pdf?id=vESNKdEMGp},
  publisher = {OpenReview.net},
  title = {Multilingual Jailbreak Challenges in Large Language Models},
  url = {https://openreview.net/forum?id=vESNKdEMGp},
  year = {2024}
}

@inproceedings{tan2024rdesign,
  abstract = {While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing {RNA} sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, {RNA} design still confronts difficulties due to structural complexity and data scarcity. In this work, we propose {RDesign}, a hierarchical data-efficient representation learning framework for tertiary structure-based {RNA} design. We craft a large, well-curated benchmark dataset and design a comprehensive structural modeling approach to represent the complex {RNA} tertiary structure. More importantly, we propose a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporate extracted secondary structures with base pairs as prior knowledge to facilitate the {RNA} design process. Experimental results demonstrate that {RDesign} outperforms all baseline methods on the recovery metric, achieving a recovery rate of 41.69\% on the testing set, demonstrating its superior ability to generate {RNA} sequences with higher fidelity than baseline models.},
  author = {Cheng Tan and Yijie Zhang and Zhangyang Gao and Bozhen Hu and Siyuan Li and Zicheng Liu and Stan Z. Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tan2024rdesign.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-11-22},
  pdf = {https://openreview.net/pdf?id=RemfXx7ebP},
  publisher = {OpenReview.net},
  title = {{RDesign}: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based {RNA} Design},
  url = {https://openreview.net/forum?id=RemfXx7ebP},
  year = {2024}
}

@inproceedings{liu2024deep,
  abstract = {Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called {TGC}, which introduces deep clustering techniques to suit the interaction sequence-based batch-processing pattern of temporal graphs. Concretely, we build our {TGC} model with a powerful temporal graph encoder and a joint clustering framework which could incorporate deep embedding clustering and community detection clustering. To assess the performance of {TGC}, we construct multiple temporal graph clustering datasets and conduct experiments with typical deep graph clustering methods and temporal graph learning models. The results show the effectiveness of our {TGC} and its flexibility to balance the requirements for time and space.},
  author = {Meng Liu and Yue Liu and Ke Liang and Wenxuan Tu and Siwei Wang and Sihang Zhou and Xinwang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024deep.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ViNe1fjGME},
  publisher = {OpenReview.net},
  title = {Deep Temporal Graph Clustering},
  url = {https://openreview.net/forum?id=ViNe1fjGME},
  year = {2024}
}

@inproceedings{chen2024recursive,
  abstract = {Transformer architectures have exhibited remarkable performance in image super-resolution ({SR}). Since the quadratic computational complexity of the self-attention ({SA}) in Transformer, existing methods tend to adopt {SA} in a local region to reduce overheads. However, the local design restricts the global context exploitation, which is crucial for accurate image reconstruction. In this work, we propose the Recursive Generalization Transformer ({RGT}) for image {SR}, which can capture global spatial information and is suitable for high-resolution images. Specifically, we propose the recursive-generalization self-attention ({RG-SA}). It recursively aggregates input features into representative feature maps, and then utilizes cross-attention to extract global information. Meanwhile, the channel dimensions of attention matrices (query, key, and value) are further scaled to mitigate the redundancy in the channel domain. Furthermore, we combine the {RG-SA} with local self-attention to enhance the exploitation of the global context, and propose the hybrid adaptive integration ({HAI}) for module integration. The {HAI} allows the direct and effective fusion between features at different levels (local or global). Extensive experiments demonstrate that our {RGT} outperforms recent state-of-the-art methods quantitatively and qualitatively.},
  author = {Zheng Chen and Yulun Zhang and Jinjin Gu and Linghe Kong and Xiaokang Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024recursive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-03-06},
  pdf = {https://openreview.net/pdf?id=owziuM1nsR},
  publisher = {OpenReview.net},
  title = {Recursive Generalization Transformer for Image Super-Resolution},
  url = {https://openreview.net/forum?id=owziuM1nsR},
  year = {2024}
}

@inproceedings{2024auxnas,
  abstract = {We aim at exploiting additional auxiliary labels from an independent (auxiliary) task to boost the primary task performance which we focus on, while preserving a single task inference cost of the primary task. While most existing auxiliary learning methods are optimization-based relying on loss weights/gradients manipulation, our method is architecture-based with a flexible asymmetric structure for the primary and auxiliary tasks, which produces different networks for training and inference. Specifically, starting from two single task networks/branches (each representing a task), we propose a novel method with evolving networks where only primary-to-auxiliary links exist as the cross-task connections after convergence. These connections can be removed during the primary task inference, resulting in a single-task inference cost. We achieve this by formulating a Neural Architecture Search ({NAS}) problem, where we initialize bi-directional connections in the search space and guide the {NAS} optimization converging to an architecture with only the single-side primary-to-auxiliary connections. Moreover, our method can be incorporated with optimization-based auxiliary learning approaches. Extensive experiments with six tasks on {NYU} v2, {CityScapes}, and {Taskonomy} datasets using {VGG}, {ResNet}, and {ViT} backbones validate the promising performance.},
  author = {Yuan Gao 0015 and Weizhong Zhang and Wenhan Luo and Lin Ma 0002 and Jin-Gang Yu and Gui-Song Xia and Jiayi Ma 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024auxnas.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cINwAhrgLf},
  publisher = {OpenReview.net},
  title = {Aux-NAS: Exploiting Auxiliary Labels with Negligibly Extra Inference Cost},
  url = {https://openreview.net/forum?id=cINwAhrgLf},
  year = {2024}
}

@inproceedings{2024waxingandwaning,
  abstract = {Deep Neural Networks ({DNNs}) require large amounts of labeled data for training, making widespread deployment challenging. Self-supervised learning ({SSL}) emerges as a promising approach that leverages inherent patterns within data through diverse augmentations to train models without explicit labels. However, {SSL}'s high computation costs remain a significant impediment, particularly for resource-constrained platforms. To address this challenge, we introduce {SimWnW}, a similarity-based efficient self-supervised learning framework. {SimWnW} works by strategically removing less important regions in augmented images and feature maps, which not only reduces computation costs but also eliminates irrelevant features that might slow down the learning process, thereby accelerating model convergence. Our experimental results demonstrate that {SimWnW} effectively reduces computation costs in self-supervised model training without compromising accuracy. Specifically, {SimWnW} yields up to 54\% and 51\% computation savings in training from scratch and transfer learning tasks, respectively. The framework provides a generic solution that can be applied to various self-supervised learning methods, making it broadly applicable for efficient representation learning.},
  author = {Sheng Li 0019 and Chao Wu 0006 and Ao Li 0004 and Yanzhi Wang and Xulong Tang and Geng Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024waxingandwaning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-03-07},
  pdf = {https://openreview.net/pdf?id=TilcG5C8bN},
  publisher = {OpenReview.net},
  title = {Waxing-and-Waning: a Generic Similarity-based Framework for Efficient Self-Supervised Learning},
  url = {https://openreview.net/forum?id=TilcG5C8bN},
  year = {2024}
}

@inproceedings{chen2024decentralized,
  abstract = {The conjugate gradient method is a crucial first-order optimization method that generally converges faster than the steepest descent method, and its computational cost is much lower than that of second-order methods. However, while various types of conjugate gradient methods have been studied in Euclidean spaces and on Riemannian manifolds, there is little study for those in distributed scenarios. This paper proposes a decentralized Riemannian conjugate gradient descent (DRCGD) method that aims at minimizing a global function over the Stiefel manifold. The optimization problem is distributed among a network of agents, where each agent is associated with a local function, and the communication between agents occurs over an undirected connected graph. Since the Stiefel manifold is a non-convex set, a global function is represented as a finite sum of possibly non-convex (but smooth) local functions. The proposed method is free from expensive Riemannian geometric operations such as retractions, exponential maps, and vector transports, thereby reducing the computational complexity required by each agent. To the best of our knowledge, DRCGD is the first decentralized Riemannian conjugate gradient algorithm to achieve global convergence over the Stiefel manifold.},
  author = {Jun Chen and Haishan Ye and Mengmeng Wang and Tianxin Huang and Guang Dai and Ivor W. Tsang and Yong Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024decentralized.pdf:pdf},
  note = {DBLP last modified: 2025-02-26},
  pdf = {https://openreview.net/pdf?id=PQbFUMKLFp},
  publisher = {OpenReview.net},
  title = {Decentralized {R}iemannian Conjugate Gradient Method on the {S}tiefel Manifold},
  url = {https://openreview.net/forum?id=PQbFUMKLFp},
  year = {2024}
}

@inproceedings{zhang2024cppo,
  abstract = {The approach of Reinforcement Learning from Human Feedback (RLHF) is widely used for enhancing pre-trained Language Models (LM), enabling them to better align with human preferences. Existing RLHF-based LMs however require complete retraining whenever new queries or feedback are introduced, as human preferences may differ across different domains or topics. LM retraining is often impracticable in most real-world scenarios, due to the substantial time and computational costs involved, as well as data privacy concerns. To address this limitation, we propose Continual Proximal Policy Optimization (CPPO), a novel method that is able to continually align LM with dynamic human preferences. Specifically, CPPO adopts a weighting strategy to decide which samples should be utilized for enhancing policy learning and which should be used for solidifying past experiences. This seeks a good trade-off between policy learning and knowledge retention. Our experimental results show that CPPO outperforms strong Continuous learning (CL) baselines when it comes to consistently aligning with human preferences. Furthermore, compared to PPO, CPPO offers more efficient and stable learning in non-continual scenarios.},
  author = {Han Zhang and Yu Lei and Lin Gui and Min Yang and Yulan He and Hui Wang and Ruifeng Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024cppo.pdf:pdf},
  note = {DBLP last modified: 2025-01-14},
  pdf = {https://openreview.net/pdf?id=86zAUE80pP},
  publisher = {OpenReview.net},
  title = {{CPPO}: Continual Learning for Reinforcement Learning with Human Feedback},
  url = {https://openreview.net/forum?id=86zAUE80pP},
  year = {2024}
}

@inproceedings{chen2024inside,
  abstract = {Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' internal states for hallucination detection (INSIDE). In particular, a simple yet effective EigenScore metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space.},
  author = {Chao Chen and Kai Liu and Ze Chen and Yi Gu and Yue Wu and Mingyuan Tao and Zhihang Fu and Jieping Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024inside.pdf:pdf},
  note = {DBLP last modified: 2025-05-06},
  pdf = {https://openreview.net/pdf?id=Zj12nzlQbz},
  publisher = {OpenReview.net},
  title = {{INSIDE}: {LLM}s' Internal States Retain the Power of Hallucination Detection},
  url = {https://openreview.net/forum?id=Zj12nzlQbz},
  year = {2024}
}

@inproceedings{yuan2024spatiotemporal,
  abstract = {Spatio-temporal modeling is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPD, for spatio-temporal few-shot learning with urban knowledge transfer. Unlike conventional approaches that heavily rely on common feature extraction or intricate few-shot learning designs, our solution takes a novel approach by performing generative pre-training on a collection of neural network parameters optimized with data from source cities. We recast spatio-temporal few-shot learning as pre-training a generative diffusion model, which generates tailored neural networks guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPD employs a Transformer-based denoising diffusion model, which is model-agnostic to integrate with powerful spatio-temporal neural networks. By addressing challenges arising from data gaps and the complexity of generalizing knowledge across cities, our framework consistently outperforms state-of-the-art baselines on multiple real-world datasets for tasks such as traffic speed prediction and crowd flow prediction.},
  author = {Yuan Yuan and Chenyang Shao and Jingtao Ding and Depeng Jin and Yong Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024spatiotemporal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QyFm3D3Tzi},
  publisher = {OpenReview.net},
  title = {Spatio-Temporal Few-Shot Learning via Diffusive Neural Network Generation},
  url = {https://openreview.net/forum?id=QyFm3D3Tzi},
  year = {2024}
}

@inproceedings{wang2024diffusion,
  abstract = {Originating from the diffusion phenomenon in physics that describes particle movement, the diffusion generative models inherit the characteristics of stochastic random walk in the data space along the denoising trajectory. However, the intrinsic mutual interference among image regions contradicts the need for practical downstream application scenarios where the preservation of low-level pixel information from given conditioning is desired (e.g., customization tasks like personalized generation and inpainting based on a user-provided single image). In this work, we investigate the diffusion (physics) in diffusion (machine learning) properties and propose our Cyclic One-Way Diffusion (COW) method to control the direction of diffusion phenomenon given a pre-trained frozen diffusion model for versatile customization application scenarios, where the low-level pixel information from the conditioning needs to be preserved. Notably, unlike most current methods that incorporate additional conditions by fine-tuning the base text-to-image diffusion model or learning auxiliary networks, our method provides a novel perspective to understand the task needs and is applicable to a wider range of customization scenarios in a learning-free manner. Extensive experiment results show that our proposed COW can achieve more flexible customization based on strict visual conditions in different application settings.},
  author = {Ruoyu Wang and Yongqi Yang and Zhihao Qian and Ye Zhu and Yu Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024diffusion.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ePOjNlOjLC},
  publisher = {OpenReview.net},
  title = {Diffusion in Diffusion: Cyclic One-Way Diffusion for Text-Vision-Conditioned Generation},
  url = {https://openreview.net/forum?id=ePOjNlOjLC},
  year = {2024}
}

@inproceedings{liu2024a,
  abstract = {The potential of Large Language Model (LLM) as agents has been widely acknowledged recently. Thus, there is an urgent need to quantitatively evaluate LLMs as agents on challenging tasks in interactive environments. We present AgentBench, a multi-dimensional benchmark that consists of 8 distinct environments to assess LLM-as-Agent's reasoning and decision-making abilities. Our extensive test over 29 API-based and open-sourced (OSS) LLMs shows that, while top commercial LLMs present a strong ability of acting as agents in complex environments, there is a significant disparity in performance between them and many OSS competitors that are no larger than 70B. We identify the typical reasons of failures in environments and LLMs, showing that poor long-term reasoning, decision-making, and instruction following abilities are the main obstacles for developing usable LLM agents. Improving instruction following and training on high quality multi-round alignment data could improve agent performance. And different from existing assumptions, training on code present ambivalent impacts on different agent tasks.},
  author = {Xiao Liu and Hao Yu and Hanchen Zhang and Yifan Xu and Xuanyu Lei and Hanyu Lai and Yu Gu and Hangliang Ding and Kaiwen Men and Kejuan Yang and Shudan Zhang and Xiang Deng and Aohan Zeng and Zhengxiao Du and Chenhui Zhang and Sheng Shen and Tianjun Zhang and Yu Su and Huan Sun and Minlie Huang and Yuxiao Dong and Jie Tang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024a.pdf:pdf},
  note = {DBLP last modified: 2025-02-18},
  pdf = {https://openreview.net/pdf?id=zAdUB0aCTQ},
  publisher = {OpenReview.net},
  title = {{A}gent{B}ench: Evaluating {LLM}s as Agents},
  url = {https://openreview.net/forum?id=zAdUB0aCTQ},
  year = {2024}
}

@inproceedings{li2024toolaugmented,
  abstract = {Reward modeling is instrumental for aligning large language models with human preferences, particularly within the context of reinforcement learning from human feedback (RLHF). While conventional reward models have exhibited remarkable scalability, they often struggle with fundamental functionality such as arithmetic computation, code execution, and factual lookup. In this paper, we propose a tool-augmented preference modeling approach, named Themis, to address these limitations by empowering reward models with access to external environments, including calculators and search engines. This approach not only fosters synergy between tool utilization and reward grading but also enhances interpretive capacity and scoring reliability. Themis achieves a noteworthy overall improvement of 17.7\% across eight tasks in preference ranking and outperforms Gopher 280B by 7.3\% on TruthfulQA task in zero-shot evaluation.},
  author = {Lei Li and Yekun Chai and Shuohuan Wang and Yu Sun and Hao Tian and Ningyu Zhang and Hua Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024toolaugmented.pdf:pdf},
  note = {DBLP last modified: 2024-11-13},
  pdf = {https://openreview.net/pdf?id=d94x0gWTUX},
  publisher = {OpenReview.net},
  title = {Tool-Augmented Reward Modeling},
  url = {https://openreview.net/forum?id=d94x0gWTUX},
  year = {2024}
}

@inproceedings{wu2024f,
  abstract = {Federated Learning (FL) is a distributed machine learning technique where multiple devices train a shared global model by using their local data. FL claims that the data privacy of local participants is preserved well because local data will not be shared with either the server-side or other training participants. However, this paper discovers a pioneering finding that a model inversion (MI) attacker, who acts as a benign participant, can invert the shared global model and obtain the data belonging to other participants. Therefore, it is important to evaluate such data-leakage risks of an FL system before using it. To alleviate this issue, we propose FedInverse to evaluate whether the FL global model can be inverted by MI attackers. In particular, FedInverse can be optimized by leveraging the Hilbert-Schmidt independence criterion (HSIC) as a regularizer to adjust the diversity of the MI attack generator. We test FedInverse with three typical MI attackers, GMI, KED-MI, and VMI, and the experiments show our FedInverse method can successfully obtain the data belonging to other participants.},
  author = {Di Wu and Jun Bai and Yiliao Song and Junjun Chen and Wei Zhou and Yong Xiang and Atul Sajjanhar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024f.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nTNgkEIfeb},
  publisher = {OpenReview.net},
  title = {{F}ed{I}nverse: Evaluating Privacy Leakage in Federated Learning},
  url = {https://openreview.net/forum?id=nTNgkEIfeb},
  year = {2024}
}

@inproceedings{liu2024chain,
  abstract = {Learning from human preferences is important for language models to match human needs and to align with human and social values. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sequences of sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences, with significant improvements on summarization and dialogue benchmarks and markedly preferred results in human evaluations.},
  author = {Hao Liu and Carmelo Sferrazza and Pieter Abbeel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024chain.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6xfe4IVcOu},
  publisher = {OpenReview.net},
  title = {Chain of Hindsight aligns Language Models with Feedback},
  url = {https://openreview.net/forum?id=6xfe4IVcOu},
  year = {2024}
}

@inproceedings{liu2024ringattention,
  abstract = {Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Blockwise RingAttention, which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. This approach enables efficient processing of long contextual information that would otherwise be prohibitively expensive with traditional transformer architectures.},
  author = {Hao Liu and Matei Zaharia and Pieter Abbeel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024ringattention.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WsRHpHH4s0},
  publisher = {OpenReview.net},
  title = {RingAttention with Blockwise Transformers for Near-Infinite Context},
  url = {https://openreview.net/forum?id=WsRHpHH4s0},
  year = {2024}
}

@inproceedings{liu2024one,
  abstract = {Designing a single model to address multiple tasks has been a long-standing objective in artificial intelligence. Recently, large language models have demonstrated exceptional capability in solving different tasks within the language domain. However, a unified model for various graph tasks remains underexplored, primarily due to the challenges unique to the graph learning domain. First, graph data from different areas carry distinct attributes and follow different distributions. Such discrepancy makes it hard to represent graphs in a single representation space. Second, tasks on graphs diversify into node, link, and graph tasks, requiring distinct embedding strategies. Finally, an appropriate graph prompting paradigm for in-context learning is unclear. To address these challenges, we propose One For All (OFA), the first general-purpose graph foundation model that can solve graph problems across diverse domains and tasks. Specifically, OFA proposes text-attributed graphs to unify different graph data by describing nodes and edges with natural language and uses language models to encode the diverse and possibly cross-domain text attributes to feature vectors in the same embedding space. Furthermore, OFA introduces the concept of nodes-of-interest to standardize different tasks with a single task representation. For in-context learning on graphs, OFA introduces a novel graph prompting paradigm that appends prompting substructures to the input graph, which enables it to address varied tasks without fine-tuning. We train the OFA model using graph data from multiple domains (including citation networks, molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs well across different tasks, making it the first general-purpose across-domains classification model on graphs.},
  address = {Vienna, Austria},
  author = {Hao Liu and Jiarui Feng and Lecheng Kong and Ningyue Liang and Dacheng Tao and Yixin Chen and Muhan Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024one.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=4IT2pgc9v6},
  publisher = {OpenReview.net},
  title = {One For All: Towards Training One Graph Model For All Classification Tasks},
  url = {https://openreview.net/forum?id=4IT2pgc9v6},
  year = {2024}
}

@inproceedings{zhang2024resimad,
  abstract = {Domain shifts such as sensor type changes and geographical situation variations are prevalent in Autonomous Driving (AD), which poses a challenge since AD model relying on the previous domain knowledge can be hardly directly deployed to a new domain without additional costs. In this paper, we provide a new perspective and approach of alleviating the domain shifts, by proposing a Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the implicit reconstruction process is based on the knowledge from the previous old domain, aiming to convert the domain-related knowledge into domain-invariant representations, e.g., 3D scene-level meshes. Besides, the point clouds simulation process of multiple new domains is conditioned on the above reconstructed 3D meshes, where the target-domain-like simulation samples can be obtained, thus reducing the cost of collecting and annotating new-domain data for the subsequent perception process. For experiments, we consider different cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes, etc, to verify the zero-shot target-domain perception using ReSimAD. Results demonstrate that our method is beneficial to boost the domain generalization ability, even promising for 3D pre-training.},
  address = {Vienna, Austria},
  author = {Bo Zhang and Xinyu Cai and Jiakang Yuan and Donglin Yang and Jianfei Guo and Xiangchao Yan and Renqiu Xia and Botian Shi and Min Dou and Tao Chen and Si Liu and Junchi Yan and Yu Qiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024resimad.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1d2cLKeNgY},
  publisher = {OpenReview.net},
  title = {ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation},
  url = {https://openreview.net/forum?id=1d2cLKeNgY},
  year = {2024}
}

@inproceedings{chen2024its,
  abstract = {Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the acoustic modality. Experimental evidence collected from various ASR tasks shows that UADF surpasses existing fusion mechanisms in several ways. It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in LLM and addressing the poor generalization relied with sole modality during fusion. We also demonstrate that UADF seamlessly adapts to audio-visual speech recognition.},
  address = {Vienna, Austria},
  author = {Chen Chen and Ruizhe Li and Yuchen Hu and Sabato Marco Siniscalchi and Pin-Yu Chen and Ensiong Chng and Chao-Han Huck Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024its.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QqjFHyQwtF},
  publisher = {OpenReview.net},
  title = {It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition},
  url = {https://openreview.net/forum?id=QqjFHyQwtF},
  year = {2024}
}

@inproceedings{wang2024magic,
  abstract = {Vanilla image completion approaches exhibit sensitivity to large missing regions, attributed to the limited availability of reference information for plausible generation. To mitigate this, existing methods incorporate the extra cue as a guidance for image completion. Despite improvements, these approaches are often restricted to employing a single modality (e.g., segmentation or sketch maps), which lacks scalability in leveraging multi-modality for more plausible completion. In this paper, we propose a novel, simple yet effective method for Multi-modal Guided Image Completion, dubbed {MaGIC}, which not only supports a wide range of single modality as the guidance (e.g., text, canny edge, sketch, segmentation, depth, and pose), but also adapts to arbitrarily customized combination of these modalities (i.e., arbitrary multi-modality) for image completion. For building {MaGIC}, we first introduce a modality-specific conditional {U-Net} ({MCU-Net}) that injects single-modal signal into a {U-Net} denoiser for single-modal guided image completion. Then, we devise a consistent modality blending ({CMB}) method to leverage modality signals encoded in multiple learned {MCU-Nets} through gradient guidance in latent space. Our {CMB} is training-free, thereby avoids the cumbersome joint re-training of different modalities, which is the secret of {MaGIC} to achieve exceptional flexibility in accommodating new modalities for completion. Experiments show the superiority of {MaGIC} over state-of-the-art methods and its generalization to various completion tasks.},
  address = {Vienna, Austria},
  author = {Hao Wang and Yongsheng Yu and Tiejian Luo and Heng Fan and Libo Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024magic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=o7x0XVlCpX},
  publisher = {OpenReview.net},
  title = {{MaGIC}: Multi-modality Guided Image Completion},
  url = {https://openreview.net/forum?id=o7x0XVlCpX},
  year = {2024}
}

@inproceedings{wang2024pflrm,
  abstract = {We propose a Pose-Free Large Reconstruction Model ({PF-LRM}) for reconstructing a {3D} object from a few unposed images even with little visual overlap, while simultaneously estimating the relative camera poses in ~1.3 seconds on a single {A100} {GPU}. {PF-LRM} is a highly scalable method utilizing self-attention blocks to exchange information between {3D} object tokens and {2D} image tokens; we predict a coarse point cloud for each view, and then use a differentiable Perspective-n-Point ({PnP}) solver to obtain camera poses. When trained on a huge amount of multi-view posed data of ~1M objects, {PF-LRM} shows strong cross-dataset generalization ability, and outperforms baseline methods by a large margin in terms of pose prediction accuracy and {3D} reconstruction quality on various unseen evaluation datasets. We also demonstrate our model's applicability in downstream text/image-to-{3D} task with fast feed-forward inference.},
  address = {Vienna, Austria},
  author = {Peng Wang and Hao Tan and Sai Bi and Yinghao Xu and Fujun Luan and Kalyan Sunkavalli and Wenping Wang and Zexiang Xu and Kai Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024pflrm.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=noe76eRcPC},
  publisher = {OpenReview.net},
  title = {{PF-LRM}: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction},
  url = {https://openreview.net/forum?id=noe76eRcPC},
  year = {2024}
}

@inproceedings{chen2024understanding,
  abstract = {Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. Through extensive experiments on synthetic noisy datasets, we discover that while slight noise in pre-training can benefit in-domain ({ID}) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain ({OOD}) performance, where training and testing data distribution are different. The reason is that noise in pre-training shapes the feature space differently, which affects model generalization capabilities. To mitigate the malignant effect of noise and improve generalization on both {ID} and {OOD} tasks, we propose a light-weight black-box tuning method ({NMTune}) to affine the feature space, considering one may not be able to fully fine-tune or even access the pre-trained models. Practical experiments on popular vision and language models that are pre-trained on noisy data show the effectiveness of our approach.},
  address = {Vienna, Austria},
  author = {Hao Chen and Jindong Wang and Ankit Shah and Ran Tao and Hongxin Wei and Xing Xie and Masashi Sugiyama and Bhiksha Raj},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024understanding.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=TjhUtloBZU},
  publisher = {OpenReview.net},
  title = {Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks},
  url = {https://openreview.net/forum?id=TjhUtloBZU},
  year = {2024}
}

@inproceedings{li2024improved,
  abstract = {Gradient clipping is a key mechanism that is essential to differentially private training techniques in federated learning. Two popular strategies are per-sample clipping, which clips the mini-batch gradient, and per-update clipping, which clips each user's model update. However, there has not been a thorough theoretical analysis of these two clipping methods. In this work, we rigorously analyze the impact of these two clipping techniques on the convergence of a popular federated learning algorithm {FedAvg} under standard stochastic noise and gradient dissimilarity assumptions. Our analysis provides convergence guarantees for both clipping methods given any arbitrary clipping threshold. Specifically, we show that per-sample clipping is guaranteed to converge to the neighborhood of the stationary point, with the size dependent on the stochastic noise, gradient dissimilarity, and clipping threshold. For per-update clipping, we show that convergence to the stationary point can be guaranteed with a sufficiently small stepsize, at the cost of more communication rounds. Our theoretical results provide new insights into understanding the effect of the two clipping methods on federated learning and their implications for differential privacy.},
  address = {Vienna, Austria},
  author = {Bo Li and Xiaowen Jiang and Mikkel N. Schmidt and Tommy Sonne Alstr{\o}m and Sebastian U. Stich},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024improved.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=BdPvGRvoBC},
  publisher = {OpenReview.net},
  title = {An improved analysis of per-sample and per-update clipping in federated learning},
  url = {https://openreview.net/forum?id=BdPvGRvoBC},
  year = {2024}
}

@inproceedings{liu2024what,
  abstract = {Instruction tuning is a standard technique employed to align large language models to end tasks and user preferences after the initial pretraining phase. Recent research indicates the critical role of data engineering in instruction tuning -- when appropriately selected, only limited data is necessary to achieve superior performance. However, we still lack a principled understanding of what makes good instruction tuning data for alignment, and how we should select data automatically and effectively. In this work, we delve deeply into automatic data selection strategies for alignment. We start with controlled studies to measure data across three dimensions: complexity, quality, and diversity, along which we examine existing methods and introduce novel techniques for enhanced data measurement. Subsequently, we propose a simple strategy to select data samples based on the measurement. We present {Deita} (short for Data-Efficient Instruction Tuning for Alignment), a series of models fine-tuned from {LLaMA} models using data samples automatically selected with our proposed approach. When assessed through both automatic metrics and human evaluation, {Deita} performs better or on par with the state-of-the-art open-source alignment models such as {Vicuna} and {WizardLM} with only {6K} training data samples -- 10x less than the data used in the baselines. We anticipate this work to provide clear guidelines and tools on automatic data selection, aiding researchers and practitioners in achieving data-efficient alignment.},
  address = {Vienna, Austria},
  author = {Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024what.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=BTKAeLqLMw},
  publisher = {OpenReview.net},
  title = {What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
  url = {https://openreview.net/forum?id=BTKAeLqLMw},
  year = {2024}
}

@inproceedings{wang2024topological,
  abstract = {Graph Neural Networks ({GNNs}) have shown great promise in learning node embeddings for link prediction ({LP}). While numerous studies improve the overall {GNNs}' {LP} performance, none have explored their varying performance across different nodes and the underlying reasons. To this end, we demystify which nodes perform better from the perspective of their local topology. Despite the widespread belief that low-degree nodes exhibit worse {LP} performance, we surprisingly observe an inconsistent performance trend. This prompts us to propose a node-level metric, Topological Concentration ({TC}), based on the intersection of the local subgraph of each node with the ones of its neighbors. We empirically demonstrate that {TC} correlates with {LP} performance more than other node-level topological metrics, better identifying low-performing nodes than using degree. With {TC}, we discover a novel topological distribution shift issue in which nodes' newly joined neighbors tend to become less interactive with their existing neighbors, compromising the generalizability of node embeddings for {LP} at testing time. To make the computation of {TC} scalable, we further propose Approximated Topological Concentration ({ATC}) and justify its efficacy in approximating {TC} with reduced computation complexity. Given the positive correlation between node {TC} and its {LP} performance, we explore the potential of boosting {LP} performance via enhancing {TC} by re-weighting edges in the message-passing and discuss its effectiveness with limitations.},
  address = {Vienna, Austria},
  author = {Yu Wang and Tong Zhao and Yuying Zhao and Yunchao Liu and Xueqi Cheng and Neil Shah and Tyler Derr},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024topological.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-06-11},
  pdf = {https://openreview.net/pdf?id=apA6SSXx2e},
  publisher = {OpenReview.net},
  title = {A Topological Perspective on Demystifying {GNN}-Based Link Prediction Performance},
  url = {https://openreview.net/forum?id=apA6SSXx2e},
  year = {2024}
}

@inproceedings{eli2024efficient,
  abstract = {Catastrophic overfitting ({CO}) in single-step adversarial training ({AT}) results in abrupt drops in the adversarial test accuracy (even down to 0\%). For models trained with multi-step {AT}, it has been observed that the loss function behaves locally linearly with respect to the input, this is however lost in single-step {AT}. To address {CO} in single-step {AT}, several methods have been proposed to enforce local linearity of the loss via regularization. However, these regularization terms considerably slow down training due to Double Backpropagation. Instead, in this work, we introduce a regularization term, called {ELLE}, to mitigate {CO} effectively and efficiently in classical {AT} evaluations, as well as some more difficult regimes, e.g., large adversarial perturbations and long training schedules. Our regularization term can be theoretically linked to curvature of the loss function and is computationally cheaper than previous methods by avoiding Double Backpropagation. Our thorough experimental validation demonstrates that our work does not suffer from {CO}, even in challenging settings where previous works suffer from it. We also notice that adapting our regularization parameter during training ({ELLE-A}) greatly improves the performance, specially in large Œµ setups.},
  address = {Vienna, Austria},
  author = {El{\'i}as Abad-Rocamora and Fanghui Liu and Grigorios Chrysos and Pablo M. Olmos and Volkan Cevher},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/eli2024efficient.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=SZzQz8ikwg},
  publisher = {OpenReview.net},
  title = {Efficient local linearity regularization to overcome catastrophic overfitting},
  url = {https://openreview.net/forum?id=SZzQz8ikwg},
  year = {2024}
}

@inproceedings{abbahaddou2024bounding,
  abstract = {Graph Neural Networks ({GNN}s) have demonstrated state-of-the-art performance in graph representation learning tasks. This paper theoretically defines ``expected robustness'' in attributed graphs and relates it to adversarial robustness. The authors derive an upper bound of robustness for Graph Convolutional Networks and Graph Isomorphism Networks under node feature attacks. The paper proposes Graph Convolutional Orthonormal Robust Networks ({GCORN}) and introduces a probabilistic method to estimate expected robustness. {GCORN} outperforms existing defense methods against node feature attacks.},
  author = {Yassine Abbahaddou and Sofiane Ennadir and Johannes F. Lutzeyer and Michalis Vazirgiannis and Henrik Bostr{\"o}m},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/abbahaddou2024bounding.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DfPtC8uSot},
  publisher = {OpenReview.net},
  title = {{Bounding the Expected Robustness of Graph Neural Networks Subject to Node Feature Attacks}},
  url = {https://openreview.net/forum?id=DfPtC8uSot},
  year = {2024}
}

@inproceedings{abbas2024effective,
  abstract = {Utilizing massive web-scale datasets has led to unprecedented performance gains in machine learning models, but also imposes outlandish compute requirements for their training. In order to improve training and data efficiency, we here push the limits of pruning large-scale multimodal datasets for training {CLIP}-style models. We propose a pruning method where we aim to obtain optimal dataset coverage by assessing sample complexity based on concept clusters. Our approach demonstrates significant improvements in training efficiency while maintaining model performance.},
  author = {Amro Kamal Mohamed Abbas and Evgenia Rusak and Kushal Tirumala and Wieland Brendel and Kamalika Chaudhuri and Ari S. Morcos},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/abbas2024effective.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CtOA9aN8fr},
  publisher = {OpenReview.net},
  title = {{Effective pruning of web-scale datasets based on complexity of concept clusters}},
  url = {https://openreview.net/forum?id=CtOA9aN8fr},
  year = {2024}
}

@inproceedings{abbaspourazad2024largescale,
  abstract = {This paper focuses on tracking biosignals using wearable devices through self-supervised learning on unlabeled sensor data from the Apple Heart and Movement Study ({AHMS}). The authors trained foundation models for photoplethysmography ({PPG}) and electrocardiogram ({ECG}) using data from approximately 141K participants collected over 3 years. The approach aims to enhance future wearable devices by reducing the reliance on labeled data and addresses the challenge of limited medical datasets through large-scale foundation model training.},
  author = {Salar Abbaspourazad and Oussama Elachqar and Andrew C. Miller and Saba Emrani and Udhyakumar Nallasamy and Ian Shapiro},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/abbaspourazad2024largescale.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pC3WJHf51j},
  publisher = {OpenReview.net},
  title = {{Large-scale Training of Foundation Models for Wearable Biosignals}},
  url = {https://openreview.net/forum?id=pC3WJHf51j},
  year = {2024}
}

@inproceedings{abdin2024kitab,
  abstract = {This paper studies {LLM}s' ability to answer constraint satisfaction queries for information retrieval. The authors introduce {KITAB}, a new dataset with book-related data across 600+ authors and 13,000 queries. Experiments on {GPT}-4 and {GPT}-3.5 reveal limitations in constraint satisfaction, showing that state-of-the-art {LLM}s struggle with answering constraint satisfaction queries for finding information.},
  author = {Marah I. Abdin and Suriya Gunasekar and Varun Chandrasekaran and Jerry Li and Mert Yuksekgonul and Rahee Ghosh Peshawaria and Ranjita Naik and Besmira Nushi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/abdin2024kitab.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=b3kDP3IytM},
  publisher = {OpenReview.net},
  title = {{{KITAB}: Evaluating {LLM}s on Constraint Satisfaction for Information Retrieval}},
  url = {https://openreview.net/forum?id=b3kDP3IytM},
  year = {2024}
}

@inproceedings{abdulaal2024causal,
  abstract = {Scientific discovery hinges on the effective integration of metadata and data. This paper introduces the Causal Modelling Agent ({CMA}), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models ({LLM}s) with the data-driven modelling of Deep Structural Causal Models ({DSCM}s) for the task of causal discovery. The approach combines domain knowledge encoded through {LLM}s with empirical data analysis to improve causal graph discovery.},
  author = {Ahmed Abdulaal and Adamos Hadjivasiliou and Nina Montana-Brown and Tiantian He and Ayodeji Ijishakin and Ivana Drobnjak and Daniel C. Castro and Daniel C. Alexander},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/abdulaal2024causal.pdf:pdf},
  note = {DBLP last modified: 2024-10-07},
  pdf = {https://openreview.net/pdf?id=pAoqRlTBtY},
  publisher = {OpenReview.net},
  title = {{Causal Modelling Agents: Causal Graph Discovery through Synergising Metadata- and Data-driven Reasoning}},
  url = {https://openreview.net/forum?id=pAoqRlTBtY},
  year = {2024}
}

@inproceedings{acharya2024oracle,
  abstract = {We study online prediction where individuals arrive sequentially and need label predictions, with each individual associated with various intersecting groups (age, sex, race, etc.). The goal is to make predictions with regret guarantees both overall and simultaneously for each group's sub-sequence. While previous work provided attractive regret guarantees but were computationally intractable on large model classes, this paper shows that a simple modification of the sleeping experts technique yields an efficient reduction to obtaining diminishing external regret without group considerations.},
  author = {Krishna Acharya and Eshwar Ram Arunachaleswaran and Sampath Kannan and Aaron Roth and Juba Ziani},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/acharya2024oracle.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HrRKc9ei7h},
  publisher = {OpenReview.net},
  title = {{Oracle Efficient Algorithms for Groupwise Regret}},
  url = {https://openreview.net/forum?id=HrRKc9ei7h},
  year = {2024}
}

@inproceedings{adams2024point2ssm,
  abstract = {We present Point2SSM, a novel unsupervised learning approach for constructing correspondence-based statistical shape models ({SSM}s) directly from raw point clouds. {SSM} is crucial in clinical research, enabling population-level analysis of morphological variation in bones and organs. Traditional methods of {SSM} construction have limitations, including the requirement of noise-free surface meshes or binary volumes, reliance on assumptions or templates, and prolonged inference times. Point2SSM overcomes these barriers by providing a data-driven solution that infers {SSM}s directly from raw point clouds, reducing inference burdens and increasing applicability as point clouds are more easily acquired.},
  author = {Jadie Adams and Shireen Y. Elhabian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/adams2024point2ssm.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=DqziS8DG4M},
  publisher = {OpenReview.net},
  title = {{Point2SSM}: Learning Morphological Variations of Anatomies from Point Clouds},
  url = {https://openreview.net/forum?id=DqziS8DG4M},
  year = {2024}
}

@inproceedings{adila2024zeroshot,
  abstract = {Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use language models to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings---without any supervision. Empirically, we evaluate RoboShot on nine image and {NLP} classification tasks and show an average improvement of 15.98\% over several zero-shot baselines.},
  author = {Dyah Adila and Changho Shin and Linrong Cai and Frederic Sala},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/adila2024zeroshot.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fCeUoDr9Tq},
  publisher = {OpenReview.net},
  title = {{Zero-Shot Robustification of Zero-Shot Models}},
  url = {https://openreview.net/forum?id=fCeUoDr9Tq},
  year = {2024}
}

@inproceedings{adilova2024layerwise,
  abstract = {Averaging neural network parameters is an intuitive method for fusing the knowledge of two independent models. It is most prominently used in federated learning. If models are averaged at the end of training, this can only lead to a good performing model if the loss surface of interest is very particular, i.e., the loss in the midpoint between the two models needs to be sufficiently low. This is impossible to guarantee for the non-convex losses of state-of-the-art networks. For averaging models trained on vastly different datasets, it was proposed to average only the parameters of particular layers or combinations of layers, resulting in better performing models. To get a better understanding of the effect of layer-wise averaging, we analyse the performance of the models that result from averaging single layers, or groups of layers. Based on our empirical and theoretical investigation, we introduce a novel notion of the layer-wise linear connectivity, and show that deep networks do not have layer-wise barriers between them.},
  author = {Linara Adilova and Maksym Andriushchenko and Michael Kamp and Asja Fischer and Martin Jaggi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/adilova2024layerwise.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=LfmZh91tDI},
  publisher = {OpenReview.net},
  title = {{Layer-wise linear mode connectivity}},
  url = {https://openreview.net/forum?id=LfmZh91tDI},
  year = {2024}
}

@inproceedings{adiya2024bidirectional,
  abstract = {We introduce a method to generate temporally coherent human animation from a single image, a video, or a random noise. This problem has been formulated as modeling of an auto-regressive generation, i.e., to regress past frames to decode future frames. However, such unidirectional generation is highly prone to motion drifting over time, generating unrealistic human animation with significant artifacts such as appearance distortion. We claim that bidirectional temporal modeling enforces temporal coherence on a generative network by largely suppressing the appearance ambiguity. To prove our claim, we design a novel human animation framework using a denoising diffusion model: a neural network learns to generate the image of a person by denoising temporal Gaussian noises whose intermediate results are cross-conditioned bidirectionally between consecutive frames. In the experiments, our method demonstrates strong performance compared to existing unidirectional approaches with realistic temporal coherence.},
  author = {Tserendorj Adiya and Jae Shin Yoon and Jungeun Lee and Sanghun Kim and Hwasup Lim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/adiya2024bidirectional.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=yQDFsuG9HP},
  publisher = {OpenReview.net},
  title = {{Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation}},
  url = {https://openreview.net/forum?id=yQDFsuG9HP},
  year = {2024}
}

@inproceedings{agafonov2024advancing,
  abstract = {We present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, typical in machine learning. We establish theoretical lower bounds and prove that our algorithm achieves optimal convergence in both gradient and Hessian inexactness in this key setting. We further introduce a tensor generalization for stochastic higher-order derivatives. When the oracles are non-stochastic, the proposed tensor algorithm matches the global convergence of Nesterov Accelerated Tensor method. Both algorithms allow for approximate solutions of their auxiliary subproblems with verifiable conditions on the accuracy of the solution.},
  author = {Artem Agafonov and Dmitry Kamzolov and Alexander V. Gasnikov and Ali Kavis and Kimon Antonakopoulos and Volkan Cevher and Martin Tak√°ƒç},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/agafonov2024advancing.pdf:pdf},
  keywords = {Second-order methods, convex optimization, stochastic optimization, Cubic Newton Method, high-order methods, tensor methods, lower bounds},
  month = {5},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=otU31x3fus},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness},
  url = {https://openreview.net/forum?id=otU31x3fus},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{agarwal2024onpolicy,
  abstract = {Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks, and task-agnostic distillation for instruction-tuning.},
  author = {Rishabh Agarwal and Nino Vieillard and Yongchao Zhou and Piotr Stanczyk and Sabela Ramos Garea and Matthieu Geist and Olivier Bachem},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/agarwal2024onpolicy.pdf:pdf},
  keywords = {knowledge distillation, language models, self-generated mistakes, distribution mismatch, reinforcement learning},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3zKtaqxLhW},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes},
  url = {https://openreview.net/forum?id=3zKtaqxLhW},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{aghajohari2024loqa,
  abstract = {In various real-world scenarios, interactions among agents often resemble the dynamics of general-sum games, where each agent strives to optimize its own utility. Despite the ubiquitous relevance of such settings, decentralized machine learning algorithms have struggled to find equilibria that maximize individual utility while preserving social welfare. In this paper we introduce Learning with Opponent Q-Learning Awareness (LOQA), a novel reinforcement learning algorithm tailored to optimizing an agent's individual utility while fostering cooperation among adversaries in partially competitive environments. LOQA assumes that each agent samples actions proportionally to their action-value function Q. Experimental results demonstrate the effectiveness of LOQA at achieving state-of-the-art performance in benchmark scenarios such as the Iterated Prisoner's Dilemma and the Coin Game. LOQA achieves these outcomes with a significantly reduced computational footprint compared to previous works, making it a promising approach for practical multi-agent applications.},
  author = {Milad Aghajohari and Juan Agustin Duque and Tim Cooijmans and Aaron C. Courville},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/aghajohari2024loqa.pdf:pdf},
  keywords = {multi-agent reinforcement learning, Q-learning, game theory, opponent awareness, cooperation},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FDQF6A1s6M},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {LOQA: Learning with Opponent Q-Learning Awareness},
  url = {https://openreview.net/forum?id=FDQF6A1s6M},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{ahn2024linear,
  abstract = {Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized *shallow* Transformer model.},
  author = {Kwangjun Ahn and Xiang Cheng and Minhak Song and Chulhee Yun and Ali Jadbabaie and Suvrit Sra},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ahn2024linear.pdf:pdf},
  keywords = {Transformer, optimization, Adam, clipping, heavy-tailed noise, directional smoothness},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0uI5415ry7},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Linear attention is (maybe) all you need (to understand Transformer optimization)},
  url = {https://openreview.net/forum?id=0uI5415ry7},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{ahn2024linear,
  abstract = {Estimating individual treatment effects in clinical data is essential for understanding how different patients uniquely respond to treatments and identifying the most effective interventions for specific patient subgroups, thereby enhancing the precision and personalization of healthcare. However, counterfactual data are not accessible, and the true calculation of causal effects cannot be performed at the individual level. This paper proposes a linear algebraic framework to generate counterfactual longitudinal data that exactly matches pre-treatment factual data. Using simulated LDL cholesterol datasets, the authors show that their method significantly outperforms the most cited methods of counterfactual generation. They also provide a formula that can estimate the time-varying variance of individual treatment effects, interpreted as a confidence level in the generated counterfactuals compared to true values.},
  author = {Jong-Hoon Ahn and Akshay Vashist},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ahn2024linear.pdf:pdf},
  keywords = {counterfactual generation, individual treatment effect, synthetic data, Gaussian mixture model, causal inference},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PoDkdFQIu3},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {A Linear Algebraic Framework for Counterfactual Generation},
  url = {https://openreview.net/forum?id=PoDkdFQIu3},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{aiello2024jointly,
  abstract = {In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge.},
  author = {Emanuele Aiello and Lili Yu and Yixin Nie and Armen Aghajanyan and Barlas Oguz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/aiello2024jointly.pdf:pdf},
  keywords = {Large Multimodal Models, Joint Training, Interleaved Image-Text Generation, Autoregressive Models},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5jcav5RcKw},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Jointly Training Large Autoregressive Multimodal Models},
  url = {https://openreview.net/forum?id=5jcav5RcKw},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{akhalwaya2024topological,
  abstract = {Topological data analysis (TDA) is a powerful technique for extracting complex and valuable shape-related summaries of high-dimensional data. However, the computational demands of classical algorithms for computing TDA are exorbitant, and quickly become impractical for high-order characteristics.},
  author = {Ismail Yunus Akhalwaya and Shashanka Ubaru and Kenneth L. Clarkson and Mark S. Squillante and Vishnu Jejjala and Yang-Hui He and Kugendran Naidoo and Vasileios Kalantzis and Lior Horesh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/akhalwaya2024topological.pdf:pdf},
  keywords = {Topological data analysis, quantum computing, unsupervised learning, feature extraction},
  month = {5},
  note = {Oral Presentation; DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=dLrhRIMVmB},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Topological data analysis on noisy quantum computers},
  url = {https://openreview.net/forum?id=dLrhRIMVmB},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{akinwande2024understanding,
  abstract = {Zero-shot learning in prompted vision-language models has achieved impressive performance. These methods suffer relatively little from overfitting... We show that we can explain such performance via classical PAC-Bayes bounds.},
  author = {Victor Akinwande and Yiding Jiang and Dylan Sam and J. Zico Kolter},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/akinwande2024understanding.pdf:pdf},
  keywords = {generalization, prompt engineering, PAC-Bayes, foundation models},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=a745RnSFLT},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Understanding prompt engineering may not require rethinking generalization},
  url = {https://openreview.net/forum?id=a745RnSFLT},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{akkerman2024dynamic,
  abstract = {Large discrete action spaces remain a central challenge in reinforcement learning. Existing approaches can handle unstructured spaces with up to a few million actions. However, real-world applications in logistics, production, and transportation have combinatorial action spaces that grow beyond millions of actions.},
  author = {Fabian Akkerman and Julius Luy and Wouter van Heeswijk and Maximilian Schiffer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/akkerman2024dynamic.pdf:pdf},
  keywords = {Structured large discrete action space, Reinforcement learning, Neighborhood search},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=80wh3jjCZf},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces},
  url = {https://openreview.net/forum?id=80wh3jjCZf},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{alhafez2024timeefficient,
  abstract = {Stateful policies play an important role in reinforcement learning, such as handling partially observable environments, enhancing robustness, or imposing an inductive bias directly into the policy structure. The conventional method for training stateful policies is Backpropagation Through Time (BPTT), which comes with significant drawbacks, such as slow training due to sequential gradient propagation and the occurrence of vanishing or exploding gradients.},
  author = {Firas Al-Hafez and Guoping Zhao and Jan Peters and Davide Tateo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/alhafez2024timeefficient.pdf:pdf},
  keywords = {Reinforcement learning, stateful policies, policy gradient, time-efficient training},
  month = {5},
  note = {DBLP last modified: 2024-08-07; arXiv:2311.04082},
  pdf = {https://openreview.net/pdf?id=5liV2xUdJL},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Time-Efficient Reinforcement Learning with Stochastic Stateful Policies},
  url = {https://openreview.net/forum?id=5liV2xUdJL},
  venue = {Vienna, Austria},
  year = {2024}
}

@inproceedings{alabdulmohsin2024clip,
  abstract = {We study data-balancing for mitigating biases in contrastive language-image pretraining ({CLIP}), identifying areas of strength and limitation. First, we reaffirm prior conclusions that {CLIP} can inadvertently absorb stereotypes. To counter this, we present a novel algorithm, called Multi-Modal Moment Matching ({M4}), designed to reduce both representation and association biases in multimodal data. We use {M4} to conduct an in-depth analysis taking into account various factors, such as the model, representation, and data size. Our study also explores the dynamic nature of how {CLIP} learns/unlearns biases. In particular, we find that fine-tuning is effective in countering representation biases, though its impact diminishes for association biases. Also, data balancing has a mixed impact on quality: it tends to improve classification but can hurt retrieval. Interestingly, data and architectural improvements seem to mitigate the negative impact of data balancing on performance; e.g. applying {M4} to {SigLIP}-{B}/16 with data quality filters improves {COCO} image-to-text retrieval @5 from 86\% (without data balancing) to 87\% and {ImageNet} 0-shot classification from 77\% to 77.5\%! Finally, we conclude with recommendations for improving the efficacy of data balancing in multimodal systems.},
  author = {Ibrahim Alabdulmohsin and Xiao Wang and Andreas Peter Steiner and Priya Goyal and Alexander D'Amour and Xiaohua Zhai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/alabdulmohsin2024clip.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FIGXAxr9E4},
  publisher = {OpenReview.net},
  title = {{CLIP} the Bias: How Useful is Balancing Data in Multimodal Learning?},
  url = {https://openreview.net/forum?id=FIGXAxr9E4},
  year = {2024}
}

@inproceedings{albergo2024multimarginal,
  abstract = {Given a set of {K} probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption. In addition, the multimarginal perspective enables an efficient algorithm for optimizing the dynamical transport cost in the ordinary two-marginal setting. We demonstrate these capacities with several numerical examples.},
  author = {Michael Samuel Albergo and Nicholas Matthew Boffi and Michael Lindsey and Eric Vanden-Eijnden},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/albergo2024multimarginal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FHqAzWl2wE},
  publisher = {OpenReview.net},
  title = {Multimarginal Generative Modeling with Stochastic Interpolants},
  url = {https://openreview.net/forum?id=FHqAzWl2wE},
  year = {2024}
}

@inproceedings{alemohammad2024selfconsuming,
  abstract = {Seismic advances in generative {AI} algorithms for imagery, text, and other data types have led to the temptation to use {AI}-synthesized data to train next-generation models. Repeating this process creates an autophagous ('self-consuming') loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and whether the samples from previous-generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that \emph{without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease.} We term this condition Model Autophagy Disorder ({MAD}), by analogy to mad cow disease, and show that appreciable {MAD}ness arises in just a few generations.},
  author = {Sina Alemohammad and Josue Casco-Rodriguez and Lorenzo Luzi and Ahmed Imtiaz Humayun and Hossein Babaei and Daniel LeJeune and Ali Siahkoohi and Richard G. Baraniuk},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/alemohammad2024selfconsuming.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ShjMHfmPs0},
  publisher = {OpenReview.net},
  title = {Self-Consuming Generative Models Go {MAD}},
  url = {https://openreview.net/forum?id=ShjMHfmPs0},
  year = {2024}
}

@inproceedings{alikhasi2024unveiling,
  abstract = {In reinforcement learning, agents often learn policies for specific tasks without the ability to generalize this knowledge to related tasks. This paper introduces an algorithm that attempts to address this limitation by decomposing neural networks encoding policies for {M}arkov {D}ecision {P}rocesses into reusable sub-policies, which are used to synthesize temporally extended actions, or options. We consider neural networks with piecewise linear activation functions, so that they can be mapped to an equivalent tree that is similar to oblique decision trees. Since each node in such a tree serves as a function of the input of the tree, each sub-tree is a sub-policy of the main policy. We turn each of these sub-policies into options by wrapping it with while-loops of varied number of iterations. Given the large number of options, we propose a selection mechanism based on minimizing the {L}evin loss for a uniform policy on these options. Empirical results in two grid-world domains where exploration can be difficult confirm that our method can identify useful options, thereby accelerating the learning process on similar but different tasks.},
  author = {Mahdi Alikhasi and Levi Lelis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/alikhasi2024unveiling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=a8VETFwcVR},
  publisher = {OpenReview.net},
  title = {Unveiling Options with Neural Network Decomposition},
  url = {https://openreview.net/forum?id=a8VETFwcVR},
  year = {2024}
}

@inproceedings{alleman2024task,
  abstract = {The utility of a learned neural representation depends on how well its geometry supports performance in downstream tasks. This geometry depends on the structure of the inputs, the structure of the target outputs, and on the architecture of the network. By studying the learning dynamics of networks with one hidden layer, we discovered that the network's activation function has an unexpectedly strong impact on the representational geometry: {T}anh networks tend to learn representations that reflect the structure of the target outputs, while {ReLU} networks retain more information about the structure of the raw inputs. This difference is consistently observed across a broad class of parameterized tasks in which we modulated the degree of alignment between the geometry of the task inputs and that of the task labels. We analyzed the learning dynamics in weight space and show how the differences between the networks with {T}anh and {ReLU} nonlinearities arise from the asymmetric saturation of {ReLU}, which leads feature neurons to specialize for different regions of input space. Feature neurons in {T}anh networks, by contrast, tend to inherit the task label structure. Consequently, when the target outputs are low dimensional, {T}anh networks generate neural representations that are more disentangled than those obtained with a {ReLU} nonlinearity. Our findings shed light on the interplay between input-output geometry, nonlinearity, and learned representations in neural networks.},
  author = {Matteo Alleman and Jack Lindsey and Stefano Fusi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/alleman2024task.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=k9t8dQ30kU},
  publisher = {OpenReview.net},
  title = {Task structure and nonlinearity jointly determine learned representational geometry},
  url = {https://openreview.net/forum?id=k9t8dQ30kU},
  year = {2024}
}

@inproceedings{alman2024how,
  abstract = {In the classical transformer attention scheme, we are given three $n \times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \times d$ size matrix $D^{-1} \exp(QK^\top) V$ where $D = \mathrm{diag}( \exp(QK^\top) {\bf 1}_n )$. Here, $\exp()$ is applied entry-wise and ${\bf 1}_n$ denotes a length-$n$ vector whose entries are all ones. Intuitively, attention computation captures pairwise information between words in a sentence, but not higher-order information. Indeed, recent work has shown that attention units cannot solve simple problems about detecting triples of connected words. In this work, we study a generalization of attention which captures triple-wise correlations. The generalization is based on computations involving tensors defined by tuples of words.},
  author = {Josh Alman and Zhao Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/alman2024how.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=v0zNCwwkaV},
  publisher = {OpenReview.net},
  title = {How to Capture Higher-order Correlations? {G}eneralizing Matrix Softmax Attention to {K}ronecker Computation},
  url = {https://openreview.net/forum?id=v0zNCwwkaV},
  year = {2024}
}

@inproceedings{almasi2024flag,
  abstract = {Modern {ML} applications increasingly rely on complex deep learning models and large datasets. There has been an exponential growth in the amount of computation needed to train the largest models. Therefore, to scale computation and data, these models are inevitably trained in a distributed manner in clusters of nodes, and their updates are aggregated before being applied to the model. However, a distributed setup is prone to {B}yzantine failures of individual nodes, components, and software. With data augmentation added to these settings, there is a critical need for robust and efficient aggregation systems. We define the quality of workers as reconstruction ratios $\in (0,1]$, and formulate aggregation as a {M}aximum {L}ikelihood {E}stimation procedure using {B}eta densities. We show that the {R}egularized form of log-likelihood wrt subspace can be approximately solved using iterative least squares solver, and provide convergence guarantees using recent {C}onvex {O}ptimization landscape results. Our empirical findings demonstrate that our approach significantly enhances the robustness of state-of-the-art {B}yzantine resilient aggregators. We evaluate our method in a distributed setup with a parameter server, and show simultaneous improvements in communication efficiency and accuracy across various tasks.},
  author = {Hamidreza Almasi and Harsh Mishra and Balajee Vamanan and Sathya N. Ravi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/almasi2024flag.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=7avlrpzWqo},
  publisher = {OpenReview.net},
  title = {Flag Aggregator: Scalable Distributed Training under Failures and Augmented Losses using Convex Optimization},
  url = {https://openreview.net/forum?id=7avlrpzWqo},
  year = {2024}
}

@inproceedings{alon2024optimal,
  abstract = {Contrastive learning is a highly successful technique for learning representations of data from labeled tuples, specifying the distance relations within the tuple. We study the sample complexity of contrastive learning, i.e. the minimum number of labeled tuples sufficient for getting high generalization accuracy. We give tight bounds on the sample complexity in a variety of settings, focusing on arbitrary distance functions, $\ell_p$-distances, and tree metrics. Our main result is an (almost) optimal bound on the sample complexity of learning $\ell_p$-distances for integer $p$. For any $p \geq 1$, we show that $\tilde{\Theta}(nd)$ labeled tuples are necessary and sufficient for learning $d$-dimensional representations of $n$-point datasets. Our results hold for an arbitrary distribution of the input samples and are based on giving the corresponding bounds on the {V}apnik-{C}hervonenkis/{N}atarajan dimension of the associated problems. We further show that the theoretical bounds on sample complexity obtained via {VC}/{N}atarajan dimension can have strong predictive power for experimental results, in contrast with the folklore belief about a substantial gap between the statistical learning theory and the practice of deep learning.},
  author = {Noga Alon and Dmitrii Avdiukhin and Dor Elboim and Orr Fischer and Grigory Yaroslavtsev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/alon2024optimal.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=NU9AYHJvYe},
  publisher = {OpenReview.net},
  title = {Optimal Sample Complexity of Contrastive Learning},
  url = {https://openreview.net/forum?id=NU9AYHJvYe},
  year = {2024}
}

@inproceedings{altabaa2024abstractors,
  abstract = {An extension of {T}ransformers is proposed that enables explicit relational reasoning through a novel module called the \emph{{A}bstractor}. At the core of the {A}bstractor is a variant of attention called \emph{relational cross-attention}. The approach is motivated by an architectural inductive bias for relational learning that disentangles relational information from object-level features. This enables explicit relational reasoning, supporting abstraction and generalization from limited data. The {A}bstractor is first evaluated on simple discriminative relational tasks and compared to existing relational architectures. Next, the {A}bstractor is evaluated on purely relational sequence-to-sequence tasks, where dramatic improvements are seen in sample efficiency compared to standard {T}ransformers. Finally, {A}bstractors are evaluated on a collection of tasks based on mathematical problem solving, where consistent improvements in performance and sample efficiency are observed.},
  author = {Awni Altabaa and Taylor Whittington Webb and Jonathan D. Cohen 0003 and John Lafferty},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/altabaa2024abstractors.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=XNa6r6ZjoB},
  publisher = {OpenReview.net},
  title = {Abstractors and relational cross-attention: An inductive bias for explicit relational reasoning in Transformers},
  url = {https://openreview.net/forum?id=XNa6r6ZjoB},
  year = {2024}
}

@inproceedings{ammar2024neco,
  abstract = {Detecting out-of-distribution ({OOD}) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that 'neural collapse', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences {OOD} data. To benefit from this interplay, we introduce {NECO}, a novel post-hoc method for {OOD} detection, which leverages the geometric properties of 'neural collapse' and of principal component spaces to identify {OOD} data. Our extensive experiments demonstrate that {NECO} achieves state-of-the-art results on both small and large-scale {OOD} detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in {OOD} detection. We plan to release the code after the anonymity period.},
  author = {Mou\"in Ben Ammar and Nacim Belkhir and Sebastian Popescu and Antoine Manzanera and Gianni Franchi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ammar2024neco.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9ROuKblmi7},
  publisher = {OpenReview.net},
  title = {NECO: NEural Collapse Based Out-of-distribution detection},
  url = {https://openreview.net/forum?id=9ROuKblmi7},
  year = {2024}
}

@inproceedings{amortila2024harnessing,
  abstract = {The theories of offline and online reinforcement learning, despite having evolved in parallel, have begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one setting often having natural counterparts in the other. However, the notion of \emph{density ratio modeling}, an emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the very existence and boundedness of density ratios relies on access to an exploratory dataset with good coverage, but the core challenge in online RL is to collect such a dataset without having one to start. In this work we show---perhaps surprisingly---that density ratio-based algorithms have online counterparts. Assuming only the existence of an exploratory distribution with good coverage, a structural condition known as \emph{coverability} (Xie et al., 2023), we give a new algorithm (GLOW) that uses density ratio realizability and value function realizability to perform sample-efficient online exploration. GLOW addresses unbounded density ratios via careful use of truncation, and combines this with optimism to guide exploration. GLOW is computationally inefficient; we complement it with a more efficient counterpart, HyGLOW, for the Hybrid RL setting (Song et al., 2023) wherein online RL is augmented with additional offline data. HyGLOW is derived as a special case of a more general meta-algorithm that provides a provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.},
  author = {Philip Amortila and Dylan J. Foster and Nan Jiang and Ayush Sekhari and Tengyang Xie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/amortila2024harnessing.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=THJEa8adBn},
  publisher = {OpenReview.net},
  title = {Harnessing Density Ratios for Online Reinforcement Learning},
  url = {https://openreview.net/forum?id=THJEa8adBn},
  year = {2024}
}

@inproceedings{amos2024never,
  abstract = {Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, *using only the downstream task data*, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.},
  author = {Ido Amos and Jonathan Berant and Ankit Gupta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/amos2024never.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=PdaPky8MUn},
  publisher = {OpenReview.net},
  title = {Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors},
  url = {https://openreview.net/forum?id=PdaPky8MUn},
  year = {2024}
}

@inproceedings{an2024diffusionnag,
  abstract = {Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS. DiffusionNAG achieves superior performance with speedups of up to 35 times when compared to the baselines on Transferable NAS benchmarks. Furthermore, when integrated into a BO-based algorithm, DiffusionNAG outperforms existing BO-based NAS approaches, particularly in the large MobileNetV3 search space on the ImageNet 1K dataset.},
  author = {Sohyun An and Hayeon Lee and Jaehyeong Jo and Seanie Lee and Sung Ju Hwang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/an2024diffusionnag.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dyG2oLJYyX},
  publisher = {OpenReview.net},
  title = {{DiffusionNAG}: Predictor-guided Neural Architecture Generation with Diffusion Models},
  url = {https://openreview.net/forum?id=dyG2oLJYyX},
  year = {2024}
}

@inproceedings{an2024hybrid,
  abstract = {Equivariant message passing neural networks have emerged as the prevailing approach for predicting chemical properties of molecules due to their ability to leverage translation and rotation symmetries, resulting in a strong inductive bias. However, the equivariant operations in each layer can impose excessive constraints on the function form and network flexibility. To address these challenges, we introduce a novel network called the Hybrid Directional Graph Neural Network (HDGNN), which effectively combines strictly equivariant operations with learnable modules. We evaluate the performance of HDGNN on the QM9 dataset and the IS2RE dataset of OC20, demonstrating its state-of-the-art performance on several tasks and competitive performance on others.},
  author = {Junyi An and Chao Qu and Zhipeng Zhou and Fenglei Cao and Yinghui Xu and Yuan Qi and Furao Shen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/an2024hybrid.pdf:pdf},
  note = {DBLP last modified: 2024-08-23},
  pdf = {https://openreview.net/pdf?id=BBD6KXIGJL},
  publisher = {OpenReview.net},
  title = {Hybrid Directional Graph Neural Network for Molecules},
  url = {https://openreview.net/forum?id=BBD6KXIGJL},
  year = {2024}
}

@inproceedings{an2024perceptionclip,
  abstract = {Vision-language models like CLIP are widely used in zero-shot image classification due to their ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better performance is still an open question. This paper draws inspiration from the human visual perception process: when classifying an object, humans first infer contextual attributes (e.g., background and orientation) which help separate the foreground object from the background, and then classify the object based on this information. Inspired by it, we observe that providing CLIP with contextual attributes improves zero-shot image classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot classification method PerceptionCLIP. Given an image, it first infers contextual attributes (e.g., background) and then performs object classification conditioning on them. Our experiments show that PerceptionCLIP achieves better generalization, group robustness, and interpretability.},
  author = {Bang An and Sicheng Zhu and Michael-Andrei Panaitescu-Liess and Chaithanya Kumar Mummadi and Furong Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/an2024perceptionclip.pdf:pdf},
  note = {DBLP last modified: 2025-01-21},
  pdf = {https://openreview.net/pdf?id=2Oiee202rd},
  publisher = {OpenReview.net},
  title = {{PerceptionCLIP}: Visual Classification by Inferring and Conditioning on Contexts},
  url = {https://openreview.net/forum?id=2Oiee202rd},
  year = {2024}
}

@inproceedings{anciukeviius2024denoising,
  abstract = {Generating 3D scenes is a challenging open problem, which requires synthesizing plausible content that is fully consistent in 3D space. While recent methods such as neural radiance fields excel at view synthesis and 3D reconstruction, they cannot synthesize plausible details in unobserved regions since they lack a generative capability. Conversely, existing generative methods are typically not capable of reconstructing detailed, large-scale scenes in the wild, as they use limited-capacity 3D scene representations, require aligned camera poses, or rely on additional regularizers. In this work, we introduce the first diffusion model able to perform fast, detailed reconstruction and generation of real-world 3D scenes. To achieve this, we make three contributions. First, we introduce a new neural scene representation, IB-planes, that can efficiently and accurately represent large 3D scenes, dynamically allocating more capacity as needed to capture details visible in each image. Second, we propose a denoising-diffusion framework to learn a prior over this novel 3D scene representation, using only 2D images without the need for any additional supervision signal such as masks or depths. This supports 3D reconstruction and generation in a unified architecture. Third, we develop a principled approach to avoid trivial 3D solutions when integrating the image-based rendering with the diffusion model, by dropping out representations of some images. We evaluate the model on several challenging datasets of real and synthetic images, and demonstrate superior results on generation, novel view synthesis and 3D reconstruction.},
  author = {Titas Anciukeviƒçius and Fabian Manhardt and Federico Tombari and Paul Henderson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/anciukeviius2024denoising.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1JbsdayvhO},
  publisher = {OpenReview.net},
  title = {Denoising Diffusion via Image-Based Rendering},
  url = {https://openreview.net/forum?id=1JbsdayvhO},
  year = {2024}
}

@inproceedings{andrade2024sparsistency,
  abstract = {Optimal Transport is a useful metric to compare probability distributions and to compute a pairing given a ground cost. Its entropic regularization variant (eOT) is crucial to have fast algorithms and reflect fuzzy/noisy matchings. This work focuses on Inverse Optimal Transport (iOT), the problem of inferring the ground cost from samples drawn from a coupling that solves an eOT problem. It is a relevant problem that can be used to infer unobserved/missing links, and to obtain meaningful information about the structure of the ground cost yielding the pairing. On one side, iOT benefits from convexity, but on the other side, being ill-posed, it requires regularization to handle the sampling noise. This work presents an in-depth theoretical study of the $\ell_1$ regularization to model for instance Euclidean costs with sparse interactions between features. Specifically, we derive a sufficient condition for the robust recovery of the sparsity of the ground cost that can be seen as a far reaching generalization of the Lasso's celebrated ``Irrepresentability Condition''. To provide additional insight into this condition (consequently on the types of recoverable costs) we work out in detail the Gaussian case. Surprisingly, varying the entropic regularizer provides evidence that the Gaussian iOT interpolates between a graphical Lasso and a classical Lasso, thereby establishing a connection between iOT and graph estimation, an important problem in ML.},
  author = {Francisco Andrade and Gabriel Peyr√© and Clarice Poon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/andrade2024sparsistency.pdf:pdf},
  note = {DBLP last modified: 2024-08-09},
  pdf = {https://openreview.net/pdf?id=wpXGPCBOTX},
  publisher = {OpenReview.net},
  title = {Sparsistency for inverse optimal transport},
  url = {https://openreview.net/forum?id=wpXGPCBOTX},
  year = {2024}
}

@inproceedings{andrew2024oneshot,
  abstract = {Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks, model architectures, or DP algorithm, and/or require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel "one-shot" approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the model architecture, task, or DP algorithm. We show that our method provides provably correct estimates for the privacy loss under the Gaussian mechanism, and we demonstrate its performance on a well-established FL benchmark dataset under several adversarial threat models.},
  author = {Galen Andrew and Peter Kairouz and Sewoong Oh and Alina Oprea and Hugh Brendan McMahan and Vinith Menon Suriyakumar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/andrew2024oneshot.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=0BqyZSWfzo},
  publisher = {OpenReview.net},
  title = {One-shot Empirical Privacy Estimation for Federated Learning},
  url = {https://openreview.net/forum?id=0BqyZSWfzo},
  year = {2024}
}

@inproceedings{angelopoulos2024conformal,
  abstract = {We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.},
  author = {Anastasios Nikolas Angelopoulos and Stephen Bates and Adam Fisch and Lihua Lei and Tal Schuster},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/angelopoulos2024conformal.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=33XGfHLtZg},
  publisher = {OpenReview.net},
  title = {Conformal Risk Control},
  url = {https://openreview.net/forum?id=33XGfHLtZg},
  year = {2024}
}

@inproceedings{antoniades2024neuroformer,
  abstract = {State-of-the-art systems neuroscience experiments yield large-scale multimodal data, and these data sets require new tools for analysis. Inspired by the success of large pretrained models in vision and language domains, we reframe the analysis of large-scale, cellular-resolution neuronal spiking data into an autoregressive spatiotemporal generation problem. Neuroformer is a multimodal, multitask generative pretrained transformer (GPT) model that is specifically designed to handle the intricacies of data in systems neuroscience. It scales linearly with feature size, can process an arbitrary number of modalities, and is adaptable to downstream tasks, such as predicting behavior. We first trained Neuroformer on simulated datasets, and found that it both accurately predicted simulated neuronal circuit activity, and also intrinsically inferred the underlying neural circuit connectivity, including direction. When pretrained to decode neural responses, the model predicted the behavior of a mouse with only few-shot fine-tuning, suggesting that the model begins learning how to do so directly from the neural representations themselves, without any explicit supervision. We used an ablation study to show that joint training on neuronal responses and behavior boosted performance, highlighting the model's ability to associate behavioral and neural representations in an unsupervised manner. These findings show that Neuroformer can analyze neural datasets and their emergent properties, informing the development of models and hypotheses associated with the brain.},
  author = {Antonis Antoniades and Yiyi Yu and Joseph Canzano and William Yang Wang and Spencer L. Smith},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/antoniades2024neuroformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=W8S8SxS9Ng},
  publisher = {OpenReview.net},
  title = {Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data},
  url = {https://openreview.net/forum?id=W8S8SxS9Ng},
  year = {2024}
}

@inproceedings{arora2024zoology,
  abstract = {Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and gated-convolution language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In fine-grained analysis, we find 82\% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. ``Hakuna Matata means no worries Hakuna Matata it means no'' $\rightarrow$ ??. On this task, termed ``associative recall'', we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions can perfectly solve synthetic tests for AR capability. To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4\% of the gap to attention, while maintaining sub-quadratic scaling.},
  author = {Simran Arora and Sabri Eyuboglu and Aman Timalsina and Isys Johnson and Michael Poli and James Zou and Atri Rudra and Christopher R{\'e}},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/arora2024zoology.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LY3ukUANko},
  publisher = {OpenReview.net},
  title = {{Zoology}: {Measuring} and {Improving} {Recall} in {Efficient} {Language} {Models}},
  url = {https://openreview.net/forum?id=LY3ukUANko},
  year = {2024}
}

@inproceedings{arora2024leveraging,
  abstract = {Binary classification involves predicting the label of an instance based on whether the model score for the positive class exceeds a threshold chosen based on the application requirements (e.g., maximizing recall for a precision bound). However, model scores are often not aligned with the true positivity rate. This is especially true when the training involves a differential sampling across classes or there is distributional drift between train and test settings. In this paper, we provide theoretical analysis and empirical evidence of the dependence of model score estimation bias on both uncertainty and score itself. Further, we formulate the decision boundary selection in terms of both model score and uncertainty, prove that it is NP-hard, and present algorithms based on dynamic programming and isotonic regression. Evaluation of the proposed algorithms on three real-world datasets yield 25\%-40\% gain in recall at high precision bounds over the traditional approach of using model score alone.},
  author = {Gundeep Arora and Srujana Merugu and Anoop Saladi and Rajeev Rastogi},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/arora2024leveraging.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nsNyDvNQTc},
  publisher = {OpenReview.net},
  title = {{Leveraging} {Uncertainty} {Estimates} {To} {Improve} {Classifier} {Performance}},
  url = {https://openreview.net/forum?id=nsNyDvNQTc},
  year = {2024}
}

@inproceedings{ge2024highdimensional,
  abstract = {We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the Hessian and gradient matrices. Moreover, in multi-layer settings this alignment occurs per layer, with the final layer's outlier eigenspace evolving over the course of training, and exhibiting rank deficiency when the SGD converges to sub-optimal classifiers.},
  author = {G{\'e}rard Ben Arous and Reza Gheissari and Jiaoyang Huang and Aukosh Jagannath},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/ge2024highdimensional.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=MHjigVnI04},
  publisher = {OpenReview.net},
  title = {{High-dimensional} {SGD} aligns with emerging outlier eigenspaces},
  url = {https://openreview.net/forum?id=MHjigVnI04},
  year = {2024}
}

@inproceedings{asadulaev2024neural,
  abstract = {We introduce a novel neural network-based algorithm to compute optimal transport (OT) plans for general cost functionals. In contrast to common Euclidean costs, i.e., $\ell^1$ or $\ell^2$, such functionals provide more flexibility and allow using auxiliary information, such as class labels, to construct the required transport map. Existing methods for general cost functionals are discrete and do not provide an out-of-sample estimation. We address the challenge of designing a continuous OT approach for general cost functionals in high-dimensional spaces, such as images. We construct two example functionals: one to map distributions while preserving the class-wise structure and the other one to preserve the given data pairs. Additionally, we provide the theoretical error analysis for our recovered transport plans.},
  author = {Arip Asadulaev and Alexander Korotin and Vage Egiazarian and Petr Mokrov and Evgeny Burnaev},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/asadulaev2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gIiz7tBtYZ},
  publisher = {OpenReview.net},
  title = {{Neural} {Optimal} {Transport} with {General} {Cost} {Functionals}},
  url = {https://openreview.net/forum?id=gIiz7tBtYZ},
  year = {2024}
}

@inproceedings{asai2024selfrag,
  abstract = {Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
  author = {Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/asai2024selfrag.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=hSyW5go0v8},
  publisher = {OpenReview.net},
  title = {{Self-RAG}: {Learning} to {Retrieve}, {Generate}, and {Critique} through {Self-Reflection}},
  url = {https://openreview.net/forum?id=hSyW5go0v8},
  year = {2024}
}

@inproceedings{ashcroft2024modelling,
  abstract = {Vector drawings are innately interactive as they preserve creational cues. Despite this desirable property they remain relatively under explored due to the difficulties in modeling complex vector drawings. This is in part due to the primarily sequential and auto-regressive nature of existing approaches failing to scale beyond simple drawings. In this paper, we define generative models over highly complex vector drawings by first representing them as 'stroke-clouds' -- sets of arbitrary cardinality comprised of semantically meaningful strokes. The dimensionality of the strokes is a design choice that allows the model to adapt to a range of complexities. We learn to encode these set of strokes into compact latent codes by a probabilistic reconstruction procedure backed by De-Finetti's Theorem of Exchangability.},
  author = {Alexander Ashcroft and Ayan Das and Yulia Gryaditskaya and Zhiyu Qu and Yi-Zhe Song},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/ashcroft2024modelling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=O2jyuo89CK},
  publisher = {OpenReview.net},
  title = {{Modelling} complex vector drawings with stroke-clouds},
  url = {https://openreview.net/forum?id=O2jyuo89CK},
  year = {2024}
}

@inproceedings{ashkboos2024slicegpt,
  abstract = {Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation we show that SliceGPT can remove up to 25\% of the model parameters (including embeddings) for LLAMA-2 70B, OPT 66B and Phi-2 models while maintaining 99\%, 99\% and 90\% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA-2 70B to 64\% of that of the dense model; on 40GB A100 GPUs we reduce it to 66\%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models.},
  author = {Saleh Ashkboos and Maximilian L. Croci and Marcelo Gennari do Nascimento and Torsten Hoefler and James Hensman},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/ashkboos2024slicegpt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vXxardq6db},
  publisher = {OpenReview.net},
  title = {{SliceGPT}: {Compress} {Large} {Language} {Models} by {Deleting} {Rows} and {Columns}},
  url = {https://openreview.net/forum?id=vXxardq6db},
  year = {2024}
}

@inproceedings{ashok2024tactis2,
  abstract = {We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.},
  author = {Arjun Ashok and {\'E}tienne Marcotte and Valentina Zantedeschi and Nicolas Chapados and Alexandre Drouin},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/ashok2024tactis2.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xtOydkE1Ku},
  publisher = {OpenReview.net},
  title = {{TACTiS-2}: {Better}, {Faster}, {Simpler} {Attentional} {Copulas} for {Multivariate} {Time} {Series}},
  url = {https://openreview.net/forum?id=xtOydkE1Ku},
  year = {2024}
}

@inproceedings{atzmon2024approximately,
  abstract = {Integrating a notion of symmetry into point cloud neural networks is a provably effective way to improve their generalization capability. Of particular interest are E(3) equivariant point cloud networks where Euclidean transformations applied to the inputs are preserved in the outputs. Recent efforts aim to extend networks that are E(3) equivariant, to accommodate inputs made of multiple parts, each of which exhibits local E(3) symmetry. In practical settings, however, the partitioning into individually transforming regions is unknown a priori. Errors in the partition prediction would unavoidably map to errors in respecting the true input symmetry. To this end, we introduce APEN: a general framework for constructing approximate piecewise-E(3) equivariant point networks. Our framework offers an adaptable design to guaranteed bounds on the resulting piecewise E(3) equivariance approximation errors. Our primary insight is that functions which are equivariant with respect to a finer partition (compared to the unknown true partition) will also maintain equivariance in relation to the true partition. As a result, the equivariance approximation error can be bounded solely in terms of (i) uncertainty quantification of the partition prediction, and (ii) bounds on the probability of failing to suggest a proper subpartition of the ground truth one. We demonstrate the practical effectiveness of APEN using two data types exemplifying part-based symmetry: (i) real-world scans of room scenes containing multiple furniture-type objects; and, (ii) human motions, characterized by articulated parts exhibiting rigid movement. Our empirical results demonstrate the advantage of integrating piecewise E(3) symmetry into network design, showing a distinct improvement in generalization accuracy compared to prior works for both classification and segmentation tasks.},
  author = {Matan Atzmon and Jiahui Huang and Francis Williams and Or Litany},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/atzmon2024approximately.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=aKJEHWmBEf},
  publisher = {OpenReview.net},
  title = {{Approximately} {Piecewise} {E(3)} {Equivariant} {Point} {Networks}},
  url = {https://openreview.net/forum?id=aKJEHWmBEf},
  year = {2024}
}

@inproceedings{raphae2024the,
  abstract = {Partially Observable Markov Decision Processes (POMDPs) are used to model environments where the state cannot be perceived, necessitating reasoning based on past observations and actions. However, remembering the full history is generally intractable due to the exponential growth in the history space. Maintaining a probability distribution that models the belief over the current state can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is often intractable. We introduce the Wasserstein Belief Updater, an RNN-free RL algorithm for POMDPs that learns a representation of the history via an approximation of the belief update in a reliable latent space model. Our algorithm provides theoretical guarantees for learning the optimal value function. We evaluate our approach on several POMDP benchmarks and show that it is competitive with state-of-the-art methods while being more efficient in terms of memory and computation.},
  author = {Rapha{\"e}l Avalos and Florent Delgrange and Ann Now{\'e} and Guillermo A. P{\'e}rez and Diederik M. Roijers},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/raphae2024the.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KrtGfTGaGe},
  publisher = {OpenReview.net},
  title = {{The} {Wasserstein} {Believer}: {Learning} {Belief} {Updates} for {Partially} {Observable} {Environments} through {Reliable} {Latent} {Space} {Models}},
  url = {https://openreview.net/forum?id=KrtGfTGaGe},
  year = {2024}
}

@inproceedings{azerbayev2024llemma,
  abstract = {We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the {MATH} benchmark Llemma outperforms all known openly released models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.},
  author = {Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen Marcus McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/azerbayev2024llemma.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4WnqRR915j},
  publisher = {OpenReview.net},
  title = {{Llemma}: An Open Language Model for Mathematics},
  url = {https://openreview.net/forum?id=4WnqRR915j},
  year = {2024}
}

@inproceedings{baader2024expressivity,
  abstract = {Convex relaxations are a key component of training and certifying provably safe neural networks. However, despite substantial progress, a wide and poorly understood accuracy gap to standard networks remains, raising the question of whether this is due to fundamental limitations of convex relaxations. Initial work investigating this question focused on the simple and widely used {IBP} relaxation. It revealed that some univariate, convex, continuous piecewise linear ({CPWL}) functions cannot be encoded by any {ReLU} network such that its {IBP}-analysis is precise. To explore whether this limitation is shared by more advanced convex relaxations, we conduct the first in-depth study on the expressive power of {ReLU} networks across all commonly used convex relaxations. We show that: (i) more advanced relaxations allow a larger class of univariate functions to be expressed as precisely analyzable {ReLU} networks, (ii) more precise relaxations can allow exponentially larger solution spaces of {ReLU} networks encoding the same functions, and (iii) even using the most precise single-neuron relaxations, it is impossible to construct precisely analyzable {ReLU} networks that express multivariate, convex, monotone {CPWL} functions.},
  author = {Maximilian Baader and Mark Niklas Mueller and Yuhao Mao and Martin Vechev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/baader2024expressivity.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=awHTL3Hpto},
  publisher = {OpenReview.net},
  title = {Expressivity of {ReLU}-Networks under Convex Relaxations},
  url = {https://openreview.net/forum?id=awHTL3Hpto},
  year = {2024}
}

@inproceedings{babaiee2024unveiling,
  abstract = {Recent advances in depthwise-separable convolutional neural networks ({DS-CNNs}) have led to novel architectures, that surpass the performance of classical {CNNs}, by a considerable scalability and accuracy margin. This paper reveals another striking property of {DS-CNN} architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. Through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. Astonishingly, the patterns converged into a few main clusters, each resembling the difference of {Gaussian} ({DoG}) functions, and their first and second-order derivatives. Notably, we were able to classify over 95\% and 90\% of the filters from state-of-the-art {ConvNextV2} and {ConvNeXt} models, respectively. This finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long proposed for the vision systems of mammals. Our results thus deepen our understanding of the emergent properties of trained {DS-CNNs} and provide a bridge between artificial and biological visual processing systems. More broadly, they pave the way for more interpretable and biologically-inspired neural network designs in the future.},
  author = {Zahra Babaiee and Peyman M. Kiasari and Daniela Rus and Radu Grosu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/babaiee2024unveiling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4VgBjsOC8k},
  publisher = {OpenReview.net},
  title = {Unveiling the Unseen: Identifiable Clusters in Trained Depthwise Convolutional Kernels},
  url = {https://openreview.net/forum?id=4VgBjsOC8k},
  year = {2024}
}

@inproceedings{bacchiocchi2024learning,
  abstract = {We study principal-agent problems in which a principal commits to an outcome-dependent payment scheme---called contract---in order to induce an agent to take a costly, unobservable action leading to favorable outcomes. We consider a generalization of the classical (single-round) version of the problem in which the principal interacts with the agent by committing to contracts over multiple rounds. The principal has no information about the agent, and they have to learn an optimal contract by only observing the outcome realized at each round. We focus on settings where the size of the agent's action space is small, and we design an algorithm that learns an approximately-optimal contract with high probability in a number of rounds polynomial in the size of the outcome space, when the number of actions is constant. The algorithm solves an open problem by Zhu et al. (2022), and it can also provide a regret bound of $\tilde{O}(T^{4/5})$ in the related online learning setting.},
  author = {Francesco Bacchiocchi and Matteo Castiglioni and Alberto Marchesi and Nicola Gatti},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bacchiocchi2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WKuimaBj4I},
  publisher = {OpenReview.net},
  title = {Learning Optimal Contracts: How to Exploit Small Action Spaces},
  url = {https://openreview.net/forum?id=WKuimaBj4I},
  year = {2024}
}

@inproceedings{backurs2024efficiently,
  abstract = {Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \subset \mathbb{R}^d$, output a differentially private ({DP}) data structure which approximates $\sum_{x \in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\|x-y\|_2^2/\sigma^2}$ (also known as {DP} kernel density estimation), or a distance function such as $f(x,y) = \|x-y\|_2$, among others. Our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. The unifying approach behind our results is leveraging `low-dimensional structures' present in the specific functions $f$ that we study, using tools such as provable dimensionality reduction, approximation theory, and one-dimensional decomposition of the functions. Our algorithms empirically exhibit improved query times and accuracy over prior state of the art. We also present an application to {DP} classification. Our experiments demonstrate that the simple methodology of classifying based on average similarity is orders of magnitude faster than prior {DP-SGD} based approaches for comparable accuracy.},
  author = {Arturs Backurs and Zinan Lin and Sepideh Mahabadi and Sandeep Silwal and Jakub Tarnawski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/backurs2024efficiently.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HMe5CJv9dQ},
  publisher = {OpenReview.net},
  title = {Efficiently Computing Similarities to Private Datasets},
  url = {https://openreview.net/forum?id=HMe5CJv9dQ},
  year = {2024}
}

@inproceedings{bae2024dirichlet,
  abstract = {For learning with noisy labels, the transition matrix, which explicitly models the relation between noisy label distribution and clean label distribution, has been utilized to achieve the statistical consistency of either the classifier or the risk. Previous researches have focused more on how to estimate this transition matrix well, rather than how to utilize it. We propose good utilization of the transition matrix is crucial and suggest a new utilization method based on resampling, coined {RENT}. Specifically, we first demonstrate current utilizations can have potential limitations for implementation. As an extension to Reweighting, we suggest the {Dirichlet} distribution-based per-sample Weight Sampling ({DWS}) framework, and compare reweighting and resampling under {DWS} framework. With the analyses from {DWS}, we propose {RENT}, a REsampling method with Noise Transition matrix. Empirically, {RENT} consistently outperforms existing transition matrix utilization methods, which includes reweighting, on various benchmark datasets.},
  author = {HeeSun Bae and Seungjae Shin and Byeonghu Na and Il-Chul Moon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bae2024dirichlet.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=A4mJuFRMN8},
  publisher = {OpenReview.net},
  title = {{Dirichlet}-based Per-Sample Weighting by Transition Matrix for Noisy Label Learning},
  url = {https://openreview.net/forum?id=A4mJuFRMN8},
  year = {2024}
}

@inproceedings{baek2024why,
  abstract = {Sharpness-Aware Minimization ({SAM}) is most known for achieving state-of-the-art performances on natural image and language tasks. However, its most pronounced improvements (of tens of percent) is rather in the presence of label noise. Understanding {SAM}'s label noise robustness requires a departure from characterizing the robustness of minimas lying in ``flatter'' regions of the loss landscape. In particular, the peak performance under label noise occurs with early stopping, far before the loss converges. We decompose {SAM}'s robustness into two effects: one induced by changes to the logit term and the other induced by changes to the network Jacobian. The first can be observed in linear logistic regression where {SAM} provably up-weights the gradient contribution from clean examples. Although this explicit up-weighting is also observable in neural networks, when we intervene and modify {SAM} to remove this effect, surprisingly, we see no visible degradation in performance. This suggests that the network Jacobian effect is the major driver of {SAM}'s robustness.},
  author = {Christina Baek and J. Zico Kolter and Aditi Raghunathan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/baek2024why.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3aZCPl3ZvR},
  publisher = {OpenReview.net},
  title = {Why is {SAM} Robust to Label Noise?},
  url = {https://openreview.net/forum?id=3aZCPl3ZvR},
  year = {2024}
}

@inproceedings{baharlouei2024fferm,
  abstract = {Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence. While numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these methods are not amenable to stochastic optimization due to the complex and nonlinear structure of constraints and regularizers. Here, the term ``stochastic'' refers to the ability of the algorithm to work with small mini-batches of data. Motivated by the limitation of existing literature, this paper presents a unified stochastic optimization framework for fair empirical risk minimization based on f-divergence measures (f-{FERM}). The proposed stochastic algorithm enjoys theoretical convergence guarantees. In addition, our experiments demonstrate the superiority of fairness-accuracy tradeoffs offered by f-{FERM} for almost all batch sizes (ranging from full-batch to batch size of one). Moreover, we show that our framework can be extended to the case where there is a distribution shift from training to the test data. Our extension is based on a distributionally robust optimization reformulation of f-{FERM} objective under $L_p$ norms as uncertainty sets. Again, in this distributionally robust setting, f-{FERM} not only enjoys theoretical convergence guarantees but also outperforms other baselines in the literature in the tasks involving distribution shifts.},
  author = {Sina Baharlouei and Shivam Patel and Meisam Razaviyayn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/baharlouei2024fferm.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=s90VIdza2K},
  publisher = {OpenReview.net},
  title = {f-{FERM}: A Scalable Framework for Robust Fair Empirical Risk Minimization},
  url = {https://openreview.net/forum?id=s90VIdza2K},
  year = {2024}
}

@inproceedings{baheti2024leftover,
  abstract = {Reinforcement Learning with Human Feedback ({RLHF}) is the most prominent method for Language Model ({LM}) alignment. However, {RLHF} is an unstable and data-hungry process that continually requires new high-quality {LM}-generated data for finetuning. We introduce Advantage-Leftover Lunch {RL} ({A-LoL}), a new class of offline policy gradient algorithms that enable {RL} training on any pre-existing data. By assuming the entire {LM} output sequence as a single action, {A-LoL} allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using {LM}'s value estimate, {A-LoL} only trains on positive advantage (leftover) data points, making it resilient to noise. {A-LoL} is very easy to implement over standard cross entropy loss using two key improvements: (1) sequence-level advantage and (2) importance weight (ratio of target {LM}'s and initial reference {LM} probabilities). On the commonly-used {RLHF} benchmark, Helpful and Harmless Assistant ({HHA}), {LMs} trained with {A-LoL} methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Overall, {A-LoL} is an easy-to-implement, sample-efficient, and stable {LM} training recipe.},
  author = {Ashutosh Baheti and Ximing Lu and Faeze Brahman and Ronan {Le Bras} and Maarten Sap and Mark O. Riedl},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/baheti2024leftover.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZDGKPbF0VQ},
  publisher = {OpenReview.net},
  title = {Leftover Lunch: Advantage-based Offline Reinforcement Learning for Language Models},
  url = {https://openreview.net/forum?id=ZDGKPbF0VQ},
  year = {2024}
}

@inproceedings{bai2024local,
  abstract = {Distributed optimization approaches for saddle point problems have gained popularity due to their critical role in machine learning. However, existing works mostly target smooth unconstrained objectives in Euclidean space, while ML problems often involve constraints or non-smooth regularization, requiring composite optimization. Additionally, while non-smooth regularization often induces structure like sparsity, standard aggregation schemes in distributed optimization break this structure. To address these issues, we propose Federated Dual Extrapolation (FeDualEx), an extra-step primal-dual algorithm with local updates, which is the first to encompass both saddle point optimization and composite objectives under the distributed paradigm.},
  author = {Site Bai and Brian Bullins},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bai2024local.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kklwv4c4dI},
  publisher = {OpenReview.net},
  title = {Local Composite Saddle Point Optimization},
  url = {https://openreview.net/forum?id=kklwv4c4dI},
  year = {2024}
}

@inproceedings{bai2024benchmarking,
  abstract = {While prior federated learning ({FL}) methods mainly consider client heterogeneity, we focus on the {*Federated Domain Generalization} ({DG}) task, which introduces train-test heterogeneity in the {FL} context. Existing evaluations in this field are limited in terms of the scale of the clients and dataset diversity. Thus, we propose a Federated {DG} benchmark that aim to test the limits of current methods with high client heterogeneity, large numbers of clients, and diverse datasets. Towards this objective, we introduce a novel data partition method that allows us to distribute any domain dataset among few or many clients while controlling client heterogeneity. We then introduce and apply our methodology to evaluate 14 {DG} methods, which include centralized {DG} methods adapted to the {FL} context, {FL} methods that handle client heterogeneity, and methods designed specifically for Federated {DG} on 7 datasets. Our results suggest that, despite some progress, significant performance gaps remain in Federated {DG}, especially when evaluating with a large number of clients, high client heterogeneity, or more realistic datasets. Furthermore, our extendable benchmark code will be publicly released to aid in benchmarking future Federated {DG} approaches.},
  author = {Ruqi Bai and Saurabh Bagchi and David I. Inouye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bai2024benchmarking.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=wprSv7ichW},
  publisher = {OpenReview.net},
  title = {Benchmarking Algorithms for Federated Domain Generalization},
  url = {https://openreview.net/forum?id=wprSv7ichW},
  year = {2024}
}

@inproceedings{bai2024hypo,
  abstract = {Out-of-distribution ({OOD}) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework {HYPO} ({HY}perspherical {O}{OD} generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the {OOD} generalization bound. Through extensive experiments on challenging {OOD} benchmarks, we demonstrate that our approach outperforms competitive baselines and achieves superior performance. Code is available at https://github.com/deeplearning-wisc/hypo.},
  author = {Haoyue Bai and Yifei Ming and Julian Katz-Samuels and Yixuan Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bai2024hypo.pdf:pdf},
  note = {DBLP last modified: 2025-02-06},
  pdf = {https://openreview.net/pdf?id=VXak3CZZGC},
  publisher = {OpenReview.net},
  title = {{HYPO}: Hyperspherical Out-Of-Distribution Generalization},
  url = {https://openreview.net/forum?id=VXak3CZZGC},
  year = {2024}
}

@inproceedings{bai2024sentencelevel,
  abstract = {Composed image retrieval ({CIR}) is the task of retrieving specific images by using a query that involves both a reference image and a relative caption. Most existing {CIR} models adopt the late-fusion strategy to combine visual and language features. Besides, several approaches have also been suggested to generate a pseudo-word token from the reference image, which is further integrated into the relative caption for {CIR}. However, these pseudo-word-based prompting methods have limitations when target image encompasses complex changes on reference image, e.g., object removal and attribute modification. In this work, we demonstrate that learning an appropriate sentence-level prompt for the relative caption ({SPRC}) is sufficient for achieving effective composed image retrieval. Instead of relying on pseudo-word-based prompts, we propose to leverage pretrained {V-L} models, e.g., {BLIP-2}, to generate sentence-level prompts. By concatenating the learned sentence-level prompt with the relative caption, one can readily use existing text-based image retrieval models to enhance {CIR} performance. Furthermore, we introduce both image-text contrastive loss and text prompt alignment loss to enforce the learning of suitable sentence-level prompts. Experiments show that our proposed method performs favorably against the state-of-the-art {CIR} methods on the Fashion-IQ and {CIRR} datasets.},
  author = {Yang Bai and Xinxing Xu and Yong Liu and Salman Khan and Fahad Khan and Wangmeng Zuo and Rick Siow Mong Goh and Chun-Mei Feng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bai2024sentencelevel.pdf:pdf},
  note = {DBLP last modified: 2025-03-10},
  pdf = {https://openreview.net/pdf?id=m3ch3kJL7q},
  publisher = {OpenReview.net},
  title = {Sentence-level Prompts Benefit Composed Image Retrieval},
  url = {https://openreview.net/forum?id=m3ch3kJL7q},
  year = {2024}
}

@inproceedings{bair2024adaptive,
  abstract = {Robustness and compactness are two essential attributes of deep learning models that are deployed in the real world. The goals of robustness and compactness may seem to be at odds, since robustness requires generalization across domains, while the process of compression exploits specificity in one domain. We introduce Adaptive Sharpness-Aware Pruning ({AdaSAP}), which unifies these goals through the lens of network sharpness. The {AdaSAP} method produces sparse networks that are robust to input variations which are unseen at training time. We achieve this by strategically incorporating weight perturbations in order to optimize the loss landscape. This allows the model to be both primed for pruning and regularized for improved robustness. {AdaSAP} improves the robust accuracy of pruned models on image classification by up to +6\% on {ImageNet} C and +4\% on {ImageNet} V2, and on object detection by +4\% on a corrupted Pascal {VOC} dataset, over a wide range of compression ratios, pruning criteria, and network architectures, outperforming recent pruning art by large margins.},
  author = {Anna Bair and Hongxu Yin and Maying Shen and Pavlo Molchanov and Jose M. Alvarez},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bair2024adaptive.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QFYVVwiAM8},
  publisher = {OpenReview.net},
  title = {Adaptive Sharpness-Aware Pruning for Robust Sparse Networks},
  url = {https://openreview.net/forum?id=QFYVVwiAM8},
  year = {2024}
}

@inproceedings{bakman2024federated,
  abstract = {Federated Learning ({FL}) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in {FL} mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning ({CFL}). The main challenge of {CFL} is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. We propose Federated Orthogonal Training ({FOT}) that addresses this challenge by extracting the global input subspace of each layer for old tasks and modifying the aggregated updates of new tasks such that they are orthogonal to the global principal subspace of old tasks for each layer. {FOT} achieves an average accuracy gain of up to 15\% with 27\% lower forgetting while incurring minimal computation and communication cost.},
  author = {Yavuz Faruk Bakman and Duygu Nur Yaldiz and Yahya H. Ezzeldin and Salman Avestimehr},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bakman2024federated.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nAs4LdaP9Y},
  publisher = {OpenReview.net},
  title = {Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning},
  url = {https://openreview.net/forum?id=nAs4LdaP9Y},
  year = {2024}
}

@inproceedings{bakr2024cot3dref,
  abstract = {{3D} visual grounding is the ability to localize objects in {3D} scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question ``Can we design an interpretable {3D} visual grounding framework that has the potential to mimic the human perception system?''. To this end, we formulate the {3D} visual grounding problem as a sequence-to-sequence ({Seq2Seq}) task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed {CoT3DRef}, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10\% of the data, we match the {SOTA} performance that trained on the entire data.},
  author = {Eslam Mohamed Bakr and Mohamed Ayman and Mahmoud Ahmed and Habib Slim and Mohamed Elhoseiny},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bakr2024cot3dref.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ORUiqcLpV6},
  publisher = {OpenReview.net},
  title = {{CoT3DRef}: Chain-of-Thoughts Data-Efficient {3D} Visual Grounding},
  url = {https://openreview.net/forum?id=ORUiqcLpV6},
  year = {2024}
}

@inproceedings{balachandar2024domain,
  abstract = {Machine learning models are often trained to predict the outcome resulting from a human decision. For example, if a doctor decides to test a patient for disease, will the patient test positive? A challenge is that historical decision-making determines whether the outcome is observed: we only observe test outcomes for patients doctors historically tested. Untested patients, for whom outcomes are unobserved, may differ from tested patients along observed and unobserved dimensions. We propose a Bayesian model class which captures this setting. The purpose of the model is to accurately estimate risk for both tested and untested patients. Estimating this model is challenging due to the wide range of possibilities for untested patients. To address this, we propose two domain constraints which are plausible in health settings: a prevalence constraint, where the overall disease prevalence is known, and an expertise constraint, where the human decision-maker deviates from purely risk-based decision-making only along a constrained feature set. We show theoretically and on synthetic data that domain constraints improve parameter inference. We apply our model to a case study of cancer risk prediction, showing that the model's inferred risk predicts cancer diagnoses, its inferred testing policy captures known public health policies, and it can identify suboptimalities in test allocation. Though our case study is in healthcare, our analysis reveals a general class of domain constraints which can improve model estimation in many settings.},
  author = {Sidhika Balachandar and Nikhil Garg and Emma Pierson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/balachandar2024domain.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1mNFsbvo2P},
  publisher = {OpenReview.net},
  title = {Domain constraints improve risk prediction when outcome data is missing},
  url = {https://openreview.net/forum?id=1mNFsbvo2P},
  year = {2024}
}

@inproceedings{bambade2024leveraging,
  abstract = {Optimization layers within neural network architectures have become increasingly popular for their ability to solve a wide range of machine learning tasks and to model domain-specific knowledge. However, designing optimization layers requires careful consideration as the underlying optimization problems might be infeasible during training. Motivated by applications in learning, control and robotics, this work focuses on convex quadratic programming ({QP}) layers. The specific structure of this type of optimization layer can be efficiently exploited for faster computations while still allowing rich modeling capabilities. We leverage primal-dual augmented Lagrangian techniques for computing derivatives of both feasible and infeasible {QP} solutions. More precisely, we propose a unified approach which tackles the differentiability of the closest feasible {QP} solutions in a classical $\ell_2$ sense. We then harness this approach to enrich the expressive capabilities of existing {QP} layers. More precisely, we show how differentiating through infeasible {QPs} during training enables to drive towards feasibility at test time a new range of {QP} layers. These layers notably demonstrate superior predictive performance in some conventional learning tasks. Additionally, we present alternative formulations that enhance numerical robustness, speed, and accuracy for training such layers. Along with these contributions, we provide an open-source {C++} software package called {QPLayer} for differentiating feasible and infeasible convex {QPs} and which can be interfaced with modern learning frameworks.},
  author = {Antoine Bambade and Fabian Schramm and Adrien B. Taylor and Justin Carpentier},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bambade2024leveraging.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=YCPDFfmkFr},
  publisher = {OpenReview.net},
  title = {Leveraging augmented-Lagrangian techniques for differentiating over infeasible quadratic programs in machine learning},
  url = {https://openreview.net/forum?id=YCPDFfmkFr},
  year = {2024}
}

@inproceedings{ban2024neural,
  abstract = {We study both stream-based and pool-based active learning with neural network approximations. A recent line of works proposed bandit-based approaches that transformed active learning into a bandit problem, achieving both theoretical and empirical success. However, the performance and computational costs of these methods may be susceptible to the number of classes, denoted as $K$, due to this transformation. Therefore, this paper seeks to answer the question: ``How can we mitigate the adverse impacts of $K$ while retaining the advantages of principled exploration and provable performance guarantees in active learning?'' To tackle this challenge, we propose two algorithms based on the newly designed exploitation and exploration neural networks for stream-based and pool-based active learning. Subsequently, we provide theoretical performance guarantees for both algorithms in a non-parametric setting, demonstrating a slower error-growth rate concerning $K$ for the proposed approaches. We use extensive experiments to evaluate the proposed algorithms, which consistently outperform state-of-the-art baselines.},
  author = {Yikun Ban and Ishika Agarwal and Ziwei Wu and Yada Zhu and Kommy Weldemariam and Hanghang Tong and Jingrui He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ban2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=g1S72T3FGc},
  publisher = {OpenReview.net},
  title = {Neural Active Learning Beyond Bandits},
  url = {https://openreview.net/forum?id=g1S72T3FGc},
  year = {2024}
}

@inproceedings{banerjee2024interpreting,
  abstract = {In recent years numerous methods have been developed to formally verify the robustness of deep neural networks ({DNNs}). Though the proposed techniques are effective in providing mathematical guarantees about the {DNNs}' behavior, it is not clear whether the proofs generated by these methods are human-understandable. In this paper, we bridge this gap by developing new concepts, algorithms, and representations to generate human understandable insights into the internal workings of {DNN} robustness proofs. Leveraging the proposed method, we show that the robustness proofs of standard {DNNs} rely more on spurious input features as compared to the proofs of {DNNs} trained to be robust. Furthermore, our analysis reveals that the robustness proofs of provably robust {DNNs} filter out more spurious input features compared to adversarially trained {DNNs}, sometimes even leading to pruning of semantically meaningful input features. Finally, we show that the proofs for {DNNs} combining adversarial and provably robust training tend to achieve a middle ground between the two approaches.},
  author = {Debangshu Banerjee and Avaljot Singh and Gagandeep Singh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/banerjee2024interpreting.pdf:pdf},
  note = {DBLP last modified: 2025-05-14},
  pdf = {https://openreview.net/pdf?id=Ev10F9TWML},
  publisher = {OpenReview.net},
  title = {Interpreting Robustness Proofs of Deep Neural Networks},
  url = {https://openreview.net/forum?id=Ev10F9TWML},
  year = {2024}
}

@inproceedings{bansal2024universal,
  abstract = {Typical diffusion models are trained to accept a particular form of conditioning, most commonly text, and cannot be conditioned on other modalities without retraining. In this work, we propose a universal guidance algorithm that enables diffusion models to be controlled by arbitrary guidance modalities without the need to retrain any use-specific components. We show that our algorithm successfully generates quality images with guidance functions including segmentation, face recognition, object detection, style guidance and classifier signals.},
  address = {Vienna, Austria},
  author = {Arpit Bansal and Hong-Min Chu and Avi Schwarzschild and Roni Sengupta and Micah Goldblum and Jonas Geiping and Tom Goldstein},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bansal2024universal.pdf:pdf},
  keywords = {Generative Models, Computer Vision, Diffusion Models},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: generative models. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=pzpWBbnwiJ},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Universal Guidance for Diffusion Models},
  url = {https://openreview.net/forum?id=pzpWBbnwiJ},
  year = {2024}
}

@inproceedings{bansal2024peering,
  abstract = {Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs.},
  address = {Vienna, Austria},
  author = {Hritik Bansal and John Dang and Aditya Grover},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bansal2024peering.pdf:pdf},
  keywords = {LLMs, Sparse Feedback, Ratings, Rankings, Inconsistency, Evaluation},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: generative models. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=dKl6lMwbCy},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models},
  url = {https://openreview.net/forum?id=dKl6lMwbCy},
  year = {2024}
}

@inproceedings{bansal2024llm,
  abstract = {Foundational models with billions of parameters which have been trained on large corpus of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. We propose {CALM}‚ÄîComposition to Augment Language Models‚Äîwhich introduces cross-attention between models to compose their representations and enable new capabilities.},
  address = {Vienna, Austria},
  author = {Rachit Bansal and Bidisha Samanta and Siddharth Dalmia and Nitish Gupta and Sriram Ganapathy and Abhishek Bapna and Prateek Jain and Partha Talukdar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bansal2024llm.pdf:pdf},
  keywords = {Large Language Models, Model Composition, Knowledge Augmentation},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: unsupervised, self-supervised, semi-supervised, and supervised representation learning. Published: 02 Feb 2024},
  pdf = {https://openreview.net/pdf?id=jjA4O1vJRz},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{LLM} Augmented {LLMs}: Expanding Capabilities through Composition},
  url = {https://openreview.net/forum?id=jjA4O1vJRz},
  year = {2024}
}

@inproceedings{bao2024insertnerf,
  abstract = {Generalizing Neural Radiance Fields (NeRF) to new scenes is a significant challenge that existing approaches struggle to address without extensive modifications to vanilla NeRF framework. We introduce InsertNeRF, a method for INStilling gEneRalizabiliTy into NeRF. By utilizing multiple plug-and-play HyperNet modules, InsertNeRF dynamically tailors NeRF's weights to specific reference scenes, transforming multi-scale sampling-aware features into scene-specific representations. This novel design allows for more accurate and efficient representations of complex appearances and geometries. Experiments show that this method not only achieves superior generalization performance but also provides a flexible pathway for integration with other NeRF-like systems, even in sparse input settings.},
  address = {Vienna, Austria},
  author = {Yanqi Bao and Tianyu Ding and Jing Huo and Wenbin Li and Yuxin Li and Yang Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bao2024insertnerf.pdf:pdf},
  keywords = {Neural Radiance Fields, Hypernetwork, Neural Rendering, Generalizability},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Code: https://github.com/bbbbby-99/InsertNeRF. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=aHmNpLlUlb},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{InsertNeRF}: Instilling Generalizability into {NeRF} with {HyperNet} Modules},
  url = {https://openreview.net/forum?id=aHmNpLlUlb},
  year = {2024}
}

@inproceedings{bao2024channel,
  abstract = {Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing.},
  address = {Vienna, Austria},
  author = {Yujia Bao and Srinivasan Sivanandan and Theofanis Karaletsos},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bao2024channel.pdf:pdf},
  keywords = {Vision transformer, Representation learning, Hyper spectral imaging},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Code: https://github.com/insitro/ChannelViT. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=CK5Hfb5hBG},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Channel Vision Transformers: An Image Is Worth 1 x 16 x 16 Words},
  url = {https://openreview.net/forum?id=CK5Hfb5hBG},
  year = {2024}
}

@inproceedings{bao2024fastdetectgpt,
  abstract = {Large language models (LLMs) have shown the ability to produce fluent and cogent content, presenting both productivity opportunities and societal risks. To build trustworthy AI systems, it is imperative to distinguish between machine-generated and human-authored content. The leading zero-shot detector, DetectGPT, showcases commendable performance but is marred by its intensive computational costs. In this paper, we introduce the concept of conditional probability curvature to elucidate discrepancies in word choices between LLMs and humans within a given context. Utilizing this curvature as a foundational metric, we present Fast-DetectGPT, an optimized zero-shot detector, which substitutes DetectGPT's perturbation step with a more efficient sampling step. Our evaluations on various datasets, source models, and test conditions indicate that Fast-DetectGPT not only surpasses DetectGPT by a relative around 75\% in both the white-box and black-box settings but also accelerates the detection process by a factor of 340.},
  address = {Vienna, Austria},
  author = {Guangsheng Bao and Yanbin Zhao and Zhiyang Teng and Linyi Yang and Yue Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bao2024fastdetectgpt.pdf:pdf},
  keywords = {Fake Detection, Machine-Generated Text Detection, Zero-Shot Detection},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: societal considerations including fairness, safety, privacy. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=Bpcgcr8E8Z},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Fast-{DetectGPT}: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature},
  url = {https://openreview.net/forum?id=Bpcgcr8E8Z},
  year = {2024}
}

@inproceedings{barbero2024localityaware,
  abstract = {Graph Neural Networks (GNNs) are popular models for machine learning on graphs that typically follow the message-passing paradigm, whereby the feature of a node is updated recursively upon aggregating information over its neighbors. While exchanging messages over the input graph endows GNNs with a strong inductive bias, it can also make GNNs susceptible to over-squashing, thereby preventing them from capturing long-range interactions in the given graph. To rectify this issue, graph rewiring techniques have been proposed as a means of improving information flow by altering the graph connectivity.},
  address = {Vienna, Austria},
  author = {Federico Barbero and Ameya Velingker and Amin Saberi and Michael M. Bronstein and Francesco Di Giovanni},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/barbero2024localityaware.pdf:pdf},
  keywords = {Graph Neural Networks, Message Passing Neural Networks, Over-squashing, Graph Rewiring},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: learning on graphs and other geometries \& topologies. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=4Ua4hKiAJX},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Locality-Aware Graph Rewiring in {GNNs}},
  url = {https://openreview.net/forum?id=4Ua4hKiAJX},
  year = {2024}
}

@inproceedings{barcel2024logical,
  abstract = {We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) {UHAT} (Unique Hard Attention Transformers) and (2) {AHAT} (Average Hard Attention Transformers). {UHAT} encoders are known to recognize only languages inside the circuit complexity class {$\mathsf{AC}^0$}, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, {AHAT} encoders can recognize languages outside {$\mathsf{AC}^0$}, but their expressive power still lies within the bigger circuit complexity class {$\mathsf{TC}^0$}, i.e., {$\mathsf{AC}^0$}-circuits extended by majority gates. We first show a negative result that there is an {$\mathsf{AC}^0$}-language that cannot be recognized by an {UHAT} encoder. On the positive side, we show that {UHAT} encoders can recognize a rich fragment of {$\mathsf{AC}^0$}-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from {$\mathsf{AC}^0$}. We then show that {AHAT} encoders can recognize all languages of our logic even when we enrich it with counting terms. Using these results, we obtain a characterization of which counting properties are expressible by {UHAT} and {AHAT}, in relation to regular languages.},
  address = {Vienna, Austria},
  author = {Pablo Barcel√≥ and Alexander Kozachinskiy and Anthony Widjaja Lin and Vladimir V. Podolskii},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/barcel2024logical.pdf:pdf},
  keywords = {Transformer encoders, Languages, Circuit complexity, First order logic, Linear temporal logic, Counting terms},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: foundations of machine learning. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=gbrHZq07mq},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Logical Languages Accepted by Transformer Encoders with Hard Attention},
  url = {https://openreview.net/forum?id=gbrHZq07mq},
  year = {2024}
}

@inproceedings{bardou2024relaxing,
  abstract = {Bayesian Optimization ({BO}) is typically used to optimize an unknown function $f$ that is noisy and costly to evaluate, by exploiting an acquisition function that must be maximized at each optimization step. Even if provably asymptotically optimal {BO} algorithms are efficient at optimizing low-dimensional functions, scaling them to high-dimensional spaces remains an open problem, often tackled by assuming an additive structure for $f$. By doing so, {BO} algorithms typically introduce additional restrictive assumptions on the additive structure that reduce their applicability domain. This work proposes {DuMBO}, a decentralized {BO} algorithm that aims to relax additive structure constraints while maintaining optimization performance.},
  address = {Vienna, Austria},
  author = {Anthony Bardou and Patrick Thiran and Thomas Begin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bardou2024relaxing.pdf:pdf},
  keywords = {Bayesian Optimization, Online Learning, Black-box Optimization, Decentralized Optimization},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: optimization for {ML}. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=de1218PoEl},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Relaxing the Additivity Constraints in Decentralized No-Regret High-Dimensional {B}ayesian Optimization},
  url = {https://openreview.net/forum?id=de1218PoEl},
  year = {2024}
}

@inproceedings{bastos2024beyond,
  abstract = {We present the Evolving Graph {F}ourier Transform ({EFT}), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the {L}aplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The {EFT} method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs.},
  address = {Vienna, Austria},
  author = {Anson Bastos and Kuldeep Singh and Abhishek Nadgeri and Manish Singh and Toyotaro Suzumura},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bastos2024beyond.pdf:pdf},
  keywords = {Temporal Graphs, Graph Neural Networks, Spectral Methods, Fourier Transform, Dynamic Graphs},
  month = {5},
  note = {Accepted to ICLR 2024 poster session. Primary area: learning on graphs and other geometries \& topologies. Published: 16 Jan 2024},
  pdf = {https://openreview.net/pdf?id=uvFhCUPjtI},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Beyond Spatio-Temporal Representations: Evolving {F}ourier Transform for Temporal Graphs},
  url = {https://openreview.net/forum?id=uvFhCUPjtI},
  year = {2024}
}

@inproceedings{basu2024localizing,
  abstract = {Text-to-Image Diffusion Models such as Stable-Diffusion and Imagen have achieved unprecedented quality of photorealism with state-of-the-art {FID} scores on {MS-COCO} and other generation benchmarks. Given a caption, image generation requires fine-grained knowledge about attributes such as object structure, style, and viewpoint amongst others. Where does this information reside in text-to-image generative models? In our paper, we tackle this question and understand how knowledge corresponding to distinct visual attributes is stored in large-scale text-to-image diffusion models. We adapt Causal Mediation Analysis for text-to-image models and trace knowledge about distinct visual attributes to various (causal) components in the (i) {UNet} and (ii) text-encoder of the diffusion model. In particular, we show that unlike large-language models, knowledge about different attributes is not localized in isolated components, but is instead distributed amongst a set of components in the conditional {UNet}. These sets of components are often distinct for different visual attributes (e.g., style / objects). Remarkably, we find that the text-encoder in public text-to-image models such as Stable-Diffusion contains only one causal state across different visual attributes, and this is the first self-attention layer corresponding to the last subject token of the attribute in the caption. This is in stark contrast to the causal states in other language models which are often the mid-{MLP} layers. Based on this observation of only one causal state in the text-encoder, we introduce a fast, data-free model editing method {DiffQuickFix} which can effectively edit concepts (remove or update knowledge) in text-to-image models. {DiffQuickFix} can edit (ablate) concepts in under a second with a closed-form update, providing a significant 1000x speedup and comparable editing performance to existing fine-tuning based editing methods.},
  author = {Samyadeep Basu and Nanxuan Zhao and Vlad I Morariu and Soheil Feizi and Varun Manjunatha},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/basu2024localizing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Qmw9ne6SOQ},
  publisher = {OpenReview.net},
  title = {Localizing and Editing Knowledge In Text-to-Image Generative Models},
  url = {https://openreview.net/forum?id=Qmw9ne6SOQ},
  year = {2024}
}

@inproceedings{batatia2024equivariant,
  abstract = {Graph Neural Networks ({GNNs}), especially message-passing neural networks ({MPNNs}), have emerged as powerful architectures for learning on graphs in diverse applications. However, {MPNNs} face challenges when modeling non-local interactions in systems such as large conjugated molecules, metals, or amorphous materials due to oversmoothing and oversquashing. Although Spectral {GNNs} and traditional neural networks such as recurrent neural networks and transformers mitigate these challenges, they often lack extensivity, adaptability, generalizability, computational efficiency, or fail to capture detailed structural relationships or symmetries in the data. This paper introduces Matrix Function Neural Networks ({MFNs}) as a novel approach that parameterizes non-local interactions through analytic matrix equivariant functions. Employing resolvent expansions offers a straightforward implementation and the potential for linear scaling with system size. The {MFN} architecture operates on equivariant matrix operators of graphs to capture non-local interactions effectively and achieves state-of-the-art performance in standard graph benchmarks, such as the {ZINC} and {TU} datasets, and is able to capture intricate non-local interactions in quantum systems, paving the way to new state-of-the-art force fields.},
  author = {Ilyes Batatia and Lars Leon Schaaf and Gabor Csanyi and Christoph Ortner and Felix Andreas Faber},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/batatia2024equivariant.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=yrgQdA5NkI},
  publisher = {OpenReview.net},
  title = {Equivariant Matrix Function Neural Networks},
  url = {https://openreview.net/forum?id=yrgQdA5NkI},
  year = {2024}
}

@inproceedings{batra2024proximal,
  abstract = {Training generally capable agents that thoroughly explore their environment and learn new and diverse skills is a long-term goal of robot learning. Quality Diversity Reinforcement Learning ({QD-RL}) is an emerging research area that blends the best aspects of both fields -- Quality Diversity ({QD}) provides a principled form of exploration and produces collections of behaviorally diverse agents, while Reinforcement Learning ({RL}) provides a powerful performance improvement operator enabling generalization across tasks and dynamic environments. However, existing {QD-RL} approaches have been constrained to sample efficient, deterministic off-policy {RL} algorithms and/or evolution strategies and struggle with highly stochastic environments. For the first time, this paper adapts on-policy {RL}, specifically Proximal Policy Optimization ({PPO}), to the Differentiable Quality Diversity ({DQD}) framework and proposes several changes that enable efficient optimization and discovery of novel skills on high-dimensional, stochastic robotics tasks. Their new algorithm, Proximal Policy Gradient Arborescence ({PPGA}), achieves state-of-the-art results including a 4x improvement on challenging continuous control locomotion tasks, discovering diverse high-performing gaits.},
  author = {Sumeet Batra and Bryon Tjanaka and Matthew Christopher Fontaine and Aleksei Petrenko and Stefanos Nikolaidis and Gaurav S. Sukhatme},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/batra2024proximal.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=TFKIfhvdmZ},
  publisher = {OpenReview.net},
  title = {Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning},
  url = {https://openreview.net/forum?id=TFKIfhvdmZ},
  year = {2024}
}

@inproceedings{battiloro2024latent,
  abstract = {Latent Graph Inference ({LGI}) relaxed the reliance of Graph Neural Networks ({GNNs}) on a given graph topology by dynamically learning it. However, most of {LGI} methods assume to have a (noisy, incomplete, improvable, ...) input graph to rewire and can solely learn regular graph topologies. In the wake of the success of Topological Deep Learning ({TDL}), this work studies Latent Topology Inference ({LTI}) for learning higher-order cell complexes (with sparse and not regular topology) describing multi-way interactions between data points. The authors introduce the Differentiable Cell Complex Module ({DCM}), a novel learnable function that computes cell probabilities in the complex to improve the downstream task. The {DCM} extends latent graph inference to higher-order topological structures, specifically cell complexes, which can capture more complex multi-way relationships in data beyond traditional pairwise graph connections.},
  author = {Claudio Battiloro and Indro Spinelli and Lev Telyatnikov and Michael M. Bronstein and Simone Scardapane and Paolo Di Lorenzo},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/battiloro2024latent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0JsRZEGZ7L},
  publisher = {OpenReview.net},
  title = {From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module},
  url = {https://openreview.net/forum?id=0JsRZEGZ7L},
  year = {2024}
}

@inproceedings{bazaga2024unsupervised,
  abstract = {This paper introduces {SFAVEL} (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised pretraining framework that leverages pre-trained language models to distill self-supervised features into high-quality claim-fact alignments without the need for annotations. The approach addresses fact verification by enabling a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. {SFAVEL} achieves state-of-the-art results on {FB15k-237} (+5.3\% Hits@1) and {FEVER} (+8\% accuracy) with linear evaluation, demonstrating the effectiveness of self-supervised pretraining for fact verification tasks.},
  author = {Adri√°n Bazaga and Pietro Li√≤ and Gos Micklem},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/bazaga2024unsupervised.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1mjsP8RYAw},
  publisher = {OpenReview.net},
  title = {Unsupervised Pretraining for Fact Verification by Language Model Distillation},
  url = {https://openreview.net/forum?id=1mjsP8RYAw},
  year = {2024}
}

@inproceedings{bdeir2024fully,
  abstract = {Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. While hyperbolic neural networks ({HNNs}) are promising for learning feature representations in such spaces, current {HNNs} in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. This paper proposes {HCNN}, a generalization of the convolutional neural network that learns latent feature representations in hyperbolic spaces in every layer, fully leveraging the benefits of hyperbolic geometry. Based on the Lorentz model, the authors generalize fundamental components of {CNNs} and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression. Experiments on standard vision tasks demonstrate the promising performance of the {HCNN} framework in both hybrid and fully hyperbolic settings, leading to better image representations and performance.},
  author = {Ahmad Bdeir and Kristian Schwethelm and Niels Landwehr},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/bdeir2024fully.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=ekz1hN5QNh},
  publisher = {OpenReview.net},
  title = {Fully Hyperbolic Convolutional Neural Networks for Computer Vision},
  url = {https://openreview.net/forum?id=ekz1hN5QNh},
  year = {2024}
}

@inproceedings{beaini2024towards,
  abstract = {Recently, pre-trained foundation models have enabled significant advancements in multiple fields. In molecular machine learning, however, where datasets are often hand-curated, and hence typically small, the lack of datasets with labeled features, and codebases to manage those datasets, has hindered the development of foundation models. This work presents seven novel datasets categorized by size into three distinct categories: {ToyMix}, {LargeMix} and {UltraLarge}, pushing the boundaries in both the scale and the diversity of supervised labels for molecular learning. The datasets cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature. In comparison, these datasets contain 300 times more data points than the widely used {OGB-LSC} {PCQM4Mv2} dataset, and 13 times more than the quantum-only {QM1B} dataset. The authors also created the Graphium graph machine learning library to support multi-task learning on their proposed datasets.},
  author = {Dominique Beaini and Shenyang Huang and Joao Alex Cunha and Zhiyi Li and Gabriela Moisescu-Pareja and Oleksandr Dymov and Samuel Maddrell-Mander and Callum McLean and Frederik Wenkel and Luis M√ºller and Jama Hussein Mohamud and Ali Parviz and Michael Craig and Michal Koziarski and Jiarui Lu and Zhaocheng Zhu and Cristian Gabellini and Kerstin Klaser and Josef Dean and Cas Wognum and Maciej Sypetkowski and Guillaume Rabusseau and Reihaneh Rabbany and Jian Tang and Christopher Morris and Mirco Ravanelli and Guy Wolf and Prudencio Tossou and Hadrien Mary and Therence Bois and Andrew W. Fitzgibbon and Blazej Banaszewski and Chad Martin and Dominic Masters},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/beaini2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Zc2aIcucwc},
  publisher = {OpenReview.net},
  title = {Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets},
  url = {https://openreview.net/forum?id=Zc2aIcucwc},
  year = {2024}
}

@inproceedings{beikmohammadi2024neural,
  abstract = {Stability guarantees are crucial when ensuring a fully autonomous robot does not take undesirable or potentially harmful actions. Unfortunately, global stability guarantees are hard to provide in dynamical systems learned from data, especially when the learned dynamics are governed by neural networks. This paper proposes a novel methodology to learn neural contractive dynamical systems, where their neural architecture ensures contraction, and hence, global stability. The paper introduces the Neural Contractive Dynamical System ({NCDS}), providing a flexible and stable approach for learning contractive dynamics. To efficiently scale the method to high-dimensional dynamical systems, the authors develop a variant of the variational autoencoder that learns dynamics in a low-dimensional latent representation space while retaining contractive stability after decoding. The approach extends to high-dimensional systems with obstacle avoidance capabilities.},
  author = {Hadi Beik-Mohammadi and S√∏ren Hauberg and Georgios Arvanitidis and Nadia Figueroa and Gerhard Neumann and Leonel Rozo},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/beikmohammadi2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=iAYIRHOYy8},
  publisher = {OpenReview.net},
  title = {Neural Contractive Dynamical Systems},
  url = {https://openreview.net/forum?id=iAYIRHOYy8},
  year = {2024}
}

@inproceedings{bekkers2024fast,
  abstract = {Based on the theory of homogeneous spaces, this paper derives geometrically optimal edge attributes to be used within the flexible message-passing framework and formalizes the notion of weight sharing in convolutional networks as the sharing of message functions over point-pairs that should be treated equally. The theory of homogeneous spaces tells us how to do group convolutions with feature maps over the homogeneous space of positions, position and orientations, and the group {SE}(3) itself. Among these, ‚Ñù¬≥√óS¬≤ is an optimal choice due to the ability to represent directional information, which ‚Ñù¬≥ methods cannot, and it significantly enhances computational efficiency compared to indexing features on the full {SE}(3) group. As an application of the theory, the authors develop {PONITA}, an efficient equivariant group convolutional network for processing 3D point clouds with state-of-the-art results in accuracy and speed on five different benchmarks. {PONITA} is an equivariant model that does not require working with steerable/Clebsch-Gordan methods, but has the same capabilities in that it can handle scalars and vectors equally well, and is much faster than typical steerable/tensor field networks.},
  author = {Erik J. Bekkers and Sharvaree P. Vadgama and Rob Hesselink and Putri A. van der Linden and David W. Romero},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/bekkers2024fast.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dPHLbUqGbr},
  publisher = {OpenReview.net},
  title = {Fast, Expressive {SE}(n) Equivariant Networks through Weight-Sharing in Position-Orientation Space},
  url = {https://openreview.net/forum?id=dPHLbUqGbr},
  year = {2024}
}

@inproceedings{belm2024models,
  abstract = {This paper addresses the question: Do language models still exhibit gender bias in non-stereotypical settings? The authors introduce {UnStereoEval} ({USE}), a novel framework tailored for investigating gender bias in stereotype-free scenarios. {USE} defines a sentence-level score based on pretraining data statistics to determine if sentences contain minimal word-gender associations. To systematically benchmark the fairness of popular language models in stereotype-free scenarios, they utilize {USE} to automatically generate benchmarks without any gender-related language. The results show that models demonstrate fair behavior in only 9--41\% of stereotype-free sentences, suggesting that bias does not solely stem from the presence of gender-related words. By leveraging {USE}'s sentence-level score, they also repurposed prior gender bias benchmarks (Winobias and Winogender) for non-stereotypical evaluation, revealing low fairness across all 28 evaluated models.},
  author = {Catarina G. Bel√©m and Preethi Seshadri and Yasaman Razeghi and Sameer Singh},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/belm2024models.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=w1JanwReU6},
  publisher = {OpenReview.net},
  title = {Are Models Biased on Text without Gender-related Language?},
  url = {https://openreview.net/forum?id=w1JanwReU6},
  year = {2024}
}

@inproceedings{belouadi2024automatikz,
  abstract = {Generating bitmap graphics from text has gained considerable attention, yet for scientific figures, vector graphics are often preferred. Given that vector graphics are typically encoded using low-level graphics primitives, generating them directly is difficult. To address this, we propose the use of {TikZ}, a well-known abstract graphics language that can be compiled to vector graphics, as an intermediate representation of scientific figures. {TikZ} offers human-oriented, high-level commands, thereby facilitating conditional language modeling with any large language model. To this end, we introduce {DaTikZ} the first large-scale {TikZ} dataset, consisting of 120k {TikZ} drawings aligned with captions. We fine-tune {LLaMA} on {DaTikZ}, as well as our new model {CLiMA}, which augments {LLaMA} with multimodal {CLIP} embeddings. In both human and automatic evaluation, {CLiMA} and {LLaMA} outperform commercial {GPT-4} and {Claude} 2 in terms of similarity to human-created figures, with {CLiMA} additionally improving text-image alignment. Our detailed analysis shows that all models generalize well and are not susceptible to memorization. {GPT-4} and {Claude} 2, however, tend to generate more simplistic figures compared to both humans and our models. We make our framework, {AutomaTikZ}, along with model weights and datasets, publicly available.},
  address = {Vienna, Austria},
  author = {Jonas Belouadi and Anne Lauscher and Steffen Eger},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/belouadi2024automatikz.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=v3K5TVP8kZ},
  publisher = {OpenReview.net},
  title = {{AutomaTikZ}: Text-Guided Synthesis of Scientific Vector Graphics with {TikZ}},
  url = {https://openreview.net/forum?id=v3K5TVP8kZ},
  year = {2024}
}

@inproceedings{benchetrit2024brain,
  abstract = {In the past five years, the use of generative and foundational {AI} systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging ({fMRI}) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 {Hz}) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography ({MEG}), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 {Hz}). For this, we develop an {MEG} decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an {MEG} module trained end-to-end and iii) a pretrained image generator. Our {MEG} decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain responses to images are best decoded with {DINOv2}, a recent foundational image model. Third, image retrievals and generations both suggest that high-level visual features can be decoded from {MEG} signals, although the same approach applied to 7T {fMRI} also recovers better low-level features. Overall, these results, while preliminary, provide an important step towards the decoding -- in real-time -- of the visual processes continuously unfolding within the human brain.},
  address = {Vienna, Austria},
  author = {Yohann Benchetrit and Hubert J. Banville and Jean-R{\'{e}}mi King},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/benchetrit2024brain.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3y1K6buO8c},
  publisher = {OpenReview.net},
  title = {Brain decoding: toward real-time reconstruction of visual perception},
  url = {https://openreview.net/forum?id=3y1K6buO8c},
  year = {2024}
}

@inproceedings{bencomo2024implicit,
  abstract = {Bayesian filtering approximates the true underlying behavior of a time-varying system by inverting an explicit generative model to convert noisy measurements into state estimates. This process typically requires either storage, inversion, and multiplication of large matrices or Monte Carlo estimation, neither of which are practical in high-dimensional state spaces such as the weight spaces of artificial neural networks. Here, we frame the standard Bayesian filtering problem as optimization over a time-varying objective. Instead of maintaining matrices for the filtering equations or simulating particles, we specify an optimizer that defines the Bayesian filter implicitly. In the linear-Gaussian setting, we show that every Kalman filter has an equivalent formulation using $K$ steps of gradient descent.},
  address = {Vienna, Austria},
  author = {Gianluca M. Bencomo and Jake Snell and Thomas L. Griffiths},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/bencomo2024implicit.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=auUngos7eR},
  publisher = {OpenReview.net},
  title = {Implicit Maximum a Posteriori Filtering via Adaptive Optimization},
  url = {https://openreview.net/forum?id=auUngos7eR},
  year = {2024}
}

@inproceedings{benita2024diffar,
  abstract = {We propose a diffusion probabilistic end-to-end model for generating a raw speech waveform. The proposed model is autoregressive, generating overlapping frames sequentially, where each frame is conditioned on a portion of the previously generated one. Hence, our model can effectively synthesize an unlimited speech duration while preserving high-fidelity synthesis and temporal coherence. {DiffAR} is an end-to-end model that directly works on the raw waveform without any intermediate representation such as the Mel-spectrogram. Specifically, it allows the creation of local acoustic behaviors, like vocal fry, which makes the overall waveform sounds more natural. Furthermore, the proposed diffusion model is stochastic and not deterministic; therefore, each inference generates a slightly different waveform variation, enabling abundance of valid realizations. It can operate in an unconditional mode, where no text is provided, or in a conditional mode, where text and other linguistic parameters are used as input. Experiments show that the proposed model generates speech with superior quality compared with other state-of-the-art neural speech generation systems.},
  address = {Vienna, Austria},
  author = {Roi Benita and Michael Elad and Joseph Keshet},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/benita2024diffar.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GTk0AdOYLq},
  publisher = {OpenReview.net},
  title = {{DiffAR}: Denoising Diffusion Autoregressive Model for Raw Speech Waveform Generation},
  url = {https://openreview.net/forum?id=GTk0AdOYLq},
  year = {2024}
}

@inproceedings{benoit2024unraveling,
  abstract = {Supervised learning datasets may contain multiple cues that explain the training set equally well, i.e., learning any of them would lead to the correct predictions on the training data. However, many of them can be spurious, i.e., lose their predictive power under a distribution shift and consequently fail to generalize to out-of-distribution ({OOD}) data. Recently developed ``diversification'' methods approach this problem by finding multiple diverse hypotheses that rely on different features. In this paper, we study this class of methods and identify the key components contributing to their {OOD} generalization abilities. We show that diversification methods are highly sensitive to the distribution of the unlabeled data used for diversification and can underperform significantly when away from a method-specific sweet spot. We distill the critical design factors of current state-of-the-art methods and show that diversification alone is insufficient for {OOD} generalization. The choice of the used learning algorithm, e.g., the model's architecture and pretraining, is crucial. In standard experiments, using the second-best choice leads to an up to 20\% absolute drop in accuracy. Moreover, the optimal choice of learning algorithm depends on the unlabeled data and vice versa, i.e., they are co-dependent. The above pitfalls cannot be alleviated by increasing the number of diverse hypotheses, the major feature of diversification methods.},
  address = {Vienna, Austria},
  author = {Harold Benoit and Liangze Jiang and Andrei Atanov and O\v{g}uzhan Fatih Kar and Mattia Rigotti and Amir Zamir},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/benoit2024unraveling.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Lvf7GnaLru},
  publisher = {OpenReview.net},
  title = {Unraveling the Key Components of {OOD} Generalization via Diversification},
  url = {https://openreview.net/forum?id=Lvf7GnaLru},
  year = {2024}
}

@inproceedings{benton2024nearly,
  abstract = {Denoising diffusions are a powerful method to generate approximate samples from high-dimensional data distributions. Recent results provide polynomial bounds on their convergence rate, assuming $L^2$-accurate scores. Until now, the tightest bounds were either superlinear in the data dimension or required strong smoothness assumptions. We provide the first convergence bounds which are linear in the data dimension (up to logarithmic factors) assuming only finite second moments of the data distribution. We show that diffusion models require at most $\tilde{O}(\frac{d \log^2(1/\delta)}{\varepsilon^2})$ steps to approximate an arbitrary distribution on $\mathbb{R}^d$ corrupted with Gaussian noise of variance $\delta$ to within $\varepsilon^2$ in {KL} divergence. Our proof extends the Girsanov-based methods of previous works. We introduce a refined treatment of the error from discretizing the reverse {SDE} inspired by stochastic localization.},
  address = {Vienna, Austria},
  author = {Joe Benton and Valentin De Bortoli and Arnaud Doucet and George Deligiannidis},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/benton2024nearly.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=r5njV3BsuD},
  publisher = {OpenReview.net},
  title = {Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization},
  url = {https://openreview.net/forum?id=r5njV3BsuD},
  year = {2024}
}

@inproceedings{berglund2024reversal,
  abstract = {We expose a surprising failure of generalization in auto-regressive large language models ({LLMs}). If a model is trained on a sentence of the form ``A is B'', it will not automatically generalize to the reverse direction ``B is A''. This is the Reversal Curse. For instance, if a model is trained on ``Valentina Tereshkova was the first woman to travel to space'', it will not automatically be able to answer the question, ``Who was the first woman to travel to space?''. Moreover, the likelihood of the correct answer (``Valentina Tereshkova'') will not be higher than for a random name. We provide evidence for the Reversal Curse by finetuning {GPT-3} and {Llama-1} on fictitious statements such as ``Uriah Hawthorne is the composer of Abyssal Melodies'' and showing that they fail to correctly answer ``Who composed Abyssal Melodies?''. The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate {ChatGPT} ({GPT-3.5} and {GPT-4}) on questions about real-world celebrities, such as ``Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]'' and the reverse ``Who is Mary Lee Pfeiffer's son?'' {GPT-4} correctly answers questions like the former 79\% of the time, compared to 33\% for the latter.},
  address = {Vienna, Austria},
  author = {Lukas Berglund and Meg Tong and Maximilian Kaufmann and Mikita Balesni and Asa Cooper Stickland and Tomasz Korbak and Owain Evans},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/berglund2024reversal.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GPKTIktA0k},
  publisher = {OpenReview.net},
  title = {The Reversal Curse: {LLMs} trained on ``{A} is {B}'' fail to learn ``{B} is {A}''},
  url = {https://openreview.net/forum?id=GPKTIktA0k},
  year = {2024}
}

@inproceedings{bergmeister2024efficient,
  abstract = {Most existing graph generation methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity. Drawing inspiration from natural growth patterns, we model graph generation as a progressive expansion process, starting from a single node and iteratively growing it to a complete graph. We present a new neural network architecture, Local {PPGN}, tailored to our approach to graph generation.},
  address = {Vienna, Austria},
  author = {Andreas Bergmeister and Karolis Martinkus and Nathana\"{e}l Perraudin and Roger Wattenhofer},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/bergmeister2024efficient.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2XkTz7gdpc},
  publisher = {OpenReview.net},
  title = {Efficient and Scalable Graph Generation through Iterative Local Expansion},
  url = {https://openreview.net/forum?id=2XkTz7gdpc},
  year = {2024}
}

@inproceedings{bernasconi2024bandits,
  abstract = {The bandits with knapsack ({BwK}) framework models online decision-making problems in which an agent makes a sequence of decisions subject to resource consumption constraints. The traditional model assumes that each action consumes a non-negative amount of resources and the process ends when the initial budgets are fully depleted. We study a natural generalization of the {BwK} framework which allows non-monotonic resource utilization, i.e., resources can be replenished by a positive amount. We propose a best-of-both-worlds primal-dual template that can handle any online learning problem with replenishment for which a suitable primal regret minimizer exists. In particular, we provide the first positive results for the case of adversarial inputs by showing that our framework guarantees a constant competitive ratio $\alpha$ when $B = \Omega(T)$ or when the possible per-round replenishment is a positive constant.},
  address = {Vienna, Austria},
  author = {Martino Bernasconi and Matteo Castiglioni and Andrea Celli and Federico Fusco},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/bernasconi2024bandits.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=yBIJRIYTqa},
  publisher = {OpenReview.net},
  title = {Bandits with Replenishable Knapsacks: the Best of both Worlds},
  url = {https://openreview.net/forum?id=yBIJRIYTqa},
  year = {2024}
}

@inproceedings{bertrand2024stability,
  abstract = {Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models will be trained on both clean and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets -- from classical training on real data to self-consuming generative models trained on purely synthetic data. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion of clean training data (w.r.t. synthetic data) is large enough. We empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows and state-of-the-art diffusion models on {CIFAR-10} and {FFHQ}.},
  address = {Vienna, Austria},
  author = {Quentin Bertrand and Avishek Joey Bose and Alexandre Duplessis and Marco Jiralerspong and Gauthier Gidel},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/bertrand2024stability.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=JORAfH2xFd},
  publisher = {OpenReview.net},
  title = {On the Stability of Iterative Retraining of Generative Models on their own Data},
  url = {https://openreview.net/forum?id=JORAfH2xFd},
  year = {2024}
}

@inproceedings{bthune2024dpsgd,
  abstract = {State-of-the-art approaches for training Differentially Private ({DP}) Deep Neural Networks ({DNN}) face difficulties to estimate tight bounds on the sensitivity of the network's layers, and instead rely on a process of per-sample gradient clipping. This clipping process not only biases the direction of gradients but also proves costly both in memory consumption and in computation. To provide sensitivity bounds and bypass the drawbacks of the clipping process, we propose to rely on {L}ipschitz constrained networks. Our theoretical analysis reveals an unexplored link between the {L}ipschitz constant with respect to their input and the one with respect to their parameters. By bounding the {L}ipschitz constant of each layer with respect to its parameters, we prove that we can train these networks with privacy guarantees. Our analysis not only allows the computation of the aforementioned sensitivities at scale, but also provides guidance on how to maximize the gradient-to-noise ratio for fixed privacy guarantees. To facilitate the application of {L}ipschitz networks and foster robust and certifiable learning under privacy guarantees, we provide a Python package that implements building blocks allowing the construction and private training of such networks.},
  author = {Louis B√©thune and Thomas Massena and Thibaut Boissin and Aur√©lien Bellet and Franck Mamalet and Yannick Prudent and Corentin Friedrich and Mathieu Serrurier and David Vigouroux},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bthune2024dpsgd.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BEyEziZ4R6},
  publisher = {OpenReview.net},
  title = {{DP-SGD} Without Clipping: The {L}ipschitz Neural Network Way},
  url = {https://openreview.net/forum?id=BEyEziZ4R6},
  year = {2024}
}

@inproceedings{bevilacqua2024efficient,
  abstract = {Subgraph {GNN}s are provably expressive neural architectures that learn graph representations from sets of subgraphs. Unfortunately, their applicability is hampered by the computational complexity associated with performing message passing on many subgraphs. In this paper, we consider the problem of learning to select a small subset of the large set of possible subgraphs in a data-driven fashion. We first motivate the problem by proving that there are families of {WL}-indistinguishable graphs for which there exist efficient subgraph selection policies: small subsets of subgraphs that can already identify all the graphs within the family. We then propose a new approach, called {P}olicy-{L}earn, that learns how to select subgraphs in an iterative manner by leveraging reinforcement learning techniques. Our experimental evaluation demonstrates that our approach significantly reduces the computational complexity of subgraph {GNN}s while preserving their predictive performance on various graph learning tasks.},
  author = {Beatrice Bevilacqua and Moshe Eliasof and Eli A. Meirom and Bruno Ribeiro and Haggai Maron},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bevilacqua2024efficient.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gppLqZLQeY},
  publisher = {OpenReview.net},
  title = {Efficient Subgraph {GNN}s by Learning Effective Selection Policies},
  url = {https://openreview.net/forum?id=gppLqZLQeY},
  year = {2024}
}

@inproceedings{bhargava2024when,
  abstract = {Offline reinforcement learning ({RL}) allows agents to learn effective, return-maximizing policies from a static dataset. Three popular algorithms for offline {RL} are Conservative {Q}-Learning ({CQL}), Behavior Cloning ({BC}), and Decision Transformer ({DT}), from the class of {Q}-Learning, Imitation Learning, and Sequence Modeling respectively. A key open question is: which algorithm is preferred under what conditions? We study this question empirically by exploring the performance of these algorithms across the commonly used {D}4{RL} and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality, task complexity, and stochasticity. Our findings suggest that the conventional wisdom around these algorithms is often incomplete or inaccurate. We find that {DT} can perform as well or better than {CQL} and {BC} even when trained on suboptimal data, and that all three algorithms benefit from data augmentation techniques.},
  author = {Prajjwal Bhargava and Rohan Chitnis and Alborz Geramifard and Shagun Sodhani and Amy Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bhargava2024when.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vpV7fOFQy4},
  publisher = {OpenReview.net},
  title = {When should we prefer Decision Transformers for Offline Reinforcement Learning?},
  url = {https://openreview.net/forum?id=vpV7fOFQy4},
  year = {2024}
}

@inproceedings{bhattacharyya2024look,
  abstract = {Multi-modal language models ({LM}) have recently shown promising performance in high-level reasoning tasks on videos. However, existing methods still fall short in tasks like causal or compositional spatiotemporal reasoning over actions, in which model predictions need to be grounded in fine-grained low-level details, such as object motions and object interactions. In this work, we propose training an {LM} end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities. We show that a two-stream video encoder with spatiotemporal attention is effective at capturing the required static and motion-based cues in the video. We demonstrate the effectiveness of our approach on four challenging video reasoning benchmarks: {ACRE}, {CATER}, Something-Else and {STAR}. Our method outperforms the prior state-of-the-art, that is based on highly task-specific architectures, by a large margin---highlighting how our general-purpose model can perform varied and complex spatiotemporal reasoning tasks in videos including causal, compositional and situated reasoning.},
  author = {Apratim Bhattacharyya and Sunny Panchal and Reza Pourreza and Mingu Lee and Pulkit Madan and Roland Memisevic},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bhattacharyya2024look.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jhPvuc7kxB},
  publisher = {OpenReview.net},
  title = {Look, Remember and Reason: Grounded Reasoning in Videos with Language Models},
  url = {https://openreview.net/forum?id=jhPvuc7kxB},
  year = {2024}
}

@inproceedings{bhattamishra2024understanding,
  abstract = {In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can match the performance of gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models ({LLM}s). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a *teaching sequence*, i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement *two distinct* algorithms to solve a *single* task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples.},
  author = {Satwik Bhattamishra and Arkil Patel and Phil Blunsom and Varun Kanade},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bhattamishra2024understanding.pdf:pdf},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ekeyCgeRfC},
  publisher = {OpenReview.net},
  title = {Understanding In-Context Learning in Transformers and {LLM}s by Learning to Learn Discrete Functions},
  url = {https://openreview.net/forum?id=ekeyCgeRfC},
  year = {2024}
}

@inproceedings{bian2024p,
  abstract = {Neural surface reconstruction is sensitive to the camera pose noise, even if state-of-the-art pose estimators like {COLMAP} or {ARK}it are used. More importantly, existing Pose-{NeRF} joint optimisation methods have struggled to improve pose accuracy in challenging real-world scenarios. To overcome the challenges, we introduce the pose residual field ({P}o{RF}), a novel implicit representation that uses an {MLP} for regressing pose updates. Compared with the conventional per-frame pose parameter optimisation, this new representation is more robust due to parameter sharing that leverages global information over the entire sequence. Furthermore, we propose an epipolar geometry loss to enhance the supervision that leverages the correspondences exported from {COLMAP} results without the extra computational overhead. Extensive experiments on challenging real-world datasets show that our method significantly improves pose accuracy and surface reconstruction quality compared to previous joint optimisation methods.},
  author = {Jia-Wang Bian and Wenjing Bian and Victor Adrian Prisacariu and Philip H. S. Torr},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bian2024p.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eBeECjacpw},
  publisher = {OpenReview.net},
  title = {{P}o{RF}: Pose Residual Field for Accurate Neural Surface Reconstruction},
  url = {https://openreview.net/forum?id=eBeECjacpw},
  year = {2024}
}

@inproceedings{bigelow2024incontext,
  abstract = {Large language models ({LLM}s) trained on huge corpora of text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of {LLM} capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a framework that enables us to analyze in-context learning dynamics to understand latent concepts underlying {LLM}s' behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length.},
  author = {Eric J. Bigelow and Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka and Tomer D. Ullman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bigelow2024incontext.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=62K7mALO2q},
  publisher = {OpenReview.net},
  title = {In-Context Learning Dynamics with Random Binary Sequences},
  url = {https://openreview.net/forum?id=62K7mALO2q},
  year = {2024}
}

@inproceedings{binz2024turning,
  abstract = {Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. After finetuning them on data from psychological experiments, these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. Their representations contain the information necessary to model behavior on the level of individual subjects, and finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. This suggests that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.},
  author = {Marcel Binz and Eric Schulz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/binz2024turning.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eiC4BKypf1},
  publisher = {OpenReview.net},
  title = {Turning large language models into cognitive models},
  url = {https://openreview.net/forum?id=eiC4BKypf1},
  year = {2024}
}

@inproceedings{bishnoi2024b,
  abstract = {{BroGNet} (Brownian graph neural networks) combines stochastic differential equations ({SDE}s) and Graph Neural Networks ({GNN}s) to learn Brownian dynamics directly from trajectory data. Existing neural network approaches focus only on deterministic dynamics like Newtonian or Hamiltonian systems, but {BroGNet} addresses the gap in learning stochastic dynamics. The architecture is modified to enforce linear momentum conservation of the system, which provides superior performance on learning dynamics. We demonstrate {BroGNet} on linear spring, linear spring with binary particle types, and non-linear spring systems following Brownian dynamics at finite temperatures, showing that {BroGNet} significantly outperforms proposed baselines. {BroGNet} demonstrates zero-shot generalizability to simulate unseen system sizes that are two orders of magnitude larger and to different temperatures than those used during training.},
  author = {Suresh Bishnoi and Jayadeva and Sayan Ranu and N. M. Anoop Krishnan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bishnoi2024b.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2iGiSHmeAN},
  publisher = {OpenReview.net},
  title = {{B}ro{GN}et: Momentum-Conserving Graph Neural Stochastic Differential Equation for Learning Brownian Dynamics},
  url = {https://openreview.net/forum?id=2iGiSHmeAN},
  year = {2024}
}

@inproceedings{black2024training,
  abstract = {Diffusion models are a class of flexible generative models trained with an approximation to the log-likelihood objective. However, most use cases of diffusion models are not concerned with likelihoods, but instead with downstream objectives such as human-perceived image quality or drug effectiveness. In this paper, we investigate reinforcement learning methods for directly optimizing diffusion models for such objectives. We describe how posing denoising as a multi-step decision-making problem enables a class of policy gradient algorithms, which we refer to as denoising diffusion policy optimization ({DDPO}), for optimizing diffusion models. We use {DDPO} to finetune Stable Diffusion on objectives that are difficult to express via prompting, such as image compressibility, and those derived from human feedback, such as aesthetic quality. Finally, we show that {DDPO} can improve prompt-image alignment using feedback from a vision-language model without the need for additional data collection or human annotation.},
  author = {Kevin Black and Michael Janner and Yilun Du and Ilya Kostrikov and Sergey Levine},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/black2024training.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=YCWjhGrJFD},
  publisher = {OpenReview.net},
  title = {Training Diffusion Models with Reinforcement Learning},
  url = {https://openreview.net/forum?id=YCWjhGrJFD},
  year = {2024}
}

@inproceedings{black2024zeroshot,
  abstract = {If generalist robots are to operate in truly unstructured environments, they need to be able to recognize and reason about novel objects and scenarios. Such objects and scenarios might not be present in the robot's own training data. We propose {SuSIE}, a method that leverages an image editing diffusion model to act as a high-level planner by proposing intermediate subgoals that a low-level controller attains. Specifically, we fine-tune {InstructPix2Pix} on robot data such that it outputs a hypothetical future observation given the robot's current observation and a language command. We then use the same robot data to train a low-level goal-conditioned policy to reach a given image observation. We find that when these components are combined, the resulting system exhibits robust generalization capabilities. The high-level planner utilizes its Internet-scale pre-training and visual understanding to guide the low-level goal-conditioned policy, achieving significantly better generalization than conventional language-conditioned policies. The paper achieved state-of-the-art results on the {CALVIN} benchmark, and also demonstrated robust generalization on real-world manipulation tasks, beating strong baselines that have access to privileged information or that utilize orders of magnitude more compute and training data.},
  address = {Vienna, Austria},
  author = {Kevin Black and Mitsuhiko Nakamoto and Pranav Atreya and Homer Rich Walke and Chelsea Finn and Aviral Kumar and Sergey Levine},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/black2024zeroshot.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=c0chJTSbci},
  publisher = {OpenReview.net},
  title = {Zero-Shot Robotic Manipulation with Pre-Trained Image-Editing Diffusion Models},
  url = {https://openreview.net/forum?id=c0chJTSbci},
  year = {2024}
}

@inproceedings{blanke2024interpretable,
  abstract = {Machine learning methods can be a valuable aid in the scientific process, but they need to face challenging settings where data come from inhomogeneous experimental conditions. Recent meta-learning methods have made significant progress in multi-task learning, but they rely on black-box neural networks, resulting in high computational costs and limited interpretability. We introduce {CAMEL}, a new meta-learning architecture capable of learning efficiently from multiple environments, with an affine structure with respect to the learning task. We prove that {CAMEL} can identify the physical parameters of the system, enabling interpretable learning. We demonstrate the competitive generalization performance and the low computational cost of our method by comparing it to state-of-the-art algorithms on physical systems, ranging from toy models to complex, non-analytical systems. The interpretability of our method is illustrated with original applications to parameter identification and to adaptive control and system identification.},
  address = {Vienna, Austria},
  author = {Matthieu Blanke and Marc Lelarge},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/blanke2024interpretable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nnicaG5xiH},
  publisher = {OpenReview.net},
  title = {Interpretable Meta-Learning of Physical Systems},
  url = {https://openreview.net/forum?id=nnicaG5xiH},
  year = {2024}
}

@inproceedings{blasiok2024smooth,
  abstract = {Calibration measures and reliability diagrams are two fundamental tools for measuring and interpreting the calibration of probabilistic predictors. Calibration measures quantify the degree of miscalibration, and reliability diagrams visualize the structure of this miscalibration. However, the most common constructions of reliability diagrams and calibration measures -- binning and {ECE} -- both suffer from well-known flaws (e.g., discontinuity). We show that a simple modification fixes both constructions: first smooth the observations using an {RBF} kernel, then compute the Expected Calibration Error ({ECE}) of this smoothed function. We prove that with a careful choice of bandwidth, this method yields a calibration measure that is well-behaved in the sense of a consistent calibration measure. We call this measure the {SmoothECE}. Moreover, the reliability diagram obtained from this smoothed function visually encodes the {SmoothECE}, just as binned reliability diagrams encode the {BinnedECE}. We demonstrate the effectiveness of our approach on various datasets and models, showing improved theoretical properties and practical utility over traditional calibration measures.},
  address = {Vienna, Austria},
  author = {Jaroslaw Blasiok and Preetum Nakkiran},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/blasiok2024smooth.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=XwiA1nDahv},
  publisher = {OpenReview.net},
  title = {Smooth {ECE}: Principled Reliability Diagrams via Kernel Smoothing},
  url = {https://openreview.net/forum?id=XwiA1nDahv},
  year = {2024}
}

@inproceedings{blecher2024nougat,
  abstract = {Scientific knowledge is predominantly stored in books and scientific journals, often in the form of {PDFs}. However, the {PDF} format leads to a loss of semantic information, particularly for mathematical expressions. We propose {Nougat} (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition ({OCR}) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition. Our method successfully handles complex mathematical formulations, tables, and figures commonly found in academic papers, converting them into structured markup that preserves semantic meaning and enables downstream processing.},
  address = {Vienna, Austria},
  author = {Lukas Blecher and Guillem Cucurull and Thomas Scialom and Robert Stojnic},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/blecher2024nougat.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fUtxNAKpdV},
  publisher = {OpenReview.net},
  title = {Nougat: Neural Optical Understanding for Academic Documents},
  url = {https://openreview.net/forum?id=fUtxNAKpdV},
  year = {2024}
}

@inproceedings{bleistein2024generalization,
  abstract = {Neural Controlled Differential Equations ({NCDE}) are a state-of-the-art tool for supervised learning with irregularly sampled time series. However, no theoretical analysis of their performance has been provided yet, and it remains unclear in particular how the roughness of the sampling affects their predictions. By merging the rich theory of controlled differential equations ({CDE}) and Lipschitz-based measures of the complexity of deep neural nets, we take a first step towards the theoretical understanding of {NCDE}. Our first result is a generalization bound for this class of predictors that depends on the regularity of the time series data. In a second time, we leverage the continuity of the flow of {CDEs} to provide a detailed analysis of both the sampling-induced bias and the approximation bias. Regarding this last result, we show how classical approximation results on neural nets may transfer to {NCDEs}. Our theoretical results are validated through a series of experiments, providing the first comprehensive theoretical framework for understanding the performance of neural controlled differential equations with irregularly sampled time series.},
  address = {Vienna, Austria},
  author = {Linus Bleistein and Agathe Guilloux},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bleistein2024generalization.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kILAd8RdzA},
  publisher = {OpenReview.net},
  title = {On the Generalization and Approximation Capacities of Neural Controlled Differential Equations},
  url = {https://openreview.net/forum?id=kILAd8RdzA},
  year = {2024}
}

@inproceedings{block2024butterfly,
  abstract = {This work studies training instabilities of behavior cloning with deep neural networks. We observe that minibatch {SGD} updates to the policy network during training result in sharp oscillations in long-horizon rewards, despite negligibly affecting the behavior cloning loss. We empirically disentangle the statistical and computational causes of these oscillations, and find them to stem from the chaotic propagation of minibatch {SGD} noise through unstable closed-loop dynamics. While {SGD} noise is benign in the single-step action prediction objective, it results in catastrophic error accumulation over long horizons, an effect we term gradient variance amplification ({GVA}). We demonstrate that many standard mitigation techniques do not alleviate {GVA}, but that taking an exponential moving average ({EMA}) of iterates is surprisingly effective at doing so. Furthermore, we illustrate the generality of the phenomenon by showing both the existence of {GVA} and its amelioration by {EMA} in autoregressive contexts, demonstrating that small fluctuations from minibatch {SGD} noise are amplified catastrophically in unstable feedback loops.},
  address = {Vienna, Austria},
  author = {Adam Block and Dylan J. Foster and Akshay Krishnamurthy and Max Simchowitz and Cyril Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/block2024butterfly.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CgPs04l9TO},
  publisher = {OpenReview.net},
  title = {Butterfly Effects of {SGD} Noise: Error Amplification in Behavior Cloning and Autoregression},
  url = {https://openreview.net/forum?id=CgPs04l9TO},
  year = {2024}
}

@inproceedings{blumberg2024experimental,
  abstract = {This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. We call this approach task-driven experimental design ({TADRED}). Our method learns which channels are most informative for a given task, enabling efficient acquisition protocols that are tailored to specific downstream applications rather than generic parameter estimation objectives.},
  address = {Vienna, Austria},
  author = {Stefano B. Blumberg and Paddy J. Slator and Daniel C. Alexander},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/blumberg2024experimental.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MloaGA6WwX},
  publisher = {OpenReview.net},
  title = {Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection},
  url = {https://openreview.net/forum?id=MloaGA6WwX},
  year = {2024}
}

@inproceedings{blumenfeld2024towards,
  abstract = {The majority of the research on the quantization of Deep Neural Networks ({DNNs}) is focused on reducing the precision of tensors visible by high-level frameworks (e.g., weights, activations, and gradients). However, current hardware still relies on high-accuracy core operations. Most significant is the operation of accumulating products. This high-precision accumulation operation is gradually becoming the main computational bottleneck. This is because, so far, the usage of low-precision accumulators led to a significant degradation in performance. In this work, we present a simple method to train and fine-tune high-end {DNNs}, to allow, for the first time, utilization of cheaper, 12-bit accumulators, with no significant degradation in accuracy. We show that models can be fine-tuned for inference with 12-bit accumulators, and develop methods for training with even smaller accumulators, addressing a critical bottleneck in efficient deep learning inference.},
  address = {Vienna, Austria},
  author = {Yaniv Blumenfeld and Itay Hubara and Daniel Soudry},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/blumenfeld2024towards.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oOwDQl8haC},
  publisher = {OpenReview.net},
  title = {Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators},
  url = {https://openreview.net/forum?id=oOwDQl8haC},
  year = {2024}
}

@inproceedings{bohde2024on,
  abstract = {Neural algorithmic reasoning is an emerging research direction that endows neural networks with the ability to mimic algorithmic executions step-by-step. A common paradigm in existing designs involves the use of historical embeddings in predicting the results of future execution steps. Our observation in this work is that such historical dependence intrinsically contradicts the Markov nature of algorithmic reasoning tasks. Based on this motivation, we present our {ForgetNet}, which does not use historical embeddings and thus is consistent with the Markov nature of the tasks. To address challenges in training {ForgetNet} at early stages, we further introduce {G-ForgetNet}, which uses a gating mechanism to allow for the selective integration of historical embeddings. Our extensive experiments, based on the {CLRS-30} algorithmic reasoning benchmark, demonstrate that both {ForgetNet} and {G-ForgetNet} achieve better generalization capability than existing methods. Furthermore, we investigate the behavior of the gating mechanism, highlighting its degree of alignment with our intuitions and its effectiveness for robust performance.},
  address = {Vienna, Austria},
  author = {Montgomery Bohde and Meng Liu and Alexandra Saxton and Shuiwang Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bohde2024on.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=Kn7tWhuetn},
  publisher = {OpenReview.net},
  title = {On the {M}arkov Property of Neural Algorithmic Reasoning: Analyses and Methods},
  url = {https://openreview.net/forum?id=Kn7tWhuetn},
  year = {2024}
}

@inproceedings{boixadsera2024when,
  abstract = {We investigate the capabilities of transformer models on relational reasoning tasks. In these tasks, models are trained on a set of strings encoding abstract relations, and are then tested out-of-distribution on data that contains symbols that did not appear in the training dataset. We prove that for any relational reasoning task in a large family of tasks, transformers learn the abstract relations and generalize to the test set when trained by gradient descent on sufficiently large quantities of training data. For regression tasks, we prove that transformers generalize when trained, but require astonishingly large quantities of training data. For next-token-prediction tasks with symbolic labels, we show an inverse scaling law: transformers fail to generalize as their embedding dimension increases. We demonstrate that classical {MLP} architectures trained by {SGD} or {Adam} will not generalize in template tasks on symbols unseen during training, even in the regression setting, regardless of training data size. For both settings, we propose subtle transformer modifications which can reduce the amount of data needed by adding two trainable parameters per head.},
  address = {Vienna, Austria},
  author = {Enric Boix-Adser{\`a} and Omid Saremi and Emmanuel Abbe and Samy Bengio and Etai Littwin and Joshua M. Susskind},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/boixadsera2024when.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=STUGfUz8ob},
  publisher = {OpenReview.net},
  title = {When can transformers reason with abstract symbols?},
  url = {https://openreview.net/forum?id=STUGfUz8ob},
  year = {2024}
}

@inproceedings{bolya2024window,
  abstract = {Window attention, position embeddings, and high resolution finetuning are core concepts in the modern transformer era of computer vision. However, we find that naively combining these near ubiquitous components can have a detrimental effect on performance. The issue is simple: interpolating position embeddings while using window attention is wrong. We study two state-of-the-art methods that have these three components, namely {H}iera and {V}i{TD}et, and find that both do indeed suffer from this bug. To fix it, we introduce a simple absolute window position embedding strategy, which solves the bug outright in {H}iera and allows us to increase both speed and performance of the model in {V}i{TD}et. We finally combine the two to obtain {H}iera{D}et, which achieves 61.7 box {mAP} on {COCO}, making it state-of-the-art for models that only use {I}mage{N}et-1k pretraining. This all stems from what is essentially a 3 line bug fix, which we name 'absolute win'.},
  author = {Daniel Bolya and Chaitanya Ryali and Judy Hoffman and Christoph Feichtenhofer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bolya2024window.pdf:pdf},
  keywords = {window attention, position embeddings, high resolution finetuning, image classification, video classification, object detection, instance segmentation, transformer, computer vision},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IPhm01y9a9},
  publisher = {OpenReview.net},
  title = {Window Attention is Bugged: How not to Interpolate Position Embeddings},
  url = {https://openreview.net/forum?id=IPhm01y9a9},
  year = {2024}
}

@inproceedings{bondtaylor2024diff,
  abstract = {This paper introduces $\infty$-Diff, a generative diffusion model defined in an infinite-dimensional {H}ilbert space, which can model infinite resolution data. By training on randomly sampled subsets of coordinates and denoising content only at those locations, we learn a continuous function for arbitrary resolution sampling. Unlike prior neural field-based infinite-dimensional models, which use point-wise functions requiring latent compression, our method employs non-local integral operators to map between {H}ilbert spaces, allowing spatial context aggregation. This is achieved with an efficient multi-scale function-space architecture that operates directly on raw sparse coordinates, coupled with a mollified diffusion process that smooths out irregularities. Through experiments on high-resolution datasets, we found that even at an $8\times$ subsampling rate, our model retains high-quality diffusion. This leads to significant run-time and memory savings, delivers samples with lower {FID} scores, and scales beyond the training resolution while retaining detail.},
  author = {Sam Bond-Taylor and Chris G. Willcocks},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bondtaylor2024diff.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OUeIBFhyem},
  publisher = {OpenReview.net},
  title = {$\infty$-Diff: Infinite Resolution Diffusion with Subsampled Mollified States},
  url = {https://openreview.net/forum?id=OUeIBFhyem},
  year = {2024}
}

@inproceedings{bonnet2024jumanji,
  abstract = {Open-source reinforcement learning ({RL}) environments have played a crucial role in driving progress in the development of {AI} algorithms. In modern {RL} research, there is a need for simulated environments that are performant, scalable, and modular to enable their utilization in a wider range of potential real-world applications. Therefore, we present {J}umanji, a suite of diverse {RL} environments specifically designed to be fast, flexible, and scalable. {J}umanji provides a suite of environments focusing on combinatorial problems frequently encountered in industry, as well as challenging general decision-making tasks. By leveraging the efficiency of {JAX} and hardware accelerators like {GPU}s and {TPU}s, {J}umanji enables rapid iteration of research ideas and large-scale experimentation, ultimately empowering more capable agents. Unlike existing {RL} environment suites, {J}umanji is highly customizable, allowing users to tailor the initial state distribution and problem complexity to their needs. Furthermore, we provide actor-critic baselines for each environment, accompanied by preliminary findings on scaling and generalization scenarios. {J}umanji aims to set a new standard for speed, adaptability, and scalability of {RL} environments.},
  author = {Cl√©ment Bonnet and Daniel Luo and Donal John Byrne and Shikha Surana and Sasha Abramowitz and Paul Duckworth and Vincent Coyette and Laurence Illing Midgley and Elshadai Tegegn and Tristan Kalloniatis and Omayma Mahjoub and Matthew Macfarlane and Andries Petrus Smit and Nathan Grinsztajn and Raphael Boige and Cemlyn Neil Waters and Mohamed Ali Ali Mimouni and Ulrich Armel Mbou Sob and Ruan John de Kock and Siddarth Singh and Daniel Furelos-Blanco and Victor Le and Arnu Pretorius and Alexandre Laterre},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bonnet2024jumanji.pdf:pdf},
  keywords = {reinforcement learning, JAX, combinatorial optimization, scalable environments, hardware acceleration, decision-making tasks},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=C4CxQmp9wc},
  publisher = {OpenReview.net},
  title = {Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in {JAX}},
  url = {https://openreview.net/forum?id=C4CxQmp9wc},
  year = {2024}
}

@inproceedings{bono2024endtoend,
  abstract = {Most recent work in goal oriented visual navigation resorts to large-scale machine learning in simulated environments. The main challenge lies in learning compact representations generalizable to unseen environments and in learning high-capacity perception modules capable of reasoning on high-dimensional input. The latter is particularly difficult when the goal is not given as a category ('{O}bject{N}av') but as an exemplar image ('{I}mage{N}av'), as the perception module needs to learn a comparison strategy requiring to solve an underlying visual correspondence problem. This has been shown to be difficult from reward alone or with standard auxiliary tasks. We address this problem through a sequence of two pretext tasks, which serve as a prior for what we argue is one of the main bottleneck in perception, extremely wide-baseline relative pose estimation and visibility prediction in complex scenes. The first pretext task, cross-view completion is a proxy for the underlying visual correspondence problem, while the second task addresses goal detection and finding directly. We propose a new dual encoder with a large-capacity binocular {V}i{T} model and show that correspondence solutions naturally emerge from the training signals. Experiments show significant improvements and {SOTA} performance on the two benchmarks, {I}mage{N}av and the Instance-{I}mage{N}av variant, where camera intrinsics and height differ between observation and goal.},
  author = {Guillaume Bono and Leonid Antsfeld and Boris Chidlovskii and Philippe Weinzaepfel and Christian Wolf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bono2024endtoend.pdf:pdf},
  keywords = {visual navigation, image goal navigation, correspondence, emergent behavior, binocular vision transformer, computer vision},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cphhnHjCvC},
  publisher = {OpenReview.net},
  title = {End-to-End (Instance)-Image Goal Navigation through Correspondence as an Emergent Phenomenon},
  url = {https://openreview.net/forum?id=cphhnHjCvC},
  year = {2024}
}

@inproceedings{bono2024learning,
  abstract = {Agents navigating in 3{D} environments require some form of memory, which should hold a compact and actionable representation of the history of observations useful for decision taking and planning. In most end-to-end learning approaches the representation is latent and usually does not have a clearly defined interpretation, whereas classical robotics addresses this with scene reconstruction resulting in some form of map, usually estimated with geometry and sensor models and/or learning. In this work we propose to learn an actionable representation of the scene independently of the targeted downstream task and without explicitly optimizing reconstruction. The learned representation is optimized by a blind auxiliary agent trained to navigate with it on multiple short sub episodes branching out from a waypoint and, most importantly, without any direct visual observation. We argue and show that the blindness property is important and forces the (trained) latent representation to be the only means for planning. With probing experiments we show that the learned representation optimizes navigability and not reconstruction. On downstream tasks we show that it is robust to changes in distribution, in particular the sim2real gap, which we evaluate with a real physical robot in a real office building, significantly improving performance.},
  author = {Guillaume Bono and Leonid Antsfeld and Assem Sadek and Gianluca Monaci and Christian Wolf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bono2024learning.pdf:pdf},
  keywords = {navigation, spatial representation learning, transferability, sim2real, latent representations, robotics},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8HCARN2hhw},
  publisher = {OpenReview.net},
  title = {Learning with a Mole: Transferable latent spatial representations for navigation without reconstruction},
  url = {https://openreview.net/forum?id=8HCARN2hhw},
  year = {2024}
}

@inproceedings{borde2024neural,
  abstract = {The inductive bias of a graph neural network ({GNN}) is largely encoded in its specified graph. Latent graph inference relies on latent geometric representations to dynamically rewire or infer a {GNN}'s graph to maximize the {GNN}'s predictive downstream performance, but it lacks solid theoretical foundations in terms of embedding-based representation guarantees. This paper addresses this issue by introducing a trainable deep learning architecture, coined neural snowflake, that can adaptively implement fractal-like metrics on $\mathbb{R}^d$. We prove that any given finite weights graph can be isometrically embedded by a standard {MLP} encoder. Furthermore, when the latent graph can be represented in the feature space of a sufficiently regular kernel, we show that the combined neural snowflake and {MLP} encoder do not succumb to the curse of dimensionality by using only a low-degree polynomial number of parameters in the number of nodes.},
  author = {Haitz S√°ez de Oc√°riz Borde and Anastasis Kratsios},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/borde2024neural.pdf:pdf},
  keywords = {graph neural networks, latent graph inference, trainable geometries, fractal metrics, isometric embedding, universal approximation},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=djM3WzpOmK},
  publisher = {OpenReview.net},
  title = {Neural Snowflakes: Universal Latent Graph Inference via Trainable Latent Geometries},
  url = {https://openreview.net/forum?id=djM3WzpOmK},
  year = {2024}
}

@inproceedings{bordelon2024depthwise,
  abstract = {The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses Œº{P} parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the Œº{P} parameterization. We provide experiments demonstrating that residual architectures including convolutional {R}es{N}ets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on {CIFAR}-10 and {I}mage{N}et.},
  author = {Blake Bordelon and Lorenzo Noci and Mufan Bill Li and Boris Hanin and Cengiz Pehlevan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bordelon2024depthwise.pdf:pdf},
  keywords = {hyperparameter transfer, residual networks, muP parameterization, scaling limit, dynamical mean field theory, vision transformer},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KZJehvRKGD},
  publisher = {OpenReview.net},
  title = {Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit},
  url = {https://openreview.net/forum?id=KZJehvRKGD},
  year = {2024}
}

@inproceedings{bose2024se3,
  abstract = {The computational design of novel protein structures has the potential to impact numerous scientific disciplines greatly. Toward this goal, we introduce {F}old{F}low, a series of novel generative models of increasing modeling power based on the flow-matching paradigm over 3{D} rigid motions -- i.e. the group {SE}(3) -- enabling accurate modeling of protein backbones. We first introduce {F}old{F}low-Base, a simulation-free approach to learning deterministic continuous-time dynamics and matching invariant target distributions on {SE}(3). We next accelerate training by incorporating {R}iemannian optimal transport to create {F}old{F}low-{OT}, leading to the construction of both more simple and stable flows. Finally, we design {F}old{F}low-{SFM}, coupling both {R}iemannian {OT} and simulation-free training to learn stochastic continuous-time dynamics over {SE}(3). Our family of {F}old{F}low, generative models offers several key advantages over previous approaches to the generative modeling of proteins: they are more stable and faster to train than diffusion-based approaches, and our models enjoy the ability to map any invariant source distribution to any invariant target distribution over {SE}(3). Empirically, we validate {F}old{F}low, on protein backbone generation of up to 300 amino acids leading to high-quality designable, diverse, and novel samples.},
  author = {Avishek Joey Bose and Tara Akhound-Sadegh and Guillaume Huguet and Kilian Fatras and Jarrid Rector-Brooks and Cheng-Hao Liu and Andrei Cristian Nica and Maksym Korablyov and Michael M. Bronstein and Alexander Tong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bose2024se3.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=kJFIH23hXb},
  publisher = {OpenReview.net},
  title = {{SE(3)}-Stochastic Flow Matching for Protein Backbone Generation},
  url = {https://openreview.net/forum?id=kJFIH23hXb},
  year = {2024}
}

@inproceedings{bou2024torchrl,
  abstract = {PyTorch has ascended as a premier machine learning framework, yet it lacks a native and comprehensive library for decision and control tasks suitable for large development teams dealing with complex real-world data and environments. To address this issue, we propose TorchRL, a generalistic control library for PyTorch that provides well-integrated, yet standalone components. With this library, we aim to fill the void for a PyTorch-centered ecosystem for RL practitioners to build upon. The main highlight of our contribution rests in the introduction of a new and flexible PyTorch primitive, the TensorDict, which facilitates streamlined algorithm development across the many branches of Reinforcement Learning (RL) and control. This data structure behaves as a flexible data carrier that empowers the integration of the library's components while preserving their modularity.},
  author = {Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bou2024torchrl.pdf:pdf},
  keywords = {reinforcement learning, PyTorch, decision making, TensorDict, modular library, control systems},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=QxItoEAVMb},
  publisher = {OpenReview.net},
  title = {TorchRL: A data-driven decision-making library for PyTorch},
  url = {https://openreview.net/forum?id=QxItoEAVMb},
  year = {2024}
}

@inproceedings{bradley2024qualitydiversity,
  abstract = {In many text-generation problems, users may prefer not only a single response, but a diverse range of high-quality outputs from which to choose. Quality-diversity (QD) search algorithms aim at continually improving and diversifying a population of candidates. However, the applicability of QD to qualitative domains, like creative writing, has been limited by the difficulty of algorithmically specifying measures of quality and diversity. Leveraging recent developments in language models (LMs), we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an evolutionary algorithm applies LMs to both generate variation and evaluate the quality and diversity of candidate text. Through experiments on creative writing domains, we demonstrate that QDAIF can effectively search for text that is both high-quality and diverse. Further, we show that techniques from the QD literature, such as archive-based search and illumination algorithms, can improve the effectiveness of search in creative domains. Our results suggest that QDAIF represents a step towards AI systems that can independently search, diversify, evaluate, and improve, which are traditionally human-driven processes.},
  author = {Herbie Bradley and Andrew Dai 0001 and Hannah Benita Teufel and Jenny Zhang and Koen Oostermeijer and Marco Bellagente and Jeff Clune and Kenneth O. Stanley and Gr√©gory Schott and Joel Lehman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bradley2024qualitydiversity.pdf:pdf},
  keywords = {quality-diversity search, AI feedback, evolutionary algorithms, language models, creative text generation, QDAIF},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=owokKCrGYr},
  publisher = {OpenReview.net},
  title = {Quality-Diversity through AI Feedback},
  url = {https://openreview.net/forum?id=owokKCrGYr},
  year = {2024}
}

@inproceedings{brahman2024plasma,
  abstract = {Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex and often contextualized situations, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models ({LLM}s), they are hindered by drawbacks such as costly {API} calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present {PlaSma}, a novel two-pronged approach to endow small language models with procedural knowledge and (constrained) language planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the commonsense knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a new related task, Replanning, that requires a revision of a plan to cope with a constrained situation. In both the planning and replanning settings, we show that orders-of-magnitude smaller models (770{M}-11{B} parameters) can compete and often surpass their larger teacher models' capabilities. Finally, we showcase successful application of {PlaSma} in an embodied environment, {VirtualHome}.},
  author = {Faeze Brahman and Chandra Bhagavatula and Valentina Pyatkin and Jena D. Hwang and Xiang Lorraine Li and Hirona Jacqueline Arai and Soumya Sanyal and Keisuke Sakaguchi and Xiang Ren and Yejin Choi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/brahman2024plasma.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=dFcXJgnrGB},
  publisher = {OpenReview.net},
  title = {{PlaSma}: Procedural Knowledge Models for Language-based Planning and Re-Planning},
  url = {https://openreview.net/forum?id=dFcXJgnrGB},
  year = {2024}
}

@inproceedings{brellmann2024double,
  abstract = {Temporal Difference ({TD}) algorithms are widely used in Deep Reinforcement Learning ({RL}). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in {RL} is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference ({LSTD}) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive deterministic limits of both the empirical and the true Mean-Squared Bellman Error ({MSBE}) that feature correction terms responsible for the double descent. Correction terms vanish when the $l_2$-regularization is increased or the number of unvisited states goes to zero. Numerical experiments with synthetic and small real-world environments closely match the theoretical predictions.},
  author = {David Brellmann and Elo{\"i}se Berthier and David Filliat and Goran Frehse},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/brellmann2024double.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=9RIbNmx984},
  publisher = {OpenReview.net},
  title = {On Double Descent in Reinforcement Learning with {LSTD} and Random Features},
  url = {https://openreview.net/forum?id=9RIbNmx984},
  year = {2024}
}

@inproceedings{brokman2024enhancing,
  abstract = {As neural networks grow in scale, their training becomes both computationally demanding and rich in dynamics. Amidst the flourishing interest in these training dynamics, we present a novel observation: Parameters during training exhibit intrinsic correlations over time. Capitalizing on this insight, we introduce correlation mode decomposition ({CMD}), an algorithm that clusters the parameter space into groups, termed modes, that display synchronized behavior across epochs. This enables {CMD} to efficiently represent the training dynamics of complex networks, like {ResNets} and Transformers, using only a few modes.},
  author = {Jonathan Brokman and Roy Betser and Rotem Turjeman and Tom Berkov and Ido Cohen and Guy Gilboa},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/brokman2024enhancing.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=c9xsaASm9L},
  publisher = {OpenReview.net},
  title = {Enhancing Neural Training via a Correlated Dynamics Model},
  url = {https://openreview.net/forum?id=c9xsaASm9L},
  year = {2024}
}

@inproceedings{buening2024bandits,
  abstract = {We study a strategic variant of the multi-armed bandit problem, which we coin the strategic click-bandit. This model is motivated by applications in online recommendation where the choice of recommended items depends on both the click-through rates and the post-click rewards. Like in classical bandits, rewards follow a fixed unknown distribution. However, we assume that the click-rate of each arm is chosen strategically by the arm (e.g., a host on Airbnb) in order to maximize the number of times it gets clicked. The algorithm designer does not know the post-click rewards nor the arms' actions (i.e., strategically chosen click-rates) in advance, and must learn both values over time. To solve this problem, we design an incentive-aware learning algorithm, {UCB-S}, which achieves two goals simultaneously: (a) incentivizing desirable arm behavior under uncertainty; (b) minimizing regret by learning unknown parameters. We approximately characterize all Nash equilibria of the arms under {UCB-S} and show a $\tilde{\mathcal{O}}(\sqrt{KT})$ regret bound uniformly in every equilibrium. We also show that incentive-unaware algorithms generally fail to achieve low regret in the strategic click-bandit. Finally, we support our theoretical results by simulations of strategic arm behavior which confirm the effectiveness and robustness of our proposed incentive design.},
  author = {Thomas Kleine Buening and Aadirupa Saha and Christos Dimitrakakis and Haifeng Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/buening2024bandits.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=lsxeNvYqCj},
  publisher = {OpenReview.net},
  title = {Bandits Meet Mechanism Design to Combat Clickbait in Online Recommendation},
  url = {https://openreview.net/forum?id=lsxeNvYqCj},
  year = {2024}
}

@inproceedings{burg2024most,
  abstract = {Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli ({MDS}). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area {V}4. This demonstrates that our approach can successfully find discriminative stimuli across species, stages of the visual system and recording techniques. The resulting most discriminative stimuli can be used to assign functional cell types fast and on the fly, without the need to train complex predictive models or show a large natural scene dataset, paving the way for experiments that were previously limited by experimental time. Crucially, {MDS} are interpretable: they visualize the distinctive stimulus patterns that most unambiguously identify a specific type of neuron.},
  author = {Max F. Burg and Thomas Zenkel and Michaela Vystr{\v{c}}ilov{\'a} and Jonathan Oesterle and Larissa H{\"o}fling and Konstantin F. Willeke and Jan Lause and Sarah M{\"u}ller and Paul G. Fahey and Zhiwei Ding and Kelli Restivo and Shashwat Sridhar and Tim Gollisch and Philipp Berens and Andreas S. Tolias and Thomas Euler and Matthias Bethge and Alexander S. Ecker},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/burg2024most.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=9W6KaAcYlr},
  publisher = {OpenReview.net},
  title = {Most discriminative stimuli for functional cell type clustering},
  url = {https://openreview.net/forum?id=9W6KaAcYlr},
  year = {2024}
}

@inproceedings{burkholz2024batch,
  abstract = {Normalization techniques, for which Batch Normalization ({BN}) is a popular choice, is an integral part of many deep learning architectures and contributes significantly to the learning success. We provide a partial explanation for this phenomenon by proving that training normalization parameters alone is already sufficient for universal function approximation if the number of available, potentially random features matches or exceeds the weight parameters of the target networks that can be expressed. Our bound on the number of required features does not only improve on a recent result for fully-connected feed-forward architectures but also applies to convolutional neural networks.},
  author = {Rebekka Burkholz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/burkholz2024batch.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=wOSYMHfENq},
  publisher = {OpenReview.net},
  title = {Batch normalization is sufficient for universal function approximation in {CNN}s},
  url = {https://openreview.net/forum?id=wOSYMHfENq},
  year = {2024}
}

@inproceedings{bushuiev2024learning,
  abstract = {Discovering mutations enhancing protein-protein interactions ({PPIs}) is critical for advancing biomedical research and developing improved therapeutics. While machine learning approaches have substantially advanced the field, they often struggle to generalize beyond training data in practical scenarios. The contributions of this work are three-fold. First, we construct {PPIRef}, the largest and non-redundant dataset of {3D} protein-protein interactions, enabling effective large-scale learning. Second, we leverage the {PPIRef} dataset to pre-train {PPIformer}, a new {SE}(3)-equivariant model generalizing across diverse protein-binder variants. We fine-tune {PPIformer} to predict effects of mutations on protein-protein interactions via a thermodynamically motivated adjustment of the pre-training loss function. Finally, we demonstrate the enhanced generalization of our new {PPIformer} approach by outperforming other state-of-the-art methods on new, non-leaking splits of standard labeled {PPI} mutational data and independent case studies optimizing a human antibody against {SARS-CoV-2} and increasing the thrombolytic activity of staphylokinase.},
  author = {Anton Bushuiev and Roman Bushuiev and Petr Kouba and Anatolii Filkin and Marketa Gabrielova and Michal Gabriel and Jiri Sedlar and Tomas Pluskal and Jiri Damborsky and Stanislav Mazurenko and Josef Sivic},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/bushuiev2024learning.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=xcMmebCT7s},
  publisher = {OpenReview.net},
  title = {Learning to design protein-protein interactions with enhanced generalization},
  url = {https://openreview.net/forum?id=xcMmebCT7s},
  year = {2024}
}

@inproceedings{butakov2024information,
  abstract = {The Information Bottleneck ({IB}) principle offers an information-theoretic framework for analyzing the training process of deep neural networks ({DNNs}). Its essence lies in tracking the dynamics of two mutual information ({MI}) values: between the hidden layer output and the {DNN} input/target. According to the hypothesis put forth by Shwartz-Ziv \& Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by {DNNs}. Due to the challenging nature of estimating {MI} between high-dimensional random vectors, this hypothesis was only partially verified for {NNs} of tiny sizes or specific types, such as quantized {NNs}. The paper addresses the key challenge of mutual information estimation in high-dimensional settings and proposes and justifies a lossy compression step to overcome the obstacles associated with high dimensionality in the information-theoretic approach to {DNNs}.},
  author = {Ivan Butakov and Aleksander Tolmachev and Sofia Malanchuk and Anna Neopryatnaya and Alexey A. Frolov and Kirill V. Andreev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/butakov2024information.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=huGECz8dPp},
  publisher = {OpenReview.net},
  title = {Information Bottleneck Analysis of Deep Neural Networks via Lossy Compression},
  url = {https://openreview.net/forum?id=huGECz8dPp},
  year = {2024}
}

@inproceedings{buvanesh2024enhancing,
  abstract = {Extreme Classification ({XC}) architectures, which utilize a massive One-vs-All ({OvA}) classifier layer at the output, have demonstrated remarkable performance on problems with large label sets. Nonetheless, these architectures falter on tail labels with few representative samples. This phenomenon has been attributed to factors such as classifier over-fitting and missing label bias, and solutions involving regularization and loss re-calibration have been developed. This paper explores the impact of label variance -- a previously unexamined factor -- on the tail performance in extreme classifiers. It also develops a method to systematically reduce label variance in {XC} by transferring the knowledge from a specialized tail-robust teacher model to the {OvA} classifiers. We propose a principled knowledge distillation framework, {LEVER}, which enhances the tail performance in extreme classifiers with formal guarantees on generalization.},
  author = {Anirudh Buvanesh and Rahul Chand and Jatin Prakash and Bhawna Paliwal and Mudit Dhawan and Neelabh Madan and Deepesh Hada and Vidit Jain and Sonu Mehta and Yashoteja Prabhu and Manish Gupta and Ramachandran Ramjee and Manik Varma},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/buvanesh2024enhancing.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=6ARlSgun7J},
  publisher = {OpenReview.net},
  title = {Enhancing Tail Performance in Extreme Classifiers by Label Variance Reduction},
  url = {https://openreview.net/forum?id=6ARlSgun7J},
  year = {2024}
}

@inproceedings{buyl2024fairret,
  abstract = {The framework introduces fairness regularization terms (fairrets) which quantify bias as modular, flexible objectives that are easily integrated in automatic differentiation pipelines. By employing a general definition of fairness in terms of linear-fractional statistics, a wide class of fairness metrics can be addressed. Central to the library is the paradigm of the fairness regularization term (fairrets) that quantify unfairness as differentiable {PyTorch} loss functions. These can be minimized jointly with other losses, like the binary cross-entropy error, by just adding them together. It suffices to simply choose a statistic that should be equalized across groups and a fairret that quantifies the gap.},
  author = {Maarten Buyl and MaryBeth Defrance and Tijl De Bie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/buyl2024fairret.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=NnyD0Rjx2B},
  publisher = {OpenReview.net},
  title = {fairret: a Framework for Differentiable Fairness Regularization Terms},
  url = {https://openreview.net/forum?id=NnyD0Rjx2B},
  year = {2024}
}

@inproceedings{cabannes2024scaling,
  abstract = {Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.},
  author = {Vivien Cabannes and Elvis Dohmatob and Alberto Bietti},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cabannes2024scaling.pdf:pdf},
  note = {Spotlight presentation. Primary area: Learning theory},
  pdf = {https://openreview.net/pdf?id=Tzh6xAJSll},
  publisher = {OpenReview.net},
  title = {Scaling Laws for Associative Memories},
  url = {https://openreview.net/forum?id=Tzh6xAJSll},
  year = {2024}
}

@inproceedings{cacciamani2024online,
  abstract = {We investigate the mechanism design problem faced by a principal who hires multiple agents to gather and report costly information. Then, the principal exploits the information to make an informed decision. We model this problem as a game, where the principal announces a mechanism consisting in action recommendations and a payment function, a.k.a. scoring rule. Then, each agent chooses an effort level and receives partial information about an underlying state of nature based on the effort. Finally, the agents report the information (possibly non-truthfully), the principal takes a decision based on this information, and the agents are paid according to the scoring rule. While previous work focuses on single-agent problems, we consider multi-agents settings. The key challenges include coordinating agents' efforts and aggregating correlated information. The authors develop a polynomial-time algorithm to find an optimal incentive-compatible mechanism and design a no-regret algorithm with $\tilde{O}(T^{2/3})$ regret.},
  author = {Federico Cacciamani and Matteo Castiglioni and Nicola Gatti},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cacciamani2024online.pdf:pdf},
  note = {Primary area: Reinforcement Learning},
  pdf = {https://openreview.net/pdf?id=oQKKlzxV1o},
  publisher = {OpenReview.net},
  title = {Online Information Acquisition: Hiring Multiple Agents},
  url = {https://openreview.net/forum?id=oQKKlzxV1o},
  year = {2024}
}

@inproceedings{cai2024large,
  abstract = {Recent research has highlighted the potential of large language models ({LLMs}) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as {LLMs} As Tool Makers ({LATM}), where {LLMs} create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an {LLM} acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another {LLM} acts as the tool user, which applies the tool built by the tool maker for problem-solving. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With {GPT}-4 as the tool maker and {GPT}-3.5 as the tool user, {LATM} demonstrates performance equivalent to using {GPT}-4 for both roles, but with a significantly reduced inference cost.},
  author = {Tianle Cai and Xuezhi Wang and Tengyu Ma and Xinyun Chen and Denny Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cai2024large.pdf:pdf},
  note = {Poster presentation. Primary area: General machine learning},
  pdf = {https://openreview.net/pdf?id=qV83K9d5WB},
  publisher = {OpenReview.net},
  title = {Large Language Models as Tool Makers},
  url = {https://openreview.net/forum?id=qV83K9d5WB},
  year = {2024}
}

@inproceedings{cai2024variance,
  abstract = {Machine learning approaches relying on such criteria as adversarial robustness or multi-agent settings have raised the need for solving game-theoretic equilibrium problems. Of particular relevance to these applications are methods targeting finite-sum structure, which generically arises in empirical variants of learning problems in these contexts. We study finite-sum monotone inclusion problems, which model broad classes of equilibrium problems. Such tasks are neatly unified under the umbrella of monotone inclusion, which has a rich history within optimization theory and operations research. The main contribution of this work is variants of the classical Halpern iteration that employ variance reduction to obtain improved complexity guarantees, specifically targeting scenarios where component operators are either cocoercive or Lipschitz continuous and monotone. We obtain the first variance-reduced complexity results for standard classes of monotone inclusion problems that can lead to a $\sqrt{n}$ improvement compared to methods without variance reduction.},
  author = {Xufeng Cai and Ahmet Alacaoglu and Jelena Diakonikolas},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cai2024variance.pdf:pdf},
  note = {Primary area: Optimization},
  pdf = {https://openreview.net/pdf?id=0i6Z9N5MLY},
  publisher = {OpenReview.net},
  title = {Variance Reduced Halpern Iteration for Finite-Sum Monotone Inclusions},
  url = {https://openreview.net/forum?id=0i6Z9N5MLY},
  year = {2024}
}

@inproceedings{cai2024independentset,
  abstract = {Interference is ubiquitous when conducting causal experiments over networks. Except for certain network structures, causal inference on the network in the presence of interference is difficult due to the entanglement between the treatment assignments and the interference levels. In this article, we conduct causal inference under interference on an observed, sparse, but connected network, and we propose a novel design of experiments based on an independent set. The independent-set design focuses on an independent subset of data and controls their interference exposures through the assignments to the rest (auxiliary set), which differs from conventional designs. We provide a lower bound on the size of the independent set from a greedy algorithm and justify the theoretical performance of estimators under the proposed design. The approach is capable of estimating both spillover effects and treatment effects. We justify its superiority over conventional methods and illustrate the empirical performance through simulations. Unlike most previous studies which require {SUTVA} assumptions and no interference between units, this approach does not require {SUTVA} and could be implemented in arbitrary networks.},
  author = {Chencheng Cai and Xu Zhang and Edoardo M. Airoldi},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cai2024independentset.pdf:pdf},
  note = {Primary area: Causal reasoning},
  pdf = {https://openreview.net/pdf?id=w50MQ9Vfty},
  publisher = {OpenReview.net},
  title = {Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference},
  url = {https://openreview.net/forum?id=w50MQ9Vfty},
  year = {2024}
}

@inproceedings{cai2024groot,
  abstract = {We study the problem of building a controller that can follow open-ended instructions in open-world environments. We propose to follow reference videos as instructions, which offer expressive goal specifications while eliminating the need for expensive text-gameplay annotations. A new learning framework is derived to allow learning such instruction-following controllers from gameplay videos while producing a video instruction encoder that induces a structured goal space. We implement our agent {GROOT} in a simple yet effective encoder-decoder architecture based on causal transformers. {GROOT} is evaluated against open-world counterparts and human players on a proposed Minecraft SkillForge benchmark. The Elo ratings clearly show that {GROOT} is closing the human-machine gap as well as exhibiting a 70\% winning rate over the best generalist agent baseline. The approach exhibits several interesting emergent properties, including goal composition and complex gameplay behavior synthesis.},
  author = {Shaofei Cai and Bowei Zhang and Zihao Wang and Xiaojian Ma and Anji Liu and Yitao Liang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cai2024groot.pdf:pdf},
  note = {Spotlight presentation. Primary area: Reinforcement Learning},
  pdf = {https://openreview.net/pdf?id=uleDLeiaT3},
  publisher = {OpenReview.net},
  title = {{GROOT}: Learning to Follow Instructions by Watching Gameplay Videos},
  url = {https://openreview.net/forum?id=uleDLeiaT3},
  year = {2024}
}

@inproceedings{cannistraci2024bricks,
  abstract = {It has been observed that representations learned by distinct neural networks conceal structural similarities when the models are trained under similar inductive biases. From a geometric perspective, identifying the classes of transformations and the related invariances that connect these representations is fundamental to unlocking applications, such as merging, stitching, and reusing different neural modules. We introduce a versatile method to directly incorporate a set of invariances into the representations, constructing a product space of invariant components on top of the latent representations without requiring prior knowledge about the optimal invariance to infuse. Using different similarity functions for each subspace, the method can infuse invariances to specific transformations into each component of the product space. We validate our solution on classification and reconstruction tasks, observing consistent latent similarity and downstream performance improvements in a zero-shot stitching setting. The experimental analysis comprises three modalities (vision, text, and graphs), twelve pretrained foundational models, nine benchmarks, and several architectures trained from scratch.},
  author = {Irene Cannistraci and Luca Moschella and Marco Fumero and Valentino Maiorca and Emanuele Rodol\`{a}},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cannistraci2024bricks.pdf:pdf},
  note = {Spotlight presentation. Primary area: Representation learning for computer vision, audio, language, and other modalities},
  pdf = {https://openreview.net/pdf?id=vngVydDWft},
  publisher = {OpenReview.net},
  title = {From Bricks to Bridges: Product of Invariances to Enhance Latent Space Communication},
  url = {https://openreview.net/forum?id=vngVydDWft},
  year = {2024}
}

@inproceedings{cao2024retrieval,
  abstract = {Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. We restrict phrases to contiguous sequences of words that correspond to constituent units in syntactic parse trees, ensuring that each phrase possesses a relatively complete and well-defined meaning. Experimental results show significant improvements: compared to the standard language model counterpart, our model raises the accuracy from 23.47\% to 36.27\% on {OpenbookQA}, and improves the {MAUVE} score from 42.61\% to 81.58\% in open-ended text generation. Our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines, demonstrating that retrieval is more accurate generation.},
  author = {Bowen Cao and Deng Cai and Leyang Cui and Xuxin Cheng and Wei Bi and Yuexian Zou and Shuming Shi},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cao2024retrieval.pdf:pdf},
  note = {Primary area: Representation learning for computer vision, audio, language, and other modalities},
  pdf = {https://openreview.net/pdf?id=oXYZJXDdo7},
  publisher = {OpenReview.net},
  title = {Retrieval is Accurate Generation},
  url = {https://openreview.net/forum?id=oXYZJXDdo7},
  year = {2024}
}

@inproceedings{cao2024enhancing,
  abstract = {We present a systematic framework designed to enhance human-robot perception and collaboration through the integration of logical rules and Theory of Mind ({ToM}). Logical rules provide interpretable predictions and generalize well across diverse tasks, making them valuable for learning and decision-making. Leveraging the {ToM} for understanding others' mental states, our approach facilitates effective collaboration. We employ logic rules derived from observational data to infer human goals and guide human-like agents. These rules are treated as latent variables, and a rule encoder is trained alongside a multi-agent system in the robot's mind. We assess the posterior distribution of latent rules using learned embeddings, representing entities and relations, with confidence scores for each rule indicating their consistency with observed data. Finally, we employ a hierarchical reinforcement learning model with {ToM} to plan robot actions for assisting humans. The approach diverges from previous works by focusing on activity understanding that infers mental states (e.g., intentions, desires) from behaviors, aligning closely with computational Theory of Mind. Experimental results demonstrate the effectiveness of our framework in enhancing human-{AI} collaboration across diverse collaborative scenarios.},
  author = {Chengzhi Cao and Yinghao Fu and Sheng Xu and Ruimao Zhang and Shuang Li},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cao2024enhancing.pdf:pdf},
  note = {Poster presentation. Primary area: Applications to robotics, autonomy, planning},
  pdf = {https://openreview.net/pdf?id=TWC4gLoAxY},
  publisher = {OpenReview.net},
  title = {Enhancing Human-{AI} Collaboration Through Logic-Guided Reasoning},
  url = {https://openreview.net/forum?id=TWC4gLoAxY},
  year = {2024}
}

@inproceedings{cao2024sohes,
  abstract = {Open-world entity segmentation, as an emerging computer vision task, aims at segmenting entities in images without being restricted by pre-defined classes, offering impressive generalization capabilities on unseen images and concepts. Despite its promise, existing entity segmentation methods like Segment Anything Model ({SAM}) rely heavily on costly expert annotators. To address this limitation, we introduce {SOHES}, a novel approach that eliminates the need for human annotations. {SOHES} operates in three phases: self-exploration, self-instruction, and self-correction. In the self-exploration phase, we cluster visual features from pre-trained {DINO} to generate initial pseudo-labels on unlabeled images. Then in the self-instruction phase, a segmentation model learns from the initial pseudo-labels. Finally, in the self-correction phase, we adopt a teacher-student framework to further refine the segmentation model. Beyond segmenting entities, {SOHES} also captures their constituent parts, providing a hierarchical understanding of visual entities. Using raw images as the sole training data, {SOHES} achieves unprecedented performance in self-supervised open-world segmentation. We evaluate {SOHES} on a variety of datasets, including {MS-COCO}, {LVIS}, {ADE20K}, {EntitySeg}, and {SA-1B}, demonstrating that {SOHES} significantly reduces the gap between self-supervised methods and the supervised Segment Anything Model.},
  author = {Shengcao Cao and Jiuxiang Gu and Jason Kuen and Hao Tan and Ruiyi Zhang and Handong Zhao and Ani Nenkova and Liangyan Gui and Tong Sun and Yu-Xiong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/cao2024sohes.pdf:pdf},
  note = {Primary area: Unsupervised, self-supervised, semi-supervised, and supervised representation learning},
  pdf = {https://openreview.net/pdf?id=PXNrncg2DF},
  publisher = {OpenReview.net},
  title = {{SOHES}: Self-supervised Open-world Hierarchical Entity Segmentation},
  url = {https://openreview.net/forum?id=PXNrncg2DF},
  year = {2024}
}

@inproceedings{cao2024largevocabulary,
  abstract = {Creating diverse and high-quality {3D} assets with an automatic generative model is highly desirable. Despite extensive efforts on {3D} generation, most existing works focus on the generation of a single category or a few categories. In this paper, we introduce a diffusion-based feed-forward framework for synthesizing massive categories of real-world {3D} objects with a single generative model.},
  author = {Ziang Cao and Fangzhou Hong and Tong Wu and Liang Pan and Ziwei Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cao2024largevocabulary.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=q57JLSE2j5},
  publisher = {OpenReview.net},
  title = {Large-Vocabulary {3D} Diffusion Model with Transformer},
  url = {https://openreview.net/forum?id=q57JLSE2j5},
  year = {2024}
}

@inproceedings{cao2024tempo,
  abstract = {The past decade has witnessed significant advances in time series modeling with deep learning. While achieving state-of-the-art results, the best-performing architectures vary highly across applications and domains. Meanwhile, for natural language processing, the Generative Pre-trained Transformer ({GPT}) has demonstrated impressive performance via training one general-purpose model across various textual datasets. It is intriguing to explore whether {GPT}-type architectures can be effective for time series, capturing the intrinsic dynamic attributes and leading to significant accuracy improvements. In this paper, we propose a novel framework, {TEMPO}, that can effectively learn time series representations. We focus on utilizing two essential inductive biases of the time series task for pre-trained models: (i) decomposition of the complex interaction between trend, seasonal and residual components; and (ii) introducing the design of prompts to facilitate distribution adaptation in different types of time series. {TEMPO} expands the capability for dynamically modeling real-world temporal phenomena from data within diverse domains. Our experiments demonstrate the superior performance of {TEMPO} over state-of-the-art methods on zero shot setting for a number of time series benchmark datasets.},
  author = {Defu Cao and Furong Jia and Sercan O. Arik and Tomas Pfister and Yixiang Zheng and Wen Ye and Yan Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cao2024tempo.pdf:pdf},
  note = {DBLP last modified: 2025-02-19},
  pdf = {https://openreview.net/pdf?id=YH5w12OUuU},
  publisher = {OpenReview.net},
  title = {{TEMPO}: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting},
  url = {https://openreview.net/forum?id=YH5w12OUuU},
  year = {2024}
}

@inproceedings{cao2024irad,
  abstract = {We introduce a novel approach to counter adversarial attacks, namely, image resampling. Image resampling transforms a discrete image into a new one, simulating the process of scene recapturing or rerendering as specified by a geometrical transformation. The underlying rationale behind our idea is that image resampling can alleviate the influence of adversarial perturbations while preserving essential semantic information, thereby conferring an inherent advantage in defending against adversarial attacks. To address limitations of basic resampling methods, we propose implicit representation-driven image resampling ({IRAD}). The method involves constructing an implicit continuous representation that enables us to represent any input image within a continuous coordinate space, and introducing {SampleNet}, which automatically generates pixel-wise shifts for resampling in response to different inputs. Extensive experiments demonstrate that our method significantly enhances the adversarial robustness of diverse deep models against various attacks while maintaining high accuracy on clean images.},
  author = {Yue Cao and Tianlin Li and Xiaofeng Cao and Ivor W. Tsang and Yang Liu and Qing Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cao2024irad.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jFa5KESW65},
  publisher = {OpenReview.net},
  title = {{IRAD}: Implicit Representation-driven Image Resampling against Adversarial Attacks},
  url = {https://openreview.net/forum?id=jFa5KESW65},
  year = {2024}
}

@inproceedings{cao2024physicsregulated,
  abstract = {This paper proposes the Phy-{DRL}: a physics-regulated deep reinforcement learning ({DRL}) framework for safety-critical autonomous systems. The Phy-{DRL} has three distinguished invariant-embedding designs: i) residual action policy (i.e., integrating data-driven-{DRL} action policy and physics-model-based action policy), ii) automatically constructed safety-embedded reward, and iii) physics-model-guided neural network ({NN}) editing, including link editing and activation editing. Theoretically, the Phy-{DRL} exhibits 1) a mathematically provable safety guarantee and 2) strict compliance of critic and actor networks with physics knowledge about the action-value function and action policy. Finally, we evaluate the Phy-{DRL} on a cart-pole system and a quadruped robot. The experiments validate the theoretical results and demonstrate that Phy-{DRL} provides guaranteed safety compared to purely data-driven approaches.},
  author = {Hongpeng Cao and Yanbing Mao and Lui Sha and Marco Caccamo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cao2024physicsregulated.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=5Dwqu5urzs},
  publisher = {OpenReview.net},
  title = {Physics-Regulated Deep Reinforcement Learning: Invariant Embeddings},
  url = {https://openreview.net/forum?id=5Dwqu5urzs},
  year = {2024}
}

@inproceedings{cao2024btr,
  abstract = {Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models ({LMs}) is slow and difficult to scale due to processing large amounts of retrieved text. To address this, we introduce binary token representations ({BTR}), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy from binarization, we develop new calibration techniques and training objectives that restore performance. {BTR} creates cacheable binary token representations of the retrieved passages such that the passage encoding can be precomputed offline and stored in a compact format. Experiments show that on five knowledge-intensive {NLP} tasks, {BTR} accelerates inference by up to 4x and reduces storage by over 100x while maintaining over 95\% task performance.},
  author = {Qingqing Cao and Sewon Min and Yizhong Wang and Hannaneh Hajishirzi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cao2024btr.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=3TO3TtnOFl},
  publisher = {OpenReview.net},
  title = {{BTR}: Binary Token Representations for Efficient Retrieval Augmented Language Models},
  url = {https://openreview.net/forum?id=3TO3TtnOFl},
  year = {2024}
}

@inproceedings{cao2024impact,
  abstract = {Integral reinforcement learning ({IntRL}) offers a pathway to optimal control without requiring the system dynamics or a value function, by iteratively solving a policy evaluation problem whose solution is the improved policy. {IntRL} requires precise computation of utility function integrals during policy evaluation, achieved through quadrature rules using discrete-time state samples. This paper investigates how the choice of computational method significantly impacts control performance. We draw parallels between {IntRL}'s policy iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation, showing that computational error in policy evaluation creates additional error terms in each iteration. When the utility function exists in a reproducing kernel Hilbert space ({RKHS}), we demonstrate that optimal quadrature is achievable using Bayesian quadrature with the {RKHS}-inducing kernel function. We provide convergence rate analysis for {IntRL} using both trapezoidal rule and Bayesian quadrature with Mat√©rn kernels. Results from canonical control tasks show that larger sample sizes reduce accumulated costs, with notable differences between quadrature methods.},
  author = {Wenhan Cao and Wei Pan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cao2024impact.pdf:pdf},
  note = {DBLP last modified: 2025-01-27},
  pdf = {https://openreview.net/pdf?id=xJEd8PkdNz},
  publisher = {OpenReview.net},
  title = {Impact of Computation in Integral Reinforcement Learning for Continuous-Time Control},
  url = {https://openreview.net/forum?id=xJEd8PkdNz},
  year = {2024}
}

@inproceedings{cao2024mvsformer,
  abstract = {In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.},
  author = {Chenjie Cao and Xinlin Ren and Yanwei Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cao2024mvsformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wXWfvSpYHh},
  publisher = {OpenReview.net},
  title = {MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo},
  url = {https://openreview.net/forum?id=wXWfvSpYHh},
  year = {2024}
}

@inproceedings{cardace2024neural,
  abstract = {Driven by the appealing properties of neural fields for storing and communicating 3D data, the problem of directly processing them to address tasks such as classification and part segmentation has emerged and has been investigated in recent works. Early approaches employ neural fields parameterized by shared networks trained on the whole dataset, achieving good task performance but sacrificing reconstruction quality. To improve the latter, later methods focus on individual neural fields parameterized as large Multi-Layer Perceptrons (MLPs), which are, however, challenging to process due to the high dimensionality of the weight space, intrinsic weight space symmetries, and sensitivity to random initialization. Hence, results turn out significantly inferior to those achieved by processing explicit representations, e.g., point clouds or meshes. In this work, we introduce an approach to process neural fields encoded through tri-planar hybrid representations. While processing a field with the same reconstruction quality, we achieve task performance far superior to frameworks that process large MLPs and, for the first time, almost on par with architectures handling explicit representations.},
  author = {Adriano Cardace and Pierluigi Zama Ramirez and Francesco Ballerini and Allan Zhou and Samuele Salti and Luigi Di Stefano},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cardace2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=zRkM6UcA22},
  publisher = {OpenReview.net},
  title = {Neural Processing of Tri-Plane Hybrid Neural Fields},
  url = {https://openreview.net/forum?id=zRkM6UcA22},
  year = {2024}
}

@inproceedings{careil2024towards,
  abstract = {Image codecs are typically optimized to trade-off bitrate vs distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate, we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model PerCo for 'perceptual compression', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images.},
  author = {Marl√®ne Careil and Matthew J. Muckley and Jakob Verbeek and St√©phane Lathuili√®re},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/careil2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ktdETU9JBg},
  publisher = {OpenReview.net},
  title = {Towards image compression with perfect realism at ultra-low bitrates},
  url = {https://openreview.net/forum?id=ktdETU9JBg},
  year = {2024}
}

@inproceedings{carlsson2024branchgan,
  abstract = {Recent advancements in open domain text generation have been spearheaded by Transformer-based large language models. Leveraging efficient parallelization and vast training datasets, these models achieve unparalleled text generation capabilities. However, current models are known to suffer from deficiencies such as repetitive texts, looping issues, and lack of robustness. Adversarial training through generative adversarial networks (GANs) is a proposed solution, but earlier research in this direction has predominantly focused on older architectures or narrow tasks. In this work, we propose a computationally efficient GAN approach for sequential data that utilizes the parallelization capabilities of Transformer models. Our method generates multiple branching sequences from each training sample, while incorporating typical next-step prediction loss on original data. This approach results in dense reward and loss signal for both generator and discriminator, leading to stable training dynamics. We apply our method to pre-trained language models using less than 0.01% of their original training data, and comprehensive human evaluation shows significant improvements in text quality while avoiding previously reported sparsity problems of GAN approaches.},
  author = {Fredrik Carlsson and Johan Broberg and Erik Hillbom and Magnus Sahlgren and Joakim Nivre},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/carlsson2024branchgan.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=sHEJJmzBIN},
  publisher = {OpenReview.net},
  title = {Branch-GAN: Improving Text Generation with (not so) Large Language Models},
  url = {https://openreview.net/forum?id=sHEJJmzBIN},
  year = {2024}
}

@inproceedings{caro2024brainlm,
  abstract = {We introduce the Brain Language Model ({BrainLM}), a foundation model for brain activity dynamics trained on 6,700 hours of {fMRI} recordings. Utilizing self-supervised masked-prediction training, {BrainLM} demonstrates proficiency in both fine-tuning and zero-shot inference tasks. Fine-tuning allows for the accurate prediction of clinical variables like age, anxiety, and {PTSD} as well as forecasting of future brain states. Critically, the model generalizes well to entirely new external cohorts not seen during training. In zero-shot inference mode, {BrainLM} can identify intrinsic functional networks directly from raw {fMRI} data without any network-based supervision during training. The model also generates interpretable latent representations that reveal relationships between brain activity patterns and cognitive states. Overall, {BrainLM} offers a versatile and interpretable framework for elucidating the complex spatiotemporal dynamics of human brain activity. It serves as a powerful `lens' through which massive repositories of {fMRI} data can be analyzed in new ways, enabling more effective interpretation and utilization at scale. The work demonstrates the potential of foundation models to advance computational neuroscience research.},
  author = {Josue Ortega Caro and Antonio Henrique de Oliveira Fonseca and Syed A. Rizvi and Matteo Rosati and Christopher Averill and James L. Cross and Prateek Mittal and Emanuele Zappala and Rahul Madhav Dhodapkar and Chadi Abdallah and David van Dijk},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/caro2024brainlm.pdf:pdf},
  note = {DBLP last modified: 2025-05-09},
  pdf = {https://openreview.net/pdf?id=RwI7ZEfR27},
  publisher = {OpenReview.net},
  title = {{BrainLM}: A foundation model for brain activity recordings},
  url = {https://openreview.net/forum?id=RwI7ZEfR27},
  year = {2024}
}

@inproceedings{carvalho2024reclaiming,
  abstract = {Recent works have introduced {LEAPS} and {HPRL}, systems that learn latent spaces of domain-specific languages, which are used to define programmatic policies for partially observable Markov decision processes ({POMDPs}). These systems induce a latent space while optimizing losses such as the behavior loss, which aim to achieve locality in program behavior, meaning that vectors close in the latent space should correspond to similarly behaving programs. In this paper, we show that the programmatic space, induced by the domain-specific language and requiring no training, presents values for the behavior loss similar to those observed in latent spaces presented in previous work. Moreover, algorithms searching in the programmatic space significantly outperform those in {LEAPS} and {HPRL}. We show that algorithms are more likely to stop at local maxima when searching in the latent space than when searching in the programmatic space. This implies that the optimization topology of the programmatic space, induced by the reward function in conjunction with the neighborhood function, is more conducive to search than that of the latent space.},
  author = {Tales Henrique Carvalho and Kenneth Tjhia and Levi H. S. Lelis},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/carvalho2024reclaiming.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NGVljI6HkR},
  publisher = {OpenReview.net},
  title = {Reclaiming the Source of Programmatic Policies: Programmatic versus Latent Spaces},
  url = {https://openreview.net/forum?id=NGVljI6HkR},
  year = {2024}
}

@inproceedings{castanyer2024improving,
  abstract = {Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Several exploration objectives like count-based bonuses, pseudo-counts, and state-entropy maximization are non-stationary and hence are difficult to optimize for the agent. While this issue is generally known, it is usually omitted and solutions remain under-explored. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration ({SOFE}) framework. {SOFE} requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. {SOFE} is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. We show that {SOFE} improves the performance of several exploration objectives, including count-based bonuses, pseudo-counts, and state-entropy maximization. Moreover, {SOFE} outperforms prior methods that attempt to stabilize the optimization of intrinsic objectives. We demonstrate the efficacy of {SOFE} in hard-exploration problems, including sparse-reward tasks, pixel-based observations, {3D} navigation, and procedurally generated environments.},
  author = {Roger Creus Castanyer and Joshua Romoff and Glen Berseth},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/castanyer2024improving.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=YbZxT0SON4},
  publisher = {OpenReview.net},
  title = {Improving Intrinsic Exploration by Creating Stationary Objectives},
  url = {https://openreview.net/forum?id=YbZxT0SON4},
  year = {2024}
}

@inproceedings{cen2024learning,
  abstract = {Offline reinforcement learning ({RL}) offers a promising direction for learning policies from pre-collected datasets without requiring further interactions with the environment. However, existing methods struggle to handle out-of-distribution ({OOD}) extrapolation errors, especially in sparse reward or scarce data settings. In this paper, we propose a novel training algorithm called Conservative Density Estimation ({CDE}), which addresses this challenge by explicitly imposing constraints on the state-action occupancy stationary distribution. {CDE} overcomes the limitations of existing approaches, such as the stationary distribution correction method, by addressing the support mismatch issue in marginal importance sampling. {CDE} integrates the strengths of both pessimism-based and {DICE}-based approaches, employing the principles of conservative {Q}-learning in a unique way, incorporating pessimism within the stationary distribution space to achieve a theoretically-backed conservative occupation distribution. {CDE} does not rely on Bellman update-style value estimation, favoring a direct behavior-policy-agnostic stationary distribution correction that improves performance in sparse reward scenarios.},
  author = {Zhepeng Cen and Zuxin Liu and Zitong Wang and Yihang Yao and Henry Lam and Ding Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/cen2024learning.pdf:pdf},
  note = {DBLP last modified: 2025-06-15},
  pdf = {https://openreview.net/pdf?id=4WM0OogPTx},
  publisher = {OpenReview.net},
  title = {Learning from Sparse Offline Datasets via Conservative Density Estimation},
  url = {https://openreview.net/forum?id=4WM0OogPTx},
  year = {2024}
}

@inproceedings{chae2024twotimescale,
  abstract = {Minimax problems are notoriously challenging to optimize. However, we present that the two-timescale extragradient method can be a viable solution. By utilizing dynamical systems theory, we show that it converges to points that satisfy the second-order necessary condition of local minimax points, under mild conditions that the two-timescale gradient descent ascent fails to work. This work provably improves upon all previous results on finding local minimax points, by eliminating a crucial assumption that the Hessian with respect to the maximization variable is nondegenerate. Our analysis provides theoretical guarantees for convergence to local minimax points using a two-timescale approach, which represents a significant advancement in minimax optimization theory.},
  author = {Jiseok Chae and Kyuwon Kim and Donghwan Kim},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/chae2024twotimescale.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6CIGhcJYJH},
  publisher = {OpenReview.net},
  title = {Two-timescale Extragradient for Finding Local Minimax Points},
  url = {https://openreview.net/forum?id=6CIGhcJYJH},
  year = {2024}
}

@inproceedings{chakraborty2024statistical,
  abstract = {Wasserstein Autoencoders ({WAEs}) are a variant of Variational Autoencoders ({VAEs}) that aim to improve model efficiency and interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which {WAEs} are applied -- such as natural images -- are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. In this paper, we show that one can ensure encoding and decoding guarantees, where the encoded distribution is close enough to the target latent distribution, and the generator maps back the encoded points close to the original points. Under additional regularity assumptions, we demonstrate that the approximating push-forward measure induced by the generator is close to the target distribution in the Wasserstein sense. We show that {WAE}s can achieve an excess risk that, as a function of the number of samples, depends only on the intrinsic data dimensions rather than the high dimensions of the ambient feature-space. Our theoretical results show that error rates for {WAEs} depend primarily on the intrinsic dimension of the data, providing fundamental insights into their performance on intrinsically low-dimensional data.},
  author = {Saptarshi Chakraborty and Peter L. Bartlett},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/chakraborty2024statistical.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WjRPZsfeBO},
  publisher = {OpenReview.net},
  title = {A Statistical Analysis of {Wasserstein} Autoencoders for Intrinsically Low-dimensional Data},
  url = {https://openreview.net/forum?id=WjRPZsfeBO},
  year = {2024}
}

@inproceedings{chakraborty2024parl,
  abstract = {Policy alignment in reinforcement learning is essential for ensuring that agents behave according to human preferences and values. However, existing algorithmic designs suffer from a major gap due to a lack of precise characterization of the dependence of the alignment objective on the data generated by policy trajectories. This shortfall contributes to the sub-optimal performance observed in contemporary algorithms. In this paper, we present {PARL}, a novel unified bilevel optimization-based framework formulated to address the critical issue of policy alignment in reinforcement learning using utility or preference-based feedback. The framework addresses these concerns by explicitly parameterizing the distribution of the upper alignment objective (reward design) by the lower optimal variable (optimal policy for the designed reward). From an optimization perspective, this formulation leads to a new class of stochastic bilevel problems where the stochasticity at the upper objective depends upon the lower-level variable. This work presents the first formulation of {RLHF} as a bilevel optimization problem which generalizes existing {RLHF} formulations and addresses distribution shift issues. We devise an algorithm named {A-PARL} to solve the {PARL} problem, establishing sample complexity bounds of order {O}(1/T). Empirical results demonstrate that {PARL} can address alignment concerns in {RL} with significant improvements (up to 63\% in terms of required samples) for policy alignment in large-scale environments.},
  author = {Souradip Chakraborty and Amrit Singh Bedi and Alec Koppel and Huazheng Wang and Dinesh Manocha and Mengdi Wang and Furong Huang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/chakraborty2024parl.pdf:pdf},
  note = {DBLP last modified: 2025-04-27},
  pdf = {https://openreview.net/pdf?id=ByR3NdDSZB},
  publisher = {OpenReview.net},
  title = {{PARL}: A Unified Framework for Policy Alignment in Reinforcement Learning from Human Feedback},
  url = {https://openreview.net/forum?id=ByR3NdDSZB},
  year = {2024}
}

@inproceedings{chakraborty2024sparse,
  abstract = {Recurrent Spiking Neural Networks ({RSNNs}) have emerged as a computationally efficient and brain-inspired learning model. The design of sparse {RSNNs} with fewer neurons and synapses helps reduce the computational complexity of {RSNNs}. In this paper, we present a task-agnostic methodology for designing sparse {RSNNs} by pruning an untrained (arbitrarily initialized) large model. We introduce a novel Lyapunov Noise Pruning ({LNP}) algorithm that uses graph sparsification methods and utilizes Lyapunov exponents to design a stable sparse {RSNN} from an untrained {RSNN}. The {LNP} can leverage diversity in neuronal timescales to design a sparse Heterogeneous {RSNN} ({HRSNN}). The same sparse {HRSNN} model can be trained for different tasks, such as image classification and time-series prediction. {LNP} increases computational efficiency (fewer neurons and synapses) and prediction performance of {RSNNs} compared to traditional activity-based pruning of trained dense models. This task-agnostic pruning strategy ensures the pruned network remains stable and effective across various applications without the need for extensive retraining.},
  author = {Biswadeep Chakraborty and Beomseok Kang and Harshit Kumar and Saibal Mukhopadhyay},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/chakraborty2024sparse.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0jsfesDZDq},
  publisher = {OpenReview.net},
  title = {Sparse Spiking Neural Network: Exploiting Heterogeneity in Timescales for Pruning Recurrent SNN},
  url = {https://openreview.net/forum?id=0jsfesDZDq},
  year = {2024}
}

@inproceedings{chalapathi2024scaling,
  abstract = {Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating {PDE}-constrained optimization as individual layers in neural networks, which enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. We develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts ({MoE}), which imposes the constraint over smaller decomposed domains, each solved by an `expert' through differentiable optimization. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural {PDE} solver setting for predicting the dynamics of challenging non-linear systems. Our method can scale to more complex systems while maintaining computational efficiency and physical accuracy.},
  author = {Nithin Chalapathi and Yiheng Du and Aditi S. Krishnapriyan},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/chalapathi2024scaling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=u3dX2CEIZb},
  publisher = {OpenReview.net},
  title = {Scaling physics-informed hard constraints with mixture-of-experts},
  url = {https://openreview.net/forum?id=u3dX2CEIZb},
  year = {2024}
}

@inproceedings{chan2024chateval,
  abstract = {Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models ({LLMs}), researchers have explored {LLMs}' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. We construct a multi-agent referee team called {ChatEval} to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation ({NLG}) tasks. Experiments on two benchmarks illustrate that {ChatEval} delivers superior accuracy and correlation in alignment with human assessment. The diverse role prompts (different personas) are essential in the multi-agent debate process; utilizing the same role description in the prompts can lead to a degradation in performance. Our qualitative analysis shows that {ChatEval} transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments.},
  author = {Chi-Min Chan and Weize Chen and Yusheng Su and Jianxuan Yu and Wei Xue and Shanghang Zhang and Jie Fu and Zhiyuan Liu},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/chan2024chateval.pdf:pdf},
  note = {DBLP last modified: 2024-09-19},
  pdf = {https://openreview.net/pdf?id=FQepisCUWu},
  publisher = {OpenReview.net},
  title = {{ChatEval}: Towards Better {LLM}-based Evaluators through Multi-Agent Debate},
  url = {https://openreview.net/forum?id=FQepisCUWu},
  year = {2024}
}

@inproceedings{chan2024internal,
  abstract = {Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverages internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods can be tailored to accommodate model-homogeneous FL methods such as FedAvg, FedProx, FedNova, Scaffold, and MOON, to expand their capabilities to handle the system heterogeneity. Copious experimental results validate the effectiveness of InCo Aggregation, spotlighting internal cross-layer gradients as a promising avenue to enhance the performance in heterogeneous FL.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Yun-Hin Chan and Rui Zhou and Running Zhao and Zhihan Jiang and Edith C. H. Ngai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2308.11464},
  file = {:/home/b/documents/inproceedings/chan2024internal.pdf:pdf},
  keywords = {Federated Learning, Heterogeneity, Convex optimization},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Cc0qk6r4Nd},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning},
  url = {https://openreview.net/forum?id=Cc0qk6r4Nd},
  year = {2024}
}

@inproceedings{chanda2024bayesian,
  abstract = {In a distributed machine learning setting like Federated Learning where there are multiple clients involved which update their individual weights to a single central server, often training on the entire individual client's dataset for each client becomes cumbersome. To address this issue we propose CORESET-PFEDBAYES : a personalized coreset weighted federated learning setup where the training updates for each individual clients are forwarded to the central server based on only individual client coreset based representative data points instead of the entire client data. Through theoretical analysis we present how the average generalization error is minimax optimal up to logarithm bounds (upper bounded by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2 \delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization error on the data likelihood differs from a vanilla Federated Learning setup as a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on different benchmark datasets based on a variety of recent personalized federated learning architectures show significant gains as compared to random sampling on the training data followed by federated learning, thereby indicating how intelligently selecting such training samples can help in performance. Additionally, through experiments on medical datasets our proposed method showcases some gains as compared to other submodular optimization based approaches used for subset selection on client's data.},
  address = {Vienna, Austria},
  author = {Prateek Chanda and Shrey Modi and Ganesh Ramakrishnan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chanda2024bayesian.pdf:pdf},
  keywords = {Federated learning, Personalized federated learning, Bayesian coreset, Submodularity, Variational inference, Coresets, Optimization},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uz7d2N2zul},
  publisher = {OpenReview.net},
  title = {Bayesian Coreset Optimization for Personalized Federated Learning},
  url = {https://openreview.net/forum?id=uz7d2N2zul},
  year = {2024}
}

@inproceedings{chandak2024adaptive,
  abstract = {Indirect experiments provide a valuable framework for estimating treatment effects in situations where conducting randomized control trials (RCTs) is impractical or unethical. Unlike RCTs, indirect experiments estimate treatment effects by leveraging (conditional) instrumental variables, enabling estimation through encouragement and recommendation rather than strict treatment assignment. However, the sample efficiency of such estimators depends not only on the inherent variability in outcomes but also on the varying compliance levels of users with the instrumental variables and the choice of estimator being used, especially when dealing with numerous instrumental variables. While adaptive experiment design has a rich literature for \textit{direct} experiments, in this paper we take the initial steps towards enhancing sample efficiency for \textit{indirect} experiments by adaptively designing a data collection policy over instrumental variables. Our main contribution is a practical computational procedure that utilizes influence functions to search for an optimal data collection policy, minimizing the mean-squared error of the desired (non-linear) estimator. Through experiments conducted in various domains inspired by real-world applications, we showcase how our method can significantly improve the sample efficiency of indirect experiments.},
  address = {Vienna, Austria},
  author = {Yash Chandak and Shiv Shankar and Vasilis Syrgkanis and Emma Brunskill},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chandak2024adaptive.pdf:pdf},
  keywords = {instrument variable, experiment design, indirect experiments, adaptive design},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4Zz5UELkIt},
  publisher = {OpenReview.net},
  title = {Adaptive Instrument Design for Indirect Experiments},
  url = {https://openreview.net/forum?id=4Zz5UELkIt},
  year = {2024}
}

@inproceedings{chang2024rapper,
  abstract = {Natural Language Explanation (NLE) in vision and language tasks aims to provide human-understandable explanations for the associated decision-making process. In practice, one might encounter explanations which lack informativeness or contradict visual-grounded facts, known as implausibility and hallucination problems, respectively. To tackle these challenging issues, we consider the task of visual question answering (VQA) and introduce Rapper, a two-stage Reinforced Rationale-Prompted Paradigm. By knowledge distillation, the former stage of Rapper infuses rationale-prompting via large language models (LLMs), encouraging the rationales supported by language-based facts. As for the latter stage, a unique Reinforcement Learning from NLE Feedback (RLNF) is introduced for injecting visual facts into NLE generation. Finally, quantitative and qualitative experiments on two VL-NLE benchmarks show that Rapper surpasses state-of-the-art VQA-NLE methods while providing plausible and faithful NLE.},
  address = {Vienna, Austria},
  author = {Kai-Po Chang and Chi-Pin Huang and Wei-Yuan Cheng and Fu-En Yang and Chien-Yi Wang and Yung-Hsuan Lai and Yu-Chiang Frank Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chang2024rapper.pdf:pdf},
  keywords = {Visual Question Answering, Natural Language Explanation, Reinforcement Learning, Large Language Models},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bshfchPM9H},
  publisher = {OpenReview.net},
  title = {{RAPPER}: Reinforced Rationale-Prompted Paradigm for Natural Language Explanation in Visual Question Answering},
  url = {https://openreview.net/forum?id=bshfchPM9H},
  year = {2024}
}

@inproceedings{chang2024booookscore,
  abstract = {Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators. We release code and annotations to spur more principled research on book-length summarization.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Yapei Chang and Kyle Lo and Tanya Goyal and Mohit Iyyer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.00785},
  file = {:/home/b/documents/inproceedings/chang2024booookscore.pdf:pdf},
  keywords = {Summarization, Large Language Models, Evaluation, Book-length documents},
  month = {5},
  note = {Oral presentation; {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=7Ttk3RzDeu},
  primaryclass = {cs.CL},
  publisher = {OpenReview.net},
  title = {{BooookScore}: A Systematic Exploration of Book-length Summarization in the Era of {LLM}s},
  url = {https://openreview.net/forum?id=7Ttk3RzDeu},
  year = {2024}
}

@inproceedings{chang2024adversarial,
  abstract = {Adversarial imitation learning (AIL) has stood out as a dominant framework across various imitation learning (IL) applications, with Discriminator Actor Critic (DAC) demonstrating the effectiveness of off-policy learning algorithms in improving sample efficiency and scalability to higher-dimensional observations. Despite DAC's empirical success, the original AIL objective is on-policy and DAC's ad-hoc application of off-policy training does not guarantee successful imitation. Follow-up work such as ValueDICE tackles this issue by deriving a fully off-policy AIL objective. Instead in this work, we develop a novel and principled AIL algorithm via the framework of boosting. Like boosting, our new algorithm, AILBoost, maintains an ensemble of weighted weak learners (i.e., policies) and trains a discriminator that witnesses the maximum discrepancy between the distributions of the ensemble and the expert policy. We maintain a weighted replay buffer to represent the state-action distribution induced by the ensemble, allowing us to train discriminators using the entire data collected so far. Empirically, we evaluate our algorithm on both controller state-based and pixel-based environments from the DeepMind Control Suite. AILBoost outperforms DAC on both types of environments, demonstrating the benefit of properly weighting replay buffer data for off-policy training. On state-based environments, AILBoost outperforms ValueDICE and IQ-Learn, achieving state-of-the-art performance with as little as one expert trajectory.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Jonathan D. Chang and Dhruv Sreenivas and Yingbing Huang and Kiant\'{e} Brantley and Wen Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2404.08513},
  file = {:/home/b/documents/inproceedings/chang2024adversarial.pdf:pdf},
  keywords = {Imitation Learning, Adversarial Learning, Boosting, Reinforcement Learning},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DuQkqSe9en},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Adversarial Imitation Learning via Boosting},
  url = {https://openreview.net/forum?id=DuQkqSe9en},
  year = {2024}
}

@inproceedings{chang2024how,
  abstract = {Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the results. This either produces high-frequency flickering, or texture-sticking artifacts that are not amenable to post-processing. With this in mind, we propose a novel method for preserving temporal correlations in a sequence of noise samples. This approach is materialized by a novel noise representation, dubbed $\int$-noise (integral noise), that reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses $\int$-noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed $\int$-noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation.},
  address = {Vienna, Austria},
  author = {Pascal Chang and Jingwei Tang and Markus Gross and Vinicius C. Azevedo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chang2024how.pdf:pdf},
  keywords = {Diffusion Models, Video Generation, Noise Sampling, Temporal Correlation},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=pzElnMrgSD},
  publisher = {OpenReview.net},
  title = {How {I} Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models},
  url = {https://openreview.net/forum?id=pzElnMrgSD},
  year = {2024}
}

@inproceedings{charton2024learning,
  abstract = {The predictions of small transformers, trained to calculate the greatest common divisor (GCD) of two positive integers, can be fully characterized by looking at model inputs and outputs. As training proceeds, the model learns a list $\mathcal D$ of integers, products of divisors of the base used to represent integers and small primes, and predicts the largest element of $\mathcal D$ that divides both inputs. Training distributions impact performance. Models trained from uniform operands only learn a handful of GCD (up to $38$ GCD $\leq100$). Log-uniform operands boost performance to $73$ GCD $\leq 100$, and a log-uniform distribution of outcomes (i.e. GCD) to $91$. However, training from uniform (balanced) GCD breaks explainability.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Fran√ßois Charton},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2308.15594},
  file = {:/home/b/documents/inproceedings/charton2024learning.pdf:pdf},
  keywords = {Transformer, Explainability, Mathematical Reasoning, Algorithmic Learning},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=cmcD05NPKa},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Learning the Greatest Common Divisor: Explaining Transformer Predictions},
  url = {https://openreview.net/forum?id=cmcD05NPKa},
  year = {2024}
}

@inproceedings{chattopadhyay2024bootstrapping,
  abstract = {Variational Information Pursuit (V-IP) is an interpretable-by-design framework that makes predictions by sequentially selecting a short chain of user-defined, interpretable queries about the data that are most informative for the task. The prediction is based solely on the obtained query answers, which also serve as a faithful explanation for the prediction. Applying the framework to any task requires (i) specification of a query set, and (ii) densely annotated data with query answers to train classifiers to answer queries at test time. This limits V-IP's application to small-scale tasks where manual data annotation is feasible. In this work, we focus on image classification tasks and propose to relieve this bottleneck by leveraging pretrained language and vision models. Specifically, following recent work, we propose to use GPT, a Large Language Model, to propose semantic concepts as queries for a given classification task. To answer these queries, we propose a light-weight Concept Question-Answering network (Concept-QA) which learns to answer binary queries about semantic concepts in images. We design pseudo-labels to train our Concept-QA model using GPT and CLIP (a Vision-Language Model). Empirically, we find our Concept-QA model to be competitive with state-of-the-art VQA models in terms of answering accuracy but with an order of magnitude fewer parameters. This allows for seamless integration of Concept-QA into the V-IP framework as a fast-answering mechanism. We name this method Concept-QA+V-IP. Finally, we show on several datasets that Concept-QA+V-IP produces shorter, interpretable query chains which are more accurate than V-IP trained with CLIP-based answering systems.},
  address = {Vienna, Austria},
  author = {Aditya Chattopadhyay and Kwan Ho Ryan Chan and Ren√© Vidal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chattopadhyay2024bootstrapping.pdf:pdf},
  keywords = {Interpretability, Image Classification, Large Language Models, Vision Models, Question Answering},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9bmTbVaA2A},
  publisher = {OpenReview.net},
  title = {Bootstrapping Variational Information Pursuit with Large Language and Vision Models for Interpretable Image Classification},
  url = {https://openreview.net/forum?id=9bmTbVaA2A},
  year = {2024}
}

@inproceedings{chattopadhyay2024augcal,
  abstract = {Synthetic data (Sim) drawn from simulators have emerged as a popular alternative for training models where acquiring annotated real-world images is difficult. However, transferring models trained on synthetic images to real-world applications can be challenging due to appearance disparities. A commonly employed solution to counter this Sim2Real gap is unsupervised domain adaptation, where models are trained using labeled Sim data and unlabeled Real data. Mispredictions made by such Sim2Real adapted models are often associated with miscalibration ‚Äì stemming from overconfident predictions on real data. In this paper, we introduce AUGCAL, a simple training-time patch for unsupervised adaptation that improves Sim2Real adapted models by ‚Äì (1) reducing overall miscalibration, (2) reducing overconfidence in incorrect predictions and (3) improving confidence score reliability by better guiding misclassification detection ‚Äì all while retaining or improving Sim2Real performance. Given a base Sim2Real adaptation algorithm, at training time, AUGCAL involves replacing vanilla Sim images with strongly augmented views (AUG intervention) and additionally optimizing for a training time calibration loss on augmented Sim predictions (CAL intervention). We motivate AUGCAL using a brief analytical justification of how to reduce miscalibration on unlabeled REAL data. Through our experiments, we empirically show the efficacy of AUGCAL across multiple adaptation methods, backbones, tasks and shifts.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Prithvijit Chattopadhyay and Bharat Goyal and Boglarka Ecsedi and Viraj Prabhu and Judy Hoffman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2312.06106},
  file = {:/home/b/documents/inproceedings/chattopadhyay2024augcal.pdf:pdf},
  keywords = {Domain Adaptation, Sim2Real, Uncertainty Calibration, Computer Vision},
  month = {5},
  note = {Poster presentation; {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WNQjN5HzXt},
  primaryclass = {cs.CV},
  publisher = {OpenReview.net},
  title = {{AUGCAL}: Improving {Sim2Real} Adaptation by Uncertainty Calibration on Augmented Synthetic Images},
  url = {https://openreview.net/forum?id=WNQjN5HzXt},
  year = {2024}
}

@inproceedings{chaudhari2024chameleon,
  abstract = {The integration of Machine Learning (ML) in numerous critical applications introduces a range of privacy concerns for individuals who provide their datasets for ML training purposes. One such privacy risk is Membership Inference (MI), in which an adversary seeks to determine whether a particular data point was included in the training dataset of a model. Current state-of-the-art MI approaches capitalize on access to the model's predicted confidence scores to successfully perform membership inference, and employ data poisoning to further enhance their effectiveness. In this work, we focus on the less explored and more realistic label-only setting, where the model provides only the predicted label as output. We show that existing label-only attacks are ineffective at inferring membership in the low False Positive Rate (FPR) regime. To address this challenge, we propose a new attack Chameleon that leverages a novel data poisoning strategy and an efficient query selection method to achieve significantly more accurate membership inference than existing label-only attacks, especially for low FPRs.},
  author = {Harsh Chaudhari and Giorgio Severi and Alina Oprea and Jonathan Ullman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chaudhari2024chameleon.pdf:pdf},
  keywords = {Privacy Attack, Membership Inference, Data Poisoning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4DoSULcfG6},
  primaryarea = {Societal considerations including fairness, safety, privacy},
  publisher = {OpenReview.net},
  title = {Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning},
  url = {https://openreview.net/forum?id=4DoSULcfG6},
  year = {2024}
}

@inproceedings{chaudhuri2024learning,
  abstract = {Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder œÜ* that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder Œ¶*_œÑ. We prove that non-commutativity steers the optimization towards Œ¶*_œÑ instead of œÜ*, bringing the H-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commutative invariance (NCI) can leverage source domain samples to meet the sample complexity needs of learning Œ¶*_œÑ, surpassing SOTA invariance learning algorithms for domain adaptation, at times by over 2%, approaching the performance of an oracle.},
  author = {Abhra Chaudhuri and Serban Georgescu and Anjan Dutta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/abhrac/nci},
  file = {:/home/b/documents/inproceedings/chaudhuri2024learning.pdf:pdf},
  keywords = {Invariance Learning, Domain Adaptation},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=tUVG9nGzgE},
  primaryarea = {Learning Theory},
  publisher = {OpenReview.net},
  title = {Learning Conditional Invariances through Non-Commutativity},
  url = {https://openreview.net/forum?id=tUVG9nGzgE},
  year = {2024}
}

@inproceedings{chavdarova2024primaldual,
  abstract = {Yang et al. (2023) recently showed how to use first-order gradient methods to solve general variational inequalities (VIs) under a limiting assumption that analytic solutions of specific subproblems are available. In this paper, we circumvent this assumption via a warm-starting technique where we solve subproblems approximately and initialize variables with the approximate solution found at the previous iteration. We prove the convergence of this method and show that the gap function of the last iterate of the method decreases at a rate of $\mathcal{O}(\frac{1}{\sqrt{K}})$ when the operator is $L$-Lipschitz and monotone. In numerical experiments, we show that this technique can converge much faster than its exact counterpart. Furthermore, for the cases when the inequality constraints are simple, we introduce an alternative variant of ACVI and establish its convergence under the same conditions. Finally, we relax the smoothness assumptions in Yang et al., yielding, to our knowledge, the first convergence result for VIs with general constraints that does not rely on the assumption that the operator is $L$-Lipschitz.},
  arxiv = {https://arxiv.org/abs/2210.15659},
  author = {Tatjana Chavdarova and Tong Yang and Matteo Pagliardini and Michael Jordan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/Chavdarova/I-ACVI},
  file = {:/home/b/documents/inproceedings/chavdarova2024primaldual.pdf:pdf},
  keywords = {Variational Inequalities, Primal-Dual Methods, First-Order Gradient Methods, Optimization Theory, Convergence Analysis},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=RsztjXcvUf},
  primaryarea = {Optimization},
  publisher = {OpenReview.net},
  title = {A Primal-Dual Approach to Solving Variational Inequalities with General Constraints},
  url = {https://openreview.net/forum?id=RsztjXcvUf},
  year = {2024}
}

@inproceedings{chefer2024hidden,
  abstract = {Text-to-image diffusion models have demonstrated an unparalleled ability to generate high-quality, diverse images from a textual prompt. However, the internal representations learned by these models remain an enigma. In this work, we propose Conceptor, a novel method to interpret the internal representation of a textual concept by a diffusion model. This interpretation is obtained by decomposing the concept into a small set of human-interpretable textual elements. This decomposition is derived by learning a pseudo-token that is a sparse weighted combination of tokens from the model's vocabulary, with the objective of reconstructing the images generated for the given concept. Applied over the state-of-the-art Stable Diffusion model, this decomposition reveals non-trivial and surprising structures in the representations of concepts. For example, some concepts such as "a president" or "a composer" are dominated by specific instances (e.g., "Obama", "Biden") and their interpolations. Other concepts, such as "happiness" combine associated terms that can be concrete ("family", "laughter") or abstract ("friendship", "emotion"). In addition to peering into the inner workings of Stable Diffusion, our method also enables applications such as single-image decomposition to tokens, bias detection and mitigation, and semantic image manipulation.},
  arxiv = {https://arxiv.org/abs/2306.00966},
  author = {Hila Chefer and Oran Lang and Mor Geva and Volodymyr Polosukhin and Assaf Shocher and Michal Irani and Inbar Mosseri and Lior Wolf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/hila-chefer/Conceptor},
  file = {:/home/b/documents/inproceedings/chefer2024hidden.pdf:pdf},
  keywords = {Diffusion Models, Concept Decomposition, Model Interpretability, Text-to-Image Generation},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=awWpHnEJDw},
  primaryarea = {Generative AI and Model Interpretability},
  projectpage = {https://hila-chefer.github.io/Conceptor/},
  publisher = {OpenReview.net},
  title = {The Hidden Language of Diffusion Models},
  url = {https://openreview.net/forum?id=awWpHnEJDw},
  year = {2024}
}

@inproceedings{chen2024lion,
  abstract = {Lion (Evolved Sign Momentum) is an optimizer discovered through program search and has shown promising results in training large AI models. Lion achieves results comparable to AdamW but with greater memory efficiency. However, the theoretical basis of Lion remained unclear. In this paper, we show that Lion is performing a constrained optimization and the key design choices of Lion are equivalent to performing a Hamiltonian mirror descent. Using both continuous-time and discrete-time analysis, we demonstrate that Lion is a theoretically grounded approach for minimizing a general loss function f(x) while enforcing a bound constraint ||x||_‚àû ‚â§ 1/Œª. Lion achieves this through the incorporation of decoupled weight decay, where Œª represents the weight decay coefficient. The analysis is facilitated by the development of a new Lyapunov function for the Lion updates. This analysis applies to a wide range of Lion-œÜ algorithms, where the sign(¬∑) operator in Lion is replaced by the subgradient of a convex function œÜ, leading to the solution of the general composite optimization problem min_x f(x) + œÜ*(x). Our findings provide valuable insights into the dynamics of Lion and pave the way for further enhancements and extensions of Lion-related algorithms.},
  arxiv = {https://arxiv.org/abs/2310.05898},
  author = {Lizhang Chen and Bo Liu and Kaizhao Liang and Qiang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024lion.pdf:pdf},
  keywords = {Lion Optimizer, Constrained Optimization, Lyapunov Function, Hamiltonian Mirror Descent, Optimization Theory},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=e4xS9ZarDr},
  primaryarea = {Optimization},
  publisher = {OpenReview.net},
  title = {Lion Secretly Solves a Constrained Optimization: As Lyapunov Predicts},
  url = {https://openreview.net/forum?id=e4xS9ZarDr},
  year = {2024}
}

@inproceedings{chen2024generalization,
  abstract = {Despite the widespread empirical success of ResNet, the generalization properties of deep ResNet are rarely explored beyond the lazy training regime. In this work, we investigate scaled ResNet in the limit of infinitely deep and wide neural networks, of which the gradient flow is described by a partial differential equation in the large-neural network limit, i.e., the mean-field regime. To derive the generalization bounds under this setting, our analysis necessitates a shift from the conventional time-invariant Gram matrix employed in the lazy training regime to a time-variant, distribution-dependent version. To this end, we provide a global lower bound on the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides, for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we establish the linear convergence of the empirical error and estimate the upper bound of the KL divergence over parameters distribution. Finally, we build the uniform convergence for generalization bound via Rademacher complexity. Our results offer new insights into the generalization ability of deep ResNet beyond the lazy training regime and contribute to advancing the understanding of the fundamental properties of deep neural networks.},
  arxiv = {https://arxiv.org/abs/2403.09889},
  author = {Yihang Chen and Fanghui Liu and Yiping Lu and Grigorios Chrysos and Volkan Cevher},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024generalization.pdf:pdf},
  keywords = {ResNet, Generalization Theory, Mean-Field Regime, Rademacher Complexity, Deep Neural Networks},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=tMzPZTvz2H},
  primaryarea = {Learning Theory},
  publisher = {OpenReview.net},
  title = {Generalization of Scaled Deep ResNets in the Mean-Field Regime},
  url = {https://openreview.net/forum?id=tMzPZTvz2H},
  year = {2024}
}

@inproceedings{chen2024clex,
  abstract = {Transformer-based Large Language Models (LLMs) are pioneering advances in many natural language processing tasks, however, their exceptional capabilities are restricted within the preset context window of Transformer. Position Embedding (PE) scaling methods, while effective in extending the context window to a specific length, demonstrate either notable limitations in their extrapolation abilities or sacrificing partial performance within the context window. To address these challenges, we propose Continuous Length EXtrapolation (CLEX) for LLMs. We generalise the PE scaling approaches to model the continuous dynamics by ordinary differential equations over the length scaling factor, thereby overcoming the constraints of current PE scaling methods designed for specific lengths. We can seamlessly incorporate CLEX into LLMs equipped with Rotary Position Embedding, such as LLaMA and GPT-NeoX, with negligible impact on training and inference latency. Experimental results reveal that CLEX can effectively extend the context window to over 4x or almost 8x training length, with no deterioration in performance. Furthermore, when evaluated on the practical LongBench benchmark, our model trained on a 4k length exhibits competitive performance against state-of-the-art open-source models trained on context lengths up to 32k.},
  arxiv = {https://arxiv.org/abs/2310.16450},
  author = {Guanzheng Chen and Xin Li and Zaiqiao Meng and Shangsong Liang and Lidong Bing},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/DAMO-NLP-SG/CLEX},
  file = {:/home/b/documents/inproceedings/chen2024clex.pdf:pdf},
  keywords = {Large Language Models, Position Embedding, Context Window, Length Extrapolation, Neural ODE},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wXpSidPpc5},
  primaryarea = {Natural Language Processing},
  publisher = {OpenReview.net},
  title = {CLEX: Continuous Length Extrapolation for Large Language Models},
  url = {https://openreview.net/forum?id=wXpSidPpc5},
  year = {2024}
}

@inproceedings{chen2024project,
  abstract = {Transfer learning with a small amount of target data is an effective and common approach to adapting a pre-trained model to distribution shifts. In some situations, target data labels may be expensive to obtain, so we may only have access to a limited number of target data points. To make the most of a very small target dataset, we propose Project and Probe (Pro¬≤), a lightweight, sample-efficient approach that learns a diverse set of predictive features and adapts to a target distribution by interpolating among them with a small target dataset. Specifically, Pro¬≤ first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro¬≤ then learns a linear classifier on top of these projected features using a small target dataset. Theoretically, we find that Pro¬≤ adapts to a target distribution by interpolating among source features rather than extrapolating far from the source distribution, and we show conditions under which Pro¬≤ recovers the optimal target classifier. Empirically, we evaluate Pro¬≤ on nine natural distribution shift datasets and find that it improves upon existing methods, particularly when the number of target samples is very small.},
  arxiv = {https://arxiv.org/abs/2302.05441},
  author = {Annie S. Chen and Yoonho Lee and Amrith Setlur and Sergey Levine and Chelsea Finn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024project.pdf:pdf},
  keywords = {Domain Adaptation, Transfer Learning, Sample Efficiency, Orthogonal Features, Distribution Shift},
  note = {Spotlight Presentation (top 5%)},
  pdf = {https://openreview.net/pdf?id=f6CBQYxXvr},
  primaryarea = {Transfer Learning and Domain Adaptation},
  publisher = {OpenReview.net},
  title = {Project and Probe: Sample-Efficient Adaptation by Interpolating Orthogonal Features},
  url = {https://openreview.net/forum?id=f6CBQYxXvr},
  year = {2024}
}

@inproceedings{chen2024diffusion,
  abstract = {Diffusion-based generative modeling has been achieving state-of-the-art results in various generation tasks. Most diffusion models, however, are limited to single-generation modeling. Can diffusion models be generalized with multi-modal generative training capabilities? In this work, we propose a principled way to define a diffusion model by constructing a unified multi-modal diffusion model in a common diffusion space. We define the forward diffusion process to be driven by information aggregation from multiple types of task-data, such as images for a generation task and labels for a classification task. In the reverse process, we enforce information sharing by parameterizing a shared backbone denoising network with additional modality-specific decoder heads. This structure can simultaneously learn to generate different types of multi-modal data with a multi-task loss, derived from a new multi-modal variational lower bound. We propose several multi-modal generation settings to verify our framework, including image transition, masked-image training, joint image-label and joint image-representation generative modeling. Extensive experimental results on ImageNet indicate the effectiveness of our framework for various multi-modal generative modeling.},
  arxiv = {https://arxiv.org/abs/2407.17571},
  author = {Changyou Chen and Han Ding and Bunyamin Sisman and Yi Xu and Ouye Xie and Benjamin Z. Yao and Son Dinh Tran and Belinda Zeng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024diffusion.pdf:pdf},
  keywords = {Diffusion Models, Multi-Modal Learning, Multi-Task Learning, Generative Modeling, Variational Lower Bound},
  note = {DBLP last modified: 2024-09-11},
  pdf = {https://openreview.net/pdf?id=cbv0sBIZh9},
  primaryarea = {Generative Models},
  publisher = {OpenReview.net},
  title = {Diffusion Models for Multi-Task Generative Modeling},
  url = {https://openreview.net/forum?id=cbv0sBIZh9},
  year = {2024}
}

@inproceedings{chen2024score,
  abstract = {Recent developments in offline reinforcement learning have uncovered the immense potential of diffusion modeling, which excels at representing heterogeneous behavior policies. However, sampling from diffusion policies is considerably slow because it necessitates tens to hundreds of iterative inference steps for one action. To address this challenge, we propose to extract an efficient deterministic inference policy from critic models and pretrained diffusion behavior models, leveraging the latter to directly regularize the policy gradient with the behavior distribution's score function during optimization. Our method enjoys powerful generative capabilities of diffusion modeling while completely circumventing the computationally intensive and time-consuming diffusion sampling scheme, both during training and evaluation. Extensive results on D4RL tasks show that our method boosts action sampling speed by more than 25 times compared with various leading diffusion-based methods in locomotion tasks, while still maintaining state-of-the-art performance.},
  arxiv = {https://arxiv.org/abs/2310.07297},
  author = {Huayu Chen and Cheng Lu and Zhengyi Wang and Hang Su and Jun Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/thu-ml/SRPO},
  file = {:/home/b/documents/inproceedings/chen2024score.pdf:pdf},
  keywords = {Offline Reinforcement Learning, Diffusion Models, Policy Optimization, Score Function, Behavior Regularization},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xCRr9DrolJ},
  primaryarea = {Reinforcement Learning},
  publisher = {OpenReview.net},
  title = {Score Regularized Policy Optimization through Diffusion Behavior},
  url = {https://openreview.net/forum?id=xCRr9DrolJ},
  year = {2024}
}

@inproceedings{chen2024data,
  abstract = {Dataset distillation aims to minimize the time and memory needed for training deep networks on large datasets, by creating a small set of synthetic images that has a similar generalization performance to that of the full dataset. However, current dataset distillation techniques fall short, showing a notable performance gap compared to training on the original data. In this work, we are the first to argue that the use of only one synthetic subset for distillation may not yield optimal generalization performance. This is because the training dynamics of deep networks drastically changes during training. Therefore, multiple synthetic subsets are required to capture the dynamics of training in different stages. To address this issue, we propose Progressive Dataset Distillation ({PDD}). {PDD} synthesizes multiple small sets of synthetic images, each conditioned on the previous sets, and trains the model on the cumulative union of these subsets without requiring additional training time. Our extensive experiments show that {PDD} can effectively improve the performance of existing dataset distillation methods by up to 4.3\%. In addition, our method for the first time enables generating considerably larger synthetic datasets.},
  author = {Xuxi Chen and Yu Yang and Zhangyang Wang and Baharan Mirzasoleiman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/VITA-Group/ProgressiveDD},
  file = {:/home/b/documents/inproceedings/chen2024data.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1NHgmKqOzZ},
  publisher = {OpenReview.net},
  title = {Data Distillation Can Be Like {V}odka: Distilling More Times For Better Quality},
  url = {https://openreview.net/forum?id=1NHgmKqOzZ},
  year = {2024}
}

@inproceedings{chen2024redl,
  abstract = {A newly-arising uncertainty estimation method named Evidential Deep Learning ({EDL}), which can obtain reliable predictive uncertainty in a single forward pass, has garnered increasing interest. Guided by the subjective logic theory, {EDL} obtains Dirichlet concentration parameters from deep neural networks, thus constructing a Dirichlet probability density function ({PDF}) to model the distribution of class probabilities. Despite its great success, we argue that {EDL} keeps nonessential settings in both stages of model construction and optimization. In this work, our analysis indicates that (1) in the construction of the Dirichlet {PDF}, a commonly ignored parameter termed prior weight governs the balance between leveraging the proportion of evidence and its magnitude in deriving predictive scores, and (2) in model optimization, a variance-minimized regularization term adopted by traditional {EDL} encourages the Dirichlet {PDF} to approach a Dirac delta function, potentially exacerbating overconfidence. Therefore, we propose the {R-EDL} (Relaxed-{EDL}) method by relaxing these nonessential settings. Specifically, {R-EDL} treats the prior weight as an adjustable hyper-parameter instead of a fixed scalar, and directly optimizes the expectation of the Dirichlet {PDF} provided to deprecate the variance-minimized regularization term. Extensive experiments and {SOTA} performances demonstrate the effectiveness of our method.},
  author = {Mengyuan Chen and Junyu Gao and Changsheng Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024redl.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Si3YFA641c},
  publisher = {OpenReview.net},
  title = {{R-EDL}: Relaxing Nonessential Settings of Evidential Deep Learning},
  url = {https://openreview.net/forum?id=Si3YFA641c},
  year = {2024}
}

@inproceedings{chen2024seine,
  abstract = {Recently video generation has achieved substantial progress with realistic results. Nevertheless, existing {AI}-generated videos are usually very short clips (``shot-level'') depicting a single scene. To deliver a coherent long video (``story-level''), it is desirable to have creative transition and prediction effects across different clips. This paper presents a short-to-long video diffusion model, {SEINE}, that focuses on generative transition and prediction. The goal is to generate high-quality long videos with smooth and creative transitions between scenes and varying lengths of shot-level videos. Specifically, we propose a random-mask video diffusion model to automatically generate transitions based on textual descriptions. By providing the images of different scenes as inputs, combined with text-based control, our model generates transition videos that ensure coherence and visual quality. Furthermore, the model can be readily extended to various tasks such as image-to-video animation and autoregressive video prediction. To conduct a comprehensive evaluation of this new generative task, we propose three assessing criteria for smooth and creative transition: temporal consistency, semantic similarity, and video-text semantic alignment. Extensive experiments validate the effectiveness of our approach over existing methods for generative transition and prediction, enabling the creation of story-level long videos.},
  author = {Xinyuan Chen and Yaohui Wang and Lingjun Zhang and Shaobin Zhuang and Xin Ma and Jiashuo Yu and Yali Wang and Dahua Lin and Yu Qiao and Ziwei Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024seine.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-06-23},
  pdf = {https://openreview.net/pdf?id=FNq3nIvP4F},
  publisher = {OpenReview.net},
  title = {{SEINE}: Short-to-Long Video Diffusion Model for Generative Transition and Prediction},
  url = {https://openreview.net/forum?id=FNq3nIvP4F},
  year = {2024}
}

@inproceedings{chen2024provably,
  abstract = {Risk-sensitive reinforcement learning ({RL}) aims to optimize policies that balance the expected reward and risk. In this paper, we present a novel risk-sensitive {RL} framework that employs an Iterated Conditional Value-at-Risk ({CVaR}) objective under both linear and general function approximations, enriched by human feedback. These new formulations provide a principled way to guarantee safety in each decision making step throughout the control process. Moreover, integrating human feedback into risk-sensitive {RL} framework bridges the gap between algorithmic decision-making and human participation, allowing us to also guarantee safety for human-in-the-loop systems. We propose provably sample-efficient algorithms for this Iterated {CVaR} {RL} and provide rigorous theoretical analysis.},
  archiveprefix = {arXiv},
  author = {Yu Chen and Yihan Du and Pihe Hu and Siwei Wang and Desheng Wu and Longbo Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2307.02842},
  file = {:/home/b/documents/inproceedings/chen2024provably.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-26},
  pdf = {https://openreview.net/pdf?id=vW1SkPl4kp},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Provably Efficient Iterated {CVaR} Reinforcement Learning with Function Approximation and Human Feedback},
  url = {https://openreview.net/forum?id=vW1SkPl4kp},
  year = {2024}
}

@inproceedings{chen2024simple,
  abstract = {Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a ``jumpy'' planning strategy at the higher level, which allows it to have a larger receptive field but at a lower computational cost -- a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of training and planning speed compared to the non-hierarchical Diffuser as well as other hierarchical planning methods. Moreover, we explore our model's generalization capability, particularly on how our method improves generalization capabilities on compositional out-of-distribution tasks.},
  archiveprefix = {arXiv},
  author = {Chang Chen and Fei Deng and Kenji Kawaguchi and Caglar Gulcehre and Sungjin Ahn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/changchencc/Simple-Hierarchical-Planning-with-Diffusion},
  eprint = {2401.02644},
  file = {:/home/b/documents/inproceedings/chen2024simple.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-01-23},
  pdf = {https://openreview.net/pdf?id=kXHEBK9uAY},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Simple Hierarchical Planning with Diffusion},
  url = {https://openreview.net/forum?id=kXHEBK9uAY},
  year = {2024}
}

@inproceedings{chen2024understanding,
  abstract = {Multi-modal learning has become increasingly popular due to its ability to leverage information from different data sources (e.g., text and images) to improve the model performance. Recently, {CLIP} has emerged as an effective approach that employs vision-language contrastive pretraining to learn joint image and text representations and exhibits remarkable performance in zero-shot learning and text-guided natural image generation. Despite the huge practical success of {CLIP}, its theoretical understanding remains elusive. In this paper, we formally study transferrable representation learning underlying {CLIP} and demonstrate how features from different modalities get aligned. We provide theoretical analysis showing that contrastive pretraining learns representations that align different modalities by minimizing the distance between paired data while maximizing the distance between unpaired data. Our analysis reveals the mechanism behind {CLIP}'s transferability and zero-shot capabilities, providing insights into the conditions under which vision-language contrastive learning succeeds.},
  archiveprefix = {arXiv},
  author = {Zixiang Chen and Yihe Deng and Yuanzhi Li and Quanquan Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.00927},
  file = {:/home/b/documents/inproceedings/chen2024understanding.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=S5yOuNfSA0},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Understanding Transferable Representation Learning and Zero-shot Transfer in {CLIP}},
  url = {https://openreview.net/forum?id=S5yOuNfSA0},
  year = {2024}
}

@inproceedings{chen2024compressing,
  abstract = {We introduce Least Volume---a simple yet effective regularization inspired by geometric intuition---that can reduce the necessary number of latent dimensions needed by an autoencoder without requiring any prior knowledge of the intrinsic dimensionality of the dataset. The framework is inspired by geometric intuition; specifically, the idea that packing a flat object (like a piece of paper) into a box consumes less volume than packing a curved or irregular object. We show that the Lipschitz continuity of the decoder is the key to making it work, provide a proof that {PCA} is just a linear special case of it, and reveal that it has a similar {PCA}-like importance ordering effect when applied to nonlinear models. We demonstrate the intuition behind the regularization on some pedagogical toy problems, and its effectiveness on several benchmark problems, including {MNIST}, {CIFAR-10} and {CelebA}. Our method uses spectral normalization to ensure that the Lipschitz constant of the decoder remains bound, preventing trivial compression of latent dimensions by making sure each dimension still carries its informative value in reconstruction.},
  archiveprefix = {arXiv},
  author = {Qiuyi Chen and Mark D. Fuge},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2404.17773},
  file = {:/home/b/documents/inproceedings/chen2024compressing.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jFJPd9kIiF},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Compressing Latent Space via Least Volume},
  url = {https://openreview.net/forum?id=jFJPd9kIiF},
  year = {2024}
}

@inproceedings{chen2024when,
  abstract = {Despite recent advancements in semantic segmentation, where and what pixels are hard to segment remains largely unexplored. Existing research only separates an image into easy and hard regions and empirically observes the latter are associated with object boundaries. In this paper, we conduct a comprehensive analysis of hard pixel errors, categorizing them into three types: false responses, merging mistakes, and displacements. Our findings reveal a quantitative association between hard pixels and aliasing, which is distortion caused by the overlapping of frequency components in the Fourier domain during downsampling. To identify the frequencies responsible for aliasing, we propose using the equivalent sampling rate to calculate the Nyquist frequency, which marks the threshold for aliasing. Then, we introduce the aliasing score as a metric to quantify the extent of aliasing. While positively correlated with the proposed aliasing score, three types of hard pixels exhibit different patterns. Here, we propose two novel de-aliasing filter ({DAF}) and frequency mixing ({FreqMix}) modules to alleviate aliasing degradation by accurately removing or adjusting frequencies higher than the Nyquist frequency. The {DAF} precisely removes the frequencies responsible for aliasing before downsampling, while the {FreqMix} dynamically selects high-frequency components within the encoder block. Experimental results demonstrate consistent improvements in semantic segmentation and low-light instance segmentation tasks.},
  archiveprefix = {arXiv},
  author = {Linwei Chen and Lin Gu and Ying Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/Linwei-Chen/Seg-Aliasing},
  eprint = {2403.09065},
  file = {:/home/b/documents/inproceedings/chen2024when.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-11-25},
  pdf = {https://openreview.net/pdf?id=SYBdkHcXXK},
  primaryclass = {cs.CV},
  publisher = {OpenReview.net},
  title = {When Semantic Segmentation Meets Frequency Aliasing},
  url = {https://openreview.net/forum?id=SYBdkHcXXK},
  year = {2024}
}

@inproceedings{chen2024generative,
  abstract = {Diffusion models ({DMs}) represent state-of-the-art generative models for continuous inputs. {DMs} work by constructing a Stochastic Differential Equation ({SDE}) in the input space (i.e., position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in phase space dynamics, where a phase space is defined as an augmented space encompassing both position and velocity. Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. In contrast to {DMs}, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation. This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluations. We demonstrate the effectiveness of our method across several benchmarks including {CIFAR-10}, {CelebA-HQ}, and {FFHQ}, showing improvements in both sample quality and computational efficiency.},
  archiveprefix = {arXiv},
  author = {Tianrong Chen and Jiatao Gu and Laurent Dinh and Evangelos A. Theodorou and Joshua M. Susskind and Shuangfei Zhai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.07805},
  file = {:/home/b/documents/inproceedings/chen2024generative.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=tUtGjQEDd4},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Generative Modeling with Phase Stochastic Bridge},
  url = {https://openreview.net/forum?id=tUtGjQEDd4},
  year = {2024}
}

@inproceedings{chen2024cosa,
  abstract = {Due to the limited scale and quality of video-text training corpus, most vision-language foundation models employ image-text datasets for pretraining and primarily focus on modeling visually semantic representations while disregarding temporal semantic representations and correlations. To address this issue, we propose {COSA}, a COncatenated SAmple pretrained vision-language foundation model. {COSA} can jointly model visual contents and event-level temporal cues using only image-text corpora. We achieve this by sequentially concatenating multiple image-text pairs as inputs for pretraining. This transformation effectively converts existing image-text corpora into a pseudo video-paragraph corpus, enabling richer scene transformations and explicit event-description correspondence. We demonstrate that {COSA} consistently improves performance across a broad range of downstream tasks, including long-form/short-form video-text tasks and image-text tasks such as retrieval, captioning, and question answering, and achieves state-of-the-art results on various competitive benchmarks.},
  archiveprefix = {arXiv},
  author = {Sihan Chen and Xingjian He and Handong Li and Xiaojie Jin and Jiashi Feng and Jing Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/TXH-mercury/COSA},
  eprint = {2306.09085},
  file = {:/home/b/documents/inproceedings/chen2024cosa.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-14},
  pdf = {https://openreview.net/pdf?id=bDkisS75zy},
  primaryclass = {cs.CV},
  publisher = {OpenReview.net},
  title = {{COSA}: Concatenated Sample Pretrained Vision-Language Foundation Model},
  url = {https://openreview.net/forum?id=bDkisS75zy},
  year = {2024}
}

@inproceedings{chen2024fixedbudget,
  abstract = {This paper studies best arm identification ({BAI}) in linear bandits in the fixed-budget regime under differential privacy constraints, when the arm rewards are supported on the unit interval. Given a finite budget $T$ and a privacy parameter $\epsilon>0$, the goal is to minimise the error probability in finding the arm with the largest mean after $T$ sampling rounds, subject to the constraint that the policy of the decision maker satisfies a certain $\epsilon$-differential privacy ($\epsilon$-{DP}) constraint. We construct a policy satisfying the $\epsilon$-{DP} constraint (called {DP}-{BAI}) by proposing the principle of maximum absolute determinants, and derive an upper bound on its error probability. Furthermore, we derive a minimax lower bound on the error probability, and demonstrate that the lower and the upper bounds decay exponentially in $T$, with exponents in the two bounds matching order-wise in (a) the sub-optimality gaps of the arms, (b) $\epsilon$, and (c) the dimension parameters.},
  author = {Zhirui Chen and P. N. Karthik and Yeow Meng Chee and Vincent Y. F. Tan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024fixedbudget.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vrE2fqAInO},
  publisher = {OpenReview.net},
  title = {Fixed-Budget Differentially Private Best Arm Identification},
  url = {https://openreview.net/forum?id=vrE2fqAInO},
  year = {2024}
}

@inproceedings{chen2024posthoc,
  abstract = {We address binary classification problems under group fairness constraints, which can be Demographic Parity ({DP}), Equalized Opportunity ({EOp}), or Equalized Odds ({EO}). We propose an explicit characterization of Bayes optimal classifier under fairness constraints, which turns out to be a simple modification rule of the unconstrained classifier. Our key contribution is introducing a novel instance-level measure of bias called ``bias score'', with the modification rule being a simple linear rule on top of the finite amount of bias scores. Based on this characterization, we develop a post-hoc approach that allows adaptation to fairness constraints while maintaining high accuracy. Experiments demonstrate the effectiveness of our method across multiple fairness measures.},
  author = {Wenlong Chen and Yegor Klochkov and Yang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024posthoc.pdf:pdf},
  note = {Spotlight Paper. DBLP last modified: 2024-12-06},
  pdf = {https://openreview.net/pdf?id=FM5xfcaR2Y},
  publisher = {OpenReview.net},
  title = {Post-hoc bias scoring is optimal for fair classification},
  url = {https://openreview.net/forum?id=FM5xfcaR2Y},
  year = {2024}
}

@inproceedings{chen2024flow,
  abstract = {We propose Riemannian Flow Matching ({RFM}), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. {RFM} bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind {RFM} is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing {E}uclidean case.},
  author = {Ricky T. Q. Chen and Yaron Lipman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024flow.pdf:pdf},
  note = {Outstanding Paper Honorable Mention. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=g7ohDlTITL},
  publisher = {OpenReview.net},
  title = {Flow Matching on General Geometries},
  url = {https://openreview.net/forum?id=g7ohDlTITL},
  year = {2024}
}

@inproceedings{chen2024consistency,
  abstract = {We propose {ConsisGAD}, a novel model tailored for Graph Anomaly Detection ({GAD}) in scenarios characterized by limited supervision and anchored in the principles of consistency training. Under limited supervision, {ConsisGAD} effectively leverages the abundance of unlabeled data for consistency training by incorporating a novel learnable data augmentation mechanism, thereby introducing controlled noise into the dataset. Moreover, {ConsisGAD} takes advantage of the variance in homophily distribution between normal and anomalous nodes to craft a simplified {GNN} backbone, enhancing its capability to distinguish effectively between these two classes. Comprehensive experiments on five real-world benchmark datasets demonstrate that {ConsisGAD} achieves up to 9.93\% improvements over existing methods.},
  author = {Nan Chen and Zemin Liu and Bryan Hooi and Bingsheng He and Rizal Fathony and Jun Hu and Jia Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024consistency.pdf:pdf},
  note = {DBLP last modified: 2024-11-20},
  pdf = {https://openreview.net/pdf?id=elMKXvhhQ9},
  publisher = {OpenReview.net},
  title = {Consistency Training with Learnable Data Augmentation for Graph Anomaly Detection with Limited Supervision},
  url = {https://openreview.net/forum?id=elMKXvhhQ9},
  year = {2024}
}

@inproceedings{chen2024defining,
  abstract = {Faithfully summarizing the knowledge encoded by a deep neural network ({DNN}) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable {AI}. To this end, Ren et al. (2024) have derived a series of theorems to prove that the inference score of a {DNN} can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the {DNN}. Given different {DNNs} trained for the same task, we develop a new method to extract interactions that are shared by these {DNNs}. Experiments show that the extracted interactions can better reflect common knowledge shared by different {DNNs}.},
  author = {Lu Chen and Siyu Lou and Benhao Huang and Quanshi Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024defining.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OCqyFVFNeF},
  publisher = {OpenReview.net},
  title = {Defining and extracting generalizable interaction primitives from {DNNs}},
  url = {https://openreview.net/forum?id=OCqyFVFNeF},
  year = {2024}
}

@inproceedings{chen2024biasvariance,
  abstract = {Classical wisdom in machine learning holds that the generalization error can be decomposed into bias and variance, and these two terms exhibit a trade-off. However, for an ensemble of deep learning based classification models, bias and variance are aligned at a sample level, where squared bias is approximately equal to variance for correctly classified sample points. We present empirical evidence confirming this phenomenon in a variety of deep learning models and datasets. We study this phenomenon from two theoretical perspectives: calibration and neural collapse. We show theoretically that under the assumption that the models are well calibrated, we can observe the bias-variance alignment. We also show an approximate correlation between bias and variance starting from neural collapse theory.},
  author = {Lin Chen and Michal Lukasik and Wittawat Jitkrittum and Chong You and Sanjiv Kumar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024biasvariance.pdf:pdf},
  note = {Spotlight Paper. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=i2Phucne30},
  publisher = {OpenReview.net},
  title = {On Bias-Variance Alignment in Deep Models},
  url = {https://openreview.net/forum?id=i2Phucne30},
  year = {2024}
}

@inproceedings{chen2024biased,
  abstract = {Time series data from real-world scenarios is often partially observed due to device malfunction or costly data acquisition, which can seriously impede the performance of existing approaches. Naive employment of imputation methods unavoidably involves error accumulation and leads to suboptimal solutions. We propose a Biased Temporal Convolution Graph Network that jointly captures temporal dependencies and spatial structure for time series forecasting with missing values. The method injects bias into two carefully developed modules: the Multi-Scale Instance PartialTCN and Biased {GCN}, to account for missing patterns. Experimental results show that the proposed model achieves up to 9.93\% improvements over existing methods on five real-world benchmark datasets.},
  author = {Xiaodan Chen and Xiucheng Li and Bo Liu and Zhijun Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024biased.pdf:pdf},
  note = {DBLP last modified: 2025-02-06},
  pdf = {https://openreview.net/pdf?id=O9nZCwdGcG},
  publisher = {OpenReview.net},
  title = {Biased Temporal Convolution Graph Network for Time Series Forecasting with Missing Values},
  url = {https://openreview.net/forum?id=O9nZCwdGcG},
  year = {2024}
}

@inproceedings{chen2024boosting,
  abstract = {The reasoning performance of Large Language Models ({LLMs}) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts ({BoT}), an automated prompting framework for problem solving with {LLMs} by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, {BoT} iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the {LLM} on them to explicitly revise prompting, which in turn enhances reasoning step generation, until a final answer is attained. Our experiments with {GPT}-4 and Llama2 across extensive complex mathematical problems demonstrate that {BoT} consistently achieves higher or comparable problem-solving rates than other advanced prompting approaches.},
  author = {Sijia Chen and Baochun Li and Di Niu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024boosting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qBL04XXex6},
  publisher = {OpenReview.net},
  title = {Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models},
  url = {https://openreview.net/forum?id=qBL04XXex6},
  year = {2024}
}

@inproceedings{chen2024learning,
  abstract = {As artificial intelligence ({AI}) systems play an increasingly prominent role in human decision-making, challenges surface in the realm of human-{AI} interactions. One challenge arises from the suboptimal {AI} policies due to the inadequate consideration of humans disregarding {AI} recommendations, as well as the need for {AI} to provide advice selectively when it is most pertinent. This paper presents a sequential decision-making model that (i) takes into account the human's adherence level (the probability that the human follows/rejects machine advice) and (ii) incorporates a defer option so that the machine can temporarily refrain from making advice. We provide learning algorithms that learn the optimal advice policy and make advice only at critical time stamps. Compared to problem-agnostic reinforcement learning algorithms, our specialized learning algorithms not only enjoy better theoretical convergence properties but also show strong empirical performance.},
  author = {Guanting Chen and Xiaocheng Li and Chunlin Sun and Hanzhao Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024learning.pdf:pdf},
  note = {DBLP last modified: 2025-06-13},
  pdf = {https://openreview.net/pdf?id=RgELE1dQXx},
  publisher = {OpenReview.net},
  title = {Learning to Make Adherence-aware Advice},
  url = {https://openreview.net/forum?id=RgELE1dQXx},
  year = {2024}
}

@inproceedings{chen2024teaching,
  abstract = {Large language models ({LLMs}) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose self-debugging, which teaches a large language model to debug its predicted program. In particular, we demonstrate that self-debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by leveraging code execution and explaining the generated code in natural language.},
  author = {Xinyun Chen and Maxwell Lin and Nathanael Sch\"arli and Denny Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024teaching.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KuPixIqPiq},
  publisher = {OpenReview.net},
  title = {Teaching Large Language Models to Self-Debug},
  url = {https://openreview.net/forum?id=KuPixIqPiq},
  year = {2024}
}

@inproceedings{chen2024polygcl,
  abstract = {Graph Contrastive Learning (GCL) has achieved superior performance in self-supervised graph representation learning. However, existing GCL techniques have inherent smooth characteristics due to their low-pass GNN encoder and objective based on homophily assumption, which poses challenges when applying to heterophilic graphs. In supervised learning tasks, spectral GNNs with polynomial approximation excel in both homophilic and heterophilic settings by adaptively fitting graph filters of arbitrary shapes, yet their applications in unsupervised learning are rarely explored. In this paper, we propose PolyGCL, a GCL pipeline that utilizes polynomial filters to achieve contrastive learning between low-pass and high-pass views. Specifically, PolyGCL utilizes polynomials with learnable filter functions to generate different spectral views and an objective that incorporates high-pass information through a linear combination. We theoretically prove that PolyGCL outperforms previous GCL paradigms when applied to graphs with varying levels of homophily. We conduct extensive experiments on both synthetic and real-world datasets demonstrating promising performance on homophilic and heterophilic graphs.},
  address = {Vienna, Austria},
  author = {Jingyu Chen and Runlin Lei and Zhewei Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024polygcl.pdf:pdf},
  month = {5},
  note = {{ICLR} 2024 Spotlight paper, {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=y21ZO6M86t},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{PolyGCL}: {GRAPH} {CONTRASTIVE} {LEARNING} via Learnable Spectral Polynomial Filters},
  url = {https://openreview.net/forum?id=y21ZO6M86t},
  year = {2024}
}

@inproceedings{chen2024mixsatgen,
  abstract = {The Boolean satisfiability problem (SAT) stands as a canonical NP-complete task. In particular, the scarcity of real-world SAT instances and their usefulness for tuning SAT solvers underscore the necessity for effective and efficient ways of hard instance generation, whereas existing methods either struggle to maintain plausible hardness or suffer from limited applicability. Different from the typical construction-based methods, this paper introduces an adaptive and efficient graph interpolation approach that in place modifies the raw structure of graph-represented SAT instance by replacing it with a counterpart from another instance. Specifically, it involves a two-stage matching and mixing pipeline. The matching aims to find a correspondence map of literal nodes from two instance graphs via learned features from a matching network; while the mixing stage involves iteratively exchanging clause pairs with the highest correspondence scores until a specified replacement ratio is achieved. We further show that under our matching-mixing framework, moderate randomness can avoid hardness degradation of instances by introducing Gumbel noise. Experimental results show the superiority of our method with both resemblance in structure and hardness, and general applicability.},
  address = {Vienna, Austria},
  author = {Xinyan Chen and Yang Li and Runzhong Wang and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024mixsatgen.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2025-05-21},
  pdf = {https://openreview.net/pdf?id=PXXuLvIH5r},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{MixSATGEN}: Learning Graph Mixing for {SAT} Instance Generation},
  url = {https://openreview.net/forum?id=PXXuLvIH5r},
  year = {2024}
}

@inproceedings{chen2024alpagasus,
  abstract = {Large language models (LLMs) strengthen instruction-following capability through instruction-finetuning (IFT) on supervised instruction/response data. However, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many low-quality instances with incorrect or irrelevant responses, which are misleading and detrimental to IFT. In this paper, we propose a simple and effective data selection strategy that automatically identifies and filters out low-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce AlpaGasus, which is finetuned on only 9k high-quality data filtered from the 52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as evaluated by GPT-4 on multiple test sets and the controlled human evaluation. Its 13B variant matches >90% performance of its teacher LLM (i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also provides 5.7x faster training, reducing the training time for a 7B variant from 80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the efficacy of our method across diverse datasets, base models, and LLM filters. Overall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be generally applied to instruction-tuning data, leading to faster training and better instruction-following models.},
  address = {Vienna, Austria},
  author = {Lichang Chen and Shiyang Li and Jun Yan and Hai Wang and Kalpa Gunaratna and Vikas Yadav and Zheng Tang and Vijay Srinivasan and Tianyi Zhou and Heng Huang and Hongxia Jin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024alpagasus.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FdVXgSJhvz},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{AlpaGasus}: Training a Better Alpaca with Fewer Data},
  url = {https://openreview.net/forum?id=FdVXgSJhvz},
  year = {2024}
}

@inproceedings{chen2024orderpreserving,
  abstract = {Generative Flow Networks (GFlowNets) have been introduced as a method to sample a diverse set of candidates with probabilities proportional to a given reward. However, GFlowNets can only be used with a predefined scalar reward, which can be either computationally expensive or not directly accessible, in the case of multi-objective optimization (MOO) tasks for example. Moreover, to prioritize identifying high-reward candidates, the conventional practice is to raise the reward to a higher exponent, the optimal choice of which may vary across different environments. To address these issues, we propose Order-Preserving GFlowNets (OP-GFNs), which sample with probabilities in proportion to a learned reward function that is consistent with a provided (partial) order on the candidates, thus eliminating the need for an explicit formulation of the reward function.},
  address = {Vienna, Austria},
  author = {Yihang Chen and Lukas Mauch},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024orderpreserving.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=VXDPXuq4oG},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Order-Preserving {GFlowNets}},
  url = {https://openreview.net/forum?id=VXDPXuq4oG},
  year = {2024}
}

@inproceedings{chen2024symbol,
  abstract = {Recent Meta-learning for Black-Box Optimization (MetaBBO) methods harness neural networks to meta-learn configurations of traditional black-box optimizers. Despite their success, they are inevitably restricted by the limitations of predefined hand-crafted optimizers. In this paper, we present SYMBOL, a novel framework that promotes the automated discovery of black-box optimizers through symbolic equation learning. Specifically, we propose a Symbolic Equation Generator (SEG) that allows closed-form optimization rules to be dynamically generated for specific tasks and optimization steps. Within SYMBOL, we then develop three distinct strategies based on reinforcement learning, so as to meta-learn the SEG efficiently. Extensive experiments reveal that the optimizers generated by SYMBOL not only surpass the state-of-the-art BBO and MetaBBO baselines, but also exhibit exceptional zero-shot generalization abilities across entirely unseen tasks with different problem characteristics.},
  address = {Vienna, Austria},
  author = {Jiacheng Chen and Zeyuan Ma and Hongshu Guo and Yining Ma and Jie Zhang and Yue-Jiao Gong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024symbol.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vLJcd43U7a},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{SYMBOL}: Generating Flexible Black-Box Optimizers through Symbolic Equation Learning},
  url = {https://openreview.net/forum?id=vLJcd43U7a},
  year = {2024}
}

@inproceedings{chen2024labelfree,
  address = {Vienna, Austria},
  author = {Zhikai Chen and Haitao Mao and Hongzhi Wen and Haoyu Han and Wei Jin and Haiyang Zhang and Hui Liu and Jiliang Tang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024labelfree.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-07},
  pdf = {https://openreview.net/pdf?id=hESD2NJFg8},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Label-free Node Classification on Graphs with Large Language Models (LLMs)},
  url = {https://openreview.net/forum?id=hESD2NJFg8},
  year = {2024}
}

@inproceedings{chen2024towards,
  address = {Vienna, Austria},
  author = {Yaofo Chen and Shuaicheng Niu and Yaowei Wang and Shoukai Xu and Hengjie Song and Mingkui Tan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024towards.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vePdNU3u6n},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation},
  url = {https://openreview.net/forum?id=vePdNU3u6n},
  year = {2024}
}

@inproceedings{chen2024longlora,
  address = {Vienna, Austria},
  author = {Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024longlora.pdf:pdf},
  month = {5},
  note = {{ICLR} 2024 Oral presentation, {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=6PmJoRfdaK},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
  url = {https://openreview.net/forum?id=6PmJoRfdaK},
  year = {2024}
}

@inproceedings{chen2024lrr,
  address = {Vienna, Austria},
  author = {Jianlang Chen and Xuhong Ren and Qing Guo and Felix Juefei-Xu and Di Lin and Wei Feng and Lei Ma and Jianjun Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024lrr.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3qo1pJHabg},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks},
  url = {https://openreview.net/forum?id=3qo1pJHabg},
  year = {2024}
}

@inproceedings{chen2024can,
  address = {Vienna, Austria},
  author = {Canyu Chen and Kai Shu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024can.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ccxD4mtkTU},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Can LLM-Generated Misinformation Be Detected?},
  url = {https://openreview.net/forum?id=ccxD4mtkTU},
  year = {2024}
}

@inproceedings{chen2024sudden,
  abstract = {Most interpretability research in {NLP} focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models ({MLM}s) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure ({SAS}), a naturally emerging property of {MLM}s wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire {SAS}, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities.},
  address = {Vienna, Austria},
  author = {Angelica Chen and Ravid Shwartz-Ziv and Kyunghyun Cho and Matthew L. Leavitt and Naomi Saphra},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024sudden.pdf:pdf},
  month = {5},
  note = {Spotlight paper. DBLP last modified: 2024-08-04},
  pdf = {https://openreview.net/pdf?id=MO5PiKHELW},
  publisher = {OpenReview.net},
  title = {Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in {MLM}s},
  url = {https://openreview.net/forum?id=MO5PiKHELW},
  year = {2024}
}

@inproceedings{chen2024genome,
  abstract = {Recent works have shown that Large Language Models ({LLM}s) could empower traditional neuro-symbolic models via programming capabilities to translate language into module descriptions, thus achieving strong visual reasoning results while maintaining the model's transparency and efficiency. However, these models usually exhaustively generate the entire code snippet given each new instance of a task, which is extremely ineffective. On the contrary, human beings gradually acquire knowledge that can be reused and grow into more profound skills for fast generalization to new tasks since we are an infant. Inspired by this, we propose generative neuro-symbolic visual reasoning by growing and reusing modules. Specifically, our model consists of three unique stages: module initialization, module generation, and module execution.},
  address = {Vienna, Austria},
  author = {Zhenfang Chen and Rui Sun and Wenjun Liu and Yining Hong and Chuang Gan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024genome.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MNShbDSxKH},
  publisher = {OpenReview.net},
  title = {{GENOME}: Generative Neuro-Symbolic Visual Reasoning by Growing and Reusing Modules},
  url = {https://openreview.net/forum?id=MNShbDSxKH},
  year = {2024}
}

@inproceedings{chen2024lie,
  abstract = {Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks ({DNN}s) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization ({RBN}) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite ({SPD}) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on {SPD} manifolds into three families of parameterized Lie groups. Specific normalization layers induced by these Lie groups are then proposed for {SPD} neural networks. We demonstrate the effectiveness of our approach through three sets of experiments: radar recognition, human action recognition, and electroencephalography ({EEG}) classification.},
  address = {Vienna, Austria},
  author = {Ziheng Chen and Yue Song and Yunmei Liu and Nicu Sebe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024lie.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=okYdj8Ysru},
  publisher = {OpenReview.net},
  title = {A Lie Group Approach to Riemannian Batch Normalization},
  url = {https://openreview.net/forum?id=okYdj8Ysru},
  year = {2024}
}

@inproceedings{chen2024agentverse,
  abstract = {Autonomous agents empowered by Large Language Models ({LLM}s) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework {AgentVerse} that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that {AgentVerse} can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups.},
  address = {Vienna, Austria},
  author = {Weize Chen and Yusheng Su and Jingwei Zuo and Cheng Yang 0002 and Chenfei Yuan and Chi-Min Chan and Heyang Yu and Yaxi Lu and Yi-Hsin Hung and Chen Qian and Yujia Qin and Xin Cong and Ruobing Xie and Zhiyuan Liu 0001 and Maosong Sun 0001 and Jie Zhou 0016},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024agentverse.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EHg5GDnyq1},
  publisher = {OpenReview.net},
  title = {AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors},
  url = {https://openreview.net/forum?id=EHg5GDnyq1},
  year = {2024}
}

@inproceedings{chen2024octavius,
  abstract = {Recent studies have demonstrated Large Language Models ({LLM}s) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called Octavius, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models ({MLLM}s). Specifically, we combine the well-known Mixture-of-Experts ({MoE}) and one of the representative {PEFT} techniques, i.e., {LoRA}, designing a novel {LLM}-based decoder, called {LoRA}-{MoE}, for multimodal learning. To the best of our knowledge, we are one of the pioneering efforts to introduce {MoE} into {MLLM}s to address this problem. The experimental results (about 20\% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks.},
  address = {Vienna, Austria},
  author = {Zeren Chen and Ziqin Wang and Zhen Wang 0003 and Huayang Liu and Zhenfei Yin and Si Liu 0001 and Lu Sheng and Wanli Ouyang and Jing Shao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024octavius.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rTDyN8yajn},
  publisher = {OpenReview.net},
  title = {Octavius: Mitigating Task Interference in {MLLM}s via {LoRA}-{MoE}},
  url = {https://openreview.net/forum?id=rTDyN8yajn},
  year = {2024}
}

@inproceedings{2024principled,
  abstract = {Training a high-quality deep neural network requires choosing suitable hyperparameters, which is a non-trivial and expensive process. Current works try to automatically optimize or design principles of hyperparameters, such that they can generalize to diverse unseen scenarios. However, most designs or optimization methods are agnostic to the choice of network structures, and thus largely ignore the impact of neural architectures on hyperparameters. In this work, we precisely characterize the dependence of initializations and maximal learning rates on the network architecture, which includes the network depth, width, convolutional kernel size, and connectivity patterns. We verify our principles with comprehensive experiments. More importantly, our strategy further sheds light on advancing current benchmarks for architecture design. A fair comparison of {AutoML} algorithms requires accurate network rankings. However, we demonstrate that network rankings can be easily changed by better training networks in benchmarks with our architecture-aware learning rates and initialization.},
  address = {Vienna, Austria},
  author = {Wuyang Chen 0001 and Junru Wu and Zhangyang Wang and Boris Hanin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024principled.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-06-03},
  pdf = {https://openreview.net/pdf?id=HZndRcfyNI},
  publisher = {OpenReview.net},
  title = {Principled Architecture-aware Scaling of Hyperparameters},
  url = {https://openreview.net/forum?id=HZndRcfyNI},
  year = {2024}
}

@inproceedings{2024gaining,
  abstract = {The rapid development of large language models ({LLM}s) has not only provided numerous opportunities but also presented significant challenges. This becomes particularly evident when {LLM}s inadvertently generate harmful or toxic content, either unintentionally or because of intentional inducement. Existing alignment methods usually direct {LLM}s toward the favorable outcomes by utilizing human-annotated, flawless instruction-response pairs. Conversely, this study proposes a novel alignment technique based on mistake analysis, which deliberately exposes {LLM}s to erroneous content to learn the reasons for mistakes and how to avoid them. In this case, mistakes are repurposed into valuable data for alignment, effectively helping to avoid the production of erroneous responses. Without external models or human annotations, our method leverages a model's intrinsic ability to discern undesirable mistakes and improves the safety of its generated responses.},
  address = {Vienna, Austria},
  author = {Kai Chen 0023 and Chunwei Wang and Kuo Yang and Jianhua Han and Lanqing Hong and Fei Mi and Hang Xu 0004 and Zhengying Liu and Wenyong Huang and Zhenguo Li and Dit-Yan Yeung and Lifeng Shang and Xin Jiang 0002 and Qun Liu 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024gaining.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-06-13},
  pdf = {https://openreview.net/pdf?id=aA33A70IO6},
  publisher = {OpenReview.net},
  title = {Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis},
  url = {https://openreview.net/forum?id=aA33A70IO6},
  year = {2024}
}

@inproceedings{2024geodiffusion,
  abstract = {Diffusion models have attracted significant attention due to the remarkable ability to create content and generate data for tasks like image classification. However, the usage of diffusion models to generate the high-quality object detection data remains an underexplored area, where not only image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image ({L2I}) generation with specifically designed modules to encode the semantic layouts. In this paper, we propose the GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower pre-trained text-to-image ({T2I}) diffusion models for high-quality detection data generation. Unlike previous {L2I} methods, our GeoDiffusion is able to encode not only the bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experiments demonstrate GeoDiffusion outperforms previous {L2I} methods while maintaining 4√ó training time faster. To the best of our knowledge, this is the first work to adopt diffusion models for layout-to-image generation with geometric conditions and demonstrate that {L2I}-generated images can be beneficial for improving the performance of object detectors.},
  address = {Vienna, Austria},
  author = {Kai Chen 0023 and Enze Xie and Zhe Chen and Yibo Wang and Lanqing Hong and Zhenguo Li and Dit-Yan Yeung},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024geodiffusion.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=xBfQZWeDRH},
  publisher = {OpenReview.net},
  title = {GeoDiffusion: Text-Prompted Geometric Control for Object Detection Data Generation},
  url = {https://openreview.net/forum?id=xBfQZWeDRH},
  year = {2024}
}

@inproceedings{chen2024pixart,
  abstract = {The most advanced text-to-image ({T2I}) models require significant training costs (e.g., millions of {GPU} hours), seriously hindering the fundamental innovation for the {AIGC} community while increasing {CO}2 emissions. This paper introduces {PIXART}-Œ±, a Transformer-based {T2I} diffusion model whose image generation quality is competitive with state-of-the-art image generators (e.g., Imagen, {SDXL}, and even Midjourney), reaching near-commercial application standards. Additionally, it supports high-resolution image synthesis up to 1024px resolution with low training cost. To achieve this goal, three core designs are proposed: (1) Training strategy decomposition: We devise three distinct training steps that separately optimize pixel dependency, text-image alignment, and image aesthetic quality; (2) Efficient {T2I} Transformer: We incorporate cross-attention modules into Diffusion Transformer ({DiT}) to inject text conditions and streamline the computation-intensive class-condition branch; (3) High-informative data: We emphasize the significance of concept density in text-image pairs and leverage a large Vision-Language model to auto-label dense pseudo-captions to assist text-image alignment learning. As a result, {PIXART}-Œ±'s training speed markedly surpasses existing large-scale {T2I} models, e.g., {PIXART}-Œ± only takes 10.8\% of Stable Diffusion v1.5's training time (~675 vs. ~6,250 A100 {GPU} days), saving nearly \$300,000 (\$26,000 vs. \$320,000) and reducing 90\% {CO}2 emissions.},
  address = {Vienna, Austria},
  author = {Junsong Chen and Jincheng Yu and Chongjian Ge and Lewei Yao and Enze Xie and Zhongdao Wang and James T. Kwok and Ping Luo 0002 and Huchuan Lu and Zhenguo Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024pixart.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=eAKmQPe3m1},
  publisher = {OpenReview.net},
  title = {PixArt-Œ±: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis},
  url = {https://openreview.net/forum?id=eAKmQPe3m1},
  year = {2024}
}

@inproceedings{chen2024invite,
  abstract = {Large-scale pre-trained vision foundation models, such as {CLIP}, have become de facto backbones for various vision tasks. However, due to their black-box nature, understanding the underlying rules behind these models' predictions and controlling model behaviors have remained open challenges. We present a framework for interpreting vision transformer's latent tokens with natural language. Given a latent token, our framework retains its semantic information to the final layer using transformer's local operations and retrieves the closest text for explanation. Our approach enables understanding of model visual reasoning procedure without needing additional model training or data collection. Based on the obtained interpretations, our framework allows for model editing that controls model reasoning behaviors and improves model robustness against biases and spurious correlations.},
  address = {Vienna, Austria},
  author = {Haozhe Chen and Junfeng Yang and Carl Vondrick and Chengzhi Mao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024invite.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5iENGLEJKG},
  publisher = {OpenReview.net},
  title = {{INViTE}: {INt}erpret and Control Vision-Language Models with Text Explanations},
  url = {https://openreview.net/forum?id=5iENGLEJKG},
  year = {2024}
}

@inproceedings{chen2024composed,
  abstract = {Composed image retrieval aims to search for target images based on a reference image and a text query. Current methods focus on learning fine-grained relationships through positive and negative pairs during training, using a one-to-one distance paradigm. However, this approach doesn't align with the coarse-grained retrieval process where users gradually refine their search from coarse to fine-grained feedback, compromising the recall rate. This paper introduces a unified learning approach that simultaneously models coarse- and fine-grained retrieval by considering multi-grained uncertainty. The method contains two modules: uncertainty modeling that simulates multi-grained queries by introducing identically distributed fluctuations in the feature space, and uncertainty regularization that adapts the matching objective according to the fluctuation range. This prevents the model from pushing away potential candidates in the early stage, improving recall rate. Experiments on three public datasets (FashionIQ, Fashion200k, and Shoes) show improvements of +4.03%, +3.38%, and +2.40% Recall@50 accuracy respectively over strong baselines.},
  author = {Yiyang Chen and Zhedong Zheng and Wei Ji and Leigang Qu and Tat-Seng Chua},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024composed.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Yb5KvPkKQg},
  publisher = {OpenReview.net},
  title = {Composed Image Retrieval with Text Feedback via Multi-grained Uncertainty Regularization},
  url = {https://openreview.net/forum?id=Yb5KvPkKQg},
  year = {2024}
}

@inproceedings{chen2024pathformer,
  abstract = {Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. This paper proposes Pathformer, a multi-scale Transformer with adaptive pathways that addresses this limitation. The key innovations include multi-scale division that divides time series into different temporal resolutions using patches of various sizes, dual attention performed over these patches to capture global correlations and local details as temporal dependencies, and adaptive pathways that dynamically adjust the multi-scale modeling process based on varying temporal dynamics of the input. Pathformer integrates both temporal resolution and temporal distance for comprehensive multi-scale modeling, enabling it to adaptively select and aggregate scale-specific characteristics based on different temporal dynamics. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios.},
  author = {Peng Chen and Yingying Zhang and Yunyao Cheng and Yang Shu and Yihang Wang and Qingsong Wen and Bin Yang and Chenjuan Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024pathformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=lJkOCMP2aW},
  publisher = {OpenReview.net},
  title = {Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting},
  url = {https://openreview.net/forum?id=lJkOCMP2aW},
  year = {2024}
}

@inproceedings{chen2024rethinking,
  abstract = {Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples. Previous works simply average the outputs of different models, lacking an in-depth analysis on how and why model ensemble methods can strongly improve the transferability. This paper rethinks ensemble methods in adversarial attacks and defines the common weakness of model ensemble with two properties: the flatness of loss landscape and the closeness to the local optimum of each model. Empirical and theoretical analysis show that both properties are strongly correlated with the transferability. Based on these insights, the authors propose a Common Weakness Attack (CWA) to generate more transferable adversarial examples by promoting these two properties. Experimental results on both image classification and object detection tasks validate the effectiveness of their approach in improving adversarial transferability, especially when attacking adversarially trained models. The method is also successfully applied to attack a black-box large vision-language model, Google's Bard, demonstrating its practical effectiveness.},
  author = {Huanran Chen and Yichi Zhang and Yinpeng Dong and Xiao Yang and Hang Su and Jun Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AcJrSoArlh},
  publisher = {OpenReview.net},
  title = {Rethinking Model Ensemble in Transfer-based Adversarial Attacks},
  url = {https://openreview.net/forum?id=AcJrSoArlh},
  year = {2024}
}

@inproceedings{chen2024deepzero,
  abstract = {Zeroth-order (ZO) optimization has become a popular technique for solving machine learning (ML) problems when first-order (FO) information is difficult or impossible to obtain. However, the scalability of ZO optimization remains an open problem: Its use has primarily been limited to relatively small-scale ML problems, such as sample-wise adversarial attack generation. To our best knowledge, no prior work has demonstrated the effectiveness of ZO optimization in training deep neural networks (DNNs) without a significant decrease in performance. To overcome this roadblock, we develop {DeepZero}, a principled and practical ZO deep learning (DL) framework that can scale ZO optimization to {DNN} training from scratch through three primary innovations. First, we demonstrate the advantages of coordinate-wise gradient estimation (CGE) over randomized vector-wise gradient estimation in training accuracy and computational efficiency. Second, we propose a sparsity-induced ZO training protocol that extends the model pruning methodology using only finite differences to explore and exploit the sparse {DL} prior in {CGE}. Third, we develop the methods of feature reuse and forward parallelization to advance the practical implementations of ZO training. Our extensive experiments show that {DeepZero} achieves state-of-the-art (SOTA) accuracy on {ResNet}-20 trained on {CIFAR}-10, approaching {FO} training performance for the first time. Furthermore, we show the practical utility of {DeepZero} in applications of certified adversarial defense and {DL}-based partial differential equation error correction, achieving 10--20\% improvement over {SOTA}.},
  author = {Aochuan Chen and Yimeng Zhang and Jinghan Jia and James Diffenderfer and Konstantinos Parasyris and Jiancheng Liu and Yihua Zhang and Zheng Zhang 0005 and Bhavya Kailkhura and Sijia Liu 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024deepzero.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=qBWhjsNPEY},
  publisher = {OpenReview.net},
  title = {{DeepZero}: Scaling Up Zeroth-Order Optimization for Deep Model Training},
  url = {https://openreview.net/forum?id=qBWhjsNPEY},
  year = {2024}
}

@inproceedings{2024disenbooth,
  abstract = {Subject-driven text-to-image generation aims to generate customized images of the given subject based on the text descriptions, which has drawn increasing attention. Existing methods mainly resort to finetuning a pretrained generative model, where the identity-relevant information (e.g., the boy) and the identity-irrelevant information (e.g., the background or the pose of the boy) are entangled in the latent embedding space. However, the highly entangled latent embedding may lead to failure in subject-driven text-to-image generation in two ways: (i) the identity-irrelevant information hidden in the entangled embedding may dominate the generation process, resulting in generated images heavily dependent on the irrelevant information while ignoring the given text descriptions; (ii) the identity-relevant information carried in the entangled embedding can not be appropriately preserved, resulting in identity change of the subject in the generated images. To tackle the problems, we propose {DisenBooth}, an identity-preserving disentangled tuning framework for subject-driven text-to-image generation. Different from previous works that utilize an entangled embedding to denoise each image, {DisenBooth} instead utilizes disentangled embeddings to respectively preserve the subject identity and capture the identity-irrelevant information. We further design novel weak denoising and contrastive embedding auxiliary tuning objectives to achieve the disentanglement. Extensive experiments show that our proposed {DisenBooth} framework outperforms baseline models for subject-driven text-to-image generation with the identity-preserved embedding. Additionally, by combining the identity-preserved embedding and identity-irrelevant embedding, {DisenBooth} demonstrates more generation flexibility and controllability.},
  author = {Hong Chen 0011 and Yipeng Zhang 0003 and Simin Wu and Xin Wang 0019 and Xuguang Duan and Yuwei Zhou and Wenwu Zhu 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024disenbooth.pdf:pdf},
  note = {DBLP last modified: 2025-03-25},
  pdf = {https://openreview.net/pdf?id=FlhjUkC7vH},
  publisher = {OpenReview.net},
  title = {{DisenBooth}: Identity-Preserving Disentangled Tuning for Subject-Driven Text-to-Image Generation},
  url = {https://openreview.net/forum?id=FlhjUkC7vH},
  year = {2024}
}

@inproceedings{chen2024boosting,
  abstract = {Unsupervised graph anomaly detection has attracted increasing attention. Existing methods primarily focus on local inconsistency mining ({LIM}), based on the intuition that establishing high similarities between abnormal nodes and their neighbors is difficult. However, the message passing employed by graph neural networks ({GNNs}) results in local anomaly signal loss, as {GNNs} tend to make connected nodes similar, which conflicts with the {LIM} intuition. This paper introduces {GADAM}, a novel framework that not only resolves the conflict between {LIM} and message passing but also leverages message passing to augment anomaly detection through a transformative approach to anomaly mining beyond {LIM}. {GADAM} includes an efficient {MLP}-based {LIM} approach to obtain local anomaly scores in a conflict-free way and a novel approach to capture anomaly signals from a global perspective. Moreover, we introduce a hybrid attention based adaptive message passing, enabling nodes to selectively absorb abnormal or normal signals from their surroundings. Extensive experiments conducted on nine benchmark datasets, including two large-scale {OGB} datasets, demonstrate that {GADAM} surpasses existing state-of-the-art methods in terms of both effectiveness and efficiency.},
  author = {Jingyan Chen and Guanghui Zhu and Chunfeng Yuan and Yihua Huang 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chen2024boosting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CanomFZssu},
  publisher = {OpenReview.net},
  title = {Boosting Graph Anomaly Detection with Adaptive Message Passing},
  url = {https://openreview.net/forum?id=CanomFZssu},
  year = {2024}
}

@inproceedings{2024towards,
  abstract = {Controllable layout generation refers to the process of creating a plausible visual arrangement of elements within a graphic design (e.g., document and web designs) with constraints representing design intentions. Prior works either tackle the problem from the autoregressive generation perspective or model layout generation as a discrete diffusion process, both of which suffer from limited diversity and poor generation quality. In this work, we propose {LACE} ({LA}yout {C}onstraint diffusion mod{E}l), a unified model to handle a broad range of layout generation tasks, such as arranging elements with specified attributes and refining or completing a coarse layout design. Built upon a diffusion model, {LACE} formulates various controllable layout generation tasks as conditional generation processes in continuous space. Compared with existing methods that use discrete diffusion models, continuous state-space design can enable the incorporation of differentiable aesthetic constraint functions in training. Specifically, we propose two aesthetic constraint loss functions that promote global alignment and minimize overlap in the layout. These functions serve as constraints during both the training and post-processing phases. Our extensive experiments show that {LACE} produces high-quality layouts and outperforms existing state-of-the-art baselines on public benchmarks.},
  author = {Jian Chen 0043 and Ruiyi Zhang 0002 and Yufan Zhou 0001 and Changyou Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024towards.pdf:pdf},
  note = {DBLP last modified: 2025-05-26},
  pdf = {https://openreview.net/pdf?id=kJ0qp9Xdsh},
  publisher = {OpenReview.net},
  title = {Towards Aligned Layout Generation via Diffusion Model with Aesthetic Constraints},
  url = {https://openreview.net/forum?id=kJ0qp9Xdsh},
  year = {2024}
}

@inproceedings{cheng2024sociodojo,
  abstract = {We introduce {SocioDojo}, an open-ended lifelong learning environment for developing ready-to-deploy autonomous agents capable of performing human-like analysis and decision-making on societal topics such as economics, finance, politics, and culture. {SocioDojo} includes (1) information sources from news, social media, reports, etc., (2) a knowledge base built from books, journals, and encyclopedias, plus a toolbox of Internet and knowledge graph search interfaces, (3) 30K high-quality time series in finance, economy, society, and polls, which support a novel task called ``hyperportfolio'', that can reliably and scalably evaluate societal analysis and decision-making power of agents, inspired by portfolio optimization with time series as assets to ``invest''. We propose a novel Analyst-Assistant-Actuator architecture for the hyperportfolio task, and a Hypothesis \& Proof prompting for producing in-depth analyses on input news, articles, etc. Through extensive experiments, we demonstrate the significant alignment of our benchmark with human preferences and show the advantage of our design choices.},
  author = {Junyan Cheng and Peter Chin 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024sociodojo.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=s9z0HzWJJp},
  publisher = {OpenReview.net},
  title = {{SocioDojo}: Building Lifelong Analytical Agents with Real-world Text and Time Series},
  url = {https://openreview.net/forum?id=s9z0HzWJJp},
  year = {2024}
}

@inproceedings{cheng2024bridging,
  abstract = {Cognitive science studies have suggested that symbolic representation in the human brain does not appear out of nowhere; rather, there is a gradual transition from neural perception to preliminary symbols and eventually to symbolic languages over the course of human evolution. The transitional representation, preliminary symbols, is essential in connecting neural and symbolic representation. A transitional representation should be in between that (1) contains high-dimensional details of the input and (2) implies structural information about the semantics of the input. This work introduces a novel Transitional Dictionary Learning ({TDL}) framework that can implicitly learn symbolic knowledge, such as visual parts and relations, by reconstructing the input as a combination of parts with implicit relations. We use a game-theoretic diffusion model to decompose the input into visual parts using the dictionaries learned by the Expectation Maximization ({EM}) algorithm, implemented as the online prototype clustering, based on the decomposition results. We also propose two metrics, clustering information gain, and heuristic shape score to evaluate the model. Experiments are conducted on three abstract compositional visual object datasets, which require the model to utilize the compositionality of data instead of simply exploiting visual features. Our results show huge improvements compared to the state-of-the-art part segmentation baselines, which struggle to process abstract objects that lack distinct visual features. We also conduct human evaluations; the results demonstrate significantly improved interpretability of the proposed method and the proposed metrics are consistent with human assessments.},
  author = {Junyan Cheng and Peter Chin 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024bridging.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uqxBTcWRnj},
  publisher = {OpenReview.net},
  title = {Bridging Neural and Symbolic Representations with Transitional Dictionary Learning},
  url = {https://openreview.net/forum?id=uqxBTcWRnj},
  year = {2024}
}

@inproceedings{cheng2024multilinear,
  abstract = {Despite the remarkable capabilities of deep neural networks in image recognition, the dependence on activation functions remains a largely unexplored area. Current state-of-the-art architectures rely on activation functions that are applied element-wise. While there has been limited exploration of models that do not require activation functions, such as {Polynomial Networks}, they have yet to perform on par with modern architectures. This study introduces {MONet}, a novel network using {Multilinear Operators} that relies solely on multilinear operators. The core layer of {MONet}, termed the {Mu-Layer}, captures multiplicative interactions of the elements of the input token. {MONet} captures high-degree interactions of the input elements, and the proposed model outperforms prior polynomial networks and performs on par with modern architectures. Furthermore, our empirical analysis demonstrates the theoretical underpinnings of our approach, validating the effectiveness of multilinear operators in deep learning. We evaluate {MONet} on a series of image recognition and scientific computing benchmarks, where it achieves competitive performance. The ability of {MONet} to remove nonlinear activation functions while maintaining competitive performance opens new research directions in neural network design.},
  author = {Yixin Cheng and Grigorios Chrysos 0002 and Markos Georgopoulos and Volkan Cevher},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024multilinear.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bbCL5aRjUx},
  publisher = {OpenReview.net},
  title = {Multilinear Operator Networks},
  url = {https://openreview.net/forum?id=bbCL5aRjUx},
  year = {2024}
}

@inproceedings{cheng2024learning,
  abstract = {Neural methods have shown significant merit in solving combinatorial optimization ({CO}) problems, including the {Bin} {Packing} {Problem} ({BPP}). However, most existing {ML}-based approaches focus on geometric {BPP} like {3DBPP}, neglecting complex vector {BPP}. In this study, we introduce a vector {BPP} variant called {Class-Constrained} {Bin} {Packing} {Problem} ({CCBPP}), dealing with items of both classes and sizes, and the objective is to pack the items in the least amount of bins respecting the bin capacity and the number of different classes that it can hold. To enhance the efficiency and practicality of solving {CCBPP}, we propose a learning-based {Encoder-Decoder} {Model}. The {Encoder} employs a {Graph} {Convolution} {Network} ({GCN}) to generate a heat-map, representing probabilities of different items packing together. The {Decoder} decodes and fine-tunes the solution through {Cluster} {Decode} and {Active} {Search} methods, thereby producing high-quality solutions for {CCBPP} instances. Extensive experiments demonstrate that our proposed method consistently yields high-quality solutions for various kinds of {CCBPP} with a very small gap from the optimal. Moreover, our {Encoder-Decoder} {Model} also shows promising performance on one practical application of {CCBPP}, the {Manufacturing} {Order} {Consolidation} {Problem} ({OCP}).},
  author = {Hanni Cheng and Ya Cong and Weihao Jiang and Shiliang Pu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6hvtSLkKeZ},
  publisher = {OpenReview.net},
  title = {Learning to solve {Class-Constrained} {Bin} {Packing} {Problems} via {Encoder-Decoder} {Model}},
  url = {https://openreview.net/forum?id=6hvtSLkKeZ},
  year = {2024}
}

@inproceedings{cheng2024adapting,
  abstract = {We explore how continued pre-training on domain-specific corpora influences large language models, revealing that training on raw corpora endows the model with domain knowledge, but drastically hurts its prompting ability for question answering. Taking inspiration from human learning via reading comprehension--practice after reading improves the ability to answer questions based on learned knowledge--we propose a simple method for transforming raw corpora into reading comprehension texts. Each raw text is enriched with a series of tasks related to its content. We continue pretraining with a mixture of the original corpus and the reading comprehension texts. This method, highly scalable and applicable to any pre-training corpora, consistently enhances performance across various tasks in three different domains: biomedicine, finance, and law. Notably, our 7{B} language model achieves competitive performance with domain-specific models of much larger scales, such as {BloombergGPT}-50{B}. Furthermore, we demonstrate that domain-specific reading comprehension texts can improve the model's performance even on general benchmarks, showing the potential to develop a general model across even more domains.},
  author = {Daixuan Cheng and Shaohan Huang and Furu Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024adapting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=y886UXPEZ0},
  publisher = {OpenReview.net},
  title = {Adapting Large Language Models via Reading Comprehension},
  url = {https://openreview.net/forum?id=y886UXPEZ0},
  year = {2024}
}

@inproceedings{cheng2024momentum,
  abstract = {Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. {FedAvg} and {SCAFFOLD} are two prominent algorithms to address these challenges. This paper explores the utilization of momentum to enhance the performance of {FedAvg} and {SCAFFOLD}. When all clients participate in the training process, we demonstrate that incorporating momentum allows {FedAvg} to converge without relying on the assumption of bounded data heterogeneity even using a constant local learning rate. This is novel and fairly surprising as existing analyses for {FedAvg} require bounded data heterogeneity even with diminishing local learning rates. In partial client participation, we show that momentum enables {SCAFFOLD} to converge provably faster without imposing any additional assumptions. Furthermore, we use momentum to develop new variance-reduced extensions of {FedAvg} and {SCAFFOLD}, which exhibit state-of-the-art convergence rates. To ensure simplicity and practicality in implementations, we only introduce momentum to the local {SGD} steps, avoiding any inclusion of impractical elements, such as gradient computation of multiple minibatches or solving local problems to high precision. Remarkably, this straightforward approach effectively alleviates the necessity for stringent assumptions regarding bounded data heterogeneity, leading to noteworthy improvements in convergence rates.},
  author = {Ziheng Cheng and Xinmeng Huang and Pengfei Wu and Kun Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024momentum.pdf:pdf},
  note = {DBLP last modified: 2025-05-28},
  pdf = {https://openreview.net/pdf?id=TdhkAcXkRi},
  publisher = {OpenReview.net},
  title = {Momentum Benefits {Non-iid} Federated Learning Simply and Provably},
  url = {https://openreview.net/forum?id=TdhkAcXkRi},
  year = {2024}
}

@inproceedings{cheng2024ucnerf,
  abstract = {While multi-camera setups are widely used in applications like autonomous driving to expand sensing capabilities, applying {Neural} {Radiance} {Field} ({NeRF}) techniques to multi-camera systems remains very challenging. This is primarily due to inherent under-calibration issues in multi-camera setups, including inconsistent imaging effects from separately calibrated image signal processing units in diverse cameras, and system errors from mechanical vibrations during driving that affect relative camera poses. To address these challenges, we present {UC-NeRF}, a novel method tailored for novel view synthesis in under-calibrated multi-view camera systems of autonomous driving. {UC-NeRF} incorporates a layer-based color correction to rectify color inconsistency in different image regions and a virtual warping to generate more viewpoint-diverse but color-consistent virtual views for color correction and {3D} recovery. The virtual warping strategy generates viewpoint-diverse yet color-consistent observations for each camera at each moment, offering stronger constraints on the latent codes for color correction. Additionally, we design spatiotemporally constrained pose refinement for more robust and accurate pose calibration in multi-camera systems. In under-calibrated multi-camera systems, standard {NeRF} quality significantly degrades with color discrepancies, object ghosts, and wrong geometry, while {UC-NeRF} achieves high-quality rendering and accurate geometry in these challenging cases.},
  author = {Kai Cheng and Xiaoxiao Long and Wei Yin and Jin Wang and Zhiqiang Wu and Yuexin Ma and Kaixuan Wang and Xiaozhi Chen and Xuejin Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024ucnerf.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bLKcCe7hYh},
  publisher = {OpenReview.net},
  title = {{UC-NERF}: Neural Radiance Field for Under-Calibrated Multi-View Cameras in Autonomous Driving},
  url = {https://openreview.net/forum?id=bLKcCe7hYh},
  year = {2024}
}

@inproceedings{cheng2024tuvf,
  abstract = {Texturing is an important aspect of {3D} asset generation. However, while high-fidelity geometry generation has made significant progress recently, texture generation has received relatively less attention. In this work, we present {TUVF} ({Texture} {UV} {Radiance} {Fields}), a method that can generate textures in a learnable {UV} sphere space rather than directly on the {3D} shape. This allows the texture to be disentangled from the underlying shape and transferable to other shapes that share the same {UV} space, i.e., from the same category. Given a {3D} shape, {TUVF} synthesizes realistic, high-fidelity, diverse, and {3D} consistent textures. Our goal is to facilitate a controllable texture generation process, such that one texture code can correspond to a particular appearance style independent of any input shapes from a category. The method integrates the {UV} sphere space with the radiance field, which provides a more efficient and accurate representation of textures than traditional texture maps. Instead of learning from reconstruction with multi-view datasets, our method leverages {GANs} for learning from {2D} single-view image collections. Extensive evaluations demonstrate that our method achieves superior performance compared to existing state-of-the-art methods for texture synthesis, control, and editing.},
  author = {An-Chieh Cheng and Xueting Li and Sifei Liu and Xiaolong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024tuvf.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dN4vpVTvWX},
  publisher = {OpenReview.net},
  title = {{TUVF}: Learning Generalizable Texture {UV} Radiance Fields},
  url = {https://openreview.net/forum?id=dN4vpVTvWX},
  year = {2024}
}

@inproceedings{cheng2024robusttsf,
  abstract = {Time series forecasting is an important task whose techniques have been applied to electricity forecasting, trajectory prediction, labor planning, etc. However, most time series forecasting techniques assume clean training data without anomalies, which is unrealistic since the collected time series data can be contaminated in practice. The forecasting model will be inferior if it is directly trained by time series with anomalies. Thus it is essential to develop methods to automatically learn a robust forecasting model from the contaminated data. In this paper, we first statistically define three types of anomalies, then theoretically and experimentally analyze the loss robustness and sample robustness when these anomalies exist. Based on our analyses, we propose {RobustTSF}, a simple and efficient algorithm to learn a robust forecasting model. In contrast to the detection-imputation-retraining pipeline, {RobustTSF} identifies informative samples by assessing the variance between the original input time series and its trend. Subsequently, it employs a robust loss function to improve the forecasting process. To the best of our knowledge, this is the first study to extend the theory of learning with noisy labels ({LNL}) to time series forecasting, which builds a bridge between {LNL} and time series forecasting with anomalies ({TSFA}) tasks. Extensive experiments show that our method is highly robust and outperforms all existing approaches.},
  author = {Hao Cheng and Qingsong Wen and Yang Liu and Liang Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024robusttsf.pdf:pdf},
  note = {DBLP last modified: 2024-10-09},
  pdf = {https://openreview.net/pdf?id=ltZ9ianMth},
  publisher = {OpenReview.net},
  title = {{RobustTSF}: Towards Theory and Design of Robust Time Series Forecasting with Anomalies},
  url = {https://openreview.net/forum?id=ltZ9ianMth},
  year = {2024}
}

@inproceedings{cheng2024causaltime,
  abstract = {Time-series causal discovery ({TSCD}) is a fundamental problem in machine learning, where the goal is to infer causality from temporal data. However, current synthetic datasets cannot properly evaluate or predict the algorithms' performance on real data. To address this challenge, we introduce the {CausalTime} pipeline to generate time-series that highly resemble real data and provide ground truth causal graphs for quantitative performance evaluation. The {CausalTime} pipeline begins from real observations in a specific scenario and produces a matching benchmark dataset through a four-step process. We harness deep neural networks along with normalizing flow to accurately capture realistic dynamics, then extract hypothesized causal graphs by performing importance analysis on the neural network or leveraging prior knowledge. We derive the ground truth causal graphs by splitting the causal model into causal term, residual term, and noise term. Using the fitted network and the derived causal graph, we generate corresponding versatile time-series proper for algorithm assessment. In our experiments, we validate the fidelity of the generated data through qualitative and quantitative experiments, followed by a benchmarking of existing {TSCD} algorithms using these generated datasets. {CausalTime} offers a feasible solution to evaluating {TSCD} algorithms in real applications and can be generalized to a wide range of fields.},
  author = {Yuxiao Cheng and Ziqian Wang and Tingxiong Xiao and Qin Zhong and Jinli Suo and Kunlun He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024causaltime.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iad1yyyGme},
  publisher = {OpenReview.net},
  title = {{CausalTime}: Realistically Generated Time-series for Benchmarking of Causal Discovery},
  url = {https://openreview.net/forum?id=iad1yyyGme},
  year = {2024}
}

@inproceedings{cheng2024consistent,
  abstract = {We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by {Instruct} {Pix2Pix}'s image transfer via editing instruction, we adapt this paradigm to the video domain. By extending the {Prompt-to-Prompt} technique to videos, we efficiently generate paired samples, each with an input video and its edited counterpart. To ensure temporal consistency in long videos, we introduce the {Long} {Video} {Sampling} {Correction} during sampling, which maintains consistent long videos across batches. Our method surpasses current methods like {Tune-A-Video}, heralding substantial progress in text-based video-to-video editing. This work suggests exciting avenues for further exploration and deployment in practical video editing applications.},
  author = {Jiaxin Cheng and Tianjun Xiao and Tong He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024consistent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IoKRezZMxF},
  publisher = {OpenReview.net},
  title = {Consistent Video-to-Video Transfer Using Synthetic Dataset},
  url = {https://openreview.net/forum?id=IoKRezZMxF},
  year = {2024}
}

@inproceedings{cheng2024conditional,
  abstract = {This paper studies the challenging problem of estimating causal effects from observational data, in the presence of unobserved confounders. The two-stage least square ({TSLS}) method and its variants with a standard instrumental variable ({IV}) are commonly used to eliminate confounding bias, including the bias caused by unobserved confounders, but they rely on the linearity assumption. Besides, the strict condition of unconfounded instruments posed on a standard {IV} is too strong to be practical. We propose a novel confounding balance representation learning method for non-linear conditional {IV} ({CIV}) regression in average causal effect estimation from observational data with latent confounders. Specifically, we use a conditional {IV} ({CIV}) to relax the unconfounded instrument condition of standard {IV} and propose a non-linear {CIV} regression with {Confounding} {Balancing} {Representation} {Learning}, {CBRL}.{CIV}, for jointly eliminating the confounding bias from unobserved confounders and balancing the observed confounders, without the linearity assumption. The {CBRL}.{CIV} method addresses the imbalance problems of observed confounders and the confounding bias of unobserved confounders by using a balancing representation learning network to obtain a balancing representation of the observed confounders and removing the confounding bias caused by unobserved confounders simultaneously in a non-linear system. Extensive experiments on synthetic and real-world datasets demonstrate the effectiveness of our method.},
  author = {Debo Cheng and Ziqi Xu and Jiuyong Li and Lin Liu and Jixue Liu and Thuc Duy Le},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024conditional.pdf:pdf},
  note = {DBLP last modified: 2024-12-30},
  pdf = {https://openreview.net/pdf?id=qDhq1icpO8},
  publisher = {OpenReview.net},
  title = {Conditional Instrumental Variable Regression with Representation Learning for Causal Inference},
  url = {https://openreview.net/forum?id=qDhq1icpO8},
  year = {2024}
}

@inproceedings{cheng2024progressive3d,
  abstract = {Recent text-to-{3D} generation methods achieve impressive {3D} content creation capacity thanks to the advances in image diffusion models and optimizing strategies. However, current methods struggle to generate correct {3D} content for a complex prompt in semantics, i.e., a prompt describing multiple interacted objects binding with different attributes. In this work, we propose a general framework named {Progressive3D}, which decomposes the entire generation into a series of locally progressive editing steps to create precise {3D} content for complex prompts, and constrains the content change to only occur in regions determined by user-defined region prompts in each editing step. Furthermore, we propose an overlapped semantic component suppression ({OSCS}) technique to encourage the optimization process to focus more on the semantic differences between prompts. For a specific editing step, the framework edits the pre-trained source representation in the {3D} space determined by the user-defined region prompt according to the semantic difference between the source prompt and the target prompt. Extensive experiments demonstrate that the proposed {Progressive3D} framework generates precise {3D} content for prompts with complex semantics and is general for various text-to-{3D} methods driven by different {3D} representations, including {NeRF}-based, {SDF}-based, and {DMTet}-based methods.},
  author = {Xinhua Cheng and Tianyu Yang and Jianan Wang and Yu Li and Lei Zhang and Jian Zhang and Li Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cheng2024progressive3d.pdf:pdf},
  note = {DBLP last modified: 2024-10-11},
  pdf = {https://openreview.net/pdf?id=O072Rc8uUy},
  publisher = {OpenReview.net},
  title = {{Progressive3D}: Progressively Local Editing for Text-to-{3D} Content Creation with Complex Semantic Prompts},
  url = {https://openreview.net/forum?id=O072Rc8uUy},
  year = {2024}
}

@inproceedings{chhabra2024what,
  abstract = {Classification models are ubiquitously deployed in society and necessitate high utility, fairness, and robustness performance. Current research efforts mainly focus on improving model architectures and learning algorithms on fixed datasets to achieve this goal. In contrast, in this paper, we address an orthogonal yet crucial problem: given a fixed convex learning model (or a convex surrogate for a non-convex model) and a function of interest, we assess what data benefits the model by interpreting the feature space, and then aim to improve performance as measured by this function. To this end, we propose the use of influence estimation models for interpreting the classifier's performance from the perspective of the data feature space. Additionally, we propose data selection approaches based on influence that enhance model utility, fairness, and robustness. Through extensive experiments on synthetic and real-world datasets, we validate and demonstrate the effectiveness of our approaches not only for conventional classification scenarios, but also under more challenging scenarios such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning.},
  author = {Anshuman Chhabra and Peizhao Li and Prasant Mohapatra and Hongfu Liu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/chhabra2024what.pdf:pdf},
  keywords = {Data Selection, Interpretability, Fairness, Robustness},
  note = {ICLR 2024 Oral Presentation},
  pdf = {https://openreview.net/pdf/c9c086d91e0480dcd349f7bb625a5031fabcc53a.pdf},
  publisher = {OpenReview.net},
  title = {"What Data Benefits My Classifier?" Enhancing Model Performance and Interpretability through Influence-Based Data Selection},
  url = {https://openreview.net/forum?id=HE9eUQlAvo},
  year = {2024}
}

@inproceedings{chi2024adapting,
  abstract = {In this paper, we aim to adapt a model at test-time using a few unlabeled data to address distribution shifts. To tackle the challenges of extracting domain knowledge from a limited amount of data, it is crucial to utilize correlated information from pre-trained backbones and source domains.},
  author = {Zhixiang Chi and Li Gu and Tao Zhong and Huan Liu and Yuanhao Yu and Konstantinos N. Plataniotis and Yang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/chi2024adapting.pdf:pdf},
  keywords = {Distribution shifts, Domain generalization, Visual prompt, Foundation model, Test-time adaptation},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=sSaN4gxuEf},
  publisher = {OpenReview.net},
  title = {Adapting to Distribution Shift by Visual Domain Prompt Generation},
  url = {https://openreview.net/forum?id=sSaN4gxuEf},
  year = {2024}
}

@inproceedings{chidambaram2024limitations,
  abstract = {Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to be overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures such as temperature scaling. While temperature scaling is frequently used because of its simplicity, it is often outperformed by modified training schemes. In this work, we identify a specific bottleneck for the performance of temperature scaling. We show that for empirical risk minimizers for a general set of distributions in which the supports of classes have overlaps, the performance of temperature scaling degrades with the amount of overlap between classes, and asymptotically becomes no better than random when there are a large number of classes. On the other hand, we prove that optimizing a modified form of the empirical risk induced by the Mixup data augmentation technique can in fact lead to reasonably good calibration performance, showing that training-time calibration may be necessary in some situations. We also verify that our theoretical results reflect practice by showing that Mixup significantly outperforms empirical risk minimization (with respect to multiple calibration metrics) on image classification benchmarks with class overlaps introduced in the form of label noise.},
  author = {Muthu Chidambaram and Rong Ge},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/chidambaram2024limitations.pdf:pdf},
  keywords = {calibration, temperature scaling, mixup, label noise},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=zavLQJ1XjB},
  publisher = {OpenReview.net},
  title = {On the Limitations of Temperature Scaling for Distributions with Overlaps},
  url = {https://openreview.net/forum?id=zavLQJ1XjB},
  year = {2024}
}

@inproceedings{chijiwa2024transferring,
  abstract = {Training deep neural networks (DNNs) is computationally expensive, which is problematic especially when performing duplicated or similar training runs in model ensemble or fine-tuning pre-trained models, for example. Once we have trained one DNN on some dataset, we have its learning trajectory (i.e., a sequence of intermediate parameters during training) which may potentially contain useful information for learning the dataset. However, there has been no attempt to utilize such information of a given learning trajectory for another training. In this paper, we formulate the problem of "transferring" a given learning trajectory from one initial parameter to another one (named *learning transfer problem*) and derive the first algorithm to approximately solve it by matching gradients successively along the trajectory via permutation symmetry. We empirically show that the transferred parameters achieve non-trivial accuracy before any direct training, and can be trained significantly faster than training from scratch.},
  author = {Daiki Chijiwa},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  eprint = {arXiv:2305.14122},
  file = {:/home/b/documents/inproceedings/chijiwa2024transferring.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=bWNJFD1l8M},
  publisher = {OpenReview.net},
  title = {Transferring Learning Trajectories of Neural Networks},
  url = {https://openreview.net/forum?id=bWNJFD1l8M},
  year = {2024}
}

@inproceedings{chlenski2024fast,
  abstract = {Hyperbolic geometry is gaining traction in machine learning due to its capacity to effectively capture hierarchical structures in real-world data. Hyperbolic spaces, where neighborhoods grow exponentially, offer substantial advantages and have consistently delivered state-of-the-art results across diverse applications. However, hyperbolic classifiers often grapple with computational challenges. Methods reliant on Riemannian optimization frequently exhibit sluggishness, stemming from the increased computational demands of operations on Riemannian manifolds. In response to these challenges, we present HyperDT, a novel extension of decision tree algorithms into hyperbolic space. Crucially, HyperDT eliminates the need for computationally intensive Riemannian optimization, numerically unstable exponential and logarithmic maps, or pairwise comparisons between points by leveraging inner products to adapt Euclidean decision tree algorithms to hyperbolic space.},
  author = {Philippe Chlenski and Ethan Turok and Antonio Khalil Moretti and Itsik Pe'er},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  eprint = {arXiv:2310.13841},
  file = {:/home/b/documents/inproceedings/chlenski2024fast.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=TTonmgTT9X},
  publisher = {OpenReview.net},
  title = {Fast Hyperboloid Decision Tree Algorithms},
  url = {https://openreview.net/forum?id=TTonmgTT9X},
  year = {2024}
}

@inproceedings{cho2024corn,
  abstract = {Nonprehensile manipulation is essential for manipulating objects that are too thin, large, or otherwise ungraspable in the wild. To sidestep the difficulty of contact modeling in conventional modeling-based approaches, reinforcement learning (RL) has recently emerged as a promising alternative. However, previous RL approaches either lack the ability to generalize over diverse object shapes, or use simple action primitives that limit the diversity of robot motions. Furthermore, using RL over diverse object geometry is challenging due to the high cost of training a policy that takes in high-dimensional sensory inputs. We propose a novel contact-based object representation and pretraining pipeline to tackle this. To enable massively parallel training, we leverage a lightweight patch-based transformer architecture for our encoder that processes point clouds, thus scaling our training across thousands of environments. Compared to learning from scratch, or other shape representation baselines, our representation facilitates both time- and data-efficient learning. We validate the efficacy of our overall system by zero-shot transferring the trained policy to novel real-world objects.},
  author = {Yoonyoung Cho and Junhyek Han and Yoontae Cho and Beomjoon Kim},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  eprint = {arXiv:2403.10760},
  file = {:/home/b/documents/inproceedings/cho2024corn.pdf:pdf},
  keywords = {Robotics, Nonprehensile manipulation, Contact representation, Reinforcement learning},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=KTtEICH4TO},
  publisher = {OpenReview.net},
  title = {CORN: Contact-based Object Representation for Nonprehensile Manipulation of General Unseen Objects},
  url = {https://openreview.net/forum?id=KTtEICH4TO},
  year = {2024}
}

@inproceedings{cho2024querying,
  abstract = {Active learning, a paradigm within machine learning, aims to select and query unlabeled data to enhance model performance strategically. A crucial selection strategy leverages the model's predictive uncertainty, reflecting the informativeness of a data point. While the sample's distance to the decision boundary intuitively measures predictive uncertainty, its computation becomes intractable for complex decision boundaries formed in multiclass classification tasks. This paper introduces the least disagree metric (LDM), the smallest probability of predicted label disagreement. We propose an asymptotically consistent estimator for LDM under mild assumptions. The estimator boasts computational efficiency and straightforward implementation for deep learning models using parameter perturbation. The LDM-based active learning algorithm queries unlabeled data with the smallest LDM, achieving state-of-the-art overall performance across various datasets and deep architectures, as demonstrated in their experimental evaluations.},
  author = {Seong Jin Cho and Gwangsu Kim and Junghyun Lee and Jinwoo Shin and Chang D. Yoo},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  eprint = {arXiv:2401.09787},
  file = {:/home/b/documents/inproceedings/cho2024querying.pdf:pdf},
  keywords = {Active learning, Uncertainty estimation, Deep learning},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=THUBTfSAS2},
  publisher = {OpenReview.net},
  title = {Querying Easily Flip-flopped Samples for Deep Active Learning},
  url = {https://openreview.net/forum?id=THUBTfSAS2},
  year = {2024}
}

@inproceedings{cho2024noise,
  abstract = {Text-guided diffusion models have become a popular tool in image synthesis, known for producing high-quality and diverse images. However, their application to editing real images often encounters hurdles primarily due to the text condition deteriorating the reconstruction quality and subsequently affecting editing fidelity. Null-text Inversion (NTI) has made strides in this area, but it fails to capture spatial context and requires computationally intensive per-timestep optimization. Addressing these challenges, we present Noise Map Guidance (NMG), an inversion method rich in a spatial context, tailored for real-image editing. Significantly, NMG achieves this without necessitating optimization, yet preserves the editing quality. Our empirical investigations highlight NMG's adaptability across various editing techniques and its robustness to variants of DDIM inversions.},
  author = {Hansam Cho and Jonghyun Lee and Seoung Bum Kim and Tae-Hyun Oh and Yonghyun Jeong},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  eprint = {arXiv:2402.04625},
  file = {:/home/b/documents/inproceedings/cho2024noise.pdf:pdf},
  keywords = {Diffusion models, Image editing, Text-to-image generation},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=mhgm0IXtHw},
  publisher = {OpenReview.net},
  title = {Noise Map Guidance: Inversion with Spatial Context for Real Image Editing},
  url = {https://openreview.net/forum?id=mhgm0IXtHw},
  year = {2024}
}

@inproceedings{cho2024spatiallyaware,
  abstract = {Episodic memory plays a crucial role in various cognitive processes, such as the ability to mentally recall past events. While cognitive science emphasizes the significance of spatial context in the formation and retrieval of episodic memory, the current primary approach to implementing episodic memory in AI systems is through transformers that store temporally ordered experiences, which overlooks the spatial dimension. As a result, it is unclear how the underlying structure could be extended to incorporate the spatial axis beyond temporal order alone and thereby what benefits can be obtained. To address this, this paper explores the use of Spatially-Aware Transformer models that incorporate spatial information. These models enable the creation of place-centric episodic memory that considers both temporal and spatial dimensions. Adopting this approach, we demonstrate that memory utilization efficiency can be improved, leading to enhanced accuracy in various place-centric downstream tasks. Additionally, we propose the Adaptive Memory Allocator, a memory management method based on reinforcement learning that aims to optimize efficiency of memory utilization. Our experiments demonstrate the advantages of our proposed model in various environments and across multiple downstream tasks, including prediction, generation, reasoning, and reinforcement learning.},
  author = {Junmo Cho and Jaesik Yoon and Sungjin Ahn},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  eprint = {arXiv:2402.15160},
  file = {:/home/b/documents/inproceedings/cho2024spatiallyaware.pdf:pdf},
  keywords = {Embodied agents, Episodic memory, Spatial reasoning, Transformers},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=Ts95eXsPBc},
  publisher = {OpenReview.net},
  title = {Spatially-Aware Transformers for Embodied Agents},
  url = {https://openreview.net/forum?id=Ts95eXsPBc},
  year = {2024}
}

@inproceedings{choi2024analyzing,
  abstract = {Optimal Transport (OT) problem aims to find a transport plan that bridges two distributions while minimizing a given cost function. OT theory has been widely utilized in generative modeling. In the beginning, OT distance has been used as a measure for assessing the distance between data and generated distributions. Recently, OT transport map between data and prior distributions has been utilized as a generative model. These OT-based generative models share a similar adversarial training objective. In this paper, we begin by unifying these OT-based adversarial methods within a single framework. Then, we elucidate the role of each component in training dynamics through a comprehensive analysis of this unified framework. Moreover, we suggest a simple but novel method that improves the previously best-performing OT-based model. Intuitively, our approach conducts a gradual refinement of the generated distribution, progressively aligning it with the data distribution. Our approach achieves a FID score of 2.51 on CIFAR-10 and 5.99 on CelebA-HQ-256, outperforming unified OT-based adversarial approaches.},
  author = {Jaemoo Choi and Jaewoong Choi and Myungjoo Kang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  eprint = {arXiv:2310.02611},
  file = {:/home/b/documents/inproceedings/choi2024analyzing.pdf:pdf},
  keywords = {Optimal transport, Generative adversarial networks, Generative modeling},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=jODehvtTDx},
  publisher = {OpenReview.net},
  title = {Analyzing and Improving Optimal-Transport-based Adversarial Networks},
  url = {https://openreview.net/forum?id=jODehvtTDx},
  year = {2024}
}

@inproceedings{choi2024dictionary,
  abstract = {While backpropagation (BP) has achieved widespread success in deep learning, it faces two prominent challenges: computational inefficiency and biological implausibility. In response to these challenges, local supervision, encompassing Local Learning (LL) and Forward Learning (FL), has emerged as a promising research direction. LL employs module-wise BP to achieve competitive results yet relies on module-wise auxiliary networks, which increase memory and parameter demands. Conversely, FL updates layer weights without BP and auxiliary networks but falls short of BP's performance. This paper proposes a simple yet effective objective within a contrastive learning framework for local supervision without auxiliary networks. Given the insight that the existing contrastive learning framework for local supervision is susceptible to task-irrelevant information without auxiliary networks, we present DICTIONARY CONTRASTIVE LEARNING (DCL) that optimizes the similarity between local features and label embeddings. Our method using static label embeddings yields substantial performance improvements in the FL scenario, outperforming state-of-the-art FL approaches. Moreover, our method using adaptive label embeddings closely approaches the performance achieved by LL while achieving superior memory and parameter efficiency.},
  author = {Suhwan Choi and Myeongho Jeon and Yeonjung Hwang and Jeonglyul Oh and Sungjun Lim and Joonseok Lee and Myungjoo Kang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/choi2024dictionary.pdf:pdf},
  note = {ICLR 2024 Spotlight Paper},
  pdf = {https://openreview.net/pdf?id=Gg7cXo3S8l},
  publisher = {OpenReview.net},
  title = {Dictionary Contrastive Learning for Efficient Local Supervision without Auxiliary Networks},
  url = {https://openreview.net/forum?id=Gg7cXo3S8l},
  year = {2024}
}

@inproceedings{choi2024conditional,
  abstract = {Time series imputation presents a significant challenge because it requires capturing the underlying temporal dynamics from partially observed time series data. Among the recent successes of imputation methods based on generative models, the information bottleneck (IB) framework offers a well-suited theoretical foundation for multiple imputations, allowing us to account for the uncertainty associated with the imputed values. However, directly applying the IB framework to time series data without considering their temporal context can lead to a substantial loss of temporal dependencies, which, in turn, can degrade the overall imputation performance. To address such a challenge, we propose a novel conditional information bottleneck (CIB) approach for time series imputation, which aims to mitigate the potentially negative consequences of the regularization constraint by focusing on reducing the redundant information conditioned on the temporal context. We provide a theoretical analysis of its effect by adapting variational decomposition. We use the resulting insight and propose a novel deep learning method that can approximately achieve the proposed CIB objective for time series imputation as a combination of evidence lower bound and novel temporal kernel-enhanced contrastive optimization. Our experiments, conducted on multiple real-world datasets, consistently demonstrate that our method significantly improves imputation performance (including both interpolation and extrapolation), and also enhances classification performance based on the imputed values.},
  author = {MinGyu Choi and Changhee Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/choi2024conditional.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=K1mcPiDdOJ},
  publisher = {OpenReview.net},
  title = {Conditional Information Bottleneck Approach for Time Series Imputation},
  url = {https://openreview.net/forum?id=K1mcPiDdOJ},
  year = {2024}
}

@inproceedings{choi2024sparse,
  abstract = {Given the ever-increasing size of modern neural networks, the significance of sparse architectures has surged due to their accelerated inference speeds and minimal memory demands. When it comes to global pruning techniques, Iterative Magnitude Pruning (IMP) still stands as a state-of-the-art algorithm despite its simple nature, particularly in extremely sparse regimes. In light of the recent finding that the two successive matching IMP solutions are linearly connected without a loss barrier, we propose Sparse Weight Averaging with Multiple Particles (SWAMP), a straightforward modification of IMP that achieves performance comparable to an ensemble of two IMP solutions. For every iteration, we concurrently train multiple sparse models, referred to as particles, using different batch orders yet the same matching ticket, and then weight average such models to produce a single mask. We demonstrate that our method consistently outperforms existing baselines across different sparsities through extensive experiments on various data and neural network structures.},
  author = {Moonseok Choi and Hyungi Lee and Giung Nam and Juho Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/choi2024sparse.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=Y9t7MqZtCR},
  publisher = {OpenReview.net},
  title = {Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning},
  url = {https://openreview.net/forum?id=Y9t7MqZtCR},
  year = {2024}
}

@inproceedings{choi2024lotabench,
  abstract = {Large language models (LLMs) have recently received considerable attention as alternative solutions for task planning. However, comparing the performance of language-oriented task planners becomes difficult, and there exists a dearth of detailed exploration regarding the effects of various factors such as pre-trained model selection and prompt construction. To address this, we propose a benchmark system for automatically quantifying performance of task planning for home-service embodied agents. Task planners are tested on two pairs of datasets and simulators: 1) ALFRED and AI2-THOR, 2) an extension of Watch-And-Help and VirtualHome. Using the proposed benchmark system, we perform extensive experiments with LLMs and prompts, and explore several enhancements of the baseline planner. We expect that the proposed benchmark tool would accelerate the development of language-oriented task planners.},
  author = {Jaewoo Choi and Youngwoo Yoon and Hyobin Ong and Jaehong Kim and Minsu Jang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/choi2024lotabench.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=ADSxCpCu9s},
  publisher = {OpenReview.net},
  title = {{LoTa-Bench}: Benchmarking Language-oriented Task Planners for Embodied Agents},
  url = {https://openreview.net/forum?id=ADSxCpCu9s},
  year = {2024}
}

@inproceedings{choquettechoo2024correlated,
  abstract = {Differentially private learning algorithms inject noise into the learning process. While the most common private learning algorithm, DP-SGD, adds independent Gaussian noise in each iteration, recent work on matrix factorization mechanisms has shown empirically that introducing correlations in the noise can greatly improve their utility. We characterize the asymptotic learning utility for any choice of the correlation function, giving precise analytical bounds for linear regression and as the solution to a convex program for general convex functions. Using these bounds, we show how correlated noise provably improves upon vanilla DP-SGD as a function of problem parameters such as the effective dimension and condition number. We also derive an analytical expression for the near-optimal correlation function that circumvents the cubic complexity of the semi-definite program used to optimize the noise correlation matrix in previous work. We validate our theory with experiments on private deep learning, matching or outperforming prior work while being efficient both in terms of compute and memory.},
  author = {Christopher A. Choquette-Choo and Krishnamurthy Dj Dvijotham and Krishna Pillutla and Arun Ganesh and Thomas Steinke and Abhradeep Guha Thakurta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/choquettechoo2024correlated.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=xHmCdSArUC},
  publisher = {OpenReview.net},
  title = {Correlated Noise Provably Beats Independent Noise for Differentially Private Learning},
  url = {https://openreview.net/forum?id=xHmCdSArUC},
  year = {2024}
}

@inproceedings{choquettechoo2024privacy,
  abstract = {Privacy amplification exploits randomness in data selection to provide tighter differential privacy (DP) guarantees. This analysis is key to DP-SGD's success in machine learning, but, is not readily applicable to the newer state-of-the-art algorithms. This is because these algorithms, known as DP-FTRL, use the matrix mechanism to add correlated noise instead of independent noise as in DP-SGD. In this paper, we propose MMCC, the first algorithm to analyze privacy amplification via sampling for any generic matrix mechanism. MMCC is nearly tight in that it approaches a lower bound as epsilon approaches zero. To analyze correlated outputs in MMCC, we prove that they can be analyzed as if they were independent, by conditioning them on prior outputs. Our conditional composition theorem has broad utility: we use it to show that the noise added to binary-tree-DP-FTRL can asymptotically match the noise added to DP-SGD with amplification.},
  author = {Christopher A. Choquette-Choo and Arun Ganesh and Thomas Steinke and Abhradeep Guha Thakurta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/choquettechoo2024privacy.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=xUzWmFdglP},
  publisher = {OpenReview.net},
  title = {Privacy Amplification for Matrix Mechanisms},
  url = {https://openreview.net/forum?id=xUzWmFdglP},
  year = {2024}
}

@inproceedings{choukroun2024foundation,
  abstract = {In recent years, Artificial Intelligence has undergone a paradigm shift with the rise of foundation models, which are trained on large amounts of data, typically in a self-supervised way, and can then be adapted to a wide range of downstream tasks. In this work, we propose the first foundation model for Error Correction Codes. This model is trained on multiple codes and can then be applied to an unseen code. To enable this, we extend the Transformer architecture in multiple ways: (1) a code-invariant initial embedding, which is also position- and length-invariant, (2) a learned modulation of the attention maps that is conditioned on the Tanner graph, and (3) a length-invariant code-aware noise prediction module that is based on the parity-check matrix. Our experimental evaluation demonstrates that the proposed foundation model achieves competitive performance on multiple codes, including unseen codes, and can serve as a universal decoder for error correction across different code families.},
  author = {Yoni Choukroun and Lior Wolf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/choukroun2024foundation.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=7KDuQPrAF3},
  publisher = {OpenReview.net},
  title = {A Foundation Model for Error Correction Codes},
  url = {https://openreview.net/forum?id=7KDuQPrAF3},
  year = {2024}
}

@inproceedings{chowdhury2024retrievalguided,
  abstract = {Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (synthesis recipe), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of hardware design complexities -- from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) -- requires a nuanced synthesis recipe guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We propose a retrieval-guided reinforcement learning approach for logic synthesis that adjusts agent recommendations using nearest neighbor similarity scores for improved synthesis recipe optimization.},
  author = {Animesh Basak Chowdhury and Marco Romanelli and Benjamin Tan and Ramesh Karri and Siddharth Garg},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chowdhury2024retrievalguided.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=0t1O8ziRZp},
  publisher = {OpenReview.net},
  title = {Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization},
  url = {https://openreview.net/forum?id=0t1O8ziRZp},
  year = {2024}
}

@inproceedings{chowdhury2024enhancing,
  abstract = {Fairness, especially group fairness, is an important consideration in the context of machine learning systems. The most commonly adopted group fairness-enhancing techniques are in-processing methods that rely on a mixture of a fairness objective (e.g., demographic parity) and a task-specific objective (e.g., cross-entropy) during the training process. However, when data arrives in an online fashion -- one instance at a time -- optimizing such fairness objectives poses several challenges. In particular, group fairness objectives are defined using expectations of predictions across different demographic groups. In the online setting, where the algorithm has access to a single instance at a time, estimating the group fairness objective requires additional storage and significantly more computation (e.g., forward/backward passes) than the task-specific objective at every time step. In this paper, we propose Aranyani, an ensemble of oblique decision trees, to make fair decisions in online settings. The hierarchical tree structure of Aranyani enables parameter isolation and allows us to efficiently compute the fairness gradients using aggregate statistics of previous decisions, eliminating the need for additional storage and forward/backward passes.},
  author = {Somnath Basu Roy Chowdhury and Nicholas Monath and Ahmad Beirami and Rahul Kidambi and Kumar Avinava Dubey and Amr Ahmed and Snigdha Chaturvedi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chowdhury2024enhancing.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=E1NxN5QMOE},
  publisher = {OpenReview.net},
  title = {Enhancing Group Fairness in Online Settings Using Oblique Decision Forests},
  url = {https://openreview.net/forum?id=E1NxN5QMOE},
  year = {2024}
}

@inproceedings{chu2024gpavatar,
  abstract = {Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control expressions and postures. Existing methods, categorized into 2D-based warping, mesh-based, and neural rendering approaches, present challenges in maintaining multi-view consistency, incorporating non-facial information, and generalizing to new identities. In this paper, we propose GPAvatar, a framework that reconstructs 3D head avatars from one or several images in a single forward pass. The key idea of this work is to introduce a dynamic point-based expression field driven by a point cloud to precisely and effectively capture expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion module in the tri-planes canonical field to leverage information from multiple input images. Our experimental evaluation demonstrates that GPAvatar achieves faithful identity reconstruction, precise expression control, and multi-view consistency, showing promising results for free-viewpoint rendering and novel view synthesis.},
  author = {Xuangeng Chu and Yu Li and Ailing Zeng and Tianyu Yang and Lijian Lin and Yunfei Liu and Tatsuya Harada},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chu2024gpavatar.pdf:pdf},
  note = {ICLR 2024 Conference Paper},
  pdf = {https://openreview.net/pdf?id=hgehGq2bDv},
  publisher = {OpenReview.net},
  title = {{GPAvatar}: Generalizable and Precise Head Avatar from Image(s)},
  url = {https://openreview.net/forum?id=hgehGq2bDv},
  year = {2024}
}

@inproceedings{chu2024image,
  abstract = {The advent of large pre-trained models has brought about a paradigm shift in both visual representation learning and natural language processing. However, clustering unlabeled images, as a fundamental and classic machine learning problem, still lacks an effective solution, particularly for large-scale datasets. In this paper, we propose a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as {CLIP} and cluster images effectively and efficiently at scale. We first developed a novel algorithm to estimate the number of clusters in a given dataset. We then show that the pre-trained features are significantly more structured by further optimizing the rate reduction objective. The resulting features may significantly improve the clustering accuracy, e.g., from 57\% to 66\% on {ImageNet}-1k. Furthermore, by leveraging {CLIP}'s multimodality bridge between image and text, we develop a simple yet effective self-labeling algorithm that produces meaningful text labels for the clusters. Through extensive experiments, we show that our pipeline works well on standard datasets such as {CIFAR}-10, {CIFAR}-100, and {ImageNet}-1k. It also extends to datasets without predefined labels, such as {LAION}-Aesthetics and {WikiArts}.},
  author = {Tianzhe Chu and Shengbang Tong and Tianjiao Ding and Xili Dai and Benjamin David Haeffele and Ren{\'{e}} Vidal and Yi Ma},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chu2024image.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ptCIlV24YZ},
  publisher = {OpenReview.net},
  title = {Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models},
  url = {https://openreview.net/forum?id=ptCIlV24YZ},
  year = {2024}
}

@inproceedings{chuang2024dola,
  abstract = {Despite their impressive capabilities, large language models ({LLMs}) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained {LLMs} that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an {LLMs} has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers ({DoLa}) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. {DoLa} consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of {LLaMA} family models on {TruthfulQA} by 12--17\% absolute points, demonstrating its potential in making {LLMs} reliably generate truthful facts.},
  author = {Yung-Sung Chuang and Yujia Xie and Hongyin Luo and Yoon Kim and James R. Glass and Pengcheng He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chuang2024dola.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Th6NyL07na},
  publisher = {OpenReview.net},
  title = {{DoLa}: Decoding by Contrasting Layers Improves Factuality in Large Language Models},
  url = {https://openreview.net/forum?id=Th6NyL07na},
  year = {2024}
}

@inproceedings{chun2024improved,
  abstract = {Image-Text Matching ({ITM}) task, a fundamental vision-language ({VL}) task, suffers from the inherent ambiguity arising from multiplicity and imperfect annotations. Deterministic functions are not sufficiently powerful to capture ambiguity, prompting the exploration of probabilistic embeddings to tackle the challenge. However, the existing probabilistic {ITM} approach encounters two key shortcomings; the burden of heavy computations due to the Monte Carlo approximation, and the loss saturation issue in the face of abundant false negatives. To overcome the issues, this paper presents an improved Probabilistic Cross-Modal Embeddings (named {PCME}++) by introducing a new probabilistic distance with a closed-form solution. In addition, two optimization techniques are proposed to enhance {PCME}++ further: first, the incorporation of pseudo-positives to prevent the negative effect under massive false negatives; second, mixed sample data augmentation for probabilistic matching. Experimental results on {MS-COCO} Caption and two extended benchmarks, {CxC} and {ECCV} Caption, demonstrate the effectiveness of {PCME}++ compared to state-of-the-art {ITM} methods. The robustness of {PCME}++ is also evaluated under noisy image-text correspondences. In addition, the potential applicability of {PCME}++ in automatic prompt-filtering for zero-shot classification is shown.},
  author = {Sanghyuk Chun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chun2024improved.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ft1mr3WlGM},
  publisher = {OpenReview.net},
  title = {Improved Probabilistic Image-Text Representations},
  url = {https://openreview.net/forum?id=ft1mr3WlGM},
  year = {2024}
}

@inproceedings{chung2024decomposed,
  abstract = {Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the conjugate gradient method ({CG}), one of the most popular Krylov subspace methods, is based on the idea of minimizing the residual error in the Krylov subspace. However, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. In this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and Krylov subspace methods. Specifically, we prove that if the tangent space at a denoised sample by Tweedie's formula forms a Krylov subspace, then the {CG} initialized with the denoised data ensures the data consistency while enabling faster convergence for large-scale inverse problems.},
  author = {Hyungjin Chung and Suhyeon Lee and Jong Chul Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chung2024decomposed.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DsEhqQtfAG},
  publisher = {OpenReview.net},
  title = {Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems},
  url = {https://openreview.net/forum?id=DsEhqQtfAG},
  year = {2024}
}

@inproceedings{chung2024rethinking,
  abstract = {Dataset distillation offers a potential means to enhance data efficiency in deep learning. Recent studies have shown its ability to counteract backdoor risks present in original training samples. In this study, we delve into the theoretical aspects of backdoor attacks and dataset distillation based on kernel methods. We introduce two new theory-driven trigger pattern generation methods specialized for dataset distillation. Following a comprehensive set of analyses and experiments, we show that our optimization-based trigger design framework informs effective backdoor attacks on dataset distillation. Notably, datasets poisoned by our designed trigger prove resilient against conventional backdoor attack detection and mitigation methods.},
  author = {Ming-Yu Chung and Sheng-Yen Chou and Chia-Mu Yu and Pin-Yu Chen and Sy-Yen Kuo and Tsung-Yi Ho},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chung2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=iCNOK45Csv},
  publisher = {OpenReview.net},
  title = {Rethinking Backdoor Attacks on Dataset Distillation: A Kernel Method Perspective},
  url = {https://openreview.net/forum?id=iCNOK45Csv},
  year = {2024}
}

@inproceedings{chytas2024pooling,
  abstract = {Small sample sizes are common in many disciplines, which necessitates pooling roughly similar datasets across multiple institutions to study weak but relevant associations between images and disease outcomes. Such data often manifest shift/imbalance in covariates (i.e., secondary non-imaging data). Controlling for such nuisance variables is common within standard statistical analysis, but the ideas do not directly apply to overparameterized models. Consequently, recent work has shown how strategies from invariant representation learning provides a meaningful starting point, but the current repertoire of methods is limited to accounting for shifts/imbalances in just a couple of covariates at a time. In this paper, we show how viewing this problem from the perspective of Category theory provides a simple and effective solution that completely avoids elaborate multi-stage training pipelines that would otherwise be needed.},
  author = {Sotirios Panagiotis Chytas and Vishnu Suresh Lokhande and Vikas Singh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/chytas2024pooling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2Mo7v69otj},
  publisher = {OpenReview.net},
  title = {Pooling Image Datasets with Multiple Covariate Shift and Imbalance},
  url = {https://openreview.net/forum?id=2Mo7v69otj},
  year = {2024}
}

@inproceedings{clark2024directly,
  abstract = {We present Direct Reward Fine-Tuning ({DRaFT}), a simple and effective method for fine-tuning diffusion models to maximize differentiable reward functions, such as scores from human preference models. We first show that it is possible to backpropagate the reward function gradient through the full sampling procedure, and that doing so achieves strong performance on a variety of rewards, outperforming reinforcement learning-based approaches. We then propose more efficient variants of {DRaFT}: {DRaFT}-K, which truncates backpropagation to only the last K steps of sampling, and {DRaFT}-{LV}, which obtains lower-variance gradient estimates for the case when K=1. We show that our methods work well for a variety of reward functions and can be used to substantially improve the aesthetic quality of images generated by Stable Diffusion 1.4. Finally, we draw connections between our approach and prior work, providing a unifying perspective on the design space of gradient-based fine-tuning algorithms.},
  author = {Kevin Clark and Paul Vicol and Kevin Swersky and David J. Fleet},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/clark2024directly.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1vmSEVL19f},
  publisher = {OpenReview.net},
  title = {Directly Fine-Tuning Diffusion Models on Differentiable Rewards},
  url = {https://openreview.net/forum?id=1vmSEVL19f},
  year = {2024}
}

@inproceedings{cohen2024posterior,
  abstract = {Image restoration problems are typically ill-posed in the sense that each degraded image can be restored in infinitely many valid ways. To accommodate this, many works generate a diverse set of outputs by attempting to randomly sample from the posterior distribution of natural images given the degraded input. Here we argue that this strategy is commonly of limited practical value because of the heavy tail of the posterior distribution. Consider for example inpainting a missing region of the sky in an image. Since there is a high probability that the missing region contains no object but clouds, any set of samples from the posterior would be entirely dominated by (practically identical) completions of sky. However, arguably, presenting users with only one clear sky completion, along with several alternative solutions such as airships, birds, and balloons, would better outline the set of possibilities. In this paper, we initiate the study of meaningfully diverse image restoration. We explore several post-processing approaches that can be combined with any diverse image restoration method to yield semantically meaningful diversity. Moreover, we propose a practical approach for allowing diffusion based image restoration methods to generate meaningfully diverse outputs, while incurring only negligible computational overhead. We conduct extensive user studies to analyze the proposed techniques, and find the strategy of reducing similarity between outputs to be significantly favorable over posterior sampling.},
  author = {Noa Cohen and Hila Manor and Yuval Bahat and Tomer Michaeli},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cohen2024posterior.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ff2g30cZxj},
  publisher = {OpenReview.net},
  title = {From Posterior Sampling to Meaningful Diversity in Image Restoration},
  url = {https://openreview.net/forum?id=ff2g30cZxj},
  year = {2024}
}

@inproceedings{cole2024scorebased,
  abstract = {While score-based generative models ({SGMs}) have achieved remarkable successes in enormous image generation tasks, their mathematical foundations are still limited. In this paper, we analyze the approximation and generalization of {SGMs} in learning a family of sub-Gaussian probability distributions. We introduce a measure of complexity for probability distributions in terms of their relative density with respect to the standard Gaussian measure. We prove that if the log-relative density can be locally approximated by a neural network whose parameters can be suitably bounded, then the distribution generated by empirical score matching approximates the target distribution in total variation with a dimension-independent rate. We illustrate our theory through examples, which include certain mixtures of Gaussians. An essential ingredient of our proof is to derive a dimension-free deep neural network approximation rate for the true score function associated with the forward process, which is interesting in its own right.},
  author = {Frank Cole and Yulong Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cole2024scorebased.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wG12xUSqrI},
  publisher = {OpenReview.net},
  title = {Score-based generative models break the curse of dimensionality in learning a family of sub-Gaussian distributions},
  url = {https://openreview.net/forum?id=wG12xUSqrI},
  year = {2024}
}

@inproceedings{cong2024flatten,
  abstract = {Text-to-video editing aims to edit the visual appearance of a source video conditional on textual prompts. A major challenge in this task is to ensure that all frames in the edited video are visually consistent. Most recent works apply advanced text-to-image diffusion models to this task by inflating {2D} spatial attention in the {U}-Net into spatio-temporal attention. Although temporal context can be added through spatio-temporal attention, it may introduce some irrelevant information for each patch and therefore cause inconsistency in the edited video. In this paper, for the first time, we introduce optical flow into the attention module in the diffusion model's {U}-Net to address the inconsistency issue for text-to-video editing. Our method, {FLATTEN}, enforces the patches on the same flow path across different frames to attend to each other in the attention module, thus improving the visual consistency in the edited videos.},
  author = {Yuren Cong and Mengmeng Xu and Christian Simon and Shoufa Chen and Jiawei Ren and Yanping Xie and Juan-Manuel P{\'{e}}rez-R{\'{u}}a and Bodo Rosenhahn and Tao Xiang and Sen He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cong2024flatten.pdf:pdf},
  note = {DBLP last modified: 2025-03-06},
  pdf = {https://openreview.net/pdf?id=JgqftqZQZ7},
  publisher = {OpenReview.net},
  title = {{FLATTEN}: optical {FLow}-guided {ATTENtion} for consistent text-to-video editing},
  url = {https://openreview.net/forum?id=JgqftqZQZ7},
  year = {2024}
}

@inproceedings{jeanre2024assessing,
  abstract = {The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. In order to draw reliable conclusions based on empirical ROC analysis, accurately evaluating the uncertainty level related to statistical versions of the ROC curves of interest is absolutely necessary, especially for applications with considerable societal impact such as Face Recognition. In this article, we prove asymptotic guarantees for empirical ROC curves of similarity functions as well as for by-product metrics useful to assess fairness. We also explain that, because the false acceptance/rejection rates are of the form of U-statistics in the case of similarity scoring, the naive bootstrap approach may jeopardize the assessment procedure. A dedicated recentering technique must be used instead. Beyond the theoretical analysis carried out, various experiments using real face image datasets provide strong empirical evidence of the practical relevance of the methods promoted here, when applied to several ROC-based measures such as popular fairness metrics.},
  author = {Jean-R\'{e}my Conti and St\'{e}phan Cl\'{e}men\c{c}on},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jeanre2024assessing.pdf:pdf},
  keywords = {Uncertainty, Face, Recognition, Performance, ROC, Fairness, Bootstrap},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lAhQCHuANV},
  publisher = {OpenReview.net},
  title = {Assessing Uncertainty in Similarity Scoring: Performance \& Fairness in Face Recognition},
  url = {https://openreview.net/forum?id=lAhQCHuANV},
  year = {2024}
}

@inproceedings{corrado2024understanding,
  abstract = {Recently, data augmentation (DA) has emerged as a method for leveraging domain knowledge to inexpensively generate additional data in reinforcement learning (RL) tasks, often yielding substantial improvements in data efficiency. While prior work has demonstrated the utility of incorporating augmented data directly into model-free RL updates, it is not well-understood when a particular DA strategy will improve data efficiency. In this paper, we seek to identify general aspects of DA responsible for observed learning improvements. Our study focuses on sparse-reward tasks with dynamics-invariant data augmentation functions, serving as an initial step towards a more general understanding of DA and its integration into RL training. Experimentally, we isolate three relevant aspects of DA: state-action coverage, reward density, and the number of augmented transitions generated per update (the augmented replay ratio). From our experiments, we draw two conclusions: (1) increasing state-action coverage often has a much greater impact on data efficiency than increasing reward density, and (2) decreasing the augmented replay ratio substantially improves data efficiency. In fact, certain tasks in our empirical study are solvable only when the replay ratio is sufficiently low.},
  author = {Nicholas Corrado and Josiah P. Hanna},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/corrado2024understanding.pdf:pdf},
  keywords = {reinforcement learning, data augmentation, model-free},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=sVEu295o70},
  publisher = {OpenReview.net},
  title = {Understanding when Dynamics-Invariant Data Augmentations Benefit Model-free Reinforcement Learning Updates},
  url = {https://openreview.net/forum?id=sVEu295o70},
  year = {2024}
}

@inproceedings{corso2024deep,
  abstract = {Accurate blind docking has the potential to lead to new biological breakthroughs, but for this promise to be realized, docking methods must generalize well across the proteome. Existing benchmarks, however, fail to rigorously assess generalizability. Therefore, we develop DockGen, a new benchmark based on the ligand-binding domains of proteins, and we show that existing machine learning-based docking models have very weak generalization abilities. We carefully analyze the scaling laws of ML-based docking and show that, by scaling data and model size, as well as integrating synthetic data strategies, we are able to significantly increase the generalization capacity and set new state-of-the-art performance across benchmarks. Further, we propose Confidence Bootstrapping, a new training paradigm that solely relies on the interaction between diffusion and confidence models and exploits the multi-resolution generation process of diffusion models. We demonstrate that Confidence Bootstrapping significantly improves the ability of ML-based docking methods to dock to unseen protein classes, edging closer to accurate and generalizable blind docking methods.},
  author = {Gabriele Corso and Arthur Deng and Nicholas Polizzi and Regina Barzilay and Tommi S. Jaakkola},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/corso2024deep.pdf:pdf},
  keywords = {generalization, molecular docking, protein-ligand binding, diffusion models, benchmark, bootstrapping, self-training},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UfBIxpTK10},
  publisher = {OpenReview.net},
  title = {Deep Confident Steps to New Pockets: Strategies for Docking Generalization},
  url = {https://openreview.net/forum?id=UfBIxpTK10},
  year = {2024}
}

@inproceedings{corso2024particle,
  abstract = {In light of the widespread success of generative models, a significant amount of research has gone into speeding up their sampling time. However, generative models are often sampled multiple times to obtain a diverse set incurring a cost that is orthogonal to sampling time. We tackle the question of how to improve diversity and sample efficiency by moving beyond the common assumption of independent samples. We propose particle guidance, an extension of diffusion-based generative sampling where a joint-particle time-evolving potential enforces diversity. We analyze theoretically the joint distribution that particle guidance generates, how to learn a potential that achieves optimal diversity, and the connections with methods in other disciplines. Empirically, we test the framework both in the setting of conditional image generation, where we are able to increase diversity without affecting quality, and molecular conformer generation, where we reduce the state-of-the-art median error by 13\% on average.},
  author = {Gabriele Corso and Yilun Xu and Valentin de Bortoli and Regina Barzilay and Tommi S. Jaakkola},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/corso2024particle.pdf:pdf},
  keywords = {diffusion models, sampling, diversity, generative models, molecular conformers},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=KqbCvIFBY7},
  publisher = {OpenReview.net},
  title = {Particle Guidance: non-{I}.{I}.{D}. Diverse Sampling with Diffusion Models},
  url = {https://openreview.net/forum?id=KqbCvIFBY7},
  year = {2024}
}

@inproceedings{coste2024reward,
  abstract = {Reinforcement learning from human feedback (RLHF) is a standard approach for fine-tuning large language models to follow instructions. As part of this process, learned reward models are used to approximately model human preferences. However, as imperfect representations of the "true" reward, these learned reward models are susceptible to overoptimization. We conduct a systematic study investigating whether combining ensembles with conservative optimization can help mitigate overoptimization. We build upon the work of Gao et al. (2023) who studied this phenomenon in a synthetic human feedback setup with a significantly larger "gold" reward model acting as the true reward (instead of humans) and showed that overoptimization remains a persistent problem regardless of the size of the proxy reward model and training data used. We extend their setup to include 25\% label noise to better mirror real-world conditions and perform a systematic study. Our results indicate that not only does ensemble-based conservative optimization help mitigate overoptimization, it also results in improved performance.},
  author = {Thomas Coste and Usman Anwar and Robert Kirk and David Krueger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/coste2024reward.pdf:pdf},
  keywords = {RLHF, reward models, overoptimization, ensemble methods, human feedback},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dcjtMYkpXx},
  publisher = {OpenReview.net},
  title = {Reward Model Ensembles Help Mitigate Overoptimization},
  url = {https://openreview.net/forum?id=dcjtMYkpXx},
  year = {2024}
}

@inproceedings{rarecs2024discretization,
  abstract = {We study contextual stochastic optimization problems where optimization problems have uncertain parameters stemming from unknown, context-dependent distributions. Due to the inherent uncertainty in these problems, one is often interested not only in minimizing expected cost, but also to be robust and protect against worst case scenarios. We propose a novel method that combines the learning stage with knowledge of the downstream optimization task. The method prescribes decisions which aim to maximize the likelihood that the cost is below a (user-controlled) threshold. The key idea is to discretize the feasible region into subsets so that the uncertain objective function can be well approximated deterministically within each subset, and devise a secondary optimization problem to prescribe decisions by integrating the information from the individual approximations. We provide theoretical guarantees bounding the underlying regret of decisions proposed by our method.},
  author = {Rare\c{s} Cristian and Georgia Perakis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rarecs2024discretization.pdf:pdf},
  keywords = {contextual optimization, stochastic optimization, robust optimization, uncertainty},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ueTdErd5Ib},
  publisher = {OpenReview.net},
  title = {A Discretization Framework for Robust Contextual Stochastic Optimization},
  url = {https://openreview.net/forum?id=ueTdErd5Ib},
  year = {2024}
}

@inproceedings{andre2024unprocessing,
  abstract = {Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Our findings cast doubt on the narrative of progress in algorithmic fairness and raise questions about the complexity of proposed methods.},
  author = {Andr\'{e} F. Cruz and Moritz Hardt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/andre2024unprocessing.pdf:pdf},
  keywords = {algorithmic fairness, postprocessing, error rate parity, fairness-accuracy tradeoffs},
  note = {DBLP last modified: 2024-08-04},
  pdf = {https://openreview.net/pdf?id=jr03SfWsBS},
  publisher = {OpenReview.net},
  title = {Unprocessing Seven Years of Algorithmic Fairness},
  url = {https://openreview.net/forum?id=jr03SfWsBS},
  year = {2024}
}

@inproceedings{cui2024analysis,
  abstract = {We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem. First, we provide a tight closed-form characterization of the learnt velocity field, when parametrized by a shallow denoising auto-encoder trained on a finite number n of samples from the target distribution. Building on this analysis, we provide a sharp description of the corresponding generative flow, which pushes the base Gaussian density forward to an approximation of the target density. In particular, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture, which we show decays as $\Theta_n(1/n)$. Finally, this rate is shown to be Bayes-optimal.},
  author = {Hugo Cui and Florent Krzakala and Eric Vanden-Eijnden and Lenka Zdeborov\'{a}},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cui2024analysis.pdf:pdf},
  keywords = {flow-based generative models, sample complexity, theoretical analysis, Gaussian mixtures},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ndCJeysCPe},
  publisher = {OpenReview.net},
  title = {Analysis of Learning a Flow-based Generative Model from Limited Sample Complexity},
  url = {https://openreview.net/forum?id=ndCJeysCPe},
  year = {2024}
}

@inproceedings{cui2024clap,
  abstract = {We investigate a new practical learning scenario where data distributed across different sources/clients are typically generated with various modalities. Existing research on learning from multi-source data mostly assumes that each client owns data of all modalities, which may largely limit its practicability. In light of the expensiveness and sparsity of multimodal data, we propose patchwork learning to jointly learn from fragmented multimodal data in distributed clients. Considering the concerns about data privacy, patchwork learning aims to impute incomplete multimodal data for diverse downstream tasks without accessing the raw data directly. Local clients could miss different modality combinations. Due to the statistical heterogeneity induced by non-i.i.d. data, the imputation is more challenging since the learned dependencies fail to adapt to the imputation of other clients. To address these challenges, we propose CLAP (Collaborative Adaptation for Patchwork Learning), a method that enables effective collaboration among clients with different missing modality patterns while preserving data privacy.},
  author = {Sen Cui and Abudukelimu Wuerkaixi and Weishen Pan and Jian Liang and Lei Fang and Changshui Zhang and Fei Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cui2024clap.pdf:pdf},
  keywords = {federated learning, multimodal learning, patchwork learning, missing modalities, data privacy},
  note = {DBLP last modified: 2024-10-22},
  pdf = {https://openreview.net/pdf?id=8EyRkd3Qj2},
  publisher = {OpenReview.net},
  title = {{CLAP}: Collaborative Adaptation for Patchwork Learning},
  url = {https://openreview.net/forum?id=8EyRkd3Qj2},
  year = {2024}
}

@inproceedings{cundy2024sequencematch,
  abstract = {The maximum-likelihood (MLE) objective used to train autoregressive models does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): this leads to compounding error during autoregressive generation. We formulate sequence generation as an imitation learning (IL) problem, which allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework allows us to incorporate backtracking by introducing a backspace action into the generation process, which further mitigates the compounding error problem by allowing the model to revert a sampled token if it takes the sequence OOD. The resulting method, SequenceMatch, can be implemented without adversarial training or major architectural changes. Empirically, SequenceMatch training leads to improvements over MLE on text generation with language models and arithmetic.},
  author = {Chris Cundy and Stefano Ermon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/cundy2024sequencematch.pdf:pdf},
  keywords = {imitation learning, autoregressive models, sequence generation, backtracking, language models},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FJWT0692hw},
  publisher = {OpenReview.net},
  title = {{SequenceMatch}: Imitation Learning for Autoregressive Sequence Modelling with Backtracking},
  url = {https://openreview.net/forum?id=FJWT0692hw},
  year = {2024}
}

@inproceedings{daems2024variational,
  abstract = {We present a novel variational framework for performing inference in (neural) stochastic differential equations ({SDE}s) driven by {M}arkov-approximate fractional {B}rownian motion (f{BM}). {SDE}s offer a versatile tool for modeling real-world continuous-time dynamic systems with inherent noise and randomness. Combining {SDE}s with the powerful inference capabilities of variational methods, enables the learning of representative distributions through stochastic gradient descent. However, conventional {SDE}s typically assume the underlying noise to follow a {B}rownian motion ({BM}), which hinders their ability to capture long-term dependencies. We address this limitation by proposing to use fractional {B}rownian motion (f{BM}), which extends {BM} to encompass non-{M}arkovian dynamics with memory effects.},
  arxiv = {2310.12975},
  author = {Rembert Daems and Manfred Opper and Guillaume Crevecoeur and Tolga Birdal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/daems2024variational.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=rtx8B94JMS},
  publisher = {OpenReview.net},
  title = {Variational Inference for {SDE}s Driven by Fractional Noise},
  url = {https://openreview.net/forum?id=rtx8B94JMS},
  year = {2024}
}

@inproceedings{daheim2024model,
  abstract = {Models trained on different datasets can be merged by a weighted-averaging of their parameters, but why does it work and when can it fail? Here, we connect the inaccuracy of weighted-averaging to mismatches in the gradients and propose a new uncertainty-based scheme to improve the performance by reducing the mismatch. The connection also reveals implicit assumptions in other schemes such as averaging, task arithmetic, and {F}isher-weighted averaging. Our new method gives consistent improvements for large language models and vision transformers, both in terms of performance and robustness to hyperparameters.},
  arxiv = {2310.12808},
  author = {Nico Daheim and Thomas M{\"o}llenhoff and Edoardo M. Ponti and Iryna Gurevych and Mohammad Emtiyaz Khan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/daheim2024model.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=D7KJmfEDQP},
  publisher = {OpenReview.net},
  title = {Model Merging by Uncertainty-Based Gradient Matching},
  url = {https://openreview.net/forum?id=D7KJmfEDQP},
  year = {2024}
}

@inproceedings{dai2024gene,
  abstract = {Gene regulatory network inference ({GRNI}) is a challenging problem, particularly owing to the presence of zeros in single-cell {RNA} sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts), which may bias {GRNI} by distorting the joint distribution of the measured gene expressions. Existing approaches typically handle dropout error via imputation, which may introduce spurious relations as the true joint distribution is generally unidentifiable. To tackle this issue, we introduce a causal graphical model to characterize the dropout mechanism, namely, {C}ausal {D}ropout {M}odel. We provide a simple yet effective theoretical result: interestingly, the conditional independence ({CI}) relations in the data with dropouts, after deleting the samples with zero values (regardless if technical or not) for the conditioned variables, are asymptotically identical to the {CI} relations in the original data without dropouts.},
  arxiv = {2403.15500},
  author = {Haoyue Dai and Ignavier Ng and Gongxu Luo and Peter Spirtes and Petar Stojanov and Kun Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dai2024gene.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; Oral presentation},
  pdf = {https://openreview.net/pdf?id=gFR4QwK53h},
  publisher = {OpenReview.net},
  title = {Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View},
  url = {https://openreview.net/forum?id=gFR4QwK53h},
  year = {2024}
}

@inproceedings{dai2024safe,
  abstract = {With the development of large language models ({LLM}s), striking a balance between the performance and safety of {AI} systems has never been more critical. However, the inherent tension between the objectives of helpfulness and harmlessness presents a significant challenge during {LLM} training. To address this issue, we propose Safe Reinforcement Learning from Human Feedback (Safe {RLHF}), a novel algorithm for human value alignment. Safe {RLHF} explicitly decouples human preferences regarding helpfulness and harmlessness, effectively avoiding the crowdworkers' confusion about the tension and allowing us to train separate reward and cost models. We formalize the safety concern of {LLM}s as an optimization task of maximizing the reward function while satisfying specified cost constraints.},
  arxiv = {2310.12773},
  author = {Josef Dai and Xuehai Pan and Ruiyang Sun and Jiaming Ji and Xinbo Xu and Mickel Liu and Yizhou Wang and Yaodong Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dai2024safe.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=TyFrPOKYXw},
  publisher = {OpenReview.net},
  title = {Safe {RLHF}: Safe Reinforcement Learning from Human Feedback},
  url = {https://openreview.net/forum?id=TyFrPOKYXw},
  year = {2024}
}

@inproceedings{dai2024incontext,
  abstract = {In-context learning is a promising approach for online policy learning of offline reinforcement learning ({RL}) methods, which can be achieved at inference time without gradient optimization. However, this method is hindered by significant computational costs resulting from the gathering of large training trajectory sets and the need to train large {T}ransformer models. We address this challenge by introducing an In-context Exploration-Exploitation ({ICEE}) algorithm, designed to optimize the efficiency of in-context policy learning. Unlike existing models, {ICEE} performs an exploration-exploitation trade-off at inference time within a {T}ransformer model, without the need for explicit {B}ayesian inference.},
  arxiv = {2403.06826},
  author = {Zhenwen Dai and Federico Tomasi and Sina Ghiassian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dai2024incontext.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uIKZSStON3},
  publisher = {OpenReview.net},
  title = {In-context Exploration-Exploitation for Reinforcement Learning},
  url = {https://openreview.net/forum?id=uIKZSStON3},
  year = {2024}
}

@inproceedings{dai2024graphical,
  abstract = {Integrating information while recognizing dependence from multiple data sources and enhancing the predictive performance of the multi-output regression are challenging tasks. Multioutput {G}aussian {P}rocess ({MOGP}) methods offer outstanding solutions with tractable predictions and uncertainty quantification. However, their practical applications are hindered by high computational complexity and storage demand. Additionally, there exist model mismatches in existing {MOGP} models when dealing with non-{G}aussian data. To improve the model representation ability in terms of flexibility, optimality, and scalability, this paper introduces a novel multi-output regression framework, termed Graphical {MOGP} ({GMOGP}), which is empowered by: (i) Generating flexible {G}aussian process priors consolidated from identified parents, (ii) providing dependent processes with attention-based graphical representations, and (iii) achieving {P}areto optimal solutions of kernel hyperparameters via a distributed learning framework.},
  author = {Yijue Dai and Wenzhong Yan and Feng Yin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dai2024graphical.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=6N8TW504aa},
  publisher = {OpenReview.net},
  title = {Graphical Multioutput Gaussian Process with Attention},
  url = {https://openreview.net/forum?id=6N8TW504aa},
  year = {2024}
}

@inproceedings{dai2024enhancing,
  abstract = {One-shot Federated Learning ({OFL}) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In {OFL}, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote {OFL}, we introduce a novel framework, {C}o-{B}oosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, {C}o-{B}oosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial attack manner.},
  arxiv = {2402.15070},
  author = {Rong Dai and Yonggang Zhang and Ang Li and Tongliang Liu and Xun Yang and Bo Han},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dai2024enhancing.pdf:pdf},
  note = {DBLP last modified: 2025-01-29},
  pdf = {https://openreview.net/pdf?id=tm8s3696Ox},
  publisher = {OpenReview.net},
  title = {Enhancing One-Shot Federated Learning Through Data and Ensemble Co-Boosting},
  url = {https://openreview.net/forum?id=tm8s3696Ox},
  year = {2024}
}

@inproceedings{daigavane2024symphony,
  abstract = {We present Symphony, an {E}(3) equivariant autoregressive generative model for {3D} molecular geometries that iteratively builds a molecule from molecular fragments. Existing autoregressive models such as {G}-{S}ch{N}et and {G}-{S}phere{N}et for molecules utilize rotationally invariant features to respect the {3D} symmetries of molecules. In contrast, Symphony uses message-passing with higher-degree {E}(3)-equivariant features. This allows a novel representation of probability distributions via spherical harmonic signals to efficiently model the {3D} geometry of molecules. The paper shows that Symphony is able to accurately generate small molecules from the {QM}9 dataset, outperforming existing autoregressive models and approaching the performance of diffusion models.},
  arxiv = {2311.16199},
  author = {Ameya Daigavane and Song Kim and Mario Geiger and Tess E. Smidt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/daigavane2024symphony.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MIEnYtlGyv},
  publisher = {OpenReview.net},
  title = {Symphony: Symmetry-Equivariant Point-Centered Spherical Harmonics for {3D} Molecule Generation},
  url = {https://openreview.net/forum?id=MIEnYtlGyv},
  year = {2024}
}

@inproceedings{dalal2024planseqlearn,
  abstract = {Large Language Models ({LLM}s) have been shown to be capable of performing high-level planning for long-horizon robotics tasks, yet existing methods require access to a pre-defined skill library (e.g. picking, placing, pulling, pushing, navigating). However, {LLM} planning does not address how to design or learn those behaviors, which remains challenging particularly in long-horizon settings. Furthermore, for many tasks of interest, the robot needs to be able to adjust its behavior in a fine-grained manner, requiring the agent to be capable of modifying low-level control actions. Can we instead use the internet-scale knowledge from {LLM}s for high-level policies, guiding reinforcement learning ({RL}) policies to efficiently solve robotic control tasks online without requiring a pre-determined set of skills? In this paper, we propose Plan-Seq-Learn ({PSL}): a modular approach that uses motion planning to bridge the gap between abstract language and learned low-level control for solving long-horizon robotics tasks from scratch.},
  arxiv = {2405.01534},
  author = {Murtaza Dalal and Tarun Chiruvolu and Devendra Singh Chaplot and Ruslan Salakhutdinov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dalal2024planseqlearn.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hQVCCxQrYN},
  publisher = {OpenReview.net},
  title = {Plan-Seq-Learn: Language Model Guided {RL} for Solving Long Horizon Robotics Tasks},
  url = {https://openreview.net/forum?id=hQVCCxQrYN},
  year = {2024}
}

@inproceedings{dang2024instructdet,
  abstract = {We propose {InstructDET}, a data-centric method for referring object detection ({ROD}) that localizes target objects based on user instructions. While deriving from referring expressions ({REC}), the instructions we leverage are greatly diversified to encompass common user intentions related to object detection. For one image, we produce tremendous instructions that refer to every single object and different combinations of multiple objects. Each instruction and its corresponding object bounding boxes (bbxs) constitute one training data pair. In order to encompass common detection expressions, we involve emerging vision-language model ({VLM}) and large language model ({LLM}) to generate instructions guided by text prompts and object bbxs, as the generalizations of foundation models are effective to produce human-like expressions (e.g., describing object property, category, and relationship).},
  arxiv = {2310.05136},
  author = {Ronghao Dang and Jiangyan Feng and Haodong Zhang and Chongjian Ge and Lin Song and Lijun Gong and Chengju Liu and Qijun Chen and Feng Zhu and Rui Zhao and Yibing Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dang2024instructdet.pdf:pdf},
  note = {DBLP last modified: 2025-02-14},
  pdf = {https://openreview.net/pdf?id=hss35aoQ1Y},
  publisher = {OpenReview.net},
  title = {{InstructDET}: Diversifying Referring Object Detection with Generalized Instructions},
  url = {https://openreview.net/forum?id=hss35aoQ1Y},
  year = {2024}
}

@inproceedings{dao2024flashattention2,
  abstract = {Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4√ó compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2√ó speedup compared to FlashAttention, reaching 50-73% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72% model FLOPs utilization).},
  author = {Tri Dao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dao2024flashattention2.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mZn2Xyh9Ec},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {{FlashAttention-2}: Faster Attention with Better Parallelism and Work Partitioning},
  url = {https://openreview.net/forum?id=mZn2Xyh9Ec},
  year = {2024}
}

@inproceedings{timothe2024vision,
  abstract = {We identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
  author = {Timoth{\'e}e Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/timothe2024vision.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; Outstanding Paper Award at ICLR 2024},
  pdf = {https://openreview.net/pdf?id=2dnO3LLiJ1},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Vision Transformers Need Registers},
  url = {https://openreview.net/forum?id=2dnO3LLiJ1},
  year = {2024}
}

@inproceedings{darlow2024dam,
  abstract = {We propose the DAM -- a neural model that takes randomly sampled histories and outputs an adjustable basis composition as a continuous function of time for forecasting to non-fixed horizons. The DAM involves three key components: (1) a flexible approach for using randomly sampled histories from a long-tail distribution, that enables an efficient global perspective of the underlying temporal dynamics while retaining focus on the recent history; (2) a transformer backbone that is trained on these actively sampled histories to produce, as representational output; (3) the basis coefficients of a continuous function of time. We address the challenge of scaling time series forecasting models to forecast accurately for multiple distinct domains and datasets with potentially different underlying collection procedures, patterns, and prediction requirements -- we call this general task universal forecasting. A single univariate DAM, trained on 25 time series datasets, either outperformed or closely matched existing state-of-the-art models at multivariate long-term forecasting across 18 datasets, including 8 held-out for zero-shot transfer, even though these models were trained to specialise for each dataset-horizon combination.},
  author = {Luke Nicholas Darlow and Qiwen Deng and Ahmed Hassan and Martin Asenov and Rajkarn Singh and Artjom Joosen and Adam Barker and Amos J. Storkey},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/darlow2024dam.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4NhMhElWqP},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {{DAM}: Towards a Foundation Model for Forecasting},
  url = {https://openreview.net/forum?id=4NhMhElWqP},
  year = {2024}
}

@inproceedings{datta2024can,
  abstract = {Binary neural networks (BNN) have emerged as an attractive computing paradigm for a wide range of low-power vision tasks. However, state-of-the-art BNNs do not yield any sparsity, and induce a significant number of non-binary operations. On the other hand, activation sparsity can be provided by spiking neural networks (SNN), that too have gained significant traction in recent times. Thanks to this sparsity, SNNs when implemented on neuromorphic hardware, have the potential to be significantly more power-efficient compared to traditional artificial neural networks. In this paper, we present a novel training framework for sparse binary activation neural networks (BANN) that achieves state-of-the-art in the trade-off between performance and compute efficiency. Our approach uses a novel variant of the Hoyer regularizer to train sparse binary activation neural networks. We estimate the threshold of each BANN layer as the Hoyer extremum of a clipped version of its activation map, where the clipping value is trained using gradient descent with our Hoyer regularizer. This approach shifts the activation values away from the threshold, thereby mitigating the effect of noise that can otherwise degrade the BANN accuracy. Our BANN outperforms existing BNNs, SNNs, and adder neural networks that also avoid energy-expensive multiplication operations.},
  author = {Gourav Datta and Zeyu Liu and Peter Anthony Beerel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/datta2024can.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lGUyAuuTYZ},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Can we get the best of both Binary Neural Networks and Spiking Neural Networks for Efficient Computer Vision?},
  url = {https://openreview.net/forum?id=lGUyAuuTYZ},
  year = {2024}
}

@inproceedings{davidson2024evaluating,
  abstract = {We introduce an approach to evaluate language model (LM) agency using negotiation games. This approach better reflects real-world use cases and addresses some of the shortcomings of alternative LM benchmarks. Negotiation games enable us to study multi-turn, and cross-model interactions, modulate complexity, and side-step accidental evaluation data leakage. We used our approach to test six widely used and publicly accessible LMs, evaluating performance and alignment in both self-play and cross-play settings. Noteworthy findings include: (i) only closed-source models tested here were able to complete these tasks; (ii) cooperative bargaining games proved to be most challenging to the models; (iii) even the most powerful models sometimes lose to weaker opponents. Our approach creates a dynamic, co-evolving benchmark that automatically adapts in difficulty with advances in language modeling power.},
  author = {Tim R. Davidson and Veniamin Veselovsky and Michal Kosinski and Robert West},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/davidson2024evaluating.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3ZqKxMHcAg},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Evaluating Language Model Agency Through Negotiations},
  url = {https://openreview.net/forum?id=3ZqKxMHcAg},
  year = {2024}
}

@inproceedings{deb2024contextual,
  abstract = {We investigate Neural Contextual Bandits (NeuCBs) and show neural bandit algorithms with provable regret guarantees and significantly better empirical performance. Building on recent works that showed a reduction from contextual bandits to online regression under a realizability assumption, we show an O(log T) regret for online regression with almost convex losses that satisfy the QG (Quadratic Growth) condition, a generalization of the PL (Polyak-≈Åojasiewicz) condition, and that have a unique minima. Using existing results for wide networks, we show that one can readily achieve an O(‚àöT) regret for online regression with square loss, which implies an O(‚àöK T^{3/4}) regret for Neural Contextual Bandits. Although not directly applicable to wide networks since they do not have unique minima, we show that adding a suitable small random perturbation to the network predictions surprisingly makes the loss satisfy QG with unique minima. Based on perturbed prediction, we show an O(log T) regret for online regression with both squared loss and KL loss, and subsequently convert these respectively to √ï(‚àöKT) and √ï(‚àöKL* + K) regret for NeuCB, where L* is the loss of the best policy.},
  author = {Rohan Deb and Yikun Ban and Shiliang Zuo and Jingrui He and Arindam Banerjee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/deb2024contextual.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5ep85sakT3},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Contextual Bandits with Online Neural Regression},
  url = {https://openreview.net/forum?id=5ep85sakT3},
  year = {2024}
}

@inproceedings{dehdashtian2024fairerclip,
  abstract = {Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Speed: FairerCLIP lends itself to an iterative optimization involving closed-form solvers, which leads to 4√ó-10√ó faster training than the existing methods. 3) Performance: Empirically, FairerCLIP achieves appreciable accuracy gains on benchmark fairness and spurious correlation datasets over their respective baselines.},
  author = {Sepehr Dehdashtian and Lan Wang and Vishnu Naresh Boddeti},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dehdashtian2024fairerclip.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HXoq9EqR9e},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {{FairerCLIP}: Debiasing {CLIP}'s Zero-Shot Predictions using Functions in {RKHSs}},
  url = {https://openreview.net/forum?id=HXoq9EqR9e},
  year = {2024}
}

@inproceedings{dekoninck2024controlled,
  abstract = {As Large Language Models (LLMs) are deployed more widely, customization with respect to vocabulary, style, and character becomes more important. In this work we introduce model arithmetic, a novel inference framework for composing and biasing LLMs without the need for model (re)training or highly specific datasets. In addition to avoiding these costs, model arithmetic allows for more precise control of generated text than direct prompting and prior controlled text generation (CTG) techniques. Using model arithmetic, we can express prior CTG techniques as simple formulas and naturally extend them to new and more effective formulations. We also show that speculative sampling, a technique for efficient LLM sampling, extends to our setting, enabling highly efficient text generation with multiple composed models with only marginal overhead over a single model. Our empirical evaluation demonstrates that model arithmetic allows fine-grained control of generated text while outperforming state-of-the-art on the task of toxicity reduction.},
  author = {Jasper Dekoninck and Marc Fischer and Luca Beurer-Kellner and Martin T. Vechev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dekoninck2024controlled.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=SLw9fp4yI6},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Controlled Text Generation via Language Model Arithmetic},
  url = {https://openreview.net/forum?id=SLw9fp4yI6},
  year = {2024}
}

@inproceedings{delattre2024lipschitzvariancemargin,
  abstract = {Deep neural networks are hindered by unsteady predictions when faced with noisy inputs and adversarial attacks, where the certified radius is a crucial indicator of model robustness. Randomized smoothing provides a promising framework by relying on noise injection into inputs to obtain a smoothed and robust classifier. In this work, we show that the variance introduced by the Monte-Carlo sampling in the randomized smoothing procedure estimate closely interacts with two other important properties of the classifier: its Lipschitz constant and margin. We emphasize the dual impact of the Lipschitz constant of the base classifier on both the smoothed classifier and the empirical variance. To increase the certified robust radius, we introduce a different way to convert logits to probability vectors for the base classifier to leverage the variance-margin trade-off. We leverage the use of Bernstein's concentration inequality along with enhanced Lipschitz bounds for randomized smoothing. Experimental results show a significant improvement in certified accuracy compared to current state-of-the-art methods. Our novel certification procedure allows us to use pre-trained models with randomized smoothing, effectively improving the current certification radius in a zero-shot manner.},
  author = {Blaise Delattre and Alexandre Araujo and Quentin Barth{\'e}lemy and Alexandre Allauzen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/delattre2024lipschitzvariancemargin.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=C36v8541Ns},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing},
  url = {https://openreview.net/forum?id=C36v8541Ns},
  year = {2024}
}

@inproceedings{gre2024language,
  abstract = {It has long been established that predictive models can be transformed into lossless compressors and vice versa. Recent machine learning has focused on training increasingly large self-supervised language models with impressive predictive capabilities, making them well-positioned to be strong compressors. In this work, we advocate for viewing the prediction problem through the lens of compression and evaluate the compression capabilities of large (foundation) models. We show that large language models are powerful general-purpose predictors and that the compression viewpoint provides novel insights into scaling laws, tokenization, and in-context learning. We compare arithmetic coding-based language model compressors to competitive general-purpose lossless compressors like gzip and LZMA2, as well as specialized lossless compressors for image and audio data. Chinchilla 70B, while trained primarily on text, compresses ImageNet patches to 43.4% and LibriSpeech samples to 16.4% of their raw size, beating domain-specific compressors like PNG (58.5%) or FLAC (30.3%), respectively. This demonstrates the emergent compression capabilities of foundation models and their potential as general-purpose compressors.},
  author = {Gr{\'e}goire Del{\'e}tang and Anian Ruoss and Paul-Ambroise Duquenne and Elliot Catt and Tim Genewein and Christopher Mattern and Jordi Grau-Moya and Li Kevin Wenliang and Matthew Aitchison and Laurent Orseau and Marcus Hutter and Joel Veness},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gre2024language.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jznbgiynus},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Language Modeling Is Compression},
  url = {https://openreview.net/forum?id=jznbgiynus},
  year = {2024}
}

@inproceedings{delfosse2024adaptive,
  abstract = {Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competitive to DDQN and Rainbow.},
  author = {Quentin Delfosse and Patrick Schramowski and Martin Mundt and Alejandro Molina and Kristian Kersting},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/delfosse2024adaptive.pdf:pdf},
  note = {ICLR 2024 spotlight presentation. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=g90ysX1sVs},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Adaptive Rational Activations to Boost Deep Reinforcement Learning},
  url = {https://openreview.net/forum?id=g90ysX1sVs},
  year = {2024}
}

@inproceedings{deng2024fast,
  abstract = {Updating truncated Singular Value Decomposition ({SVD}) has extensive applications in representation learning. The continuous evolution of massive-scaled data matrices in practical scenarios highlights the importance of aligning {SVD}-based models with fast-paced updates. Recent methods for updating truncated {SVD} can be recognized as Rayleigh-Ritz projection procedures where their projection matrices are augmented based on the original singular vectors. However, the updating process in these methods densifies the update matrix and applies the projection to all singular vectors, resulting in inefficiency. This paper presents a novel method for dynamically approximating the truncated {SVD} of a sparse and temporally evolving matrix.},
  archiveprefix = {arXiv},
  author = {Haoran Deng and Yang Yang and Jiahe Li and Cheng Chen and Weihao Jiang and Shiliang Pu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2401.09703},
  file = {:/home/b/documents/inproceedings/deng2024fast.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CX2RgsS29V},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Fast Updating Truncated {SVD} for Representation Learning with Sparse Matrices},
  url = {https://openreview.net/forum?id=CX2RgsS29V},
  year = {2024}
}

@inproceedings{deng2024perceptual,
  abstract = {Human visual recognition system shows astonishing capability of compressing visual information into a set of tokens containing rich representations without label supervision. One critical driving principle behind it is perceptual grouping. Despite being widely used in computer vision in the early 2010s, it remains a mystery whether perceptual grouping can be leveraged to derive a neural visual recognition backbone that generates as powerful representations. In this paper, we propose the Perceptual Group Tokenizer, a model that entirely relies on grouping operations to extract visual features and perform self-supervised representation learning, where a series of grouping operations are used to iteratively hypothesize the context for pixels or superpixels to refine feature representations.},
  archiveprefix = {arXiv},
  author = {Zhiwei Deng and Ting Chen and Yang Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2311.18296},
  file = {:/home/b/documents/inproceedings/deng2024perceptual.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NnYaYVODyV},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Perceptual Group Tokenizer: Building Perception with Iterative Grouping},
  url = {https://openreview.net/forum?id=NnYaYVODyV},
  year = {2024}
}

@inproceedings{deng2024polynormer,
  abstract = {Graph transformers ({GT}s) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks ({GNN}s). However, typical {GT} models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear {GT}s recently proposed, they still lag behind {GNN} counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of {GT}s, we propose Polynormer, a polynomial-expressive {GT} model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme to learn high-degree equivariant polynomials whose coefficients are controlled by attention scores.},
  archiveprefix = {arXiv},
  author = {Chenhui Deng and Zichao Yue and Zhiru Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2403.01232},
  file = {:/home/b/documents/inproceedings/deng2024polynormer.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hmv1LpNfXa},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Polynormer: Polynomial-Expressive Graph Transformer in Linear Time},
  url = {https://openreview.net/forum?id=hmv1LpNfXa},
  year = {2024}
}

@inproceedings{desai2024defense,
  abstract = {Different approaches to reduce a model's memory footprint have been explored: historically, methods for creating sparse networks through pruning have been popular, and recently randomized parameter-sharing ({RPS}) methods have gained traction for model compression at the start of training. We comprehensively assess the trade-off between memory and accuracy across {RPS}, pruning techniques, and building smaller models. We find that {RPS}, which is both data and model-agnostic, consistently outperforms smaller models and all moderately informed pruning strategies such as {MAG}, {SNIP}, {SYNFLOW}, and {GRASP}. Our empirical evaluation shows that 100{GB} sized embeddings can be reduced to 10{MB} without loss of quality. We provide theoretical analysis showing that parameter sharing has better memory-capacity trade-off in linear models.},
  archiveprefix = {arXiv},
  author = {Aditya Desai and Anshumali Shrivastava},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.11611},
  file = {:/home/b/documents/inproceedings/desai2024defense.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ypAT2ixD4X},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {In defense of parameter sharing for model-compression},
  url = {https://openreview.net/forum?id=ypAT2ixD4X},
  year = {2024}
}

@inproceedings{dettmers2024spqr,
  abstract = {Recent advances in large language model ({LLM}) pretraining have led to high-quality {LLM}s with impressive abilities. By compressing such {LLM}s via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation ({SpQR}), a new compressed format and quantization technique which enables for the first time near-lossless compression of {LLM}s across model scales, while reaching similar compression levels to previous methods. {SpQR} works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1\% in perplexity for highly-accurate {LLaMA} and Falcon {LLM}s.},
  archiveprefix = {arXiv},
  author = {Tim Dettmers and Ruslan Svirschevski and Vage Egiazarian and Denis Kuznedelev and Elias Frantar and Saleh Ashkboos and Alexander Borzunov and Torsten Hoefler and Dan Alistarh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2306.03078},
  file = {:/home/b/documents/inproceedings/dettmers2024spqr.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Q1u25ahSuy},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{SpQR}: A Sparse-Quantized Representation for Near-Lossless {LLM} Weight Compression},
  url = {https://openreview.net/forum?id=Q1u25ahSuy},
  year = {2024}
}

@inproceedings{dexter2024precise,
  abstract = {Stochastic Gradient Descent ({SGD}) stands as a cornerstone optimization algorithm with proven real-world empirical successes but relatively limited theoretical understanding. Recent research has illuminated a key factor contributing to its practical efficacy: the implicit regularization it instigates. Several studies have investigated the linear stability property of {SGD} in the vicinity of a stationary point as a predictive proxy for sharpness and generalization error in overparameterized neural networks. In this paper, we delve deeper into the relationship between linear stability and sharpness. More specifically, we meticulously delineate the necessary and sufficient conditions for linear stability, contingent on hyperparameters of {SGD} and the geometry of the loss function. Towards this end, we introduce a novel coherence measure of the loss Hessian that encapsulates pertinent geometric properties of the loss function that are relevant to the linear stability of {SGD}. Notably, compared to previous works, our analysis relies on significantly milder assumptions and is applicable for a broader class of loss functions than known before, encompassing not only mean-squared error but also cross-entropy loss.},
  archiveprefix = {arXiv},
  author = {Gregory Dexter and Borja Ocejo and S. Sathiya Keerthi and Aman Gupta and Ayan Acharya and Rajiv Khanna},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2401.12332},
  file = {:/home/b/documents/inproceedings/dexter2024precise.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UMOlFJzLfL},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {A Precise Characterization of {SGD} Stability Using Loss Surface Geometry},
  url = {https://openreview.net/forum?id=UMOlFJzLfL},
  year = {2024}
}

@inproceedings{di2024varianceaware,
  abstract = {Dueling bandits is a prominent framework for decision-making involving preferential feedback, a valuable feature that fits various applications involving human interaction, such as ranking, information retrieval, and recommendation systems. While substantial efforts have been made to minimize the cumulative regret in dueling bandits, a notable gap in the current research is the absence of regret bounds that account for the inherent uncertainty in pairwise comparisons between the dueling arms. Intuitively, greater uncertainty suggests a higher level of difficulty in the problem. To bridge this gap, this paper studies the problem of contextual dueling bandits, where the binary comparison of dueling arms is generated from a generalized linear model ({GLM}). We propose a new {SupLinUCB}-type algorithm that enjoys computational efficiency and a variance-aware regret bound $\tilde{O}\big(d\sqrt{\sum_{t=1}^T\sigma_t^2} + d\big)$, where $\sigma_t$ is the variance of the pairwise comparison at round $t$.},
  archiveprefix = {arXiv},
  author = {Qiwei Di and Tao Jin and Yue Wu and Heyang Zhao and Farzad Farnoud and Quanquan Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.00968},
  file = {:/home/b/documents/inproceedings/di2024varianceaware.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rDH7dIFn20},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Variance-aware Regret Bounds for Stochastic Contextual Dueling Bandits},
  url = {https://openreview.net/forum?id=rDH7dIFn20},
  year = {2024}
}

@inproceedings{di2024pessimistic,
  abstract = {Offline reinforcement learning ({RL}), where the agent aims to learn the optimal policy based on the data collected by a behavior policy, has attracted increasing attention in recent years. While offline {RL} with linear function approximation has been extensively studied with optimal results achieved under certain assumptions, many works shift their interest to offline {RL} with non-linear function approximation. However, limited works on offline {RL} with non-linear function approximation have instance-dependent regret guarantees. In this paper, we propose an oracle-efficient algorithm, dubbed Pessimistic Nonlinear Least-Square Value Iteration ({PNLSVI}), for offline {RL} with non-linear function approximation. Our algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. Our algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation.},
  archiveprefix = {arXiv},
  author = {Qiwei Di and Heyang Zhao and Jiafan He and Quanquan Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.01380},
  file = {:/home/b/documents/inproceedings/di2024pessimistic.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4kLVvIh8cp},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=4kLVvIh8cp},
  year = {2024}
}

@inproceedings{didolkar2024cycle,
  abstract = {Developing deep learning models that effectively learn object-centric representations, akin to human cognition, remains a challenging task. Existing approaches facilitate object discovery by representing objects as fixed-size vectors, called ``slots'' or ``object files''. While these approaches have shown promise in certain scenarios, they still exhibit certain limitations. First, they rely on architectural priors which can be unreliable and usually require meticulous engineering to identify the correct objects. Second, there has been a notable gap in investigating the practical utility of these representations in downstream tasks. To address the first limitation, we introduce a method that explicitly optimizes the constraint that each object in a scene should be associated with a distinct slot. We formalize this constraint by introducing consistency objectives which are cyclic in nature. By integrating these consistency objectives into various existing slot-based object-centric methods, we showcase substantial improvements in object-discovery performance. These enhancements consistently hold true across both synthetic and real-world scenes, underscoring the effectiveness and adaptability of the proposed approach. To tackle the second limitation, we apply the learned object-centric representations from the proposed method to two downstream reinforcement learning tasks, demonstrating considerable performance enhancements compared to conventional slot-based and monolithic representation learning methods.},
  archiveprefix = {arXiv},
  author = {Aniket Rajiv Didolkar and Anirudh Goyal and Yoshua Bengio},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2306.02204},
  file = {:/home/b/documents/inproceedings/didolkar2024cycle.pdf:pdf},
  note = {ICLR 2024 poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=f1xnBr4WD6},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Cycle Consistency Driven Object Discovery},
  url = {https://openreview.net/forum?id=f1xnBr4WD6},
  year = {2024}
}

@inproceedings{dimitrakopoulos2024implicit,
  author = {Panagiotis Dimitrakopoulos and Giorgos Sfikas and Christophoros Nikou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dimitrakopoulos2024implicit.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5KUiMKRebi},
  publisher = {OpenReview.net},
  title = {Implicit Neural Representation Inference for Low-Dimensional {B}ayesian Deep Learning},
  url = {https://openreview.net/forum?id=5KUiMKRebi},
  year = {2024}
}

@inproceedings{ding2024consistency,
  abstract = {Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning ({RL}). However, the inference process of diffusion model can be slow, which hinders its usage in {RL} with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical {RL} settings: offline, offline-to-online and online. For offline {RL}, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online {RL}, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online {RL}, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.},
  author = {Zihan Ding and Chi Jin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ding2024consistency.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=v8jdwkUNXb},
  publisher = {OpenReview.net},
  title = {Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning},
  url = {https://openreview.net/forum?id=v8jdwkUNXb},
  year = {2024}
}

@inproceedings{ding2024saflex,
  abstract = {Data augmentation is crucial in deep learning, but most existing methods use fixed transformation pipelines that can introduce noise or label errors. We introduce {SAFLEX} (Self-Adaptive Augmentation via Feature Label EXtrapolation), which learns sample weights and soft labels for augmented samples using efficient bilevel optimization. {SAFLEX} reduces noise and label errors from upstream augmentation with minimal computational cost and seamlessly integrates with common strategies like {RandAug}, {CutMix}, and generative models. Our approach excels across diverse datasets including natural images, medical images, and tabular data, showing particular strength in few-shot learning and out-of-distribution generalization.},
  author = {Mucong Ding and Bang An and Yuancheng Xu and Anirudh Satheesh and Furong Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ding2024saflex.pdf:pdf},
  note = {DBLP last modified: 2025-01-21},
  pdf = {https://openreview.net/pdf?id=qL6brrBDk2},
  publisher = {OpenReview.net},
  title = {{SAFLEX}: Self-Adaptive Augmentation via Feature Label Extrapolation},
  url = {https://openreview.net/forum?id=qL6brrBDk2},
  year = {2024}
}

@inproceedings{ding2024hybrid,
  abstract = {Large language models ({LLMs}) excel in most {NLP} tasks but require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices tend to lag behind in terms of response quality. We propose a hybrid inference approach that combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements.},
  author = {Dujian Ding and Ankur Mallick and Chi Wang and Robert Sim and Subhabrata Mukherjee and Victor R{\"u}hle and Laks V. S. Lakshmanan and Ahmed Hassan Awadallah},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ding2024hybrid.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=02f3mUtqnM},
  publisher = {OpenReview.net},
  title = {Hybrid {LLM}: Cost-Efficient and Quality-Aware Query Routing},
  url = {https://openreview.net/forum?id=02f3mUtqnM},
  year = {2024}
}

@inproceedings{ding2024patched,
  abstract = {We propose an effective denoising diffusion model for generating high-resolution images (e.g., 1024√ó512), trained on small-size image patches (e.g., 64√ó64). The algorithm is named Patch-{DM}, in which a new feature collage strategy is designed to avoid boundary artifacts when synthesizing large-size images. Feature collage systematically crops and combines partial features of neighboring patches to predict the features of a shifted image patch, allowing seamless generation of the entire image due to overlap in the patch feature space. Patch-{DM} produces high-quality image synthesis results on newly collected dataset of nature images (1024√ó512), as well as standard benchmarks of {LHQ} (1024√ó1024), {FFHQ} (1024√ó1024), {LSUN}-Bedroom, {LSUN}-Church (256√ó256), achieving state-of-the-art {FID} scores compared to previous patch-based generation methods.},
  author = {Zheng Ding and Mengqi Zhang and Jiajun Wu and Zhuowen Tu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ding2024patched.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TgSRPRz8cI},
  publisher = {OpenReview.net},
  title = {Patched Denoising Diffusion Models For High-Resolution Image Synthesis},
  url = {https://openreview.net/forum?id=TgSRPRz8cI},
  year = {2024}
}

@inproceedings{dong2024rayleigh,
  abstract = {Graph-level anomaly detection has applications in various domains such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the spectral properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. We re-investigate the spectral differences between anomalous and normal graphs and observe a significant disparity in the accumulated spectral energy between these two classes. We prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs.},
  author = {Xiangyu Dong and Xingyi Zhang and Sibo Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024rayleigh.pdf:pdf},
  note = {DBLP last modified: 2025-02-07},
  pdf = {https://openreview.net/pdf?id=4UIBysXjVq},
  publisher = {OpenReview.net},
  title = {Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection},
  url = {https://openreview.net/forum?id=4UIBysXjVq},
  year = {2024}
}

@inproceedings{dong2024rethinking,
  abstract = {Information-theoretic generalization analysis has achieved success in characterizing the generalization capabilities of noisy and iterative learning algorithms. However, current advancements are mostly restricted to average-case scenarios and necessitate the stringent bounded loss assumption, leaving a gap with regard to computationally tractable {PAC} generalization analysis, especially for long-tailed loss distributions. We bridge this gap by introducing a novel class of {PAC} bounds through leveraging loss entropies. These bounds simplify the computation of key information metrics in previous {PAC} information-theoretic bounds to one-dimensional variables, thereby enhancing computational tractability.},
  author = {Yuxin Dong and Tieliang Gong and Hong Chen and Shujian Yu and Chen Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GWSIo2MzuH},
  publisher = {OpenReview.net},
  title = {Rethinking Information-theoretic Generalization: Loss Entropy Induced {PAC} Bounds},
  url = {https://openreview.net/forum?id=GWSIo2MzuH},
  year = {2024}
}

@inproceedings{dong2024polyvoice,
  abstract = {We present {PolyVoice}, a language model-based framework for speech-to-speech translation ({S2ST}) that comprises three decoder-only language models: a translation language model, a duration language model, and a speech synthesis language model. The framework uses discretized speech units generated in a fully unsupervised way, enabling support for unwritten languages. For speech synthesis, we adopt the {VALL-E X} approach and build a unit-based audio language model, granting the framework the ability to preserve voice characteristics and speaking style of the original speech. Experimental results on {Chinese}‚Üí{English} and {English}‚Üí{Spanish} language pairs demonstrate that our method outperforms state-of-the-art encoder-decoder models, producing voice-cloned speech with high translation and audio quality.},
  author = {Qianqian Dong and Zhiying Huang and Qi Tian and Chen Xu and Tom Ko and Yunlong Zhao and Siyuan Feng and Tang Li and Kexin Wang and Xuxin Cheng and Fengpeng Yue and Ye Bai and Xi Chen and Lu Lu and Zejun Ma and Yuping Wang and Mingxuan Wang and Yuxuan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024polyvoice.pdf:pdf},
  note = {DBLP last modified: 2025-01-20},
  pdf = {https://openreview.net/pdf?id=hCrFG9cyuC},
  publisher = {OpenReview.net},
  title = {{PolyVoice}: Language Models for Speech to Speech Translation},
  url = {https://openreview.net/forum?id=hCrFG9cyuC},
  year = {2024}
}

@inproceedings{dong2024versatile,
  abstract = {Most existing causal discovery methods rely on the assumption of no latent confounders, limiting their applicability in solving real-life problems. We introduce a novel, versatile framework for causal discovery that accommodates the presence of causally-related hidden variables almost everywhere in the causal network (for instance, they can be effects of observed variables), based on rank information of covariance matrix over observed variables. The proposed framework is able to identify causal relations between latent variables using rank constraints, providing a significant advancement over existing methods that assume no latent confounders.},
  author = {Xinshuai Dong and Biwei Huang and Ignavier Ng and Xiangchen Song and Yujia Zheng and Songyao Jin and Roberto Legaspi and Peter Spirtes and Kun Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024versatile.pdf:pdf},
  note = {DBLP last modified: 2025-02-07},
  pdf = {https://openreview.net/pdf?id=FhQSGhBlqv},
  publisher = {OpenReview.net},
  title = {A Versatile Causal Discovery Framework to Allow Causally-Related Hidden Variables},
  url = {https://openreview.net/forum?id=FhQSGhBlqv},
  year = {2024}
}

@inproceedings{dong2024dreamllm,
  abstract = {{DreamLLM} is a learning framework that first achieves versatile Multimodal Large Language Models ({MLLMs}) empowered with frequently overlooked synergy between multimodal comprehension and creation. {DreamLLM} operates on two fundamental principles: (1) the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space, circumventing limitations of external feature extractors like {CLIP}; and (2) fostering generation of raw, interleaved documents with both text and image contents. As a result, {DreamLLM} is the first {MLLM} capable of generating free-form interleaved content, achieving superior performance as a zero-shot multimodal generalist.},
  author = {Runpei Dong and Chunrui Han and Yuang Peng and Zekun Qi and Zheng Ge and Jinrong Yang and Liang Zhao and Jianjian Sun and Hongyu Zhou and Haoran Wei and Xiangwen Kong and Xiangyu Zhang and Kaisheng Ma and Li Yi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024dreamllm.pdf:pdf},
  note = {DBLP last modified: 2024-10-28},
  pdf = {https://openreview.net/pdf?id=y01KGvd9Bw},
  publisher = {OpenReview.net},
  title = {{DreamLLM}: Synergistic Multimodal Comprehension and Creation},
  url = {https://openreview.net/forum?id=y01KGvd9Bw},
  year = {2024}
}

@inproceedings{dong2024fastelectra,
  abstract = {{ELECTRA} pre-trains language models by detecting tokens in a sequence that have been replaced by an auxiliary model. Although {ELECTRA} offers a significant boost in efficiency, its potential is constrained by the training cost brought by the auxiliary model. Notably, this model, which is jointly trained with the main model, only serves to assist the training of the main model and is discarded post-training. This results in a substantial amount of training cost being expended in vain. To mitigate this issue, we propose {Fast-ELECTRA}, which leverages an existing language model as the auxiliary model. To construct a learning curriculum for the main model, we smooth its output distribution via temperature scaling following a descending schedule. Our approach rivals the performance of state-of-the-art {ELECTRA}-style pre-training methods, while significantly eliminating the computation and memory cost brought by the joint training of the auxiliary model. Our method also reduces the sensitivity to hyper-parameters and enhances the pre-training stability.},
  author = {Chengyu Dong and Liyuan Liu and Hao Cheng and Jingbo Shang and Jianfeng Gao and Xiaodong Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024fastelectra.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8OBuqbLb8h},
  publisher = {OpenReview.net},
  title = {{Fast-ELECTRA} for Efficient Pre-training},
  url = {https://openreview.net/forum?id=8OBuqbLb8h},
  year = {2024}
}

@inproceedings{dong2024toward,
  abstract = {How to conduct teacher training for knowledge distillation is still an open problem. It has been widely observed that a best-performing teacher does not necessarily yield the best-performing student, suggesting a fundamental discrepancy between the current teacher training practice and the ideal teacher training strategy. To fill this gap, we explore the feasibility of training a teacher that is oriented toward student performance with empirical risk minimization ({ERM}). Our analyses are inspired by the recent findings that the effectiveness of knowledge distillation hinges on the teacher's capability to approximate the true label distribution of training inputs. We theoretically establish that {ERM} minimizer can approximate the true label distribution of training data as long as the feature extractor of the learner network is {Lipschitz} continuous and is robust to feature transformations. In light of our theory, we propose a teacher training method {SoTeacher} which incorporates {Lipschitz} regularization and consistency regularization into {ERM}. Experiments on benchmark datasets using various knowledge distillation algorithms and teacher-student pairs confirm that {SoTeacher} can improve student accuracy consistently.},
  author = {Chengyu Dong and Liyuan Liu and Jingbo Shang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024toward.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wsWGcw6qKD},
  publisher = {OpenReview.net},
  title = {Toward Student-oriented Teacher Network Training for Knowledge Distillation},
  url = {https://openreview.net/forum?id=wsWGcw6qKD},
  year = {2024}
}

@inproceedings{dong2024aligndiff,
  abstract = {Aligning agent behaviors with diverse human preferences remains a challenging problem in reinforcement learning ({RL}), owing to the inherent abstractness and mutability of human preferences. To address these issues, we propose {AlignDiff}, a novel framework that leverages {RL} from Human Feedback ({RLHF}) to quantify human preferences, covering abstractness, and utilizes them to guide diffusion planning for zero-shot behavior customizing, covering mutability. {AlignDiff} can accurately match user-customized behaviors and efficiently switch from one to another. To build the framework, we first establish the multi-perspective human feedback datasets, which contain comparisons for the attributes of diverse behaviors, and then train an attribute strength model to predict quantified relative strengths. After relabeling behavioral datasets with relative strengths, we proceed to train an attribute-conditioned diffusion model, which serves as a planner with the attribute strength model as a director for preference aligning at the inference phase. We evaluate {AlignDiff} on various locomotion tasks and demonstrate its superior performance on preference matching, switching, and covering compared to other baselines. Its capability of completing unseen downstream tasks under human instructions also showcases the promising potential for human-{AI} collaboration.},
  author = {Zibin Dong and Yifu Yuan and Jianye Hao and Fei Ni and Yao Mu and Yan Zheng and Yujing Hu and Tangjie Lv and Changjie Fan and Zhipeng Hu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024aligndiff.pdf:pdf},
  note = {{DBLP} last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=bxfKIYfHyx},
  publisher = {OpenReview.net},
  title = {{AlignDiff}: Aligning Diverse Human Preferences via Behavior-Customisable Diffusion Model},
  url = {https://openreview.net/forum?id=bxfKIYfHyx},
  year = {2024}
}

@inproceedings{dong2024rethinking,
  abstract = {The expressivity of Graph Neural Networks ({GNNs}) has been studied broadly in recent years to reveal the design principles for more powerful {GNNs}. Graph canonization is known as a typical approach to distinguish non-isomorphic graphs, yet rarely adopted when developing expressive {GNNs}. This paper proposes to maximize the expressivity of {GNNs} by graph canonization, then the power of such {GNNs} is studies from the perspective of model stability. A stable {GNN} will map similar graphs to close graph representations in the vectorial space, and the stability of {GNNs} is critical to generalize their performance to unseen graphs. We theoretically reveal the trade-off of expressivity and stability in graph-canonization-enhanced {GNNs}. Then we introduce a notion of universal graph canonization as the general solution to address the trade-off and characterize a widely applicable sufficient condition to solve the universal graph canonization. In many popular graph benchmark datasets, graph canonization successfully enhances {GNNs} and provides highly competitive performance, indicating the capability and great potential of proposed method in general graph representation learning. In graph datasets where the sufficient condition holds, {GNNs} enhanced by universal graph canonization consistently outperform {GNN} baselines and successfully improve the {SOTA} performance up to 31\%, providing the optimal solution to numerous challenging real-world graph analytical tasks like gene network representation learning in bioinformatics.},
  author = {Zehao Dong and Muhan Zhang and Philip R. O. Payne and Michael A. Province and Carlos Cruchaga and Tianyu Zhao and Fuhai Li and Yixin Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024rethinking.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nTwb2vBLOV},
  publisher = {OpenReview.net},
  title = {Rethinking the Power of Graph Canonization in Graph Representation Learning with Stability},
  url = {https://openreview.net/forum?id=nTwb2vBLOV},
  year = {2024}
}

@inproceedings{dong2024leveraging,
  abstract = {Multi-cellular robot design aims to create robots comprised of numerous cells that can be efficiently controlled to perform diverse tasks. Previous research has demonstrated the ability to generate robots for various tasks, but these approaches often optimize robots directly in the vast design space, resulting in robots with complicated morphologies that are hard to control. In response, this paper presents a novel coarse-to-fine method for designing multi-cellular robots. Initially, this strategy seeks optimal coarse-grained robots and progressively refines them. To mitigate the challenge of determining the precise refinement juncture during the coarse-to-fine transition, we introduce the Hyperbolic Embeddings for Robot Design ({HERD}) framework. {HERD} unifies robots of various granularity within a shared hyperbolic space and leverages a refined Cross-Entropy Method for optimization. This framework enables our method to autonomously identify areas of exploration in hyperbolic space and concentrate on regions demonstrating promise. We evaluate our framework on 15 tasks in the benchmark {EvoGym} and demonstrate that our method significantly outperforms previous state-of-the-art algorithms in terms of both sample efficiency and final performance on most tasks.},
  author = {Heng Dong and Junyu Zhang and Chongjie Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dong2024leveraging.pdf:pdf},
  note = {{DBLP} last modified: 2025-06-26},
  pdf = {https://openreview.net/pdf?id=q9jQPA6zPK},
  publisher = {OpenReview.net},
  title = {Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design},
  url = {https://openreview.net/forum?id=q9jQPA6zPK},
  year = {2024}
}

@inproceedings{doshi2024grok,
  abstract = {We study robust generalization in deep learning, where the number of trainable parameters is very large. We consider an interpretable model where generalizing representations are understood analytically and are easily distinguishable from memorizing ones. We consider multi-layer perceptron ({MLP}) and Transformer architectures trained on modular arithmetic tasks where label corruption is introduced by incorrectly computing some of the operations. We show that it is possible for the network to memorize the corrupted labels and achieve 100\% generalization at the same time. The memorizing neurons can be identified and pruned, lowering the accuracy on corrupted data and improving the accuracy on uncorrupted data. Our work shows that grokking on modular addition dataset is remarkably robust to label corruption -- even without explicit regularization, the model can generalize to near 100\% accuracy with sizable label corruption. In many cases, the network surprisingly manages to correct some of the corrupted examples, resulting in test accuracy exceeding training accuracy. We provide mechanistic interpretations showing that regularization methods such as weight decay, dropout, and {BatchNorm} force the network to ignore the corrupted data during optimization, achieving 100\% accuracy on the uncorrupted dataset.},
  author = {Darshil Doshi and Aritra Das and Tianyu He and Andrey Gromov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/doshi2024grok.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UHjE5v5MB7},
  publisher = {OpenReview.net},
  title = {To Grok or not to Grok: Disentangling Generalization and Memorization on Corrupted Algorithmic Datasets},
  url = {https://openreview.net/forum?id=UHjE5v5MB7},
  year = {2024}
}

@inproceedings{dou2024diffusion,
  abstract = {Diffusion models have achieved tremendous success in generating high-dimensional data like images, videos and audio. These models provide powerful data priors that can solve linear inverse problems in zero shot through Bayesian posterior sampling. However, exact posterior sampling for diffusion models is intractable. Current solutions often hinge on approximations that are either computationally expensive or lack strong theoretical guarantees. In this work, we introduce an efficient diffusion sampling algorithm for linear inverse problems that is guaranteed to be asymptotically accurate. We reveal a link between Bayesian posterior sampling and Bayesian filtering in diffusion models, proving the former as a specific instance of the latter. Our method, termed filtering posterior sampling, leverages sequential Monte Carlo methods to solve the corresponding filtering problem. It seamlessly integrates with all Markovian diffusion samplers, requires no model re-training, and guarantees accurate samples from the Bayesian posterior as particle counts rise. Empirical tests demonstrate that our method generates better or comparable results than leading zero-shot diffusion posterior samplers on tasks like image inpainting, super-resolution, and deblurring.},
  author = {Zehao Dou and Yang Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dou2024diffusion.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=tplXNcHZs1},
  publisher = {OpenReview.net},
  title = {Diffusion Posterior Sampling for Linear Inverse Problem Solving: A Filtering Perspective},
  url = {https://openreview.net/forum?id=tplXNcHZs1},
  year = {2024}
}

@inproceedings{du2024role,
  abstract = {In self-supervised learning ({SSL}), masked image modeling ({MIM}) has gained popularity alongside contrastive learning methods. {MIM} involves reconstructing masked regions of input images using their unmasked portions. A notable subset of {MIM} methodologies employs discrete tokens as the reconstruction target, but the theoretical underpinnings of this choice remain underexplored. This paper explores the role of these discrete tokens, aiming to unravel their benefits and limitations. Building upon the connection between {MIM} and contrastive learning, we provide a comprehensive theoretical understanding on how discrete tokenization affects the model's generalization capabilities. We show that discrete tokens that align well with data classes enhance connectivity among intra-class samples, thereby improving the downstream performance. Conversely, incorrectly specified tokenizers may cause confusion between inter-class samples, resulting in poorer downstream performance. We design a novel metric, named token-class alignment similarity ({TCAS}), by measuring the discrepancy in distribution between true patch labels and the discrete token labels assigned by the tokenizer. {TCAS} can be used to directly compare the quality of different tokenizers without training. Furthermore, we design an easy-to-implement tokenizer with its corresponding {MIM} method named {ClusterMIM}, which demonstrates its efficacy through enhanced performance across different benchmark datasets and different {ViT} backbones.},
  author = {Tianqi Du and Yifei Wang and Yisen Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/du2024role.pdf:pdf},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=WNLAkjUm19},
  publisher = {OpenReview.net},
  title = {On the Role of Discrete Tokenization in Visual Representation Learning},
  url = {https://openreview.net/forum?id=WNLAkjUm19},
  year = {2024}
}

@inproceedings{du2024cascading,
  abstract = {Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading {RL} framework, which considers the impact of user states and state transition into decisions. To tackle this challenge, we delve into the properties of value functions, and design an oracle {BestPerm} to efficiently find the optimal item list. Equipped with {BestPerm}, we develop two algorithms {CascadingVI} and {CascadingBPI}, which are both computation-efficient and sample-efficient, and provide near-optimal regret and sample complexity guarantees. We conduct experiments on both synthetic and real-world datasets to demonstrate the empirical superiority of our algorithms compared with baselines.},
  author = {Yihan Du and R. Srikant and Wei Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/du2024cascading.pdf:pdf},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=KjOAHlKMF5},
  publisher = {OpenReview.net},
  title = {Cascading Reinforcement Learning},
  url = {https://openreview.net/forum?id=KjOAHlKMF5},
  year = {2024}
}

@inproceedings{du2024how,
  abstract = {Using unlabeled data to regularize the machine learning model has demonstrated the promise for improving safety and reliability. Particularly, using unlabeled data for out-of-distribution ({OOD}) detection is an important topic for trustworthy machine learning, but there is currently a lack of research on formally understanding how unlabeled data helps {OOD} detection. This paper bridges the gap by introducing a new learning framework {SAL} (Separate And Learn) that offers both strong theoretical guarantees and empirical effectiveness. The key idea is to separate candidate outliers from unlabeled data and then train an {OOD} classifier using the candidate outliers and the labeled in-distribution ({ID}) data. We provide rigorous error bounds from the lens of separability and learnability, formally justifying the two components in the algorithm. Our theory shows that {SAL} can separate the candidate outliers with small error rates, which leads to a generalization guarantee for the learned {OOD} classifier. Empirically, {SAL} achieves strong performance compared to competitive baselines and achieves state-of-the-art performance on common benchmarks, echoing the theoretical insights.},
  author = {Xuefeng Du and Zhen Fang and Ilias Diakonikolas and Yixuan Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/du2024how.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jlEjB8MVGa},
  publisher = {OpenReview.net},
  title = {How Does Unlabeled Data Provably Help Out-of-Distribution Detection?},
  url = {https://openreview.net/forum?id=jlEjB8MVGa},
  year = {2024}
}

@inproceedings{du2024image2sentence,
  abstract = {The task of composed image retrieval (CIR) aims to retrieve images based on the query image and the text describing the users' intent. Existing methods have made great progress with the advanced large vision-language (VL) model in CIR task, however, they generally suffer from two main issues: lack of labeled triplets for model training and difficulty of deployment on resource-restricted environments when deploying the large vision-language model.},
  address = {Vienna, Austria},
  author = {Yongchao Du and Min Wang and Wengang Zhou and Shuping Hui and Houqiang Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/du2024image2sentence.pdf:pdf},
  keywords = {zero-shot, composed image retrieval, asymmetrical},
  month = {5},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=5BXAXOpaWu},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Image2Sentence based Asymmetrical Zero-shot Composed Image Retrieval},
  url = {https://openreview.net/forum?id=5BXAXOpaWu},
  year = {2024}
}

@inproceedings{du2024neural,
  abstract = {We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. Our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients. In contrast to current machine learning approaches which enforce PDE constraints by minimizing the numerical quadrature of the residuals in the spatiotemporal domain, we leverage Parseval's identity and introduce a new training strategy through a spectral loss. Our spectral loss enables more efficient differentiation through the neural network, and substantially reduces training complexity. At inference time, the computational cost of our method remains constant, regardless of the spatiotemporal resolution of the domain. Our experimental results demonstrate that our method significantly outperforms previous machine learning approaches in terms of speed and accuracy by one to two orders of magnitude on multiple different problems. When compared to numerical solvers of the same accuracy, our method demonstrates a 10√ó increase in performance speed.},
  address = {Vienna, Austria},
  author = {Yiheng Du and Nithin Chalapathi and Aditi S. Krishnapriyan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/du2024neural.pdf:pdf},
  keywords = {spectral methods, PDEs, neural operators, self-supervised learning},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=2DbVeuoa6a},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Neural Spectral Methods: Self-supervised learning in the spectral domain},
  url = {https://openreview.net/forum?id=2DbVeuoa6a},
  year = {2024}
}

@inproceedings{du2024remasker,
  abstract = {We propose ReMasker, a new method of imputing missing values in tabular data by extending the masked autoencoding framework. Compared with prior work, ReMasker is both simple -- besides the missing values (i.e., naturally masked), we randomly re-mask another set of values, optimize the autoencoder by reconstructing this re-masked set, and apply the trained model to predict the missing values; and effective -- with extensive evaluation on benchmark datasets, we show that ReMasker performs on par with or outperforms state-of-the-art methods in terms of both imputation fidelity and utility under various missingness settings, while its performance advantage often increases with the ratio of missing data. We further explore theoretical justification for its effectiveness, showing that ReMasker tends to learn missingness-invariant representations of tabular data.},
  address = {Vienna, Austria},
  author = {Tianyu Du and Luca Melis and Ting Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/du2024remasker.pdf:pdf},
  keywords = {tabular data, missing data imputation, masked autoencoding, data preprocessing},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=KI9NqjLVDT},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {ReMasker: Imputing Tabular Data with Masked Autoencoding},
  url = {https://openreview.net/forum?id=KI9NqjLVDT},
  year = {2024}
}

@inproceedings{dusell2024stack,
  abstract = {Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.},
  address = {Vienna, Austria},
  author = {Brian DuSell and David Chiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dusell2024stack.pdf:pdf},
  keywords = {transformers, hierarchical patterns, context-free languages, attention mechanisms, syntax},
  month = {5},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=XVhm3X8Fum},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns},
  url = {https://openreview.net/forum?id=XVhm3X8Fum},
  year = {2024}
}

@inproceedings{du2024video,
  abstract = {We present an approach for enabling visual planning for complex long-horizon tasks in the space of generated videos and language, leveraging recent advances in large generative models pretrained on Internet-scale data. We present video language planning (VLP), an algorithm that consists of a tree search procedure, where we train (i) vision-language models to serve as both policies and value functions, and (ii) text-to-video models as dynamics models. VLP takes as input a long-horizon task instruction and current image observation, and outputs a long video plan that provides detailed multimodal (video and language) specifications that describe how to complete the final task. Generated video plans can be translated into real robot actions via goal-conditioned policies, conditioned on each intermediate frame of the generated video.},
  address = {Vienna, Austria},
  author = {Yilun Du and Sherry Yang and Pete Florence and Fei Xia and Ayzaan Wahid and Brian Ichter and Pierre Sermanet and Tianhe Yu and Pieter Abbeel and Joshua B. Tenenbaum and Leslie Pack Kaelbling and Andy Zeng and Jonathan Tompson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/du2024video.pdf:pdf},
  keywords = {video planning, robotics, vision-language models, long-horizon tasks, multimodal planning},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=9pKtcJcMP3},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Video Language Planning},
  url = {https://openreview.net/forum?id=9pKtcJcMP3},
  year = {2024}
}

@inproceedings{duan2024denevil,
  abstract = {Large Language Models (LLMs) have made unprecedented breakthroughs, yet their increasing integration into everyday life might raise societal risks due to generated unethical content. Despite extensive study on specific issues like bias, the intrinsic values of LLMs remain largely unexplored from a moral philosophy perspective. This work delves into ethical values utilizing Moral Foundation Theory. Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities and elicit the violation of ethics in a generative manner, revealing their underlying value inclinations. On such a basis, we construct MoralPrompt, a high-quality dataset comprising 2,397 prompts covering 500+ value principles, and then benchmark the intrinsic values across a spectrum of LLMs.},
  address = {Vienna, Austria},
  author = {Shitong Duan and Xiaoyuan Yi and Peng Zhang and Tun Lu and Xing Xie and Ning Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/duan2024denevil.pdf:pdf},
  keywords = {ethical AI, large language models, moral foundation theory, AI safety, value alignment},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=m3RRWWFaVe},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Denevil: towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning},
  url = {https://openreview.net/forum?id=m3RRWWFaVe},
  year = {2024}
}

@inproceedings{dumouchelle2024neur2ro,
  abstract = {Robust optimization provides a mathematical framework for modeling and solving decision-making problems under worst-case uncertainty. This work addresses two-stage robust optimization (2RO) problems (also called adjustable robust optimization), wherein first-stage and second-stage decisions are made before and after uncertainty is realized, respectively. This results in a nested min-max-min optimization problem which is extremely challenging computationally, especially when the decisions are discrete. We propose Neur2RO, an efficient machine learning-driven instantiation of column-and-constraint generation (CCG), a classical iterative algorithm for 2RO. Specifically, we learn to estimate the value function of the second-stage problem via a novel neural network architecture that is easy to optimize over by design.},
  address = {Vienna, Austria},
  author = {Justin Dumouchelle and Esther Julien and Jannis Kurtz and Elias Boutros Khalil},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dumouchelle2024neur2ro.pdf:pdf},
  keywords = {robust optimization, two-stage optimization, neural networks, combinatorial optimization, machine learning},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=T5Xb0iGCCv},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Neur2RO: Neural Two-Stage Robust Optimization},
  url = {https://openreview.net/forum?id=T5Xb0iGCCv},
  year = {2024}
}

@inproceedings{dutt2024fairtune,
  abstract = {Training models with robust group fairness properties is crucial in ethically sensitive application areas such as medical diagnosis. Despite the growing body of work aiming to minimise demographic bias in AI, this problem remains challenging. A key reason for this challenge is the fairness generalisation gap: High-capacity deep learning models can fit all training data nearly perfectly, and thus also exhibit perfect fairness during training. In this case, bias emerges only during testing when generalisation performance differs across sub-groups. This motivates us to take a bi-level optimisation perspective on fair learning: Optimising the learning strategy based on validation fairness. Specifically, we consider the highly effective workflow of adapting pre-trained models to downstream medical imaging tasks using parameter-efficient fine-tuning (PEFT) techniques. We propose FairTune, a framework to optimise the choice of PEFT parameters with respect to fairness.},
  address = {Vienna, Austria},
  author = {Raman Dutt and Ondrej Bohdal and Sotirios A. Tsaftaris and Timothy M. Hospedales},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dutt2024fairtune.pdf:pdf},
  keywords = {fairness, medical imaging, parameter-efficient fine-tuning, bias mitigation, ethical AI},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=ArpwmicoYW},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {FairTune: Optimizing Parameter Efficient Fine Tuning for Fairness in Medical Image Analysis},
  url = {https://openreview.net/forum?id=ArpwmicoYW},
  year = {2024}
}

@inproceedings{duvvuri2024combining,
  abstract = {Adaptive regularization based optimization methods such as full-matrix Adagrad which use gradient second-moment information hold significant potential for fast convergence in deep neural network (DNN) training, but are memory intensive and computationally demanding for large neural nets. We develop a technique called Combining AxeS PReconditioners (CASPR), which optimizes matrix-shaped DNN parameters by finding different preconditioners for each mode/axis of the parameter and combining them using a Kronecker-sum based approximation. We show tighter convergence guarantees in stochastic optimization compared to a Kronecker product based preconditioner, Shampoo, which arises as a special case of CASPR. CASPR shows significant improvement in training and generalization performance compared to existing practical adaptive regularization based methods such as Shampoo and Adam in a variety of tasks.},
  address = {Vienna, Austria},
  author = {Sai Surya Duvvuri and Devvrit and Rohan Anil and Cho-Jui Hsieh and Inderjit S. Dhillon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/duvvuri2024combining.pdf:pdf},
  keywords = {optimization, preconditioners, Kronecker approximation, deep learning, adaptive methods},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=8j9hz8DVi8},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Combining Axes Preconditioners through Kronecker Approximation for Deep Learning},
  url = {https://openreview.net/forum?id=8j9hz8DVi8},
  year = {2024}
}

@inproceedings{dyballa2024learning,
  abstract = {The static synaptic connectivity of neuronal circuits stands in direct contrast to the dynamics of their function. As in changing community interactions, different neurons can participate actively in various combinations to effect behaviors at different times. We introduce an unsupervised approach to learn the dynamic affinities between neurons in live, behaving animals, and to reveal which communities form among neurons at different times. The inference occurs in two major steps. First, pairwise non-linear affinities between neuronal traces from brain-wide calcium activity are organized by non-negative tensor factorization (NTF). Each factor specifies which groups of neurons are most likely interacting for an inferred interval in time, and for which animals. Finally, a generative model that allows for weighted community detection is applied to the functional motifs produced by NTF to reveal a dynamic functional connectome.},
  address = {Vienna, Austria},
  author = {Luciano Dyballa and Samuel Lang and Alexandra Haslund-Gourley and Eviatar Yemini and Steven W. Zucker},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dyballa2024learning.pdf:pdf},
  keywords = {neuroscience, functional connectome, tensor factorization, community detection, neural networks},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=ZwhHSOHMTM},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Learning dynamic representations of the functional connectome in neurobiological networks},
  url = {https://openreview.net/forum?id=ZwhHSOHMTM},
  year = {2024}
}

@inproceedings{early2024inherently,
  abstract = {Conventional Time Series Classification ({TSC}) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning ({MIL}) to overcome this issue, and propose a new framework called {MILLET}: Multiple Instance Learning for Locally Explainable Time series classification. We apply {MILLET} to existing deep learning {TSC} models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate {MILLET} on 85 {UCR} {TSC} datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show {MILLET} produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with {MILLET}, which is available on {GitHub}, is the first to develop general {MIL} methods for {TSC} and apply them to an extensive variety of domains.},
  author = {Joseph Early and Gavin K. C. Cheung and Kurt Cutajar and Hanting Xie and Jas Kandola and Niall Twomey},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/early2024inherently.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=xriGRsoAza},
  publisher = {OpenReview.net},
  title = {Inherently Interpretable Time Series Classification via Multiple Instance Learning},
  url = {https://openreview.net/forum?id=xriGRsoAza},
  year = {2024}
}

@inproceedings{eftekhar2024selective,
  abstract = {Embodied {AI} models often employ off the shelf vision backbones like {CLIP} to encode their visual observations. Although such general purpose representations encode rich syntactic and semantic information about the scene, much of this information is often irrelevant to the specific task at hand. This introduces noise within the learning process and distracts the agent's focus from task-relevant visual cues. Inspired by selective attention in humans‚Äîthe process through which people filter their perception based on their experiences, knowledge, and the task at hand‚Äîwe introduce a parameter-efficient approach to filter visual stimuli for embodied {AI}. Our approach induces a task-conditioned bottleneck using a small learnable codebook module. This codebook is trained jointly to optimize task reward and acts as a task-conditioned selective filter over the visual observation. Our experiments showcase state-of-the-art performance for object goal navigation and object displacement across 5 benchmarks, {ProcTHOR}, {ArchitecTHOR}, {RoboTHOR}, {AI2-iTHOR}, and {ManipulaTHOR}. The filtered representations produced by the codebook are also able generalize better and converge faster when adapted to other simulation environments such as Habitat. Our qualitative analyses show that agents explore their environments more effectively and their representations retain task-relevant information like target object recognition while ignoring superfluous information about other objects.},
  author = {Ainaz Eftekhar and Kuo-Hao Zeng and Jiafei Duan and Ali Farhadi and Aniruddha Kembhavi and Ranjay Krishna},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/eftekhar2024selective.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=kC5nZDU5zf},
  publisher = {OpenReview.net},
  title = {Selective Visual Representations Improve Convergence and Generalization for Embodied {AI}},
  url = {https://openreview.net/forum?id=kC5nZDU5zf},
  year = {2024}
}

@inproceedings{eisner2024deep,
  abstract = {Many robot manipulation tasks can be framed as geometric reasoning tasks, where an agent must be able to precisely manipulate an object into a position that satisfies the task from a set of initial conditions. Often, task success is defined based on the relationship between two objects - for instance, hanging a mug on a rack. In such cases, the solution should be equivariant to the initial position of the objects as well as the agent, and invariant to the pose of the camera. This poses a challenge for learning systems which attempt to solve this task by learning directly from high-dimensional demonstrations: the agent must learn to be both equivariant as well as precise, which can be challenging without any inductive biases about the problem. In this work, we propose a method for precise relative pose prediction which is provably {SE(3)}-equivariant, can be learned from only a few demonstrations, and can generalize across variations in a class of objects. We accomplish this by factoring the problem into learning an {SE(3)} invariant task-specific representation of the scene and then interpreting this representation with novel geometric reasoning layers which are provably {SE(3)} equivariant. We demonstrate that our method can yield substantially more precise placement predictions in simulated placement tasks than previous methods trained with the same amount of data, and can accurately represent relative placement relationships data collected from real-world demonstrations.},
  author = {Ben Eisner and Yi Yang and Todor Davchev and Mel Vecerik and Jonathan Scholz and David Held},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/eisner2024deep.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2inBuwTyL2},
  publisher = {OpenReview.net},
  title = {Deep {SE(3)}-Equivariant Geometric Reasoning for Precise Placement Tasks},
  url = {https://openreview.net/forum?id=2inBuwTyL2},
  year = {2024}
}

@inproceedings{elazar2024whats,
  abstract = {Large text corpora are the backbone of language models. However, we have a limited understanding of the content of these corpora, including general statistics, quality, social factors, and inclusion of evaluation data (contamination). In this work, we propose What's In My Big Data? ({WIMBD}), a platform and a set of sixteen analyses that allow us to reveal and compare the contents of large text corpora. {WIMBD} builds on two basic capabilities -- count and search -- at scale, which allows us to analyze more than 35 terabytes on a standard compute node. We apply {WIMBD} to ten different corpora used to train popular language models, including C4, The Pile, and {RedPajama}. Our analysis uncovers several surprising and previously undocumented findings about these corpora, including the high prevalence of duplicate, synthetic, and low-quality content, personally identifiable information, toxic language, and benchmark contamination.},
  author = {Yanai Elazar and Akshita Bhagia and Ian Magnusson and Abhilasha Ravichander and Dustin Schwenk and Alane Suhr and Evan Pete Walsh and Dirk Groeneveld and Luca Soldaini and Sameer Singh and Hannaneh Hajishirzi and Noah A. Smith and Jesse Dodge},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/elazar2024whats.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=RvfPnOkPV4},
  publisher = {OpenReview.net},
  title = {What's In My Big Data?},
  url = {https://openreview.net/forum?id=RvfPnOkPV4},
  year = {2024}
}

@inproceedings{elenter2024nearoptimal,
  abstract = {There is a rising need to constrain the behavior of machine learning systems as their adoption widens. These requirements can be imposed through constrained learning problems that are tackled by dual ascent algorithms. While these algorithms converge in objective value even in non-convex settings, they do not guarantee feasible solutions. Traditionally, this issue is addressed by randomizing over all iterates, which is impractical for modern applications. In this work, we address this gap between theory and practice by characterizing the constraint violation of Lagrangian minimizers associated with optimal dual variables, despite lack of convexity. We characterize both the sub-optimality and infeasibility of primal solutions obtained by dual algorithms. For sufficiently rich parametrizations, we show that solutions obtained by dual algorithms closely approximate functional solutions in both optimal value and constraint satisfaction. We provide numerical evidence for constrained neural network training and neural architecture search problems.},
  author = {Juan Elenter and Luiz F. O. Chamon and Alejandro Ribeiro},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/elenter2024nearoptimal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fDaLmkdSKU},
  publisher = {OpenReview.net},
  title = {Near-Optimal Solutions of Constrained Learning Problems},
  url = {https://openreview.net/forum?id=fDaLmkdSKU},
  year = {2024}
}

@inproceedings{elhag2024manifold,
  abstract = {Diffusion models have shown remarkable success in generating high-quality data across various domains. However, most existing approaches are designed for data in Euclidean space, while many real-world applications involve data on manifolds. In this work, we propose Manifold Diffusion Fields ({MDF}), an approach that unlocks learning of diffusion models of data in general non-Euclidean geometries. Leveraging insights from spectral geometry analysis, we define an intrinsic coordinate system on the manifold via the eigen-functions of the Laplace-Beltrami Operator. {MDF} represents functions using an explicit parametrization formed by a set of multiple input-output pairs. This approach allows sampling continuous functions on manifolds and is invariant with respect to rigid and isometric transformations of the manifold. {MDF} generalizes to cases where the training set contains functions on different manifolds. We validate our approach on multiple datasets including challenging scientific problems like weather prediction and molecular conformation, showing that {MDF} can capture distributions of such functions with better diversity and fidelity than previous approaches.},
  author = {Ahmed A. A. Elhag and Yuyang Wang and Joshua M. Susskind and Miguel {\'A}ngel Bautista},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/elhag2024manifold.pdf:pdf},
  note = {DBLP last modified: 2024-09-02},
  pdf = {https://openreview.net/pdf?id=BZtEthuXRF},
  publisher = {OpenReview.net},
  title = {Manifold Diffusion Fields},
  url = {https://openreview.net/forum?id=BZtEthuXRF},
  year = {2024}
}

@inproceedings{engel2024faithful,
  abstract = {Two trends in explainable {AI} ({XAI}) are the use of surrogate models to approximate neural networks with more interpretable machine learning algorithms, and the use of kernel functions for explain-by-example and data attribution tasks. We combine these two trends by analyzing the use of approximate empirical neural tangent kernels ({eNTK}) for data attribution. We introduce two new random projection variants of approximate {eNTK} which allow users to tune the time and memory complexity of their calculation. We find strong connections between the neural network model and an approximation to a special kernel called the empirical neural tangent kernel, which we call the trace neural tangent kernel. We show that kernel machines using approximate neural tangent kernel as the kernel function are effective surrogate models, with the introduced trace {NTK} being the most consistent performer. Our results challenge the assumption that explanations-by-example of neural network behavior are sparse, providing evidence that every single point in the training dataset generally contributes to the behavior of the model on new data. We provide open source software allowing users to efficiently calculate kernel functions in the {PyTorch} framework.},
  author = {Andrew Engel and Zhichao Wang and Natalie S. Frank and Ioana Dumitriu and Sutanay Choudhury and Anand D. Sarwate and Tony Chiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/engel2024faithful.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=yKksu38BpM},
  publisher = {OpenReview.net},
  title = {Faithful and Efficient Explanations for Neural Networks via Neural Tangent Kernel Surrogate Models},
  url = {https://openreview.net/forum?id=yKksu38BpM},
  year = {2024}
}

@inproceedings{engelmann2024opennerf,
  abstract = {Large visual-language models ({VLMs}) like {CLIP} enable open-set image segmentation by allowing to query arbitrary concepts from images in a zero-shot manner. This goes beyond the traditional closed-set assumption, i.e., where models can only segment classes from a pre-defined training set. More recently, first works on open-vocabulary {3D} scene segmentation have emerged, which enable queries of arbitrary concepts from {3D} scene representations. In this work, we present {OpenNeRF}, which naturally operates on posed images and directly encodes {VLM} features within the {NeRF}. In contrast to prior work, we show that using pixel-wise {VLM} features (instead of global {CLIP} features) results in a less complex architecture without the need for additional {DINO} regularization. For {3D} point cloud segmentation on the Replica dataset, {OpenNeRF} outperforms recent open-vocabulary methods such as {LERF} and {OpenScene} by at least +4.9 {mIoU}. Compared to {LERF}, {OpenNeRF}'s segmentation masks are more accurate and better localized, while achieving more fine-grained classification than {OpenScene}. We also validate our method on self-captured datasets, confirming that {OpenNeRF} enables novel-view semantic segmentation in diverse real-world scenarios.},
  author = {Francis Engelmann and Fabian Manhardt and Michael Niemeyer and Keisuke Tateno and Federico Tombari},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/engelmann2024opennerf.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=SgjAojPKb3},
  publisher = {OpenReview.net},
  title = {{OpenNeRF}: Open Set {3D} Neural Scene Segmentation with Pixel-Wise Features and Rendered Novel Views},
  url = {https://openreview.net/forum?id=SgjAojPKb3},
  year = {2024}
}

@inproceedings{englesson2024robust,
  abstract = {Deep neural networks are susceptible to overfitting to label noise, resulting in reduced generalization. Two main approaches have emerged to address this: loss reweighting (which reduces the influence of noisy examples) and label correction (which replaces noisy labels with estimated true labels). In this work, we present a unified method that seamlessly combines loss reweighting and label correction. Specifically, by leveraging ideas from compositional data analysis in statistics, we frame the problem as a regression task, where loss reweighting and label correction can naturally be achieved with a shifted Gaussian label noise model. We derive the maximum likelihood estimator for this model and show how to seamlessly switch between loss reweighting, label correction, and combinations thereof. Our unified approach achieves strong performance compared to recent baselines on several noisy labeled datasets, representing a promising step towards robust deep learning in the presence of label noise.},
  author = {Erik Englesson and Hossein Azizpour},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/englesson2024robust.pdf:pdf},
  note = {DBLP last modified: 2024-12-24},
  pdf = {https://openreview.net/pdf?id=wfgZc3IMqo},
  publisher = {OpenReview.net},
  title = {Robust Classification via Regression for Learning with Noisy Labels},
  url = {https://openreview.net/forum?id=wfgZc3IMqo},
  year = {2024}
}

@inproceedings{english2024kernelised,
  abstract = {Normalising flows are non-parametric statistical models characterised by their dual capabilities of density estimation and generation. This duality requires an inherently invertible architecture, and the requirement of invertibility imposes constraints on their expressiveness, necessitating a large number of parameters and innovative architectural designs to achieve good results. The issue of over-parameterisation in flow-based models hinders their effectiveness in domains with limited data. In this paper, we present kernelised normalising flows, a novel paradigm that integrates kernels into the framework of normalising flows. Kernels can replace neural networks in coupling layers or autoregressive layers, reducing the number of parameters while maintaining or improving performance, especially in the low-data regime. We demonstrate the effectiveness of our approach on several benchmark datasets, showing that kernelised normalising flows can achieve competitive or superior performance compared to traditional neural network-based flows while using significantly fewer parameters. We also apply our method to a real-world medical dataset, the {UK} Biobank, demonstrating its potential in practical applications where data is scarce and interpretability is crucial.},
  author = {Eshant English and Matthias Kirchler and Christoph Lippert},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/english2024kernelised.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iTFdNLHE7k},
  publisher = {OpenReview.net},
  title = {Kernelised Normalising Flows},
  url = {https://openreview.net/forum?id=iTFdNLHE7k},
  year = {2024}
}

@inproceedings{errica2024tractable,
  abstract = {We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of their children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a probabilistic model. We show the model's competitiveness on scarce supervision scenarios, under missing data, and for graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.},
  address = {Vienna, Austria},
  author = {Federico Errica and Mathias Niepert},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/errica2024tractable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=h7nOCxFsPg},
  publisher = {OpenReview.net},
  title = {Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks},
  url = {https://openreview.net/forum?id=h7nOCxFsPg},
  year = {2024}
}

@inproceedings{esfandiarpoor2024followup,
  abstract = {A promising approach for improving the performance of vision-language models like CLIP for image classification is to extend the class descriptions (i.e., prompts) with related attributes, e.g., using brown sparrow instead of sparrow. However, current zero-shot methods select a subset of attributes regardless of commonalities between the target classes, potentially providing no useful information that would have helped to distinguish between them. For instance, they may use color instead of bill shape to distinguish between sparrows and wrens, which are both brown. We propose Follow-up Differential Descriptions (FuDD), a zero-shot approach that tailors the class descriptions to each dataset and leads to additional attributes that better differentiate the target classes. FuDD first identifies the ambiguous classes for each image, and then uses a Large Language Model (LLM) to generate new class descriptions that differentiate between them. The new class descriptions resolve the initial ambiguity and help predict the correct label. In our experiments, FuDD consistently outperforms generic description ensembles and naive LLM-generated descriptions on 12 datasets. We show that differential descriptions are an effective tool to resolve class ambiguities, which otherwise significantly degrade the performance. We also show that high quality natural language class descriptions produced by FuDD result in comparable performance to few-shot adaptation methods.},
  address = {Vienna, Austria},
  author = {Reza Esfandiarpoor and Stephen H. Bach},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/esfandiarpoor2024followup.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=g6rZtxaXRm},
  publisher = {OpenReview.net},
  title = {Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification},
  url = {https://openreview.net/forum?id=g6rZtxaXRm},
  year = {2024}
}

@inproceedings{eustratiadis2024neural,
  abstract = {In few-shot recognition, a classifier that has been trained on one set of classes is required to rapidly adapt and generalize to a disjoint, novel set of classes. To that end, recent studies have shown the efficacy of fine-tuning with carefully crafted adaptation architectures. However this raises the question of: How can one design the optimal adaptation strategy? In this paper, we study this question through the lens of neural architecture search (NAS). Given a pre-trained neural network, our algorithm discovers the optimal arrangement of adapters, which layers to keep frozen and which to fine-tune. We demonstrate the generality of our NAS method by applying it to both residual networks and vision transformers and report state-of-the-art performance on Meta-Dataset and Meta-Album.},
  address = {Vienna, Austria},
  author = {Panagiotis Eustratiadis and Lukasz Dudziak and Da Li and Timothy M. Hospedales},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/eustratiadis2024neural.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=T7YV5UZKBc},
  publisher = {OpenReview.net},
  title = {Neural Fine-Tuning Search for Few-Shot Learning},
  url = {https://openreview.net/forum?id=T7YV5UZKBc},
  year = {2024}
}

@inproceedings{everaert2024gio,
  abstract = {It is often advantageous to train models on a subset of the available train examples, because the examples are of variable quality or because one would like to train with fewer examples, without sacrificing performance. We present Gradient Information Optimization (GIO), a scalable, task-agnostic approach to this data selection problem that requires only a small set of (unlabeled) examples representing a target distribution. GIO begins from a natural, information-theoretic objective that is intractable in practice. Our contribution is in showing that it can be made highly scalable through a simple relaxation of the objective and a highly efficient implementation.},
  address = {Vienna, Austria},
  author = {Dante Everaert and Christopher Potts},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/everaert2024gio.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=3NnfJnbJT2},
  publisher = {OpenReview.net},
  title = {{GIO}: Gradient Information Optimization for Training Dataset Selection},
  url = {https://openreview.net/forum?id=3NnfJnbJT2},
  year = {2024}
}

@inproceedings{eyring2024unbalancedness,
  abstract = {In optimal transport (OT), a Monge map is known as a mapping that transports a source distribution to a target distribution in the most cost-efficient way. Recently, multiple neural estimators for Monge maps have been developed and applied in diverse unpaired domain translation tasks, e.g. in single-cell biology and computer vision. However, the classic OT framework enforces mass conservation, which makes it prone to outliers and limits its applicability in real-world scenarios. The latter can be particularly harmful in OT domain translation tasks, where the relative position of a sample within a distribution is explicitly taken into account. While unbalanced OT tackles this challenge in the discrete setting, its integration into neural Monge map estimators has received limited attention. We propose a theoretically grounded method to incorporate unbalancedness into any Monge map estimator. We improve existing estimators to model cell trajectories over time and to predict cellular responses to perturbations. Moreover, our approach seamlessly integrates with the OT flow matching (OT-FM) framework. While we show that OT-FM performs competitively in image translation, we further improve performance by incorporating unbalancedness (UOT-FM), which better preserves relevant features. We hence establish UOT-FM as a principled method for unpaired image translation.},
  address = {Vienna, Austria},
  author = {Luca Eyring and Dominik Klein and Theo Uscidda and Giovanni Palla and Niki Kilbertus and Zeynep Akata and Fabian J. Theis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/eyring2024unbalancedness.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-04-03},
  pdf = {https://openreview.net/pdf?id=2UnCj3jeao},
  publisher = {OpenReview.net},
  title = {Unbalancedness in Neural {M}onge Maps Improves Unpaired Domain Translation},
  url = {https://openreview.net/forum?id=2UnCj3jeao},
  year = {2024}
}

@inproceedings{fabian2024learning,
  abstract = {Learning the behavior of large agent populations is an important task for numerous research areas. Although the field of multi-agent reinforcement learning (MARL) has made significant progress towards solving these systems, solutions for many agents often remain computationally infeasible and lack theoretical guarantees. Mean Field Games (MFGs) address both of these issues and can be extended to Graphon MFGs (GMFGs) to include network structures between agents. Despite their merits, the real world applicability of GMFGs is limited by the fact that graphons only capture dense graphs. Since most empirically observed networks show some degree of sparsity, such as power law graphs, the GMFG framework is insufficient for capturing these network topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which builds on the graph theoretical concept of graphexes. Graphexes are the limiting objects to sparse graph sequences that also have other desirable features such as the small world property. Learning equilibria in these games is challenging due to the rich and sparse structure of the underlying graphs. To tackle these challenges, we design a new learning algorithm tailored to the GXMFG setup. This hybrid graphex learning approach leverages that the system mainly consists of a highly connected core and a sparse periphery. After defining the system and providing a theoretical analysis, we state our learning approach and demonstrate its learning capabilities on both synthetic graphs and real-world networks. This comparison shows that our GXMFG learning algorithm successfully extends MFGs to a highly relevant class of hard, realistic learning problems that are not accurately addressed by current MARL and MFG methods.},
  address = {Vienna, Austria},
  author = {Christian Fabian and Kai Cui and Heinz Koeppl},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fabian2024learning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-09-27},
  pdf = {https://openreview.net/pdf?id=zwU9scoU4A},
  publisher = {OpenReview.net},
  title = {Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach},
  url = {https://openreview.net/forum?id=zwU9scoU4A},
  year = {2024}
}

@inproceedings{faiz2024llmcarbon,
  abstract = {The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. In this paper, we propose an end-to-end carbon footprint projection model, LLMCarbon, which can accurately predict the carbon footprint of both dense and MoE LLMs during their training, inference, experimentation, and storage phases. LLMCarbon incorporates critical LLM, hardware, and data center parameters, such as LLM parameter count, hardware type, system power, chip area, and data center efficiency, to model both operational and embodied carbon footprints of an LLM.},
  address = {Vienna, Austria},
  author = {Ahmad Faiz and Sotaro Kaneda and Ruhan Wang and Rita Chukwunyere Osi and Prateek Sharma and Fan Chen and Lei Jiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/faiz2024llmcarbon.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=aIok3ZD9to},
  publisher = {OpenReview.net},
  title = {{LLMCarbon}: Modeling the End-to-End Carbon Footprint of Large Language Models},
  url = {https://openreview.net/forum?id=aIok3ZD9to},
  year = {2024}
}

@inproceedings{fakoor2024timevarying,
  abstract = {Real-world deployment of machine learning models is challenging because data evolves over time. While no model can work when data evolves in an arbitrary fashion, if there is some pattern to these changes, we might be able to design methods to address it. This paper addresses situations when data evolves gradually. We introduce a time-varying propensity score that can detect gradual shifts in the distribution of data which allows us to selectively sample past data to update the model -- not just similar data from the past like that of a standard propensity score but also data that evolved in a similar fashion in the past. The time-varying propensity score is quite general: we demonstrate different ways of implementing it and evaluate it on a variety of problems ranging from supervised learning (e.g., image classification problems) where data undergoes a sequence of gradual shifts, to reinforcement learning tasks (e.g., robotic manipulation and continuous control) where data shifts as the policy or the task changes.},
  address = {Vienna, Austria},
  author = {Rasool Fakoor and Jonas Mueller and Zachary Chase Lipton and Pratik Chaudhari and Alex Smola},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fakoor2024timevarying.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=m0x0rv6Iwm},
  publisher = {OpenReview.net},
  title = {Time-Varying Propensity Score to Bridge the Gap between the Past and Present},
  url = {https://openreview.net/forum?id=m0x0rv6Iwm},
  year = {2024}
}

@inproceedings{falet2024deltaai,
  abstract = {We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call Œî-amortized inference (Œî-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent, sparsity of the PGM enables local credit assignment in the agent's policy learning objective. This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update, thus speeding up training considerably. The Œî-AI objective matches the conditional distribution of a variable given its Markov blanket in a tractable learned sampler, which has the structure of a Bayesian network, with the same conditional distribution under the target PGM. As such, the trained sampler recovers marginals and conditional distributions of interest and enables inference of partial subsets of variables.},
  address = {Vienna, Austria},
  author = {Jean-Pierre R. Falet and Hae Beom Lee and Nikolay Malkin and Chen Sun and Dragos Secrieru and Dinghuai Zhang and Guillaume Lajoie and Yoshua Bengio},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/falet2024deltaai.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-08},
  pdf = {https://openreview.net/pdf?id=LemSSn8htt},
  publisher = {OpenReview.net},
  title = {Delta-{AI}: Local objectives for amortized inference in sparse graphical models},
  url = {https://openreview.net/forum?id=LemSSn8htt},
  year = {2024}
}

@inproceedings{fan2024fair,
  abstract = {Federated learning is a popular technology for training machine learning models on distributed data sources without sharing data. Vertical federated learning or feature-based federated learning applies to the cases that different data sources share the same sample ID space but differ in feature space. To ensure the data owners' long-term engagement, it is critical to objectively assess the contribution from each data source and recompense them accordingly. The Shapley value (SV) is a provably fair contribution valuation metric originated from cooperative game theory. However, computing the SV requires extensively retraining the model on each subset of data sources, which causes prohibitively high communication costs in federated learning. We propose a contribution valuation metric called vertical federated Shapley value (VerFedSV) based on SV. We show that VerFedSV not only satisfies many desirable properties for fairness but is also efficient to compute, and can be adapted to both synchronous and asynchronous vertical federated learning algorithms. Both theoretical analysis and extensive experimental results verify the fairness, efficiency, and adaptability of VerFedSV.},
  address = {Vienna, Austria},
  author = {Zhenan Fan and Huang Fang and Xinglu Wang and Zirui Zhou and Jian Pei and Michael P. Friedlander and Yong Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fan2024fair.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-28},
  pdf = {https://openreview.net/pdf?id=sLQb8q0sUi},
  publisher = {OpenReview.net},
  title = {Fair and Efficient Contribution Valuation for Vertical Federated Learning},
  url = {https://openreview.net/forum?id=sLQb8q0sUi},
  year = {2024}
}

@inproceedings{fan2024weaker,
  abstract = {We show that extragradient with multistep extrapolation solve more problems and propose a new algorithm that tackles cycling behaviors. The min-max problem has attracted increasing attention because of its applications in machine learning tasks such as generative adversarial networks ({GANs}) training. While there has been exhaustive research on convex-concave setting, problem of nonconvex-nonconcave setting faces many challenges, such as convergence to limit cycles. Given that general min-max optimization has been found to be intractable, recent research efforts have shifted towards tackling structured problems, particularly those following the weak {Minty} variational inequality (weak {MVI}). This paper proposes a new algorithm framework extending the extragradient algorithm with multi-step exploration to relax existing assumptions and tackle cyclic behaviors in optimization.},
  author = {Yifeng Fan and Yongqiang Li and Bo Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fan2024weaker.pdf:pdf},
  keywords = {extragradient, minimax, nonconvex-nonconcave, variational inequalities, saddle point problem},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RNGUbTYSjk},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Weaker {MVI} Condition: Extragradient Methods with Multi-Step Exploration},
  url = {https://openreview.net/forum?id=RNGUbTYSjk},
  year = {2024}
}

@inproceedings{fan2024salun,
  abstract = {We introduce the concept of 'weight saliency' for machine unlearning ({MU}), drawing parallels with input saliency in model explanation. This innovation directs {MU}'s attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning ({SalUn}) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting data points). {SalUn} yields a stability advantage in high-variance random data forgetting, e.g., with a 0.2\% gap compared to exact unlearning on the {CIFAR-10} dataset. Moreover, in preventing conditional diffusion models from generating harmful images, {SalUn} achieves nearly 100\% unlearning accuracy, outperforming current state-of-the-art baselines like Erased Stable Diffusion and Forget-Me-Not. To the best of our knowledge, {SalUn} is the first principled {MU} approach that can effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation tasks.},
  archiveprefix = {arXiv},
  author = {Chongyu Fan and Jiancheng Liu and Yihua Zhang and Eric Wong and Dennis Wei and Sijia Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.12508},
  file = {:/home/b/documents/inproceedings/fan2024salun.pdf:pdf},
  keywords = {machine unlearning, weight saliency, gradient-based methods, image classification, image generation, diffusion models},
  note = {DBLP last modified: 2024-08-04, Spotlight paper},
  pdf = {https://openreview.net/pdf?id=gn0mIhQGNM},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{SalUn}: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation},
  url = {https://openreview.net/forum?id=gn0mIhQGNM},
  year = {2024}
}

@inproceedings{fan2024mgtsd,
  abstract = {Recently, diffusion probabilistic models have attracted attention in generative time series forecasting due to their remarkable capacity to generate high-fidelity samples. However, the effective utilization of their strong modeling ability in the probabilistic time series forecasting task remains an open question, partially due to the challenge of instability arising from their stochastic nature. To address this challenge, we introduce a novel Multi-Granularity Time Series Diffusion ({MG-TSD}) model, which achieves state-of-the-art predictive performance by leveraging the inherent granularity levels within the data as given targets at intermediate diffusion steps to guide the learning process of diffusion models. The way to construct the targets is motivated by the observation that the forward process of the diffusion model, which sequentially corrupts the data distribution to a standard normal distribution, intuitively aligns with the process of smoothing fine-grained data into a coarse-grained representation, both of which result in a gradual loss of fine distribution features. In the study, we derive a novel multi-granularity guidance diffusion loss function and propose a concise implementation method to effectively utilize coarse-grained data across various granularity levels. More importantly, our approach does not rely on additional external data, making it versatile and applicable across various domains. Extensive experiments conducted on real-world datasets demonstrate that our {MG-TSD} model outperforms existing time series prediction methods.},
  archiveprefix = {arXiv},
  author = {Xinyao Fan and Yueying Wu and Chang Xu and Yuhao Huang and Weiqing Liu and Jiang Bian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2403.05751},
  file = {:/home/b/documents/inproceedings/fan2024mgtsd.pdf:pdf},
  keywords = {time series forecasting, diffusion models, multi-granularity, probabilistic forecasting, generative models},
  note = {DBLP last modified: 2025-04-01},
  pdf = {https://openreview.net/pdf?id=CZiY6OLktd},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{MG-TSD}: Multi-Granularity Time Series Diffusion Models with Guided Learning Process},
  url = {https://openreview.net/forum?id=CZiY6OLktd},
  year = {2024}
}

@inproceedings{fan2024decoupling,
  abstract = {Recent years have witnessed the great success of graph pre-training for graph representation learning. With hundreds of graph pre-training tasks proposed, integrating knowledge acquired from multiple pre-training tasks has become a popular research topic. In this paper, we identify two important collaborative processes for this topic: (1) select: how to select an optimal task combination from a given task pool based on their compatibility, and (2) weigh: how to weigh the selected tasks based on their importance. However, current approaches mainly focus on the task weighing process and neglect the necessary of task selection, leading to sub-optimal performance and efficiency. To address this challenge, we propose a novel instance-level framework called Weigh And Select ({WAS}), where the two collaborative processes of weighing and selecting are combined by decoupled siamese networks. Specifically, the framework first adaptively learns an optimal combination of tasks for each instance from a given task pool, based on which a customized instance-level task weighing strategy is learned. Extensive experiments on 16 graph datasets across node-level and graph-level downstream tasks demonstrated that by combining a few simple but classical tasks, {WAS} can achieve comparable performance to other leading counterparts. To the best of our knowledge, this work is the first attempt to use the weighing \& selecting strategy for integrating multiple graph pre-training tasks.},
  archiveprefix = {arXiv},
  author = {Tianyu Fan and Lirong Wu and Yufei Huang and Haitao Lin and Cheng Tan and Zhangyang Gao and Stan Z. Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2403.01400},
  file = {:/home/b/documents/inproceedings/fan2024decoupling.pdf:pdf},
  keywords = {graph pre-training, multi-task learning, graph neural networks, task selection, task weighing},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=c85tdYOOju},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks},
  url = {https://openreview.net/forum?id=c85tdYOOju},
  year = {2024}
}

@inproceedings{fang2024domainagnostic,
  abstract = {The generation of molecules with desired properties has become increasingly popular, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face challenges such as generating syntactically or chemically flawed molecules, having narrow domain focus, and struggling to create diverse and feasible molecules due to limited annotated data or external molecular databases. To address these issues, we introduce {MolGen}, a pre-trained molecular language model tailored specifically for molecule generation. Through the reconstruction of over 100 million molecular {SELFIES}, {MolGen} internalizes structural and grammatical insights. This is further enhanced by domain-agnostic molecular prefix tuning, fostering robust knowledge transfer across diverse domains. Importantly, our chemical feedback paradigm steers the model away from molecular hallucinations, ensuring alignment between the model's estimated probabilities and real-world chemical preferences. Extensive experiments on well-known benchmarks underscore {MolGen}'s optimization capabilities in properties such as penalized log{P}, {QED}, and molecular docking. Additional analyses confirm its proficiency in accurately capturing molecule distributions, discerning intricate structural patterns, and efficiently exploring the chemical space.},
  archiveprefix = {arXiv},
  author = {Yin Fang and Ningyu Zhang and Zhuo Chen and Lingbing Guo and Xiaohui Fan and Huajun Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2301.11259},
  file = {:/home/b/documents/inproceedings/fang2024domainagnostic.pdf:pdf},
  keywords = {molecular generation, chemical feedback, language models, drug design, SELFIES representation},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9rPyHyjfwP},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Domain-Agnostic Molecular Generation with Chemical Feedback},
  url = {https://openreview.net/forum?id=9rPyHyjfwP},
  year = {2024}
}

@inproceedings{fang2024functional,
  abstract = {{Tucker} decomposition is a powerful tensor model to handle multi-aspect data. It demonstrates the low-rank property by decomposing the grid-structured data as interactions between a core tensor and a set of object representations (factors). However, a fundamental assumption of such decomposition is that there are finite objects in each aspect or mode, corresponding to discrete indexes of data entries. However, real-world data is often not naturally posed in this setting. For example, geographic data is represented as continuous indexes of latitude and longitude coordinates, and cannot fit tensor models directly. To address this limitation, we propose Functional {Bayesian} {Tucker} Decomposition ({FunBaT}). We treat the continuous-indexed data as the interaction between the {Tucker} core and a group of latent functions. We use {Gaussian} processes ({GP}) as functional priors to model the latent functions. We convert each {GP} into a state-space prior by constructing an equivalent stochastic differential equation ({SDE}) to reduce computational cost. We develop an efficient inference algorithm based on the conditional expectation propagation ({CEP}) framework and {Bayesian} filter to approximate the posterior distribution of the factors.},
  archiveprefix = {arXiv},
  author = {Shikai Fang and Xin Yu and Zheng Wang and Shibo Li and Mike Kirby and Shandian Zhe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2311.04829},
  file = {:/home/b/documents/inproceedings/fang2024functional.pdf:pdf},
  keywords = {tensor decomposition, Bayesian methods, Gaussian processes, continuous-indexed data, Tucker decomposition},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZWyZeqE928},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Functional {Bayesian} {Tucker} Decomposition for Continuous-indexed Tensor Data},
  url = {https://openreview.net/forum?id=ZWyZeqE928},
  year = {2024}
}

@inproceedings{fang2024rethinking,
  abstract = {Uniformity plays an important role in evaluating learned representations, providing insights into self-supervised learning. In our quest for effective uniformity metrics, we pinpoint four principled properties that such metrics should possess. Namely, an effective uniformity metric should remain invariant to instance permutations and sample replications while accurately capturing feature redundancy and dimensional collapse. Surprisingly, we find that the uniformity metric proposed by Wang \& Isola (2020) fails to satisfy the majority of these properties. Specifically, their metric is sensitive to sample replications, and cannot account for feature redundancy and dimensional collapse correctly. To overcome these limitations, we introduce a new uniformity metric based on the {Wasserstein} distance, which satisfies all the aforementioned properties. Integrating this new metric in existing self-supervised learning methods effectively mitigates dimensional collapse and consistently improves their performance on downstream tasks involving {CIFAR-10} and {CIFAR-100} datasets.},
  archiveprefix = {arXiv},
  author = {Xianghong Fang and Jian Li and Qiang Sun and Benyou Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2403.00642},
  file = {:/home/b/documents/inproceedings/fang2024rethinking.pdf:pdf},
  keywords = {self-supervised learning, uniformity metric, Wasserstein distance, dimensional collapse, representation learning},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=3pf2hEdu8B},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Rethinking the Uniformity Metric in Self-Supervised Learning},
  url = {https://openreview.net/forum?id=3pf2hEdu8B},
  year = {2024}
}

@inproceedings{fang2024whats,
  abstract = {Proximal operators commonly appear as part of algorithmic strategies to regularize ill-posed inverse problems. While modern deep learning models have been applied to these tasks through plug-and-play or deep unrolling frameworks, there is no guarantee that a general deep network represents the proximal operator of any function, nor is there any characterization of the function for which the network might provide some approximate proximal. We address these points by developing a new class of deep neural networks -- learned proximal networks ({LPN}) -- that exactly implement the proximal operator of a general learned function. Such an {LPN} implicitly learns a regularization function for inverse problems that can be characterized and evaluated, shedding light onto what has been learned from data and improving the interpretability of learning-based solutions. We also present a new training problem, dubbed proximal matching, that provably promotes the recovery of the correct regularization term (i.e., the log of the data distribution). In contrast to {MMSE} denoising, we prove that in the limit of small scale parameter Œ≥, training a denoiser by minimizing the proximal matching loss recovers the correct log-prior. We show convergence for {PnP} reconstruction algorithms using {LPN} with minimal and verifiable assumptions.},
  archiveprefix = {arXiv},
  author = {Zhenghan Fang and Sam Buchanan and Jeremias Sulam},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.14344},
  file = {:/home/b/documents/inproceedings/fang2024whats.pdf:pdf},
  keywords = {inverse problems, proximal operators, deep neural networks, regularization, plug-and-play methods},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kNPcOaqC5r},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {What's in a Prior? Learned Proximal Networks for Inverse Problems},
  url = {https://openreview.net/forum?id=kNPcOaqC5r},
  year = {2024}
}

@inproceedings{fang2024solving,
  abstract = {Physics-informed neural networks ({PINNs}) often struggle to solve high-frequency and multi-scale partial differential equations ({PDEs}), which can be due to spectral bias during neural network training. To address this problem, we resort to the {Gaussian} process ({GP}) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the {PDE} solution with a student t mixture or {Gaussian} mixture. We apply the inverse {Fourier} transform to obtain the covariance function (by {Wiener-Khinchin} theorem). To enable efficient and scalable computation on massive collocation points, which are critical to capture high frequencies, we place the collocation points on a grid, and multiply our covariance function at each input dimension. As a result, we can derive a {Kronecker} product structure in the covariance matrix. We use {Kronecker} product properties and multilinear algebra to promote computational efficiency and scalability, without low-rank approximations. We introduce {GP-HM}, a {GP} solver for High frequency and Multi-scale {PDEs}, and demonstrate the first to realize its rationale and benefit for solving high-frequency and multi-scale {PDEs}.},
  archiveprefix = {arXiv},
  author = {Shikai Fang and Madison Cooley and Da Long and Shibo Li and Mike Kirby and Shandian Zhe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2311.04465},
  file = {:/home/b/documents/inproceedings/fang2024solving.pdf:pdf},
  keywords = {partial differential equations, Gaussian processes, physics-informed neural networks, high-frequency PDEs, spectral methods},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=q4AEBLHuA6},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Solving High Frequency and Multi-Scale {PDEs} with {Gaussian} Processes},
  url = {https://openreview.net/forum?id=q4AEBLHuA6},
  year = {2024}
}

@inproceedings{fang2024acrf,
  abstract = {This work studies the problem of explicit {NeRF} compression by reformulating the task as 3D data compression. We introduce the {ACRF} (Attributed Compression of Radiance Field) framework which focuses on compressing explicit neural 3D representations. The neural 3D structure is pruned and converted to points with features, which are further encoded using importance-guided feature encoding. The framework employs an importance-prioritized entropy model to estimate the probability distribution of transform coefficients, which are then entropy coded with an arithmetic coder using the predicted distribution. Within this framework, we present two models: {ACRF} and {ACRF-F}, to strike a balance between compression performance and encoding time budget. Explicit {NeRF} models facilitate practical {NeRF} applications with faster rendering speed, and also attract considerable attention in {NeRF} compression due to their huge storage cost.},
  author = {Guangchi Fang and Qingyong Hu and Longguang Wang and Yulan Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fang2024acrf.pdf:pdf},
  keywords = {neural radiance fields, compression, explicit NeRF, attribute compression, 3D data compression, entropy coding},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=POFrdKvpea},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{ACRF}: Compressing Explicit Neural Radiance Fields via Attribute Compression},
  url = {https://openreview.net/forum?id=POFrdKvpea},
  year = {2024}
}

@inproceedings{fang2024data,
  abstract = {Large training sets have become a cornerstone of machine learning and are the foundation for recent advances in language modeling and multimodal learning. While data curation for pre-training is often still ad-hoc, one common paradigm is to first collect a massive pool of data from the Web and then filter this candidate pool down to an actual training set via various heuristics. In this work, we study the problem of learning a data filtering network ({DFN}) for this second step of filtering a large uncurated dataset. Our key finding is that the quality of a network for filtering is distinct from its performance on downstream tasks: for instance, a model that performs well on {ImageNet} can yield worse training sets than a model with low {ImageNet} accuracy that is trained on a small amount of high-quality data. Based on our insights, we construct new data filtering networks that induce state-of-the-art image-text datasets. Specifically, our best performing dataset {DFN-5B} enables us to train state-of-the-art {CLIP} models for their compute budgets: among other improvements on a variety of tasks, a {ViT-H} trained on our dataset achieves 84.4\% zero-shot transfer accuracy on {ImageNet}, out-performing models trained on other datasets such as {LAION-2B}, {DataComp-1B}, or {OpenAI}'s {WIT}.},
  author = {Alex Fang and Albin Madappally Jose and Amit Jain and Ludwig Schmidt and Alexander T. Toshev and Vaishaal Shankar},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/fang2024data.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KAk6ngZ09F},
  publisher = {OpenReview.net},
  title = {{Data Filtering Networks}},
  url = {https://openreview.net/forum?id=KAk6ngZ09F},
  year = {2024}
}

@inproceedings{fang2024molinstructions,
  abstract = {Large Language Models ({LLMs}), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce {Mol-Instructions}, a comprehensive instruction dataset designed for the biomolecular domain. {Mol-Instructions} encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of {LLMs} concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on {LLMs}, we demonstrate the effectiveness of {Mol-Instructions} in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. {Mol-Instructions} is publicly available for ongoing research and will undergo regular updates to enhance its applicability.},
  author = {Yin Fang and Xiaozhuan Liang and Ningyu Zhang and Kangwei Liu and Rui Huang and Zhuo Chen and Xiaohui Fan and Huajun Chen},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/fang2024molinstructions.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Tlsdsb6l9n},
  publisher = {OpenReview.net},
  title = {{Mol-Instructions}: A Large-Scale Biomolecular Instruction Dataset for Large Language Models},
  url = {https://openreview.net/forum?id=Tlsdsb6l9n},
  year = {2024}
}

@inproceedings{fang2024causalstonet,
  abstract = {With the advancement of data science, the collection of increasingly complex datasets has become commonplace. In such datasets, the data dimension can be extremely high, and the underlying data generation process can be unknown and highly nonlinear. As a result, the task of making causal inference with high-dimensional complex data has become a fundamental problem in many disciplines, such as medicine, econometrics, and social science. However, the existing methods for causal inference are frequently developed under the assumption that the data dimension is low or that the underlying data generation process is linear or approximately linear. To address these challenges, this paper proposes a novel causal inference approach for dealing with high-dimensional complex data. The proposed approach is based on deep learning techniques, including sparse deep learning theory and stochastic neural networks, that have been developed in recent literature. By using these techniques, the proposed approach can address both the high dimensionality and unknown data generation process in a coherent way. Furthermore, the proposed approach can also be used when missing values are present in the datasets. Extensive numerical studies indicate that the proposed approach outperforms existing ones.},
  author = {Yaxin Fang and Faming Liang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/fang2024causalstonet.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BtZ7vCt5QY},
  publisher = {OpenReview.net},
  title = {{Causal-StoNet}: Causal Inference for High-Dimensional Complex Data},
  url = {https://openreview.net/forum?id=BtZ7vCt5QY},
  year = {2024}
}

@inproceedings{fang2024predictive,
  abstract = {The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning ({RL}), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an {RL} system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this {RL} system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. The auxiliary predictive model corresponds to the hippocampus (an area thought to learn a predictive model to support memory-guided behavior), while the encoder network and value learning network correspond to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep {RL} systems can provide an interpretable framework for modeling multi-region interactions in the brain.},
  author = {Ching Fang and Kimberly Stachenfeld},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/fang2024predictive.pdf:pdf},
  note = {{ICLR} 2024 Oral Presentation, DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=agPpmEgf8C},
  publisher = {OpenReview.net},
  title = {Predictive auxiliary objectives in deep {RL} mimic learning in the brain},
  url = {https://openreview.net/forum?id=agPpmEgf8C},
  year = {2024}
}

@inproceedings{farinhas2024nonexchangeable,
  abstract = {Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best F‚ÇÅ-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable.},
  author = {Ant√≥nio Farinhas and Chrysoula Zerva and Dennis Ulmer and Andr√© F. T. Martins},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/farinhas2024nonexchangeable.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=j511LaqEeP},
  publisher = {OpenReview.net},
  title = {Non-Exchangeable Conformal Risk Control},
  url = {https://openreview.net/forum?id=j511LaqEeP},
  year = {2024}
}

@inproceedings{fatemi2024dynamical,
  abstract = {We address causal reasoning in multivariate time series data generated by stochastic processes. Existing approaches are largely restricted to static settings, ignoring the continuity and emission of variations across time. In contrast, we propose a learning paradigm that directly establishes causation between events in the course of time. We present two key lemmas to compute causal contributions and frame them as reinforcement learning problems. Our approach offers formal and computational tools for uncovering and quantifying causal relationships in diffusion processes, subsuming various important settings such as discrete-time Markov decision processes.},
  author = {Mehdi Fatemi and Sindhu C. M. Gowda},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/fatemi2024dynamical.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lrQlLqQase},
  publisher = {OpenReview.net},
  title = {A Dynamical View of the Question of Why},
  url = {https://openreview.net/forum?id=lrQlLqQase},
  year = {2024}
}

@inproceedings{fatemi2024talk,
  abstract = {Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models ({LLMs}) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by {LLMs}. We show that {LLM} performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered.},
  author = {Bahare Fatemi and Jonathan Halcrow and Bryan Perozzi},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/fatemi2024talk.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IuXR1CCrSi},
  publisher = {OpenReview.net},
  title = {Talk like a Graph: Encoding Graphs for Large Language Models},
  url = {https://openreview.net/forum?id=IuXR1CCrSi},
  year = {2024}
}

@inproceedings{fathi2024course,
  abstract = {{Koopman} representations aim to learn features of nonlinear dynamical systems ({NLDS}) which lead to linear dynamics in the latent space. Theoretically, such features can be used to simplify many problems in modeling and control of {NLDS}. In this work we study autoencoder formulations of this problem, and different ways they can be used to model dynamics, specifically for future state prediction over long horizons. We discover several limitations of predicting future states in the latent space and propose an inference-time mechanism, which we refer to as Periodic Reencoding, for faithfully capturing long term dynamics. We justify this method both analytically and empirically via experiments in low and high dimensional {NLDS}.},
  author = {Mahan Fathi and Clement Gehring and Jonathan Pilault and David Kanaa and Pierre-Luc Bacon and Ross Goroshin},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/fathi2024course.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=A18gWgc5mi},
  publisher = {OpenReview.net},
  title = {Course Correcting {Koopman} Representations},
  url = {https://openreview.net/forum?id=A18gWgc5mi},
  year = {2024}
}

@inproceedings{federici2024latent,
  abstract = {{Markov} processes are widely used mathematical models for describing dynamic systems in various fields. However, accurately simulating large-scale systems at long time scales is computationally expensive due to the short time steps required for accurate integration. In this paper, we introduce an inference process that maps complex systems into a simplified representational space and models large jumps in time. To achieve this, we propose Time-lagged Information Bottleneck ({T-IB}), a principled objective rooted in information theory, which aims to capture relevant temporal features while discarding high-frequency information to simplify the simulation task and minimize the inference error. Our experiments demonstrate that {T-IB} learns information-optimal representations for accurately modeling the statistical properties and dynamics of the original process at a selected time lag, outperforming existing time-lagged dimensionality reduction methods.},
  author = {Marco Federici and Patrick Forr√© and Ryota Tomioka and Bastiaan S. Veeling},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/federici2024latent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bH6T0Jjw5y},
  publisher = {OpenReview.net},
  title = {Latent Representation and Simulation of {Markov} Processes via Time-Lagged Information Bottleneck},
  url = {https://openreview.net/forum?id=bH6T0Jjw5y},
  year = {2024}
}

@inproceedings{felice2024graphbased,
  abstract = {Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named {GgNet}, implementing the framework.},
  author = {Giovanni de Felice and Andrea Cini and Daniele Zambon and Vladimir V. Gusev and Cesare Alippi},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/felice2024graphbased.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CAqdG2dy5s},
  publisher = {OpenReview.net},
  title = {Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations},
  url = {https://openreview.net/forum?id=CAqdG2dy5s},
  year = {2024}
}

@inproceedings{feng2024mayfly,
  abstract = {A graph is a structure made up of vertices and edges used to represent complex relationships between entities, while a graph stream is a continuous flow of graph updates that convey evolving relationships between entities. The massive volume and high dynamism of graph streams promote research on data structures of graph summarization, which provides a concise and approximate view of graph streams with sub-linear space and linear construction time, enabling real-time graph analytics in various domains, such as social networking, financing, and cybersecurity. In this work, we propose the Mayfly, the first neural data structure for summarizing graph streams. The Mayfly replaces handcrafted data structures with better accuracy and adaptivity. To cater to practical applications, Mayfly incorporates two offline training phases. During the larval phase, the Mayfly learns basic summarization abilities from automatically and synthetically constituted meta-tasks, and in the metamorphosis phase, it rapidly adapts to real graph streams via meta-tasks. With specific configurations of information pathways, the Mayfly enables flexible support for miscellaneous graph queries, including edge, node, and connectivity queries. Extensive empirical studies show that the Mayfly significantly outperforms its handcrafted competitors.},
  author = {Yuan Feng and Yukun Cao and Hairu Wang and Xike Xie and S. Kevin Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024mayfly.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=n7Sr8SW4bn},
  publisher = {OpenReview.net},
  title = {Mayfly: a Neural Data Structure for Graph Stream Summarization},
  url = {https://openreview.net/forum?id=n7Sr8SW4bn},
  year = {2024}
}

@inproceedings{feng2024proteinligand,
  abstract = {The binding between proteins and ligands plays a crucial role in the realm of drug discovery. Previous deep learning approaches have shown promising results over traditional computationally intensive methods, but resulting in poor generalization due to limited supervised data. In this paper, we propose to learn protein-ligand binding representation in a self-supervised learning manner. Different from existing pre-training approaches which treat proteins and ligands individually, we emphasize to discern the intricate binding patterns from fine-grained interactions. Specifically, this self-supervised learning problem is formulated as a prediction of the conclusive binding complex structure given a pocket and ligand with a Transformer based interaction module, which naturally emulates the binding process. To ensure the representation of rich binding information, we introduce two pre-training tasks, i.e. atomic pairwise distance map prediction and mask ligand reconstruction, which comprehensively model the fine-grained interactions from both structure and feature space. Extensive experiments have demonstrated the superiority of our method across various binding tasks, including protein-ligand affinity prediction, virtual screening and protein-ligand docking.},
  author = {Shikun Feng and Minghao Li and Yinjun Jia and Wei-Ying Ma and Yanyan Lan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024proteinligand.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AXbN2qMNiW},
  publisher = {OpenReview.net},
  title = {Protein-ligand binding representation learning from fine-grained interactions},
  url = {https://openreview.net/forum?id=AXbN2qMNiW},
  year = {2024}
}

@inproceedings{feng2024lighthgnn,
  abstract = {Hypergraph Neural Networks ({HGNNs}) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of {HGNNs} is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the {HGNNs} and inference-efficient Multi-Layer Perceptron ({MLPs}) to eliminate the hypergraph dependency of {HGNNs} and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce {LightHGNN} and {LightHGNN+} for fast inference with low complexity. {LightHGNN} directly distills the knowledge from teacher {HGNNs} to student {MLPs} via soft labels, and {LightHGNN+} further explicitly injects reliable high-order correlations into the student {MLPs} to achieve topology-aware distillation and resistance to over-smoothing. Experiments on eight hypergraph datasets demonstrate that even without hypergraph dependency, the proposed {LightHGNNs} can still achieve competitive or even better performance than {HGNNs} and outperform vanilla {MLPs} by 16.3 on average. Extensive experiments on three graph datasets further show the average best performance of our {LightHGNNs} compared with all other methods. Experiments on synthetic hypergraphs with 5.5w vertices indicate {LightHGNNs} can run 100√ó faster than {HGNNs}, showcasing their ability for latency-sensitive deployments.},
  author = {Yifan Feng and Yihe Luo and Shihui Ying and Yue Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024lighthgnn.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lHasEfGsXL},
  publisher = {OpenReview.net},
  title = {{LightHGNN}: Distilling Hypergraph Neural Networks into {MLPs} for 100x Faster Inference},
  url = {https://openreview.net/forum?id=lHasEfGsXL},
  year = {2024}
}

@inproceedings{feng2024how,
  abstract = {Language models ({LMs}) can recall facts mentioned in context, as shown by their performance on reading comprehension tasks. When the context describes facts about more than one entity, the {LM} has to correctly bind attributes to their corresponding entity. We show, via causal experiments, that {LMs}' internal activations represent binding information by exhibiting appropriate binding {ID} vectors at the entity and attribute positions. We further show that binding {ID} vectors form a subspace and often transfer across tasks. Our results demonstrate that {LMs} learn interpretable strategies for representing symbolic knowledge in context, and that studying context activations is a fruitful direction for understanding {LM} cognition.},
  author = {Jiahai Feng and Jacob Steinhardt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024how.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=zb3b6oKO77},
  publisher = {OpenReview.net},
  title = {How do Language Models Bind Entities in Context?},
  url = {https://openreview.net/forum?id=zb3b6oKO77},
  year = {2024}
}

@inproceedings{feng2024knowledge,
  abstract = {By design, large language models ({LLMs}) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge. To this end, we propose Knowledge Card, a modular framework to plug in new factual and relevant knowledge into general-purpose {LLMs}. We first introduce knowledge cards -- specialized language models trained on corpora from specific domains and sources. Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base {LLM}. We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs. Finally, we propose two complementary integration approaches to augment the base {LLM} with the (relevant, factual) knowledge curated from the specialized {LMs}. Through extensive experiments, we demonstrate that Knowledge Card achieves state-of-the-art performance on six benchmark datasets. Ultimately, Knowledge Card framework enables dynamic synthesis and updates of knowledge from diverse domains. Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.},
  author = {Shangbin Feng and Weijia Shi and Yuyang Bai and Vidhisha Balachandran and Tianxing He and Yulia Tsvetkov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024knowledge.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=WbWtOYIzIK},
  publisher = {OpenReview.net},
  title = {Knowledge Card: Filling {LLMs}' Knowledge Gaps with Plug-in Specialized Language Models},
  url = {https://openreview.net/forum?id=WbWtOYIzIK},
  year = {2024}
}

@inproceedings{feng2024tree,
  abstract = {Cross Attention is a popular method for retrieving information from a set of context tokens for making predictions. At inference time, for each prediction, Cross Attention scans the full set of O(N) tokens. In practice, however, often only a small subset of tokens are required for good performance. Methods such as Perceiver IO are cheap at inference as they distill the information to a smaller-sized set of latent tokens L < N on which cross attention is then applied, resulting in only O(L) complexity. However, in practice, as the number of input tokens and the amount of information to distill increases, the number of latent tokens needed also increases significantly. In this work, we propose Tree Cross Attention ({TCA}) - a module based on Cross Attention that only retrieves information from a logarithmic O(log(N)) number of tokens for performing inference.},
  author = {Leo Feng and Frederick Tung and Hossein Hajimirsadeghi and Yoshua Bengio and Mohamed Osama Ahmed},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024tree.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Vw24wtSddM},
  publisher = {OpenReview.net},
  title = {Tree Cross Attention},
  url = {https://openreview.net/forum?id=Vw24wtSddM},
  year = {2024}
}

@inproceedings{feng2024embarrassingly,
  abstract = {Dataset distillation extracts a small set of synthetic training samples from a large dataset with the goal of achieving competitive performance on test data when trained on this sample. In this work, we tackle dataset distillation at its core by treating it directly as a bilevel optimization problem. Re-examining the foundational back-propagation through time method, we study the pronounced variance in the gradients, computational burden, and long-term dependencies. We introduce an improved method: Random Truncated Backpropagation Through Time ({RaT-BPTT}) to address them. {RaT-BPTT} incorporates a truncation coupled with a random window, effectively stabilizing the gradients and speeding up the optimization while covering long dependencies.},
  author = {Yunzhen Feng and Shanmukha Ramakrishna Vedantam and Julia Kempe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024embarrassingly.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PLoWVP7Mjc},
  publisher = {OpenReview.net},
  title = {Embarrassingly Simple Dataset Distillation},
  url = {https://openreview.net/forum?id=PLoWVP7Mjc},
  year = {2024}
}

@inproceedings{feng2024unveiling,
  abstract = {Prompts play a crucial role in guiding the responses of Large Language Models ({LLMs}). However, the intricate role of individual tokens in prompts, known as input saliency, in shaping the responses remains largely underexplored. Existing saliency methods either misalign with {LLM} generation objectives or rely heavily on linearity assumptions, leading to potential inaccuracies. To address this, we propose Token Distribution Dynamics ({TDD}), an elegantly simple yet remarkably effective approach to unveil and manipulate the role of prompts in generating {LLM} outputs. {TDD} leverages the robust interpreting capabilities of the language model head ({LM} head) to assess input saliency. It projects input tokens into the embedding space and then estimates their significance based on distribution dynamics over the vocabulary. We introduce three {TDD} variants: forward, backward, and bidirectional, each offering unique insights into token relevance. Extensive experiments reveal that the {TDD} surpasses state-of-the-art baselines with a big margin in elucidating the causal relationships between prompts and {LLM} outputs. Beyond mere interpretation, we apply {TDD} to two prompt manipulation tasks for controlled text generation: zero-shot toxic language suppression and sentiment steering. Empirical results underscore {TDD}'s proficiency in identifying both toxic and sentimental cues in prompts, subsequently mitigating toxicity or modulating sentiment in the generated content.},
  author = {Zijian Feng and Hanzhang Zhou and Zixiao Zhu and Junlang Qian and Kezhi Mao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/feng2024unveiling.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ap1ByuwQrX},
  publisher = {OpenReview.net},
  title = {Unveiling and Manipulating Prompt Influence in Large Language Models},
  url = {https://openreview.net/forum?id=ap1ByuwQrX},
  year = {2024}
}

@inproceedings{fernandezdecossiodiaz2024accelerated,
  abstract = {Sampling complex distributions is a fundamental challenge across numerous fields in physics, chemistry, and statistics. Parallel tempering (also called replica exchange Monte Carlo) is a popular approach for sampling multimodal distributions, which runs sequences of Markov chains at decreasing temperatures in parallel that can swap configurations. In this work, we apply similar parallel tempering ideas to Restricted Boltzmann Machines ({RBMs}), which are unsupervised architectures capable of learning complex, multimodal distributions. Inspired by Deep Tempering (an approach for deep belief networks), we propose to learn a stack of nested {RBMs} on top of the first {RBM}, using the representations of one {RBM} as data for the next level. We show that this hierarchical approach can significantly accelerate sampling from the base distribution and improve the exploration of multimodal distributions.},
  author = {Jorge Fernandez-de-Cossio-Diaz and Cl{\'e}ment Roussel and Simona Cocco and R{\'e}mi Monasson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fernandezdecossiodiaz2024accelerated.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07; Equal contribution: first two authors},
  pdf = {https://openreview.net/pdf?id=kXNJ48Hvw1},
  publisher = {OpenReview.net},
  title = {Accelerated Sampling with Stacked Restricted {Boltzmann} Machines},
  url = {https://openreview.net/forum?id=kXNJ48Hvw1},
  year = {2024}
}

@inproceedings{fesser2024effective,
  abstract = {Structural and Positional Encodings can significantly improve the performance of Graph Neural Networks in downstream tasks. Recent literature has begun to systematically investigate differences in the structural properties that these approaches encode, as well as performance trade-offs between them. However, the question of which structural properties yield the most effective encoding remains open. In this paper, we investigate this question from a geometric perspective. We propose a novel structural encoding based on discrete Ricci curvature (Local Curvature Profiles, short {LCP}). The main idea is that curvature information of edges away from the extremes of the curvature distribution, which is not being used by curvature-based rewiring methods, can be beneficial to {GNN} performance. We show that {LCP} consistently improves performance across a diverse set of datasets and {GNN} architectures, and further demonstrate that it can be combined with existing structural encodings to achieve even better results.},
  author = {Lukas Fesser and Melanie Weber},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fesser2024effective.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GIUjLsDP4Z},
  publisher = {OpenReview.net},
  title = {Effective Structural Encodings via Local Curvature Profiles},
  url = {https://openreview.net/forum?id=GIUjLsDP4Z},
  year = {2024}
}

@inproceedings{fifty2024contextaware,
  abstract = {Large Language Models like {ChatGPT} demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts visual meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label.},
  author = {Christopher Fifty and Dennis Duan and Ronald G. Junkins and Ehsan Amid and Jure Leskovec and Christopher Re and Sebastian Thrun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fifty2024contextaware.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lJYAkDVnRU},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Context-Aware Meta-Learning},
  url = {https://openreview.net/forum?id=lJYAkDVnRU},
  year = {2024}
}

@inproceedings{finkelstein2024mbr,
  abstract = {Recent research in decoding methods for Natural Language Generation ({NLG}) tasks has shown that {MAP} decoding is not optimal, because model probabilities do not always align with human preferences. Stronger decoding methods, including Quality Estimation ({QE}) reranking and Minimum Bayes' Risk ({MBR}) decoding, have since been proposed to mitigate the model-perplexity-vs-quality mismatch. While these decoding methods achieve state-of-the-art performance, they are prohibitively expensive to compute. In this work, we propose {MBR} finetuning and {QE} finetuning which distill the quality gains from these decoding methods at training time, while using an efficient decoding algorithm at inference time. Using the canonical {NLG} task of Neural Machine Translation ({NMT}), we show that even with self-training, these finetuning methods significantly outperform the base model. Moreover, when using an external {LLM} as a teacher model, these finetuning methods outperform finetuning on human-generated references.},
  author = {Mara Finkelstein and Subhajit Naskar and Mehdi Mirzazadeh and Apurva Shah and Markus Freitag},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/finkelstein2024mbr.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bkNx3O0sND},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{MBR} and {QE} Finetuning: Training-time Distillation of the Best and Most Expensive Decoding Methods},
  url = {https://openreview.net/forum?id=bkNx3O0sND},
  year = {2024}
}

@inproceedings{finlayson2024closing,
  abstract = {Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold.},
  author = {Matthew Finlayson and John Hewitt and Alexander Koller and Swabha Swayamdipta and Ashish Sabharwal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/finlayson2024closing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dONpC9GL1o},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Closing the Curious Case of Neural Text Degeneration},
  url = {https://openreview.net/forum?id=dONpC9GL1o},
  year = {2024}
}

@inproceedings{fisher2024pushing,
  abstract = {Mixup is a data augmentation strategy that employs convex combinations of training instances and their respective labels to augment the robustness and calibration of deep neural networks. Despite its widespread adoption, the nuanced mechanisms that underpin its success are not entirely understood. The observed phenomenon of Neural Collapse, where the last-layer activations and classifier of deep networks converge to a simplex equiangular tight frame ({ETF}), provides a compelling motivation to explore whether mixup induces alternative geometric configurations and whether those could explain its success. In this study, we delve into the last-layer activations of training data for deep networks subjected to mixup, aiming to uncover insights into its operational efficacy.},
  author = {Quinn LeBlanc Fisher and Haoming Meng and Vardan Papyan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fisher2024pushing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jTSKkcbEsj},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Pushing Boundaries: Mixup's Influence on Neural Collapse},
  url = {https://openreview.net/forum?id=jTSKkcbEsj},
  year = {2024}
}

@inproceedings{fleissner2024explaining,
  abstract = {Despite the growing popularity of explainable and interpretable machine learning, there is still surprisingly limited work on inherently interpretable clustering methods. Recently, there has been a surge of interest in explaining the classic k-means algorithm, leading to efficient algorithms that approximate k-means clusters using axis-aligned decision trees. However, interpretable variants of k-means have limited applicability in practice, where more flexible clustering methods are often needed to obtain useful partitions of the data. In this work, we investigate interpretable kernel clustering, and propose algorithms that construct decision trees to approximate the partitions induced by kernel k-means, a nonlinear extension of k-means.},
  author = {Maximilian Fleissner and Leena Chennuru Vankadara and Debarghya Ghoshdastidar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fleissner2024explaining.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FAGtjl7HOw},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Explaining Kernel Clustering via Decision Trees},
  url = {https://openreview.net/forum?id=FAGtjl7HOw},
  year = {2024}
}

@inproceedings{fourkioti2024camil,
  abstract = {Existing attention-based multiple instance learning ({MIL}) models used for analyzing Whole Slide Images ({WSIs}) in cancer diagnostics often overlook the contextual information of tumor and neighboring tiles, leading to misclassifications. {CAMIL} proposes a neural network architecture that incorporates neighbor-constrained attention to consider dependencies among tiles within a {WSI} and integrates contextual constraints as prior knowledge into the {MIL} model. {CAMIL} was evaluated on subtyping non-small cell lung cancer ({TCGA}-{NSCLC}) and detecting lymph node ({CAMELYON}16 and {CAMELYON}17) metastasis, achieving test {AUCs} of 97.5\%, 95.9\%, and 88.1\%, respectively, outperforming other state-of-the-art methods.},
  author = {Olga Fourkioti and Matt De Vries and Chen Jin and Daniel C. Alexander and Chris Bakal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fourkioti2024camil.pdf:pdf},
  note = {DBLP last modified: 2025-01-22},
  pdf = {https://openreview.net/pdf?id=rzBskAEmoc},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{CAMIL}: Context-Aware Multiple Instance Learning for Cancer Detection and Subtyping in Whole Slide Images},
  url = {https://openreview.net/forum?id=rzBskAEmoc},
  year = {2024}
}

@inproceedings{fraikin2024trep,
  abstract = {Multivariate time series present challenges to standard machine learning techniques, as they are often unlabeled, high dimensional, noisy, and contain missing data. T-Rep is a self-supervised method to learn time series representations at a timestep granularity. T-Rep learns vector embeddings of time alongside its feature extractor, to extract temporal features such as trend, periodicity, or distribution shifts from the signal. These time-embeddings are leveraged in pretext tasks, to incorporate smooth and fine-grained temporal dependencies in the representations, as well as reinforce robustness to missing data. T-Rep is evaluated on downstream classification, forecasting, and anomaly detection tasks where it outperforms existing self-supervised algorithms for time series.},
  author = {Archibald Fraikin and Adrien Bennetot and St√©phanie Allassonni√®re},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fraikin2024trep.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3y2TfP966N},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {T-Rep: Representation Learning for Time Series using Time-Embeddings},
  url = {https://openreview.net/forum?id=3y2TfP966N},
  year = {2024}
}

@inproceedings{frantar2024scaling,
  abstract = {The current standard approach to scaling neural network models is increasing the number of parameters while keeping the models dense. However, as models grow in size, this approach becomes increasingly infeasible due to computational and memory constraints. Sparse foundation models offer a more sustainable path to scale, but little is known about the scaling behavior of such models or about the optimal sparsity level for a given compute budget. In this work, we present the first scaling laws for sparsely-connected foundation models across model scales, sparsity levels, and training budgets. We identify the first scaling law describing the relationship between weight sparsity, number of non-zero parameters, and amount of training data.},
  author = {Elias Frantar and Carlos Riquelme Ruiz and Neil Houlsby and Dan Alistarh and Utku Evci},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/frantar2024scaling.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=i9K2ZWkYIP},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Scaling Laws for Sparsely-Connected Foundation Models},
  url = {https://openreview.net/forum?id=i9K2ZWkYIP},
  year = {2024}
}

@inproceedings{frantzen2024learning,
  abstract = {Computational models that process higher-order topological data, such as hypergraphs and simplicial complexes, can yield insights into problems where pair-wise relations are insufficient to capture the whole picture. While message-passing is a popular approach in this setting, these methods can become computationally expensive and are constrained in their expressive power. To address these limitations, we propose {SCRaWl}: a simplicial complex neural network based on random walks and fast {1D} convolutions. Due to our random walk-based design, the expressivity of our model is provably incomparable to that of existing message-passing simplicial neural networks, offering an alternative approach to learning on simplicial complexes.},
  author = {Florian Frantzen and Michael T. Schaub},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/frantzen2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OsGUnYOzii},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Learning From Simplicial Data Based on Random Walks and {1D} Convolutions},
  url = {https://openreview.net/forum?id=OsGUnYOzii},
  year = {2024}
}

@inproceedings{franzese2024minde,
  abstract = {Mutual Information ({MI}) is a fundamental measure of statistical dependence widely used in machine learning. However, estimating {MI} from data is challenging, particularly in high-dimensional settings. We present {MINDE}, a novel approach for {MI} estimation based on an original interpretation of the Girsanov theorem, which allows the use of score-based diffusion models to estimate the Kullback Leibler divergence between two densities as a difference between their score functions. The method is more accurate than the main alternatives from the literature, especially for challenging distributions, and enables the estimation of the entropy of random variables as a by-product.},
  author = {Giulio Franzese and Mustapha Bounoua and Pietro Michiardi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/franzese2024minde.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0kWd8SJq8d},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{MINDE}: Mutual Information Neural Diffusion Estimation},
  url = {https://openreview.net/forum?id=0kWd8SJq8d},
  year = {2024}
}

@inproceedings{franzmeyer2024select,
  abstract = {AI agents are commonly trained with large datasets of demonstrations of human behavior. However, not all behaviors are equally safe or desirable. Desired characteristics for an AI agent can be expressed by assigning desirability scores, which we assume are not assigned to individual behaviors but to collective trajectories. For example, in a dataset of vehicle interactions, these scores might relate to the number of incidents that occurred. We first assess the effect of each individual agent's behavior on the collective desirability score, e.g., assessing how likely an agent is to cause incidents. This allows us to selectively imitate agents with a positive effect, e.g., only imitating agents that are unlikely to cause incidents. To enable this, we propose the concept of an agent's Exchange Value, which quantifies an individual agent's contribution to the collective desirability score. The Exchange Value is the expected change in desirability score when substituting the agent for a randomly selected agent. We propose additional methods for estimating Exchange Values from real-world datasets, enabling us to learn desired imitation policies that outperform relevant baselines.},
  author = {Tim Franzmeyer and Edith Elkind and Philip Torr and Jakob Nicolaus Foerster and Joao F. Henriques},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/franzmeyer2024select.pdf:pdf},
  keywords = {multi-agent systems, imitation learning, credit assignment, learning from human data},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=L6crLU7MIE},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Select to Perfect: Imitating desired behavior from large multi-agent data},
  url = {https://openreview.net/forum?id=L6crLU7MIE},
  year = {2024}
}

@inproceedings{franzmeyer2024illusory,
  abstract = {Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of information-theoretic detectability constraints makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce Œµ-attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and of Œµ-bounded statistical detectability. We propose a novel dual ascent algorithm to learn such attacks end-to-end. Compared to existing attacks, we empirically find Œµ-attacks to be significantly harder to detect with automated methods, and a small study with human participants suggests they are similarly harder to detect for humans. Our findings suggest the need for better anomaly detectors, as well as effective hardware- and system-level defenses.},
  author = {Tim Franzmeyer and Stephen Marcus McAleer and Joao F. Henriques and Jakob Nicolaus Foerster and Philip Torr and Adel Bibi and Christian Schroeder de Witt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/franzmeyer2024illusory.pdf:pdf},
  keywords = {adversarial attacks, reinforcement learning, information theory, detectability, autonomous agents},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=F5dhGCdyYh},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Illusory Attacks: Information-theoretic detectability matters in adversarial attacks},
  url = {https://openreview.net/forum?id=F5dhGCdyYh},
  year = {2024}
}

@inproceedings{frauen2024neural,
  abstract = {Unobserved confounding is common in many applications, making causal inference from observational data challenging. As a remedy, causal sensitivity analysis is an important tool to draw causal conclusions under unobserved confounding with mathematical guarantees. We propose NeuralCSA, a neural framework for generalized causal sensitivity analysis that is compatible with a large class of sensitivity models, different treatment types, and different causal queries. The generality of NeuralCSA is achieved by learning a latent distribution shift that corresponds to a treatment intervention using two conditional normalizing flows.},
  author = {Dennis Frauen and Fergus Imrie and Alicia Curth and Valentyn Melnychuk and Stefan Feuerriegel and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/frauen2024neural.pdf:pdf},
  keywords = {causal inference, sensitivity analysis, unobserved confounding, normalizing flows, neural networks},
  note = {DBLP last modified: 2024-08-07; arXiv:2311.16026},
  pdf = {https://openreview.net/pdf?id=ikX6D1oM1c},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {A Neural Framework for Generalized Causal Sensitivity Analysis},
  url = {https://openreview.net/forum?id=ikX6D1oM1c},
  year = {2024}
}

@inproceedings{frey2024protein,
  abstract = {We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our Discrete Walk-Jump Sampling formalism combines the contrastive divergence training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the distributional conformity score to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100% of generated samples are successfully expressed and purified and 70% of functional designs show equal or improved binding affinity compared to known functional antibodies.},
  author = {Nathan C. Frey and Daniel Berenberg and Karina Zadorozhny and Joseph Kleinhenz and Julien Lafrance-Vanasse and Isidro Hotzel and Yan Wu and Stephen Ra and Richard Bonneau and Kyunghyun Cho and Andreas Loukas and Vladimir Gligorijevic and Saeed Saremi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/frey2024protein.pdf:pdf},
  keywords = {protein design, generative models, antibody discovery, discrete sampling, energy-based models, MCMC},
  note = {DBLP last modified: 2025-06-25; arXiv:2306.12360; Outstanding Paper Award ICLR 2024; Oral Presentation},
  pdf = {https://openreview.net/pdf?id=zMPHKOmQNb},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Protein Discovery with Discrete Walk-Jump Sampling},
  url = {https://openreview.net/forum?id=zMPHKOmQNb},
  year = {2024}
}

@inproceedings{fu2024theoretical,
  abstract = {Adversarial training (AT) is a canonical method for enhancing the robustness of deep neural networks (DNNs). Yet, recent studies show that AT suffers from robust overfitting, where long-term AT can be detrimental to DNN robustness. In this work, we provide a theoretical explanation for this phenomenon through the lens of neural tangent kernel (NTK). We non-trivially extend the NTK theory to adversarial training and prove that an adversarially trained wide DNN can be well approximated by a linearized DNN. For squared loss, we derive closed-form AT dynamics for the linearized DNN and reveal a new AT degeneration phenomenon: long-term AT causes a wide DNN to degenerate to one obtained without AT, thus causing robust overfitting. Based on our theoretical results, we design Adv-NTK, the first adversarial training algorithm for infinite-width DNNs.},
  author = {Shaopeng Fu and Di Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024theoretical.pdf:pdf},
  keywords = {adversarial training, neural tangent kernel, robust overfitting, wide neural networks, theoretical analysis},
  note = {DBLP last modified: 2024-08-07; arXiv:2310.06112},
  pdf = {https://openreview.net/pdf?id=1op5YGZu8X},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Theoretical Analysis of Robust Overfitting for Wide {DNN}s: An {NTK} Approach},
  url = {https://openreview.net/forum?id=1op5YGZu8X},
  year = {2024}
}

@inproceedings{fu2024convergence,
  abstract = {We provide the first theoretical guarantee for Bayesian bilevel optimization (BBO) for the prevalent bilevel framework combining Bayesian optimization at the outer level to tune hyperparameters, and inner-level stochastic gradient descent (SGD) for training the model. We prove sublinear regret bounds suggesting simultaneous convergence of the inner-level model parameters and outer-level hyperparameters to optimal configurations for generalization capability. A pivotal technical novelty in the proofs is modeling the excess risk of the SGD-trained parameters as evaluation noise during the optimization process. Our theory implies the inner unit horizon, defined as the number of SGD iterations, shapes the convergence behavior of BBO, suggesting practical guidance on configuring the inner unit horizon to enhance training efficiency and model performance.},
  author = {Shi Fu and Fengxiang He and Xinmei Tian and Dacheng Tao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024convergence.pdf:pdf},
  keywords = {Bayesian optimization, bilevel optimization, hyperparameter tuning, convergence analysis, regret bounds},
  note = {DBLP last modified: 2024-07-29; Spotlight Presentation},
  pdf = {https://openreview.net/pdf?id=fLXpXa7iiz},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Convergence of {B}ayesian Bilevel Optimization},
  url = {https://openreview.net/forum?id=fLXpXa7iiz},
  year = {2024}
}

@inproceedings{fu2024selfsupervised,
  abstract = {Speech quality estimation has recently undergone a paradigm shift from human-hearing expert designs to machine-learning models. However, current models rely mainly on supervised learning, which is time-consuming and expensive for label collection. To solve this problem, we propose VQScore, a self-supervised metric for evaluating speech based on the quantization error of a vector-quantized-variational autoencoder (VQ-VAE). The training of VQ-VAE relies on clean speech; hence, large quantization errors can be expected when the speech is distorted. To further improve correlation with real quality scores, domain knowledge of speech processing is incorporated into the model design. We found that the vector quantization mechanism could also be used for self-supervised speech enhancement (SE) model training. For SE, a novel self-distillation mechanism combined with adversarial training is introduced to improve the robustness of the encoder.},
  author = {Szu-Wei Fu and Kuo-Hsuan Hung and Yu Tsao and Yu-Chiang Frank Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024selfsupervised.pdf:pdf},
  keywords = {speech quality estimation, self-supervised learning, vector quantization, variational autoencoder, speech enhancement},
  note = {DBLP last modified: 2024-08-07; arXiv:2402.16321},
  pdf = {https://openreview.net/pdf?id=ale56Ya59q},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Self-Supervised Speech Quality Estimation and Enhancement Using Only Clean Speech},
  url = {https://openreview.net/forum?id=ale56Ya59q},
  year = {2024}
}

@inproceedings{fu2024featup,
  abstract = {Deep features often lack the spatial resolution to directly perform dense prediction tasks like segmentation and depth prediction because models aggressively pool information over large areas. We introduce FeatUp, a task- and model-agnostic framework to restore lost spatial information in deep features. We introduce two variants of FeatUp: one that guides features with high-resolution signal in a single forward pass, and one that fits an implicit model to a single image to reconstruct features at any resolution. Both approaches use a multi-view consistency loss with deep analogies to NeRFs. Our features retain their original semantics and can be swapped into existing applications to yield resolution and performance gains even without re-training. We show that FeatUp significantly improves performance on a variety of dense prediction tasks and outperforms alternative feature upsampling and image super-resolution approaches.},
  author = {Stephanie Fu and Mark Hamilton and Laura E. Brandt and Axel Feldmann and Zhoutong Zhang and William T. Freeman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024featup.pdf:pdf},
  keywords = {computer vision, feature upsampling, spatial resolution, dense prediction, multi-view consistency, implicit neural representations},
  note = {DBLP last modified: 2024-10-06; arXiv:2403.10516},
  pdf = {https://openreview.net/pdf?id=GkJiNn2QDF},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{FeatUp}: A Model-Agnostic Framework for Features at Any Resolution},
  url = {https://openreview.net/forum?id=GkJiNn2QDF},
  year = {2024}
}

@inproceedings{fu2024guiding,
  abstract = {Instruction-based image editing improves the controllability and flexibility of image manipulation via natural commands without elaborate descriptions or regional masks. However, human instructions are sometimes too brief for current methods to capture and follow. We investigate how multimodal large language models (MLLMs) facilitate edit instructions and present MLLM-Guided Image Editing (MGIE). MGIE learns to derive expressive instructions and provides explicit guidance. The editing model jointly captures this visual imagination and performs manipulation through end-to-end training. We evaluate various aspects of Photoshop-style modification, global photo optimization, and local editing. Extensive experimental results demonstrate that expressive instructions are crucial to instruction-based image editing, and MGIE can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.},
  author = {Tsu-Jui Fu and Wenze Hu and Xianzhi Du and William Yang Wang and Yinfei Yang and Zhe Gan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024guiding.pdf:pdf},
  keywords = {image editing, multimodal large language models, instruction following, computer vision, natural language processing},
  note = {DBLP last modified: 2024-07-29; arXiv:2309.17102},
  pdf = {https://openreview.net/pdf?id=S1RKWSyZ2Y},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Guiding Instruction-based Image Editing via Multimodal Large Language Models},
  url = {https://openreview.net/forum?id=S1RKWSyZ2Y},
  year = {2024}
}

@inproceedings{fu2024vcr,
  abstract = {Graph transformer has been proven as an effective graph learning method for its adoption of attention mechanism that is capable of capturing expressive representations from complex topological and feature information of graphs. However, conventional graph transformers perform dense attention for every pair of nodes, resulting in quadratic computational costs that are unaffordable for large-scale graph data. We address this issue through two key innovations: PPR tokenization that assigns each node a token list sampled by personalized PageRank, enabling mini-batch training by decoupling model training from complex graph topology; and virtual connections that rewire graphs through structure- and content-based super nodes to encode local and global contexts, long-range interaction, and heterophilous information. We formalize our Virtual Connection Ranking based Graph Transformer (VCR-Graphormer) with O(m+k log k) complexity compared to O(n¬≥) of previous works.},
  author = {Dongqi Fu and Zhigang Hua and Yan Xie and Jin Fang and Si Zhang and Kaan Sancak and Hao Wu and Andrey Malevich and Jingrui He and Bo Long},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024vcr.pdf:pdf},
  keywords = {graph transformers, personalized PageRank, mini-batch training, virtual connections, graph neural networks, scalable graph learning},
  note = {DBLP last modified: 2024-08-07; arXiv:2403.16030},
  pdf = {https://openreview.net/pdf?id=SUUrkC3STJ},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{VCR}-{G}raphormer: A Mini-batch Graph Transformer via Virtual Connections},
  url = {https://openreview.net/forum?id=SUUrkC3STJ},
  year = {2024}
}

@inproceedings{fu2024flashfftconv,
  abstract = {Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform ({FFT}) -- which allows long convolutions to run in {O}({N} log {N}) time in sequence length {N} but has poor hardware utilization. {FlashFFTConv} uses a matrix decomposition that computes the {FFT} using matrix multiply units and enables kernel fusion for long sequences, reducing {I}/{O}. The key insight is using a {Monarch} decomposition of the {FFT} that allows fusing the steps of the {FFT} convolution -- even for long sequences -- and enables efficient use of tensor cores available on modern {GPU}s. {FlashFFTConv} speeds up exact {FFT} convolutions by up to 7.93√ó over {PyTorch} and achieves up to 4.4√ó speedup end-to-end. Given the same compute budget, {FlashFFTConv} allows {Hyena}-{GPT}-s to achieve 2.3 points better perplexity on the {PILE} and {M2}-{BERT}-base to achieve 3.3 points higher {GLUE} score -- matching models with twice the parameter count.},
  author = {Daniel Y. Fu and Hermann Kumbong and Eric Nguyen and Christopher R{\'e}},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024flashfftconv.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gPKTTAfYBp},
  publisher = {OpenReview.net},
  title = {{FlashFFTConv}: Efficient Convolutions for Long Sequences with {Tensor Cores}},
  url = {https://openreview.net/forum?id=gPKTTAfYBp},
  year = {2024}
}

@inproceedings{fu2024reasonableness,
  abstract = {Multilingual large language models trained on non-parallel data yield impressive translation capabilities. Existing studies demonstrate that incidental sentence-level bilingualism within pre-training data contributes to the {LLM}s' translation abilities. However, it has also been observed that {LLM}s' translation capabilities persist even when incidental sentence-level bilingualism are excluded from the training corpus. This study comprehensively investigates the unreasonable effectiveness and the underlying mechanism for {LLM}s' translation abilities, specifically addressing the question why large language models learn to translate without parallel data, using the {BLOOM} model series as a representative example. Through extensive experiments, our findings suggest the existence of unintentional bilingualism in the pre-training corpus, especially word alignment data significantly contributes to the large language model's acquisition of translation ability.},
  author = {Tingchen Fu and Lemao Liu and Deng Cai and Guoping Huang and Shuming Shi and Rui Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024reasonableness.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3KDbIWT26J},
  publisher = {OpenReview.net},
  title = {The Reasonableness Behind Unreasonable Translation Capability of Large Language Model},
  url = {https://openreview.net/forum?id=3KDbIWT26J},
  year = {2024}
}

@inproceedings{fu20243d,
  abstract = {Most existing neural field methods train a separate network from scratch for each individual scene, which is not scalable, inefficient, and unable to yield good results given limited views. We introduce Neural Fields incorporating scene Priors ({NFPs}) to address these limitations. The {NFP} network maps any single-view {RGB}-{D} image into signed distance and radiance values. A complete scene can be reconstructed by merging individual frames in the volumetric space without a fusion module, which provides better flexibility. The key insight is to incorporate geometry and appearance priors learned from large-scale datasets during training, enabling fast adaptation to the reconstruction of a new scene with fewer views. We parameterize the distance and radiance fields as functions of the extracted features. The {NFP} network is trained in two stages: first learning the geometric prior from {RGB}-{D} pairs, then learning the appearance prior from {RGB} images. Experiments show that {NFP} can reconstruct scenes with as few as 2-5 views, significantly outperforming existing methods in both reconstruction quality and efficiency.},
  author = {Yang Fu and Shalini De Mello and Xueting Li and Amey Kulkarni and Jan Kautz and Xiaolong Wang and Sifei Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu20243d.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Nu7dDaVF5a},
  publisher = {OpenReview.net},
  title = {{3D} Reconstruction with Generalizable Neural Fields using Scene Priors},
  url = {https://openreview.net/forum?id=Nu7dDaVF5a},
  year = {2024}
}

@inproceedings{fu2024nemesis,
  abstract = {Recent advancements in vision-language models ({VLMs}) have demonstrated remarkable capabilities in multimodal understanding and generation. However, the practice of soft prompt tuning in {VLMs} has revealed a phenomenon we term the "Low-Norm Effect," where reducing the norms of certain learned prompts occasionally enhances performance, while increasing them often degrades it. This paper is the first to systematically investigate the role of norms of soft-prompt vectors in {VLMs}. Through extensive corruption experiments, we uncover this Low-Norm Effect and propose a novel method named "Normalizing the soft-prompt vectors of vision-language models (Nemesis)" to normalize soft-prompt vectors in {VLMs}. Our approach demonstrates improved domain generalization abilities and offers valuable insights for future research in soft-prompt tuning.},
  author = {Shuai Fu and Xiequn Wang and Qiushi Huang and Yu Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/fu2024nemesis.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=zmJDzPh1Dm},
  publisher = {OpenReview.net},
  title = {{Nemesis}: Normalizing the Soft-prompt Vectors of Vision-Language Models},
  url = {https://openreview.net/forum?id=zmJDzPh1Dm},
  year = {2024}
}

@inproceedings{furusawa2024mean,
  abstract = {Most existing metric learning methods suffer from high training complexity due to the combinatorial nature of pair-based loss functions. By adapting mean field theory for deep metric learning, we develop an approach to design classification-based loss functions from pair-based ones, which can be considered complementary to the proxy-based approach. Applying the mean field theory to two pair-based loss functions, we derive two new loss functions: {MeanFieldContrastive} and {MeanFieldClassWiseMultiSimilarity}. Experimental results demonstrate that our approach outperforms conventional pair-based methods and achieves competitive performance with recent proxy-based methods while requiring significantly less computational resources during training.},
  author = {Takuya Furusawa},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/furusawa2024mean.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZPdZLlNXSm},
  publisher = {OpenReview.net},
  title = {Mean Field Theory in Deep Metric Learning},
  url = {https://openreview.net/forum?id=ZPdZLlNXSm},
  year = {2024}
}

@inproceedings{furuta2024multimodal,
  abstract = {Web agents powered by foundation models hold great promise for automating web-based tasks, but their development is challenging due to the dependence on billions of exploratory interactions via online reinforcement learning and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. We propose {WebGUM}, an instruction-following multimodal agent that observes both webpage screenshots and {HTML} pages and outputs web navigation actions, such as click and type. {WebGUM} is trained by jointly finetuning an instruction-finetuned language model and a vision encoder with temporal and local perception on a large corpus of demonstrations. This recipe improves the agent's ability of grounded multimodal perception, {HTML} comprehension, and multi-step reasoning, outperforming prior works by a significant margin. On {MiniWoB}, we improve over the previous best offline methods by more than 45.8%, even outperforming online-finetuned {SoTA}, humans, and {GPT}-4-based agent. On the {WebShop} benchmark, our 3-billion-parameter model achieves superior performance to the existing {SoTA}, {PaLM}-540{B}.},
  author = {Hiroki Furuta and Kuang-Huei Lee and Ofir Nachum and Yutaka Matsuo and Aleksandra Faust and Shixiang Shane Gu and Izzeddin Gur},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/furuta2024multimodal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=efFmBWioSc},
  publisher = {OpenReview.net},
  title = {Multimodal Web Navigation with Instruction-Finetuned Foundation Models},
  url = {https://openreview.net/forum?id=efFmBWioSc},
  year = {2024}
}

@inproceedings{gadgil2024estimating,
  abstract = {Dynamic feature selection, where we sequentially query features to make accurate predictions with a minimal budget, is a promising paradigm to reduce feature acquisition costs and provide transparency into a model's predictions. The problem is challenging, however, as it requires both predicting with arbitrary feature sets and learning a policy to identify valuable selections. We take an information-theoretic perspective and prioritize features based on their mutual information with the response variable. The main challenge is implementing this policy, and we design a new approach that estimates the mutual information in a discriminative rather than generative fashion. Building on our approach, we introduce several further improvements: allowing variable feature budgets across samples, enabling non-uniform feature costs, incorporating prior information, and exploring modern architectures to handle partial inputs.},
  author = {Soham Gadgil and Ian Connick Covert and Su-In Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gadgil2024estimating.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Oju2Qu9jvn},
  publisher = {OpenReview.net},
  title = {Estimating Conditional Mutual Information for Dynamic Feature Selection},
  url = {https://openreview.net/forum?id=Oju2Qu9jvn},
  year = {2024}
}

@inproceedings{gadhikar2024masks,
  abstract = {Learning Rate Rewinding ({LRR}) has been established as a strong variant of Iterative Magnitude Pruning ({IMP}) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how {LRR} excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. In this work, we show that the ability of {LRR} to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. We prove in a simplified single hidden neuron setting that {LRR} succeeds in more cases than {IMP}, as it can escape initially problematic sign configurations.},
  author = {Advait Harshal Gadhikar and Rebekka Burkholz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gadhikar2024masks.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=qODvxQ8TXW},
  publisher = {OpenReview.net},
  title = {Masks, Signs, And Learning Rate Rewinding},
  url = {https://openreview.net/forum?id=qODvxQ8TXW},
  year = {2024}
}

@inproceedings{gan2024instructcv,
  abstract = {We develop a unified language interface for computer vision tasks that abstracts away task specific design choices and enables task execution by following natural language instructions. Our approach involves casting multiple computer vision tasks as text-to-image generation problems. The text represents an instruction describing the task, and the resulting image is a visually-encoded task output. To train our model, we pool commonly-used computer vision datasets covering a range of tasks, including segmentation, object detection, depth estimation, and classification. We use a large language model to paraphrase prompt templates that convey the specific tasks to be conducted on each image, creating a multi-modal and multi-task training dataset comprising input and output images along with annotated instructions. Following the {InstructPix2Pix} architecture, we apply instruction-tuning to a text-to-image diffusion model using our constructed dataset, steering its functionality from a generative model to an instruction-guided multi-task vision learner. Experiments demonstrate that our model, dubbed {InstructCV}, performs competitively compared to other generalist and task-specific vision models. Moreover, it exhibits compelling generalization capabilities to unseen data, categories, and user instructions.},
  author = {Yulu Gan and Sungwoo Park and Alexander Schubert and Anthony Philippakis and Ahmed M. Alaa},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gan2024instructcv.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Nu9mOSq7eH},
  publisher = {OpenReview.net},
  title = {{InstructCV}: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists},
  url = {https://openreview.net/forum?id=Nu9mOSq7eH},
  year = {2024}
}

@inproceedings{gan2024diving,
  abstract = {Semantic segmentation models exhibit subpar performance when faced with domain gaps, class imbalances, and variations in object scales and contexts, primarily due to pixel-level variance. This paper analyzes the causes of pixel-level variance and introduces the concept of "pixel learning" to concentrate on the tailored learning process of pixels to handle the pixel-level variance, enhancing the per-pixel recognition capability of segmentation models. We propose a pure pixel-level learning framework called {PiXL}, which consists of a pixel partition module to divide pixels into sub-domains, a prototype generation and selection module to prepare targets for subsequent alignment, and a pixel alignment module to guarantee pixel feature consistency intra-/inter-images, and inter-domains. Extensive evaluations across multiple learning paradigms, including unsupervised domain adaptation and semi-/fully-supervised segmentation, show that {PiXL} outperforms state-of-the-art performances, especially when annotated images are scarce.},
  author = {Chen Gan and Zihao Yin and Kelei He and Yang Gao and Junfeng Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gan2024diving.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=KBo7Z5aTV0},
  publisher = {OpenReview.net},
  title = {Diving Segmentation Model into Pixels},
  url = {https://openreview.net/forum?id=KBo7Z5aTV0},
  year = {2024}
}

@inproceedings{gandelsman2024interpreting,
  abstract = {We investigate the {CLIP} image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use {CLIP}'s text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within {CLIP}. Finally, we use this understanding to remove spurious features from {CLIP} and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.},
  author = {Yossi Gandelsman and Alexei A. Efros and Jacob Steinhardt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gandelsman2024interpreting.pdf:pdf},
  note = {{ICLR} 2024 (Oral). {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=5Ca9sSzuDp},
  publisher = {OpenReview.net},
  title = {Interpreting {CLIP}'s Image Representation via Text-Based Decomposition},
  url = {https://openreview.net/forum?id=5Ca9sSzuDp},
  year = {2024}
}

@inproceedings{gani2024llm,
  abstract = {Diffusion-based generative models have significantly advanced text-to-image generation but encounter challenges when processing lengthy and intricate text prompts describing complex scenes with multiple objects. While excelling in generating images from short, single-object descriptions, these models often struggle to faithfully capture all the nuanced details within longer and more elaborate textual inputs. In response, we present a novel approach leveraging Large Language Models ({LLMs}) to extract critical components from text prompts, including bounding box coordinates for foreground objects, detailed textual descriptions for individual objects, and a succinct background context. These components form the foundation of our layout-to-image generation model, which operates in two phases. The initial Global Scene Generation utilizes object layouts and background context to create an initial scene but often falls short in faithfully representing object characteristics as specified in the prompts. To address this limitation, we introduce an Iterative Refinement Scheme that iteratively evaluates and refines box-level content to align them with their textual descriptions, recomposing objects as needed to ensure consistency. Our evaluation on complex prompts featuring multiple objects demonstrates a substantial improvement in recall compared to baseline diffusion models. This is further validated by a user study, underscoring the efficacy of our approach in generating coherent and detailed scenes from intricate textual inputs.},
  author = {Hanan Gani and Shariq Farooq Bhat and Muzammal Naseer and Salman Khan and Peter Wonka},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gani2024llm.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mNYF0IHbRy},
  publisher = {OpenReview.net},
  title = {{LLM} Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts},
  url = {https://openreview.net/forum?id=mNYF0IHbRy},
  year = {2024}
}

@inproceedings{gao2024kw,
  abstract = {Recent studies have shown competitive performance in protein inverse folding, while most of them disregard the importance of predictive confidence, fail to cover the vast protein space, and do not incorporate common protein knowledge. Given the great success of pretrained models on diverse protein-related tasks and the fact that recovery is highly correlated with confidence, we wonder whether this knowledge can push the limits of protein design further. As a solution, we propose a knowledge-aware module that refines low-quality residues. We also introduce a memory-retrieval mechanism to save more than 50\% of the training time. We extensively evaluate our proposed method on the {CATH}, {TS50}, {TS500}, and {PDB} datasets and our results show that our {KW}-Design method outperforms the previous {PiFold} method by approximately 9\% on the {CATH} dataset. {KW}-Design is the first method that achieves 60+\% recovery on all these benchmarks.},
  author = {Zhangyang Gao and Cheng Tan and Xingran Chen and Yijie Zhang and Jun Xia and Siyuan Li and Stan Z. Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024kw.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2024-11-22},
  pdf = {https://openreview.net/pdf?id=mpqMVWgqjn},
  publisher = {OpenReview.net},
  title = {{KW}-Design: Pushing the Limit of Protein Design via Knowledge Refinement},
  url = {https://openreview.net/forum?id=mpqMVWgqjn},
  year = {2024}
}

@inproceedings{gao2024pbadet,
  abstract = {The detection of human parts (e.g., hands, face) and their correct association with individuals is an essential task, e.g., for ubiquitous human-machine interfaces and action recognition. Traditional methods often employ multi-stage processes, rely on cumbersome anchor-based systems, or do not scale well to larger part sets. This paper presents {PBADet}, a novel one-stage, anchor-free approach for part-body association detection. Building upon the anchor-free object representation across multi-scale feature maps, we introduce a singular part-to-body center offset that effectively encapsulates the relationship between parts and their parent bodies. Our design is inherently versatile and capable of managing multiple parts-to-body associations without compromising on detection accuracy or robustness. Comprehensive experiments on various datasets underscore the efficacy of our approach, which not only outperforms existing state-of-the-art techniques but also offers a more streamlined and efficient solution to the part-body association challenge.},
  author = {Zhongpai Gao and Huayi Zhou and Abhishek Sharma and Meng Zheng and Benjamin Planche and Terrence Chen and Ziyan Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024pbadet.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2025-04-16},
  pdf = {https://openreview.net/pdf?id=pPh9p8anUi},
  publisher = {OpenReview.net},
  title = {{PBADet}: A One-Stage Anchor-Free Approach for Part-Body Association},
  url = {https://openreview.net/forum?id=pPh9p8anUi},
  year = {2024}
}

@inproceedings{gao2024inducing,
  abstract = {Large vision-language models ({VLMs}) such as {GPT}-4 have achieved exceptional performance across various multi-modal tasks. However, the deployment of {VLMs} necessitates substantial energy consumption and computational resources. Once attackers maliciously induce high energy consumption and latency time (energy-latency cost) during inference of {VLMs}, it will exhaust computational resources. In this paper, we explore this attack surface about availability of {VLMs} and aim to induce high energy-latency cost during inference of {VLMs}. We find that high energy-latency cost during inference of {VLMs} can be manipulated by maximizing the length of generated sequences. To this end, we propose verbose images, with the goal of crafting an imperceptible perturbation to induce {VLMs} to generate long sentences during inference. Concretely, we design three loss objectives. First, a loss is proposed to delay the occurrence of end-of-sequence ({EOS}) token, where {EOS} token is a signal for {VLMs} to stop generating further tokens. Moreover, an uncertainty loss and a token diversity loss are proposed to increase the uncertainty over each generated token and the diversity among all tokens of the whole generated sequence, respectively, which can break output dependency at token-level and sequence-level. Furthermore, a temporal weight adjustment algorithm is proposed, which can effectively balance these losses. Extensive experiments demonstrate that our verbose images can increase the length of generated sequences by 7.87$\times$ and 8.56$\times$ compared to original images on {MS}-{COCO} and {ImageNet} datasets, which presents potential challenges for various applications.},
  author = {Kuofeng Gao and Yang Bai and Jindong Gu and Shu-Tao Xia and Philip Torr and Zhifeng Li and Wei Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024inducing.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BteuUysuXX},
  publisher = {OpenReview.net},
  title = {Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images},
  url = {https://openreview.net/forum?id=BteuUysuXX},
  year = {2024}
}

@inproceedings{gao2024variational,
  abstract = {Estimating treatment effects has numerous real-world applications in various fields, such as epidemiology and political science. While much attention has been devoted to addressing the challenge using fully observational data, there has been comparatively limited exploration of this issue in cases when the treatment is not directly observed. In this paper, we tackle this problem by developing a general variational framework, which is flexible to integrate with advanced neural network-based approaches, to identify the average dose-response function ({ADRF}) with the continuously valued error-contaminated treatments. In this model, we leverage a learnable density estimation neural network to derive its prior distribution conditioned on covariates. This module also doubles as a generalized propensity score estimator, effectively mitigating selection bias arising from observed confounding variables. Subsequently, we calculate the posterior distribution of the treatment, taking into account the observed measurement and outcome. To mitigate the impact of treatment error, we introduce a re-parametrized treatment value, replacing the error-affected one, to make more accurate predictions regarding the outcome. To demonstrate the adaptability of our framework, we incorporate two state-of-the-art {ADRF} estimation methods and rigorously assess its efficacy through extensive simulations and experiments using semi-synthetic data.},
  author = {Erdun Gao and Howard D. Bondell and Wei Huang and Mingming Gong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024variational.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=S46Knicu56},
  publisher = {OpenReview.net},
  title = {A Variational Framework for Estimating Continuous Treatment Effects with Measurement Error},
  url = {https://openreview.net/forum?id=S46Knicu56},
  year = {2024}
}

@inproceedings{gao2024deep,
  abstract = {{AlphaFold} can be used for both single-chain and multi-chain protein structure prediction, while the latter becomes extremely challenging as the number of chains increases. In this work, by taking each chain as a node and assembly actions as edges, we show that an acyclic undirected connected graph can be used to predict the structure of multi-chain protein complexes (a.k.a., protein complex modelling, {PCM}). However, there are still two challenges: 1) The huge combinatorial optimization space of $N^{N-2}$ ($N$ is the number of chains) for the {PCM} problem can easily lead to high computational cost. 2) The scales of protein complexes exhibit distribution shift due to variance in chain numbers, which calls for the generalization in modelling complexes of various chains. To address these challenges, we propose {GAPN}, a Generative Adversarial Policy Network powered by domain-specific rewards and adversarial loss through policy gradient for automatic {PCM} prediction. Specifically, {GAPN} learns to efficiently search through the immense assembly space and optimize the direct docking reward through policy gradient. Importantly, we design an adversarial reward function to enhance the receptive field of our model. In this way, {GAPN} will simultaneously focus on a specific batch of complexes and the global assembly rules learned from complexes with varied chain numbers. Empirically, we have achieved both significant accuracy (measured by {RMSD} and {TM}-Score) and efficiency improvements compared to leading {PCM} softwares.},
  author = {Ziqi Gao and Tao Feng and Jiaxuan You and Chenyi Zi and Yan Zhou and Chen Zhang and Jia Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024deep.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2025-02-11},
  pdf = {https://openreview.net/pdf?id=4MsfQ2H0lP},
  publisher = {OpenReview.net},
  title = {Deep Reinforcement Learning for Modelling Protein Complexes},
  url = {https://openreview.net/forum?id=4MsfQ2H0lP},
  year = {2024}
}

@inproceedings{gao2024trajectory,
  abstract = {In the realm of reinforcement learning ({RL}), off-policy evaluation ({OPE}) holds a pivotal position, especially in high-stake human-involved scenarios such as e-learning and healthcare. Applying {OPE} to these domains is often challenging with scarce and underrepresentative offline training trajectories. Data augmentation has been a successful technique to enrich training data. However, directly employing existing data augmentation methods to {OPE} may not be feasible, due to the Markovian nature within the offline trajectories and the desire for generalizability across diverse target policies. In this work, we propose an offline trajectory augmentation approach to specifically facilitate {OPE} in human-involved scenarios. We propose sub-trajectory mining to extract potentially valuable sub-trajectories from offline data, and diversify the behaviors within those sub-trajectories by varying coverage of the state-action space. Our work was empirically evaluated in a wide array of environments, encompassing both simulated scenarios and real-world domains like robotic control, healthcare, and e-learning, where the training trajectories include varying levels of coverage of the state-action space. By enhancing the performance of a variety of {OPE} methods, our work offers a promising path forward for tackling {OPE} challenges in situations where data may be limited or underrepresentative.},
  author = {Ge Gao and Qitong Gao and Xi Yang and Song Ju and Miroslav Pajic and Min Chi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024trajectory.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eMNN0wIyVw},
  publisher = {OpenReview.net},
  title = {On Trajectory Augmentations for Off-Policy Evaluation},
  url = {https://openreview.net/forum?id=eMNN0wIyVw},
  year = {2024}
}

@inproceedings{gao2024econtrol,
  abstract = {Modern distributed training relies heavily on communication compression to reduce the communication overhead. In this work, we study algorithms employing a popular class of contractive compressors in order to reduce communication overhead. However, the naive implementation often leads to unstable convergence or even exponential divergence due to the compression bias. Error Compensation ({EC}) is an extremely popular mechanism to mitigate the aforementioned issues during the training of models enhanced by contractive compression operators. Compared to the effectiveness of {EC} in the data homogeneous regime, the understanding of the practicality and theoretical foundations of {EC} in the data heterogeneous regime is limited. Existing convergence analyses typically rely on strong assumptions such as bounded gradients, bounded data heterogeneity, or large batch accesses, which are often infeasible in modern Machine Learning Applications. We resolve the majority of current issues by proposing {EControl}, a novel mechanism that can regulate error compensation by controlling the strength of the feedback signal. We prove fast convergence for {EControl} in standard strongly convex, general convex, and nonconvex settings without any additional assumptions on the problem or data heterogeneity. We conduct extensive numerical evaluations to illustrate the efficacy of our method and support our theoretical findings.},
  author = {Yuan Gao and Rustem Islamov and Sebastian U. Stich},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024econtrol.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lsvlvWB9vz},
  publisher = {OpenReview.net},
  title = {{EControl}: Fast Distributed Optimization with Compression and Error Control},
  url = {https://openreview.net/forum?id=lsvlvWB9vz},
  year = {2024}
}

@inproceedings{gao2024selfsupervised,
  abstract = {Pocket representations play a vital role in various biomedical applications, such as druggability estimation, ligand affinity prediction, and de novo drug design. While existing geometric features and pretrained representations have demonstrated promising results, they usually treat pockets independent of ligands, neglecting the fundamental interactions between them. However, the limited pocket-ligand complex structures available in the {PDB} database (less than 100 thousand non-redundant pairs) hampers large-scale pretraining endeavors for interaction modeling. To address this constraint, we propose a novel pocket pretraining approach that leverages knowledge from high-resolution atomic protein structures, assisted by highly effective pretrained small molecule representations. By segmenting protein structures into drug-like fragments and their corresponding pockets, we obtain a reasonable simulation of ligand-receptor interactions, resulting in the generation of over 5 million complexes. Subsequently, the pocket encoder is trained in a contrastive manner to align with the representation of pseudo-ligand furnished by some pretrained small molecule encoders. Our method, named {ProFSA}, achieves state-of-the-art performance across various tasks, including pocket druggability prediction, pocket matching, and ligand binding affinity prediction. Notably, {ProFSA} surpasses other pretraining methods by a substantial margin. Moreover, our work opens up a new avenue for mitigating the scarcity of protein-ligand complex data through the utilization of high-quality and diverse protein structure databases.},
  author = {Bowen Gao and Yinjun Jia and Yuanle Mo and Yuyan Ni and Wei-Ying Ma and Zhi-Ming Ma and Yanyan Lan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024selfsupervised.pdf:pdf},
  note = {{ICLR} 2024 (Poster). {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uMAujpVi9m},
  publisher = {OpenReview.net},
  title = {Self-supervised Pocket Pretraining via Protein Fragment-Surroundings Alignment},
  url = {https://openreview.net/forum?id=uMAujpVi9m},
  year = {2024}
}

@inproceedings{gao2024sublinear,
  abstract = {Adversarial training is a widely used strategy for making neural networks resistant to adversarial perturbations. For a neural network of width $m$, $n$ input training data in $d$ dimension, it takes $\Omega(mnd)$ time cost per training iteration for the forward and backward computation. In this paper we analyze the convergence guarantee of adversarial training procedure on a two-layer neural network with shifted ReLU activation, and shows that only $o(m)$ neurons will be activated for each input data per iteration. Furthermore, we develop an algorithm for adversarial training with time cost $o(m n d)$ per iteration by applying half-space reporting data structure.},
  author = {Yeqi Gao and Lianke Qin and Zhao Song and Yitan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024sublinear.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=N2WchST43h},
  publisher = {OpenReview.net},
  title = {A Sublinear Adversarial Training Algorithm},
  url = {https://openreview.net/forum?id=N2WchST43h},
  year = {2024}
}

@inproceedings{gao2024protein,
  author = {Ziqi Gao and Xiangguo Sun and Zijing Liu and Yu Li and Hong Cheng and Jia Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024protein.pdf:pdf},
  note = {DBLP last modified: 2025-02-18},
  pdf = {https://openreview.net/pdf?id=OHpvivXrQr},
  publisher = {OpenReview.net},
  title = {Protein Multimer Structure Prediction via Prompt Learning},
  url = {https://openreview.net/forum?id=OHpvivXrQr},
  year = {2024}
}

@inproceedings{gao2024sparseformer,
  author = {Ziteng Gao and Zhan Tong and Limin Wang and Mike Zheng Shou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gao2024sparseformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2pvECsmld3},
  publisher = {OpenReview.net},
  title = {{SparseFormer}: Sparse Visual Recognition via Limited Latent Tokens},
  url = {https://openreview.net/forum?id=2pvECsmld3},
  year = {2024}
}

@inproceedings{garg2024ticclip,
  author = {Saurabh Garg and Mehrdad Farajtabar and Hadi Pouransari and Raviteja Vemulapalli and Sachin Mehta and Oncel Tuzel and Vaishaal Shankar and Fartash Faghri},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/garg2024ticclip.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TLADT8Wrhn},
  publisher = {OpenReview.net},
  title = {{TiC-CLIP}: Continual Training of {CLIP} Models},
  url = {https://openreview.net/forum?id=TLADT8Wrhn},
  year = {2024}
}

@inproceedings{garimella2024newborn,
  author = {Manju Garimella and Denizhan Pak and Justin N. Wood and Samantha Marie Waters Wood},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/garimella2024newborn.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qhkEOCcVX9},
  publisher = {OpenReview.net},
  title = {A Newborn Embodied Turing Test for Comparing Object Segmentation Across Animals and Machines},
  url = {https://openreview.net/forum?id=qhkEOCcVX9},
  year = {2024}
}

@inproceedings{garov2024hiding,
  author = {Kostadin Garov and Dimitar Iliev Dimitrov and Nikola Jovanovic and Martin T. Vechev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/garov2024hiding.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=krx55l2A6G},
  publisher = {OpenReview.net},
  title = {Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated Learning},
  url = {https://openreview.net/forum?id=krx55l2A6G},
  year = {2024}
}

@inproceedings{gastpar2024fantastic,
  author = {Michael Gastpar and Ido Nachum and Jonathan Shafer and Thomas Weinberger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gastpar2024fantastic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NkmJotfL42},
  publisher = {OpenReview.net},
  title = {Fantastic Generalization Measures are Nowhere to be Found},
  url = {https://openreview.net/forum?id=NkmJotfL42},
  year = {2024}
}

@inproceedings{gat2024faithful,
  author = {Yair Ori Gat and Nitay Calderon and Amir Feder and Alexander Chapanin and Amit Sharma and Roi Reichart},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gat2024faithful.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UMfcdRIotC},
  publisher = {OpenReview.net},
  title = {Faithful Explanations of Black-box {NLP} Models Using {LLM}-generated Counterfactuals},
  url = {https://openreview.net/forum?id=UMfcdRIotC},
  year = {2024}
}

@inproceedings{ge2024model,
  author = {Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ge2024model.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; oral presentation},
  pdf = {https://openreview.net/pdf?id=uNrFpDPMyo},
  publisher = {OpenReview.net},
  title = {Model Tells You What to Discard: Adaptive {KV} Cache Compression for {LLMs}},
  url = {https://openreview.net/forum?id=uNrFpDPMyo},
  year = {2024}
}

@inproceedings{ge2024provable,
  author = {Jiawei Ge and Shange Tang and Jianqing Fan and Chi Jin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ge2024provable.pdf:pdf},
  note = {DBLP last modified: 2025-06-13},
  pdf = {https://openreview.net/pdf?id=rmXXKxQpOR},
  publisher = {OpenReview.net},
  title = {On the Provable Advantage of Unsupervised Pretraining},
  url = {https://openreview.net/forum?id=rmXXKxQpOR},
  year = {2024}
}

@inproceedings{ge2024maximum,
  abstract = {We address a key challenge of modern machine learning systems: achieving Out-of-Distribution (OOD) generalization -- generalizing to target data whose distribution differs from that of source data. Despite its significant importance, the fundamental question of ''what are the most effective algorithms for OOD generalization'' remains open even under the standard setting of covariate shift. This paper addresses this fundamental question by proving that, surprisingly, classical Maximum Likelihood Estimation (MLE) purely using source data (without any modification) achieves the minimax optimality for covariate shift under the well-specified setting. That is, no algorithm performs better than MLE in this setting (up to a constant factor), justifying MLE is all you need.},
  author = {Jiawei Ge and Shange Tang and Jianqing Fan and Cong Ma and Chi Jin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ge2024maximum.pdf:pdf},
  note = {Also available at arXiv:2311.15961},
  pdf = {https://openreview.net/pdf?id=eoTCKKOgIs},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Maximum Likelihood Estimation is All You Need for Well-Specified Covariate Shift},
  url = {https://openreview.net/forum?id=eoTCKKOgIs},
  year = {2024}
}

@inproceedings{ge2024making,
  abstract = {The paper addresses the challenge that while Large Language Models (LLMs) have expanded multimodal potential toward General Artificial Intelligence (AGI), current multimodal LLMs still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. The authors contend that the key to overcoming this limitation lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. They introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. The tokenizer is designed with two crucial principles: (1) Image tokens should be independent of 2D physical patch positions and be produced with a 1D causal dependency, aligning with the left-to-right autoregressive prediction mechanism in LLMs; (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, optimized for both discriminativeness and reconstruction. With SEED tokens, LLM performs scalable multimodal autoregression under its original training recipe (next-word prediction). SEED-LLaMA is produced by large-scale pretraining and instruction tuning on interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal tasks and exhibiting compositional emergent abilities such as multi-turn in-context multimodal generation.},
  author = {Yuying Ge and Sijie Zhao and Ziyun Zeng and Yixiao Ge and Chen Li and Xintao Wang and Ying Shan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ge2024making.pdf:pdf},
  note = {Also available at arXiv:2310.01218. Code and models available at https://github.com/AILab-CVC/SEED},
  pdf = {https://openreview.net/pdf?id=0Nui91LBQS},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Making {LLaMA} {SEE} and Draw with {SEED} Tokenizer},
  url = {https://openreview.net/forum?id=0Nui91LBQS},
  year = {2024}
}

@inproceedings{geadah2024parsing,
  abstract = {Unsupervised methods for dimensionality reduction of neural activity and behavior have provided unprecedented insights into the underpinnings of neural information processing. One popular approach involves the recurrent switching linear dynamical system (rSLDS) model, which describes the latent dynamics of neural spike train data using discrete switches between a finite number of low-dimensional linear dynamical systems. However, a few properties of rSLDS model limit its deployability on trial varying data, such as a fixed number of states over trials, and no latent structure nor organization of states. Here we overcome these limitations by endowing the rSLDS model with a semi-Markov discrete state process, with latent geometry, that captures key properties of stochastic processes over partitions with flexible state cardinality. We leverage partial differential equations (PDE) theory to derive an efficient, semi-parametric formulation for dynamical sufficient statistics to the discrete states. This process, combined with switching dynamics, defines our infinite recurrent switching linear dynamical system (irSLDS) model class. We first validate and demonstrate the capabilities of our model on synthetic data. Next, we turn to the analysis of mice electrophysiological data during decision-making, and uncover strong non-stationary processes underlying both within-trial and trial-averaged neural activity.},
  author = {Victor Geadah and International Brain Laboratory and Jonathan W. Pillow},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/geadah2024parsing.pdf:pdf},
  note = {Analyzed mice electrophysiological data during decision-making},
  pdf = {https://openreview.net/pdf?id=YIls9HEa52},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Parsing neural dynamics with infinite recurrent switching linear dynamical systems},
  url = {https://openreview.net/forum?id=YIls9HEa52},
  year = {2024}
}

@inproceedings{gemp2024approximating,
  abstract = {We propose the first loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows the deployment of standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms with provable guarantees. We complement our theoretical analysis with experiments demonstrating that stochastic gradient descent can outperform previous state-of-the-art approaches. Our work reformulates the approximation of Nash equilibria in a normal-form game as a stochastic non-convex optimization problem admitting unbiased Monte-Carlo estimation. This enables the use of powerful solvers and advances in parallel computing to efficiently enumerate Nash equilibria for n-player, general-sum games. This is the first such formulation that allows one-shot unbiased Monte-Carlo estimation, which is critical to introduce the use of powerful algorithms capable of solving high dimensional optimization problems.},
  author = {Ian Gemp and Luke Marris and Georgios Piliouras},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gemp2024approximating.pdf:pdf},
  note = {Also available at arXiv:2310.06689. Oral presentation at ICLR 2024},
  pdf = {https://openreview.net/pdf?id=cc8h3I3V4E},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Approximating {Nash} Equilibria in Normal-Form Games via Stochastic Optimization},
  url = {https://openreview.net/forum?id=cc8h3I3V4E},
  year = {2024}
}

@inproceedings{geng2024motion,
  abstract = {Diffusion models are capable of generating impressive images conditioned on text descriptions, and extensions of these models allow users to edit images at a relatively coarse scale. However, the ability to precisely edit the layout, position, pose, and shape of objects in images with diffusion models is still difficult. To this end, we propose motion guidance, a zero-shot technique that allows a user to specify dense, complex motion fields that indicate where each pixel in an image should move. Motion guidance works by steering the diffusion sampling process with the gradients through an off-the-shelf optical flow network. Specifically, we design a guidance loss that encourages the sample to have the desired motion, as estimated by a flow network, while also being visually similar to the source image. By simultaneously sampling from a diffusion model and guiding the sample to minimize guidance loss, motion-edited images can be obtained.},
  author = {Daniel Geng and Andrew Owens},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/geng2024motion.pdf:pdf},
  note = {Also available at arXiv:2401.18085. Code available at https://github.com/dangeng/motion_guidance. Poster presentation at ICLR 2024},
  pdf = {https://openreview.net/pdf?id=WIAO4vbnNV},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Motion Guidance: Diffusion-Based Image Editing with Differentiable Motion Estimators},
  url = {https://openreview.net/forum?id=WIAO4vbnNV},
  year = {2024}
}

@inproceedings{geng2024improving,
  abstract = {We propose Heuristic Blending (HUBL), a simple performance-improving technique for a broad class of offline RL algorithms based on value bootstrapping. HUBL modifies the Bellman operators used in these algorithms, partially replacing the bootstrapped values with heuristic ones that are estimated with Monte-Carlo returns. For trajectories with higher returns, HUBL relies more on the heuristic values and less on bootstrapping; otherwise, it leans more heavily on bootstrapping. HUBL is very easy to combine with many existing offline RL implementations by relabeling the offline datasets with adjusted rewards and discount factors. We derive a theory that explains HUBL's effect on offline RL as reducing offline RL's complexity and thus increasing its finite-sample performance. We demonstrate that HUBL consistently improves the policy quality of four state-of-the-art bootstrapping-based offline RL algorithms (ATAC, CQL, TD3+BC, and IQL), by 9\% on average over 27 datasets of the D4RL and Meta-World benchmarks.},
  author = {Sinong Geng and Aldo Pacchiano and Andrey Kolobov and Ching-An Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/geng2024improving.pdf:pdf},
  note = {Also available at arXiv:2306.00321. Spotlight paper at ICLR 2024},
  pdf = {https://openreview.net/pdf?id=MCl0TLboP1},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Improving Offline {RL} by Blending Heuristics},
  url = {https://openreview.net/forum?id=MCl0TLboP1},
  year = {2024}
}

@inproceedings{geyer2024tokenflow,
  abstract = {The generative AI revolution has recently expanded to videos. Nevertheless, current state-of-the-art video models are still lagging behind image models in terms of visual quality and user control over the generated content. In this work, we present a framework that harnesses the power of a text-to-image diffusion model for the task of text-driven video editing. Specifically, given a source video and a target text-prompt, our method generates a high-quality video that adheres to the target text, while preserving the spatial layout and motion of the input video. Our method is based on a key observation that consistency in the edited video can be obtained by enforcing consistency in the diffusion feature space. We achieve this by explicitly propagating diffusion features based on inter-frame correspondences, readily available in the model. Thus, our framework does not require any training or fine-tuning, and can work in conjunction with any off-the-shelf text-to-image editing method. We demonstrate state-of-the-art editing results on a variety of real-world videos.},
  author = {Michal Geyer and Omer Bar-Tal and Shai Bagon and Tali Dekel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/geyer2024tokenflow.pdf:pdf},
  note = {Also available at arXiv:2307.10373. Code available at https://github.com/omerbt/TokenFlow. Poster presentation at ICLR 2024},
  pdf = {https://openreview.net/pdf?id=lKK50q2MtV},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{TokenFlow}: Consistent Diffusion Features for Consistent Video Editing},
  url = {https://openreview.net/forum?id=lKK50q2MtV},
  year = {2024}
}

@inproceedings{ghaffari2024robust,
  abstract = {This paper addresses protein design as a grand challenge that involves optimization on a fitness landscape. We focus on model-based approaches where a model is trained on a training set of protein sequences and fitness values to propose candidates for exploration. Two key challenges are identified: the sparsity of high-fitness samples in training sets, and a less recognized problem of ''separation'' in the design space where the desired optimum is in a region poorly represented in training data and relatively far from highly represented low-fitness regions. We show that this separation problem is a significant bottleneck in existing model-based optimization tools and propose a new approach using a novel VAE as its search model to overcome this problem. The proposed method demonstrates advantages over prior methods in robustly finding improved samples, regardless of imbalance and separation between low- and high-fitness training samples. The approach is evaluated through comprehensive benchmarks on real and semi-synthetic protein datasets as well as solution design for physics-informed neural networks, showcasing its generality in both discrete and continuous design spaces.},
  author = {Saba Ghaffari and Ehsan Saleh and Alexander G. Schwing and Yu-Xiong Wang and Martin D. Burke and Saurabh Sinha},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ghaffari2024robust.pdf:pdf},
  note = {Also available at arXiv:2305.13650. Code available at https://github.com/sabagh1994/PGVAE. Poster presentation at ICLR 2024},
  pdf = {https://openreview.net/pdf?id=xhEN0kJh4q},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Robust Model-Based Optimization for Challenging Fitness Landscapes},
  url = {https://openreview.net/forum?id=xhEN0kJh4q},
  year = {2024}
}

@inproceedings{ghazanfari2024lipsim,
  abstract = {Recent years have seen growing interest in developing and applying perceptual similarity metrics. Research has shown the superiority of perceptual metrics over pixel-wise metrics in aligning with human perception and serving as a proxy for the human visual system. On the other hand, as perceptual metrics rely on neural networks, there is a growing concern regarding their resilience, given the established vulnerability of neural networks to adversarial attacks. It is indeed logical to infer that perceptual metrics may inherit both the strengths and shortcomings of neural networks. In this work, we demonstrate the vulnerability of state-of-the-art perceptual similarity metrics based on an ensemble of ViT-based feature extractors to adversarial attacks. We then propose a framework to train a robust perceptual similarity metric called LipSim (Lipschitz Similarity Metric) with provable guarantees. By leveraging 1-Lipschitz neural networks as the backbone, LipSim provides guarded areas around each data point and certificates for all perturbations within an ‚Ñì‚ÇÇ ball. Finally, a comprehensive set of experiments shows the performance of LipSim in terms of natural and certified scores and on the image retrieval application.},
  author = {Sara Ghazanfari and Alexandre Araujo and Prashanth Krishnamurthy and Farshad Khorrami and Siddharth Garg},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ghazanfari2024lipsim.pdf:pdf},
  note = {Also available at arXiv:2310.18274. Code available at https://github.com/SaraGhazanfari/LipSim},
  pdf = {https://openreview.net/pdf?id=0w42S2Gp70},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{LipSim}: A Provably Robust Perceptual Similarity Metric},
  url = {https://openreview.net/forum?id=0w42S2Gp70},
  year = {2024}
}

@inproceedings{ghazi2024labeldp,
  abstract = {Label differentially private (label DP) algorithms seek to preserve the privacy of the labels in a training dataset in settings where the features are known to the adversary. In this work, we study a new family of label DP training algorithms. Unlike most prior label DP algorithms that have been based on label randomization, our algorithm naturally leverages the power of the central model of DP. It interleaves gradient projection operations with private stochastic gradient descent steps in order to improve the utility of the trained model while guaranteeing the privacy of the labels. We show that such projection-based algorithms can be made practical and that they improve on the state-of-the art for label DP training in the high-privacy regime. We complement our empirical evaluation with theoretical results shedding light on the efficacy of our method through the lens of bias-variance trade-offs.},
  author = {Badih Ghazi and Yangsibo Huang and Pritish Kamath and Ravi Kumar and Pasin Manurangsi and Chiyuan Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ghazi2024labeldp.pdf:pdf},
  note = {From Google Research. Addresses label differential privacy in computational advertising systems},
  pdf = {https://openreview.net/pdf?id=JnYaF3vv3G},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{LabelDP}-{Pro}: Learning with Label Differential Privacy via Projections},
  url = {https://openreview.net/forum?id=JnYaF3vv3G},
  year = {2024}
}

@inproceedings{ghosh2024compa,
  abstract = {A fundamental characteristic of audio is its compositional nature. Audio-language models (ALMs) trained using a contrastive approach (e.g., CLAP) that learns a shared representation between audio and language modalities have improved performance in many downstream applications, including zero-shot audio classification, audio retrieval, etc. However, the ability of these models to effectively perform compositional reasoning remains largely unexplored and necessitates additional research. In this paper, we propose CompA, a collection of two expert-annotated benchmarks with a majority of real-world audio samples, to evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates how well an ALM understands the order or occurrence of acoustic events in audio, and CompA-attribute evaluates attribute-binding of acoustic events. An instance from either benchmark consists of two audio-caption pairs, where both audios have the same acoustic events but with different compositions. An ALM is evaluated on how well it matches the right audio to the right caption. Using this benchmark, we first show that current ALMs perform only marginally better than random chance, thereby struggling with compositional reasoning. Next, we propose CompA-CLAP, where we fine-tune CLAP using a novel learning method to improve its compositional reasoning abilities.},
  author = {Sreyan Ghosh and Ashish Seth and Sonal Kumar and Utkarsh Tyagi and Chandra Kiran Reddy Evuru and Ramaneswaran S. and S Sakshi and Oriol Nieto and Ramani Duraiswami and Dinesh Manocha},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ghosh2024compa.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=86NGO8qeWs},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{CompA}: Addressing the Gap in Compositional Reasoning in Audio-Language Models},
  url = {https://openreview.net/forum?id=86NGO8qeWs},
  year = {2024}
}

@inproceedings{ghugare2024closing,
  abstract = {Some reinforcement learning (RL) algorithms have the capability of recombining together pieces of previously seen experience to solve a task never seen before during training. This oft-sought property is one of the few ways in which dynamic programming based RL algorithms are considered different from supervised learning (SL) based RL algorithms. Yet, recent RL methods based on off-the-shelf SL algorithms achieve excellent results without an explicit mechanism for stitching; it remains unclear whether those methods forgo this important stitching property. This paper studies this question in the setting of goal-reaching problems. We show that the desirable stitching property corresponds to a form of generalization: after training on a distribution of (state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen together in the training data. Our analysis shows that this sort of generalization is different from i.i.d. generalization. This connection between stitching and generalization reveals why we should not expect existing RL methods based on SL to perform stitching, even in the limit of large datasets and models. We experimentally validate this result on carefully constructed datasets. This connection suggests a simple remedy, the same remedy for improving generalization in supervised learning: data augmentation. We propose a naive temporal data augmentation approach and demonstrate that adding it to RL methods based on SL enables them to successfully stitch together experience, so that they succeed in navigating between states and goals unseen together during training.},
  author = {Raj Ghugare and Matthieu Geist and Glen Berseth and Benjamin Eysenbach},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ghugare2024closing.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=qg5JENs0N4},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Closing the Gap between {TD} Learning and Supervised Learning - A Generalisation Point of View},
  url = {https://openreview.net/forum?id=qg5JENs0N4},
  year = {2024}
}

@inproceedings{ghugare2024searching,
  abstract = {Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.},
  author = {Raj Ghugare and Santiago Miret and Adriana Hugessen and Mariano Phielipp and Glen Berseth},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ghugare2024searching.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=nqlymMx42E},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Searching for High-Value Molecules Using Reinforcement Learning and Transformers},
  url = {https://openreview.net/forum?id=nqlymMx42E},
  year = {2024}
}

@inproceedings{ging2024openended,
  abstract = {The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our benchmark to a suite of vision-language models and show a detailed comparison of their abilities on object, action, and attribute classification. Our contributions aim to lay the foundation for more precise and meaningful assessments, facilitating targeted progress in the exciting field of vision-language modeling.},
  author = {Simon Ging and Mar{\'i}a Alejandra Bravo and Thomas Brox},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ging2024openended.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=EXitynZhYn},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Open-ended {VQA} benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy},
  url = {https://openreview.net/forum?id=EXitynZhYn},
  year = {2024}
}

@inproceedings{glasgow2024sgd,
  abstract = {In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a \textit{signal-finding} phase where the network is small and many of the neurons evolve independently to find features, and a \textit{signal-heavy} phase, where SGD maintains and balances the features. We leverage the simultaneous training of the layers to show that it is sufficient for only a small fraction of the neurons to learn features, since those neurons will be amplified by the simultaneous growth of their second layer weights.},
  author = {Margalit Glasgow},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/glasgow2024sgd.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=HgOJlxzB16},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{SGD} Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the {XOR} problem},
  url = {https://openreview.net/forum?id=HgOJlxzB16},
  year = {2024}
}

@inproceedings{go2024compositional,
  abstract = {As language models (LMs) become more capable, it is increasingly important to align them with human preferences. However, the dominant paradigm for training Preference Models (PMs) for that purpose suffers from fundamental limitations, such as lack of transparency and scalability, along with susceptibility to overfitting the preference dataset. We propose Compositional Preference Models (CPMs), a novel PM framework that decomposes one global preference assessment into several interpretable features, obtains scalar scores for these features from a prompted LM, and aggregates these scores using a logistic regression classifier. Through these simple steps, CPMs allow to control which properties of the preference data are used to train the preference model and to build it based on features that are believed to underlie the human preference judgment. Our experiments show that CPMs not only improve generalization and are more robust to overoptimization than standard PMs, but also that best-of-n samples obtained using CPMs tend to be preferred over samples obtained using conventional PMs. Overall, our approach demonstrates the benefits of endowing PMs with priors about which features determine human preferences while relying on LM capabilities to extract those features in a scalable and robust way.},
  author = {Dongyoung Go and Tomasz Korbak and Germ{\'a}n Kruszewski and Jos Rozen and Marc Dymetman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/go2024compositional.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=tiiAzqi6Ol},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Compositional Preference Models for Aligning {LMs}},
  url = {https://openreview.net/forum?id=tiiAzqi6Ol},
  year = {2024}
}

@inproceedings{godey2024headless,
  abstract = {Self-supervised pre-training of language models usually consists in predicting probability distributions over extensive token vocabularies. In this study, we propose an innovative method that shifts away from probability prediction and instead focuses on reconstructing input embeddings in a contrastive fashion via Contrastive Weight Tying (CWT). We apply this approach to pretrain Headless Language Models in both monolingual and multilingual contexts. Our method offers practical advantages, substantially reducing training computational requirements by up to 20 times, while simultaneously enhancing downstream performance and data efficiency. We observe a significant +1.6 GLUE score increase and a notable +2.7 LAMBADA accuracy improvement compared to classical LMs within similar compute budgets.},
  author = {Nathan Godey and {\'E}ric Villemonte de la Clergerie and Beno{\^i}t Sagot},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/godey2024headless.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=ONPECq0Rk7},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Headless Language Models: Learning without Predicting with Contrastive Weight Tying},
  url = {https://openreview.net/forum?id=ONPECq0Rk7},
  year = {2024}
}

@inproceedings{goktas2024efficient,
  abstract = {We study inverse game theory and inverse multiagent learning where the goal is to find parameters of a game's payoff functions for which the expected (resp. sampled) behavior is an equilibrium. We formulate these problems as generative-adversarial (i.e., min-max) optimization problems, for which we develop polynomial-time algorithms to solve, the former of which relies on an exact first-order oracle, and the latter, a stochastic one. We extend our approach to solve inverse multiagent simulacral learning in polynomial time and number of samples, where we seek a simulacrum, meaning parameters and an associated equilibrium that replicate the given observations in expectation. We find that our approach outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data.},
  author = {Denizalp Goktas and Amy Greenwald and Sadie Zhao and Alec Koppel and Sumitra Ganesh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/goktas2024efficient.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=JzvIWvC9MG},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Efficient Inverse Multiagent Learning},
  url = {https://openreview.net/forum?id=JzvIWvC9MG},
  year = {2024}
}

@inproceedings{goktas2024generative,
  abstract = {We introduce the use of generative adversarial learning to compute equilibria in general game-theoretic settings, specifically the generalized Nash equilibrium (GNE) in pseudo-games, and its specific instantiation as the competitive equilibrium (CE) in Arrow-Debreu competitive economies. Pseudo-games are a generalization of games in which players' actions affect not only the payoffs of other players but also their feasible action spaces. Although the computation of GNE and CE is intractable in the worst-case, i.e., PPAD-hard, in practice, many applications only require solutions with high accuracy in expectation over a distribution of problem instances. We introduce Generative Adversarial Equilibrium Solvers (GAES): a family of generative adversarial neural networks that can learn GNE and CE from only a sample of problem instances. We provide computational and sample complexity bounds, and apply the framework to finding Nash equilibria in normal-form games, CE in Arrow-Debreu competitive economies, and GNE in an environmental economic model of the Kyoto mechanism.},
  author = {Denizalp Goktas and David C. Parkes and Ian Gemp and Luke Marris and Georgios Piliouras and Romuald Elie and Guy Lever and Andrea Tacchetti},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/goktas2024generative.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=TlyiaPXaVN},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Generative Adversarial Equilibrium Solvers},
  url = {https://openreview.net/forum?id=TlyiaPXaVN},
  year = {2024}
}

@inproceedings{golchin2024time,
  abstract = {Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in measuring LLMs' real effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ 'guided instruction:' a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a 'general instruction' that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts. Further, our findings indicate that GPT-4 is contaminated with AG News, WNLI, and XSum datasets.},
  author = {Shahriar Golchin and Mihai Surdeanu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/golchin2024time.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=2Rwq6c3tvr},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Time Travel in {LLMs}: Tracing Data Contamination in Large Language Models},
  url = {https://openreview.net/forum?id=2Rwq6c3tvr},
  year = {2024}
}

@inproceedings{goldfarb2024joint,
  abstract = {In continual learning, catastrophic forgetting is affected by multiple aspects of the tasks. Previous works have analyzed separately how forgetting is affected by either task similarity or overparameterization. In contrast, our paper examines how task similarity and overparameterization jointly affect forgetting in an analyzable model. Specifically, we focus on two-task continual linear regression, where the second task is a random orthogonal transformation of an arbitrary first task (an abstraction of random permutation tasks). We derive an exact analytical expression for the expected forgetting ‚Äî and uncover a nuanced pattern. In highly overparameterized models, intermediate task similarity causes the most forgetting. However, near the interpolation threshold, forgetting decreases monotonically with the expected task similarity. We validate our findings with linear regression on synthetic data, and with neural networks on established permutation task benchmarks.},
  address = {Vienna, Austria},
  author = {Daniel Goldfarb and Itay Evron and Nir Weinberger and Daniel Soudry and Paul Hand},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/goldfarb2024joint.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=u3dHl287oB},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting - An Analytical Model},
  url = {https://openreview.net/forum?id=u3dHl287oB},
  year = {2024}
}

@inproceedings{gollakota2024efficient,
  abstract = {We give the first efficient algorithm for learning halfspaces in the testable learning model recently defined by Rubinfeld and Vasilyan [2022]. In this model, a learner certifies that the accuracy of its output hypothesis is near optimal whenever the training set passes an associated test, and training sets drawn from some target distribution must pass the test. This model is more challenging than distribution-specific agnostic or Massart noise models where the learner is allowed to fail arbitrarily if the distributional assumption does not hold. We consider the setting where the target distribution is the standard Gaussian in $d$ dimensions and the label noise is either Massart or adversarial (agnostic). For Massart noise, our tester-learner runs in polynomial time and outputs a hypothesis with (information-theoretically optimal) error $\mathrm{opt}+\epsilon$ (and extends to any fixed strongly log-concave target distribution). For adversarial noise, our tester-learner obtains error $O(\mathrm{opt})+\epsilon$ in polynomial time. Prior work on testable learning ignores the labels in the training set and checks that the empirical moments of the covariates are close to the moments of the base distribution. Here we develop new tests of independent interest that make critical use of the labels and combine them with the moment-matching approach of Gollakota et al. [2022]. This enables us to implement a testable variant of the algorithm of Diakonikolas et al. [2020a, 2020b] for learning noisy halfspaces using nonconvex SGD.},
  address = {Vienna, Austria},
  author = {Aravind Gollakota and Adam R. Klivans and Konstantinos Stavropoulos and Arsen Vasilyan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gollakota2024efficient.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=z6n1fKMMC1},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {An Efficient Tester-Learner for Halfspaces},
  url = {https://openreview.net/forum?id=z6n1fKMMC1},
  year = {2024}
}

@inproceedings{gomes2024datadriven,
  abstract = {Misclassification detection is an important problem in machine learning, as it allows for the identification of instances where the model's predictions are unreliable. However, conventional uncertainty measures such as Shannon entropy do not provide an effective way to infer the real uncertainty associated with the model's predictions. In this paper, we introduce a novel data-driven measure of uncertainty relative to an observer for misclassification detection. By learning patterns in the distribution of soft-predictions, our uncertainty measure can identify misclassified samples based on the predicted class probabilities. Interestingly, according to the proposed measure, soft-predictions corresponding to misclassified instances can carry a large amount of uncertainty, even though they may have low Shannon entropy. We demonstrate empirical improvements over multiple image classification tasks, outperforming state-of-the-art misclassification detection methods.},
  address = {Vienna, Austria},
  author = {Eduardo Dadalto C√¢mara Gomes and Marco Romanelli and Georg Pichler and Pablo Piantanida},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gomes2024datadriven.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ruGY8v10mK},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {A Data-Driven Measure of Relative Uncertainty for Misclassification Detection},
  url = {https://openreview.net/forum?id=ruGY8v10mK},
  year = {2024}
}

@inproceedings{gomez2024proper,
  abstract = {The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The _Laplacian representation_ is a promising approach to address these problems by inducing informative state encoding and intrinsic rewards for temporally-extended action discovery and reward shaping. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally recovers both the true eigenvectors and eigenvalues while eliminating the hyperparameter dependence of previous approximations. We provide theoretical guarantees for our method and we show that those results translate empirically into robust learning across multiple environments.},
  address = {Vienna, Austria},
  author = {Diego Gomez and Michael Bowling and Marlos C. Machado},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gomez2024proper.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7gLfQT52Nn},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Proper Laplacian Representation Learning},
  url = {https://openreview.net/forum?id=7gLfQT52Nn},
  year = {2024}
}

@inproceedings{gondur2024multimodal,
  abstract = {Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning latent variability into components that are either shared between or independent to each modality. We parameterize the latents of our model in the Fourier domain, and show improved latent identification using this approach over standard GP-VAE methods. We validate our model on simulated multi-modal data consisting of Poisson spike counts and MNIST images that scale and rotate smoothly over time. We show that the multi-modal GP-VAE (MM-GPVAE) is able to not only identify the shared and independent latent structure across modalities accurately, but provides good reconstructions of both images and neural rates on held-out trials. Finally, we demonstrate our framework on two real world multi-modal experimental settings: Drosophila whole-brain calcium imaging alongside tracked limb positions, and Manduca sexta spike train measurements from ten wing muscles as the animal tracks a visual stimulus.},
  address = {Vienna, Austria},
  author = {Rabia Gondur and Usama Bin Sikandar and Evan Schaffer and Mikio Christian Aoi and Stephen L. Keeley},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gondur2024multimodal.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=aGH43rjoe4},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data},
  url = {https://openreview.net/forum?id=aGH43rjoe4},
  year = {2024}
}

@inproceedings{gonon2024pathnorm,
  abstract = {This work introduces the first toolkit around path-norms that fully encompasses general DAG ReLU networks with biases, skip connections and any operation based on the extraction of order statistics: max pooling, GroupSort etc. This toolkit notably allows us to establish generalization bounds for modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on layered fully-connected networks compared to the product of operator norms, another complexity measure most commonly used. The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.},
  address = {Vienna, Austria},
  author = {Antoine Gonon and Nicolas Brisebarre and Elisa Riccietti and R√©mi Gribonval},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gonon2024pathnorm.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=hiHZVUIYik},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {A path-norm toolkit for modern networks: consequences, promises and challenges},
  url = {https://openreview.net/forum?id=hiHZVUIYik},
  year = {2024}
}

@inproceedings{gorishniy2024tabr,
  abstract = {Deep learning (DL) models for tabular data problems (e.g. classification, regression) are currently receiving increasingly more attention from researchers. However, despite the recent efforts, the non-DL algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution for these problems. One of the research directions aimed at improving the position of tabular DL involves designing so-called retrieval-augmented models. For a target object, such models retrieve other objects (e.g. the nearest neighbors) from the available training data and use their features and labels to make a better prediction. In this work, we present TabR -- essentially, a feed-forward network with a custom k-Nearest-Neighbors-like component in the middle. On a set of public benchmarks with datasets up to several million objects, TabR marks a big step forward for tabular DL: it demonstrates the best average performance among tabular DL models, becomes the new state-of-the-art on several datasets, and even outperforms GBDT models on the recently proposed "GBDT-friendly" benchmark (see Figure 1). Among the important findings and technical details powering TabR, the main ones lie in the attention-like mechanism that is responsible for retrieving the nearest neighbors and extracting valuable signal from them. In addition to the higher performance, TabR is simple and significantly more efficient compared to prior retrieval-based tabular DL models.},
  address = {Vienna, Austria},
  author = {Yury Gorishniy and Ivan Rubachev and Nikolay Kartashev and Daniil Shlenskii and Akim Kotelnikov and Artem Babenko},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gorishniy2024tabr.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rhgIgTSSxW},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{TabR}: Tabular Deep Learning Meets Nearest Neighbors},
  url = {https://openreview.net/forum?id=rhgIgTSSxW},
  year = {2024}
}

@inproceedings{gou2024critic,
  abstract = {Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.},
  address = {Vienna, Austria},
  author = {Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Nan Duan and Weizhu Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gou2024critic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Sx038qxjek},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{CRITIC}: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
  url = {https://openreview.net/forum?id=Sx038qxjek},
  year = {2024}
}

@inproceedings{gou2024tora,
  abstract = {Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22% absolute. ToRA-34B is also the first open-source model that achieves an accuracy exceeding 50% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.},
  address = {Vienna, Austria},
  author = {Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gou2024tora.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Ep0TtjVoap},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{ToRA}: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
  url = {https://openreview.net/forum?id=Ep0TtjVoap},
  year = {2024}
}

@inproceedings{gould2024successor,
  abstract = {In this work we describe successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days. For example, successor heads increment 'Monday' into 'Tuesday'. We explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms. Existing research in this area has struggled to find recurring, mechanistically interpretable large language model (LLM) components beyond small toy models. Further, existing results have led to very little insight to explain the internals of the larger models that are used in practice. In this paper, we analyze the behavior of successor heads in LLMs and find that they implement abstract representations that are common to different architectures. Successor heads form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2. We find a set of 'mod 10' features that underlie how successor heads increment in LLMs across different architectures and sizes. We perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs. Additionally, we study the behavior of successor heads on natural language data, where we find that successor heads are important for achieving a low loss on examples involving succession, and also identify interpretable polysemanticity in a Pythia successor head.},
  address = {Vienna, Austria},
  author = {Rhys Gould and Euan Ong and George Ogden and Arthur Conmy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gould2024successor.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kvcbV8KQsi},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Successor Heads: Recurring, Interpretable Attention Heads In The Wild},
  url = {https://openreview.net/forum?id=kvcbV8KQsi},
  year = {2024}
}

@inproceedings{goyal2024think,
  abstract = {Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{\rm th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{\rm th}$ token? We operationalize this idea by introducing pause tokens, a sequence of learnable tokens that do not produce an output symbol but instead give the model time to think and process. We present the results of our experiments training small- and medium-scale Transformer decoder-only language models with pause tokens on a pre-training dataset of books and web text, and subsequently evaluating on downstream tasks covering language understanding, common sense reasoning, and mathematical problem solving.},
  author = {Sachin Goyal and Ziwei Ji and Ankit Singh Rawat and Aditya Krishna Menon and Sanjiv Kumar and Vaishnavh Nagarajan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/goyal2024think.pdf:pdf},
  keywords = {{LLM} training and inference, Downstream finetuning, Pause tokens, Language models, Reasoning},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=ph04CRkPdC},
  publisher = {OpenReview.net},
  title = {Think before you speak: Training Language Models With Pause Tokens},
  url = {https://openreview.net/forum?id=ph04CRkPdC},
  year = {2024}
}

@inproceedings{grand2024lilo,
  abstract = {While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from neurosymbolic program synthesis. To guide this process, LILO learns a semantic parser that maps from natural language descriptions to syntactic patterns in the library. Altogether, LILO automates the cycle of writing, compressing, and documenting code, providing a path toward self-improving software systems.},
  author = {Gabriel Grand and Lionel Wong and Matthew Bowers and Theo X. Olausson and Muxin Liu and Joshua B. Tenenbaum and Jacob Andreas},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/grand2024lilo.pdf:pdf},
  keywords = {program synthesis, language models, library learning, compression, refactoring, abstraction, documentation, neurosymbolic},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=TqYbAWKMIe},
  publisher = {OpenReview.net},
  title = {{LILO}: Learning Interpretable Libraries by Compressing and Documenting Code},
  url = {https://openreview.net/forum?id=TqYbAWKMIe},
  year = {2024}
}

@inproceedings{grari2024fairness,
  abstract = {In the field of algorithmic fairness, significant attention has been put on group fairness criteria, such as Demographic Parity and Equalized Odds. Nevertheless, these objectives, measured as global averages, have raised concerns about persistent local disparities between sensitive groups. In this work, we address the problem of local fairness, which aims to reduce performance disparities between sensitive groups within similar regions of the covariate space. We introduce ROAD, a distributionally robust optimization approach that leverages adversarial learning to enforce local fairness. Our method trains a predictor that is robust to the worst-case reweighting of the training distribution over local regions while maintaining good predictive performance.},
  author = {Vincent Grari and Thibault Laugel and Tatsunori Hashimoto and Sylvain Lamprier and Marcin Detyniecki},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/grari2024fairness.pdf:pdf},
  keywords = {Fairness, {DRO}, Adversarial Learning, Local fairness, Distributionally robust optimization},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=xnhvVtZtLD},
  publisher = {OpenReview.net},
  title = {On the Fairness {ROAD}: Robust Optimization for Adversarial Debiasing},
  url = {https://openreview.net/forum?id=xnhvVtZtLD},
  year = {2024}
}

@inproceedings{grega2024energyconserving,
  abstract = {Lattices are architected metamaterials whose properties strongly depend on their geometrical design. The analogy between lattices and graphs enables the use of graph neural networks (GNNs) as a faster surrogate model compared to traditional methods such as finite element modelling. However, to predict the elasticity tensor, which is a fourth-order tensor used to characterise a material's stiffness, commonly used simple GNNs need to be modified. We present an energy-conserving equivariant GNN that can predict positive definite stiffness matrices of lattice metamaterials. We achieve this by using equivariant features and by introducing a Cholesky decomposition layer within our architecture.},
  author = {Ivan Grega and Ilyes Batatia and G√°bor Cs√°nyi and Sri Karlapati and Vikram S. Deshpande},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/grega2024energyconserving.pdf:pdf},
  keywords = {mechanical metamaterials, lattices, elasticity, {GNN}, equivariant, positive definite, energy conservation},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=smy4DsUbBo},
  publisher = {OpenReview.net},
  title = {Energy-conserving equivariant {GNN} for elasticity of lattice architected metamaterials},
  url = {https://openreview.net/forum?id=smy4DsUbBo},
  year = {2024}
}

@inproceedings{grigsby2024amago,
  abstract = {We introduce AMAGO, an in-context Reinforcement Learning (RL) agent that uses sequence models to tackle the challenges of generalization, long-term memory, and meta-learning. Recent works have shown that off-policy learning can make in-context RL with recurrent policies viable. Nonetheless, these approaches require extensive tuning and limit scalability by creating key bottlenecks in agents' memory capacity, planning horizon, and model size. AMAGO revisits and redesigns the off-policy in-context approach to successfully train long-sequence Transformers over entire rollouts in parallel with end-to-end RL. Our agent is scalable and applicable to a wide range of problems, and we demonstrate its strong performance empirically in meta-RL and long-term memory domains.},
  author = {Jake Grigsby and Linxi Fan and Yuke Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/grigsby2024amago.pdf:pdf},
  keywords = {reinforcement learning, meta-learning, transformers, in-context learning, sequence modeling, {Meta-RL}, Generalization, Long-term memory},
  note = {Accepted as spotlight presentation},
  pdf = {https://openreview.net/pdf?id=M6XWoEdmwf},
  publisher = {OpenReview.net},
  title = {{AMAGO}: Scalable In-Context Reinforcement Learning for Adaptive Agents},
  url = {https://openreview.net/forum?id=M6XWoEdmwf},
  year = {2024}
}

@inproceedings{grtschla2024coregd,
  abstract = {Graph Visualization, also known as Graph Drawing, aims to find geometric embeddings of graphs that optimize certain criteria. Stress is a widely used metric; stress is minimized when every pair of nodes is positioned at their shortest path distance. However, stress optimization presents computational challenges due to its inherent complexity and is usually solved using heuristics in practice. We introduce a scalable Graph Neural Network (GNN) based Graph Drawing framework with sub-quadratic runtime that can learn to optimize stress. Inspired by classical stress optimization techniques and force-directed layout algorithms, we create a coarsening hierarchy for the input graph.},
  author = {Florian Gr√∂tschla and Jo√´l Mathys and Robert Veres and Roger Wattenhofer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/grtschla2024coregd.pdf:pdf},
  keywords = {graph drawing, graph visualization, graph neural networks, stress optimization, hierarchical coarsening, Graph Visualization, Optimization, Scalability},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=vtyasLn4RM},
  publisher = {OpenReview.net},
  title = {{CoRe-GD}: A Hierarchical Framework for Scalable Graph Visualization with {GNN}s},
  url = {https://openreview.net/forum?id=vtyasLn4RM},
  year = {2024}
}

@inproceedings{gruver2024finetuned,
  abstract = {We propose fine-tuning large language models for generation of stable materials. While unorthodox, fine-tuning large language models on text-encoded atomistic data is simple to implement yet reliable, with around 90\% of sampled structures obeying physical constraints on atom positions and charges. Using energy above hull calculations from both learned ML potentials and gold-standard DFT calculations, we show that our strongest model (fine-tuned LLaMA-2 70B) can generate materials predicted to be metastable at about twice the rate (49\% vs 28\%) of CDVAE, a competing diffusion model. Because of text prompting's inherent flexibility, our models can simultaneously be used for unconditional generation of stable material, infilling of partial structures and text-conditional generation.},
  author = {Nate Gruver and Anuroop Sriram and Andrea Madotto and Andrew Gordon Wilson and C. Lawrence Zitnick and Zachary W. Ulissi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gruver2024finetuned.pdf:pdf},
  keywords = {materials science, language models, crystal structure generation, computational materials discovery, fine-tuning, generative model, large language model, stable materials, {AI} for science},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=vN9fpfqoP1},
  publisher = {OpenReview.net},
  title = {Fine-Tuned Language Models Generate Stable Inorganic Materials as Text},
  url = {https://openreview.net/forum?id=vN9fpfqoP1},
  year = {2024}
}

@inproceedings{gu2024minillm,
  abstract = {Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM.},
  author = {Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024minillm.pdf:pdf},
  keywords = {knowledge distillation, large language models, model compression, reverse {KLD}, instruction following, Large Language Models, Knowledge Distillation},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=5h0qf7IBZZ},
  publisher = {OpenReview.net},
  title = {{MiniLLM}: Knowledge Distillation of Large Language Models},
  url = {https://openreview.net/forum?id=5h0qf7IBZZ},
  year = {2024}
}

@inproceedings{gu2024low,
  abstract = {Given a matrix $M\in \mathbb{R}^{m\times n}$, the low rank matrix completion problem asks us to find a rank-$k$ approximation of $M$ as $UV^\top$ for $U\in \mathbb{R}^{m\times k}$ and $V\in \mathbb{R}^{n\times k}$ by only observing a few entries specified by a set of entries $Œ©\subseteq [m]\times [n]$. In particular, we examine an approach that is widely used in practice -- the alternating minimization framework. While the sample complexity has been subsequently improved, alternating minimization steps are required to be computed exactly. This hinders the development of more efficient algorithms and fails to depict the practical implementation of alternating minimization, where the updates are usually performed approximately in favor of efficiency.},
  author = {Yuzhou Gu and Zhao Song and Junze Yin and Lichen Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024low.pdf:pdf},
  keywords = {matrix completion, alternating minimization, low rank approximation, robust optimization, nearly linear time, Optimization},
  note = {Accepted as poster presentation},
  pdf = {https://openreview.net/pdf?id=N0gT4A0jNV},
  publisher = {OpenReview.net},
  title = {Low Rank Matrix Completion via Robust Alternating Minimization in Nearly Linear Time},
  url = {https://openreview.net/forum?id=N0gT4A0jNV},
  year = {2024}
}

@inproceedings{gu2024rt,
  abstract = {Generalization remains one of the most important desiderata for robust robot learning systems. While recently proposed approaches show promise in generalization to novel objects, semantic concepts, or visual distribution shifts, generalization to new tasks remains challenging. For example, a language-conditioned policy trained on pick-and-place tasks will not be able to generalize to a folding task, even if the arm trajectory of folding is similar to pick-and-place. Our key insight is that this kind of generalization becomes feasible if we represent the task through rough trajectory sketches. We propose a policy conditioning method using such rough trajectory sketches, which we call {RT}-Trajectory, that is practical, easy to specify, and allows the policy to effectively perform new tasks that would otherwise be challenging to perform. We find that trajectory sketches strike a balance between being detailed enough to express low-level motion-centric guidance while being coarse enough to allow the learned policy to interpret the trajectory sketch in the context of situational visual observations. In addition, we show how trajectory sketches can provide a useful interface to communicate with robotic policies -- they can be specified through simple human inputs like drawings or videos, or through automated methods such as modern image-generating or waypoint-generating methods. We evaluate {RT}-Trajectory at scale on a variety of real-world robotic tasks, and find that {RT}-Trajectory is able to perform a wider range of tasks compared to language-conditioned and goal-conditioned policies, when provided the same training data.},
  address = {Vienna, Austria},
  author = {Jiayuan Gu and Sean Kirmani and Paul Wohlhart and Yao Lu and Montserrat Gonzalez Arenas and Kanishka Rao and Wenhao Yu and Chuyuan Fu and Keerthana Gopalakrishnan and Zhuo Xu and Priya Sundaresan and Peng Xu and Hao Su and Karol Hausman and Chelsea Finn and Quan Vuong and Ted Xiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024rt.pdf:pdf},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=F1TKzG8LJO},
  publisher = {OpenReview.net},
  title = {{RT}-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches},
  url = {https://openreview.net/forum?id=F1TKzG8LJO},
  year = {2024}
}

@inproceedings{gu2024quadratic,
  abstract = {In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local {SGD}, address this issue by allowing workers to compute locally for H steps without synchronizing with others, hence reducing communication frequency. While H has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper H value can lead to generalization improvement. Yet, selecting a proper H is elusive. This work proposes a theory-grounded method for determining H, named the Quadratic Synchronization Rule ({QSR}), which recommends dynamically setting H in proportion to 1/Œ∑¬≤ as the learning rate Œ∑ decays over time. Extensive {ImageNet} experiments on {ResNet} and {ViT} show that local gradient methods with {QSR} consistently improve the test accuracy over other synchronization strategies. Compared to the standard data parallel training, {QSR} enables Local {AdamW} to cut the training time on 16 or 64 {GPUs} down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.16\% or 0.84\% higher top-1 validation accuracy.},
  address = {Vienna, Austria},
  author = {Xinran Gu and Kaifeng Lyu and Sanjeev Arora and Jingzhao Zhang and Longbo Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024quadratic.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=yroyhkhWS6},
  publisher = {OpenReview.net},
  title = {A Quadratic Synchronization Rule for Distributed Deep Learning},
  url = {https://openreview.net/forum?id=yroyhkhWS6},
  year = {2024}
}

@inproceedings{gu2024learnability,
  abstract = {Watermarking of language model outputs enables statistical detection of model-generated text, which can mitigate harms and misuses of language models. Existing watermarking strategies operate by altering the decoder of an existing language model. In this paper, we ask whether language models can directly learn to generate watermarked text, which would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, enabling watermarking for open models, where users can control the decoding procedure. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. We explore watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three distinct decoding-based watermarking strategies and various hyperparameter settings, finding that models can learn to generate watermarked text with high detectability. However, we also identify limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks.},
  address = {Vienna, Austria},
  author = {Chenchen Gu and Xiang Lisa Li and Percy Liang and Tatsunori Hashimoto},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024learnability.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=9k0krNzvlV},
  publisher = {OpenReview.net},
  title = {On the Learnability of Watermarks for Language Models},
  url = {https://openreview.net/forum?id=9k0krNzvlV},
  year = {2024}
}

@inproceedings{gu2024adopd,
  abstract = {Research in document image understanding is hindered by the scarcity of high-quality document data. To address this challenge, we introduce {ADOPD}, a comprehensive dataset for document page decomposition. {ADOPD} stands out with its data-driven approach for document taxonomy discovery during data collection, complemented by dense annotations. Our approach integrates large-scale pretrained models with a human-in-the-loop process to guarantee diversity and balance in the resulting data collection. Leveraging our data-driven document taxonomy, we collect and densely annotate document images, addressing four document image understanding tasks: Doc2Mask, Doc2Box, Doc2Tag, and Doc2Seq. Our dataset provides human-labeled entity masks, text bounding boxes, and automatically generated tags and captions. We aim to establish {ADOPD} as a cornerstone dataset for future document image understanding research, removing the data scarcity blocker for impactful research in this domain.},
  address = {Vienna, Austria},
  author = {Jiuxiang Gu and Xiangxi Shi and Jason Kuen and Lu Qi and Ruiyi Zhang and Anqi Liu and Ani Nenkova and Tong Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024adopd.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=x1ptaXpOYa},
  publisher = {OpenReview.net},
  title = {{ADOPD}: A Large-Scale Document Page Decomposition Dataset},
  url = {https://openreview.net/forum?id=x1ptaXpOYa},
  year = {2024}
}

@inproceedings{gu2024seer,
  abstract = {Imagining the future trajectory is the key for robots to make sound planning and successfully reach their goals. Therefore, text-conditioned video prediction ({TVP}) is an essential task to facilitate general robot policy learning. To tackle this task and empower robots with the ability to foresee the future, we propose a sample and computation-efficient model, named Seer, by inflating the pretrained text-to-image ({T2I}) stable diffusion models along the temporal axis. We enhance the {U-Net} and language conditioning model by incorporating computation-efficient spatial-temporal attention. Furthermore, we introduce a novel Frame Sequential Text Decomposer module that dissects a sentence's global instruction into temporally aligned sub-instructions, ensuring precise integration into each frame of generation. Our framework allows us to effectively leverage the extensive prior knowledge embedded in pretrained {T2I} models across the frames. With the adaptable-designed architecture, Seer makes it possible to generate high-fidelity, coherent, and instruction-aligned video frames by fine-tuning a few layers on a small amount of data. The experimental results on Something Something V2 ({SSv2}), Bridgedata and EpicKitchens-100 datasets demonstrate our superior video prediction performance with around 480-{GPU} hours versus {CogVideo} with over 12,480-{GPU} hours: achieving the 31\% {FVD} improvement compared to the current {SOTA} model on {SSv2} and 83.7\% average preference in the human evaluation.},
  address = {Vienna, Austria},
  author = {Xianfan Gu and Chuan Wen and Weirui Ye and Jiaming Song and Yang Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024seer.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=qHGgNyQk31},
  publisher = {OpenReview.net},
  title = {Seer: Language Instructed Video Prediction with Latent Diffusion Models},
  url = {https://openreview.net/forum?id=qHGgNyQk31},
  year = {2024}
}

@inproceedings{gu2024matryoshka,
  abstract = {Diffusion models are the de-facto approach for generating high-quality images and videos, but learning high-dimensional models remains a formidable task due to computational and optimization challenges. Existing methods often resort to training cascaded models in pixel space, or using a downsampled latent space of a separately trained auto-encoder. In this paper, we introduce Matryoshka Diffusion ({MDM}), an end-to-end framework for high-resolution image and video synthesis. We propose a diffusion process that denoises inputs at multiple resolutions jointly and uses a {NestedUNet} architecture where features and parameters for small-scale inputs are nested within those of large scales. In addition, {MDM} enables a progressive training schedule from lower to higher resolutions, which leads to significant improvements in optimization for high-resolution generation. We demonstrate the effectiveness of {MDM} by training a single pixel-space model up to 1024√ó1024 resolution on the CC12M dataset and showing strong zero-shot generalization across class-conditioned image generation, high-resolution text-to-image synthesis, and text-to-video generation applications.},
  address = {Vienna, Austria},
  author = {Jiatao Gu and Shuangfei Zhai and Yizhe Zhang and Joshua M. Susskind and Navdeep Jaitly},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024matryoshka.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=tOzCcDdH9O},
  publisher = {OpenReview.net},
  title = {Matryoshka Diffusion Models},
  url = {https://openreview.net/forum?id=tOzCcDdH9O},
  year = {2024}
}

@inproceedings{gu2024unraveling,
  abstract = {Double descent presents a counter-intuitive aspect within the machine learning domain, where test performance initially decreases and then increases again as model complexity grows. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory to account for its occurrence in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that the presence of noisy data strongly influences its occurrence. Through conducting a comprehensive analysis of the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that double descent is a consequence of the model first learning the noisy data until interpolation and then adding implicit regularization via over-parameterization, thus acquiring the capability to separate the information from the noise. We demonstrate that while small and intermediate models before the interpolation threshold follow the traditional bias-variance trade-off, over-parameterized models interpolate noisy samples among robust data, thus acquiring the capability to separate the information from the noise.},
  address = {Vienna, Austria},
  author = {Yufei Gu and Xiaoqing Zheng and Tomaso Aste},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gu2024unraveling.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=CEkIyshNbC},
  publisher = {OpenReview.net},
  title = {Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space},
  url = {https://openreview.net/forum?id=CEkIyshNbC},
  year = {2024}
}

@inproceedings{guan2024improved,
  abstract = {Online-Within-Online ({OWO}) meta learning stands for the online multi-task learning paradigm in which both tasks and data within each task become available in a sequential order. In this work, we study the {OWO} meta learning of the initialization and step size of within-task online algorithms in the non-convex setting, and provide improved regret bounds under mild assumptions of loss functions. Previous work analyzing this scenario has obtained for bounded and piecewise Lipschitz functions an averaged regret bound O((‚àöm/T^{1/4} + (log m)log T/‚àöT + V)‚àöm) across T tasks, with m iterations per task and V the task similarity. Our first contribution is to modify the existing non-convex {OWO} meta learning algorithm and improve the regret bound to O((1/T^{1/2-Œ±} + (log T)^{9/2}/T + V)‚àöm), for any Œ± ‚àà (0,1/2). Furthermore, we propose a new algorithm with regret O((log T/T + V)‚àöm) for non-convex {OWO} meta learning. This regret bound exhibits better asymptotic performance than previous ones, and holds for any bounded loss functions. Additionally, we provide a novel PAC-Bayes bound for multi-task learning via regret analysis, investigating how to attain generalization bounds for statistical meta learning through regret analysis.},
  address = {Vienna, Austria},
  author = {Jiechao Guan and Hui Xiong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guan2024improved.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=pA8Q5WiEMg},
  publisher = {OpenReview.net},
  title = {Improved Regret Bounds for Non-Convex Online-Within-Online Meta Learning},
  url = {https://openreview.net/forum?id=pA8Q5WiEMg},
  year = {2024}
}

@inproceedings{guan2024hardness,
  abstract = {Online nonconvex optimization has been an active area of research recently. Previous studies either considered the global regret with full information about the objective functions, or studied the local regret with window-smoothed objective functions, which required access to unlimited number of gradient oracles per time step. In this paper, we focus on the more challenging and practical setting, where access to only a single oracle is allowed per time step, and take the local regret of the original (i.e., unsmoothed) objective functions as the performance metric. For both settings respectively with a single exact and stochastic gradient oracle feedback, we derive lower bounds on the local regret and show that the classical online (stochastic) gradient descent algorithms are optimal. For the more challenging setting with a single function value oracle feedback, we develop an online algorithm based on a one-point running difference gradient estimator, and show that such an algorithm achieves a local regret that matches what a generic stochastic gradient oracle can achieve.},
  address = {Vienna, Austria},
  author = {Ziwei Guan and Yi Zhou and Yingbin Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guan2024hardness.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=iZgECfyHXF},
  publisher = {OpenReview.net},
  title = {On the Hardness of Online Nonconvex Optimization with Single Oracle Feedback},
  url = {https://openreview.net/forum?id=iZgECfyHXF},
  year = {2024}
}

@inproceedings{gudibande2024false,
  abstract = {An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like {ChatGPT} (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of {LMs} that imitate {ChatGPT} using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical {NLP} benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with {ChatGPT}. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base {LM} to {ChatGPT} on tasks that are not heavily supported in the imitation data. We show that these models learn to imitate {ChatGPT}'s style, but not its factual knowledge. We conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed {LMs} that can be bridged using an unwieldy amount of imitation data or by using more capable base {LMs}.},
  address = {Vienna, Austria},
  author = {Arnav Gudibande and Eric Wallace and Charlie Snell and Xinyang Geng and Hao Liu and Pieter Abbeel and Sergey Levine and Dawn Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gudibande2024false.pdf:pdf},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=Kz3yckpCN5},
  publisher = {OpenReview.net},
  title = {The False Promise of Imitating Proprietary Language Models},
  url = {https://openreview.net/forum?id=Kz3yckpCN5},
  year = {2024}
}

@inproceedings{guha2024conformal,
  abstract = {Conformal prediction (CP) for regression can be challenging, especially when the output distribution is heteroscedastic, multimodal, or skewed. Some of the issues can be addressed by estimating a distribution over the output, but in reality, such approaches can be sensitive to estimation error and yield unstable intervals. Here, we circumvent the challenges by converting regression to a classification problem and then use CP for classification to obtain CP sets for regression. To preserve the ordering of the continuous-output space, we design a new loss function and present necessary modifications to the CP classification techniques. Empirical results on many benchmarks show that this simple approach gives surprisingly good results on many practical problems.},
  address = {Vienna, Austria},
  author = {Etash Kumar Guha and Shlok Natarajan and Thomas M\"{o}llenhoff and Mohammad Emtiyaz Khan and Eugene Ndiaye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guha2024conformal.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rulxyXjf46},
  publisher = {OpenReview.net},
  title = {Conformal Prediction via Regression-as-Classification},
  url = {https://openreview.net/forum?id=rulxyXjf46},
  year = {2024}
}

@inproceedings{gui2024active,
  abstract = {Test-time adaptation (TTA) addresses distribution shifts for streaming test data in unsupervised settings. Currently, most TTA methods can only deal with minor shifts and rely heavily on heuristic and empirical studies. To advance TTA under domain shifts, we propose the novel problem setting of active test-time adaptation (ATTA) that integrates active learning within the fully TTA setting. We provide a learning theory analysis, demonstrating that incorporating limited labeled test instances enhances overall performances across test domains with a theoretical guarantee. We also present a sample entropy balancing for implementing ATTA while avoiding catastrophic forgetting (CF). We introduce a simple yet effective ATTA algorithm, known as SimATTA, using real-time sample selection techniques.},
  address = {Vienna, Austria},
  author = {Shurui Gui and Xiner Li and Shuiwang Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gui2024active.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=YHUGlwTzFB},
  publisher = {OpenReview.net},
  title = {Active Test-Time Adaptation: Theoretical Analyses and An Algorithm},
  url = {https://openreview.net/forum?id=YHUGlwTzFB},
  year = {2024}
}

@inproceedings{gumbsch2024learning,
  abstract = {Hierarchical world models can significantly improve model-based reinforcement learning (MBRL) and planning by enabling reasoning across multiple time scales. Nonetheless, the majority of state-of-the-art MBRL methods employ flat, non-hierarchical models. We propose Temporal Hierarchies from Invariant Context Kernels (THICK), an algorithm that learns a world model hierarchy via discrete latent dynamics. The lower level of THICK updates parts of its latent state sparsely in time, forming invariant contexts. The higher level exclusively predicts situations involving context changes. Our experiments demonstrate that THICK learns categorical, interpretable, temporal abstractions on the high level, while maintaining precise low-level predictions.},
  address = {Vienna, Austria},
  author = {Christian Gumbsch and Noor Sajid and Georg Martius and Martin V. Butz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gumbsch2024learning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=TjCDNssXKU},
  publisher = {OpenReview.net},
  title = {Learning Hierarchical World Models with Adaptive Temporal Abstractions from Discrete Latent Dynamics},
  url = {https://openreview.net/forum?id=TjCDNssXKU},
  year = {2024}
}

@inproceedings{guo2024revisit,
  abstract = {Recent embedding-based methods have achieved great successes in exploiting entity alignment from knowledge graph (KG) embeddings of multiple modalities. In this paper, we study embedding-based entity alignment (EEA) from a perspective of generative models. We show that EEA shares similarities with typical generative models and prove the effectiveness of the recently developed generative adversarial network (GAN)-based EEA methods theoretically. We reveal that incomplete objectives limit the capacity on both entity alignment and entity synthesis (i.e., generating new entities), and mitigate this problem by introducing a generative EEA (GEEA) framework with the proposed mutual variational autoencoder (M-VAE) as the generative model. GEEA enables entity conversion between KGs and generation of new entities from random noise vectors. Extensive experiments demonstrate that GEEA achieves state-of-the-art performance on entity alignment.},
  address = {Vienna, Austria},
  author = {Lingbing Guo and Zhuo Chen and Jiaoyan Chen and Yin Fang and Wen Zhang and Huajun Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024revisit.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=z3dfuRcGAK},
  publisher = {OpenReview.net},
  title = {Revisit and Outstrip Entity Alignment: A Perspective of Generative Models},
  url = {https://openreview.net/forum?id=z3dfuRcGAK},
  year = {2024}
}

@inproceedings{guo2024towards,
  abstract = {The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in part due to the fact that previous methods only remain effective when the total number of synthetic samples is extremely small. Since only so much information can be contained in such a small number of samples, it seems that to achieve truly lossless dataset distillation, we must develop a distillation method that remains effective as the size of the synthetic dataset grows. We present such an algorithm and elucidate why existing methods fail to generate larger, high-quality synthetic sets. The key insight is that the training stage of the trajectories we choose to match (i.e., early or late) greatly affects the effectiveness of the distilled dataset. We propose to align the difficulty of the generated patterns with the size of the synthetic dataset, successfully scaling trajectory matching-based methods to larger synthetic datasets and achieving lossless dataset distillation for the very first time.},
  address = {Vienna, Austria},
  author = {Ziyao Guo and Kai Wang and George Cazenavette and Hui Li and Kaipeng Zhang and Yang You},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024towards.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rTBL8OhdhH},
  publisher = {OpenReview.net},
  title = {Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching},
  url = {https://openreview.net/forum?id=rTBL8OhdhH},
  year = {2024}
}

@inproceedings{guo2024connecting,
  abstract = {Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation.},
  address = {Vienna, Austria},
  author = {Qingyan Guo and Rui Wang and Junliang Guo and Bei Li and Kaitao Song and Xu Tan and Guoqing Liu and Jiang Bian and Yujiu Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024connecting.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZG3RaNIsO8},
  publisher = {OpenReview.net},
  title = {Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers},
  url = {https://openreview.net/forum?id=ZG3RaNIsO8},
  year = {2024}
}

@inproceedings{guo2024owl,
  abstract = {With the rapid development of IT operations, large language models (LLMs) are becoming increasingly important for IT operational tasks. However, existing LLMs still struggle with complex, domain-specific IT operations. This paper introduces OWL, a large language model specifically designed for IT operations. We constructed the Owl-Instruct dataset containing a wide range of IT-related information and instruction-following data. To address limitations imposed by maximum input length, we propose the Homogeneous Markov Context Extension method (HMCE). Additionally, we leverage a mixture-of-adapter strategy to improve parameter-efficient tuning across different domains or tasks. Experimental results demonstrate that OWL significantly outperforms existing general-purpose LLMs on IT operations tasks while maintaining competitive performance on general benchmarks.},
  address = {Vienna, Austria},
  author = {Hongcheng Guo and Jian Yang and Jiaheng Liu and Liqun Yang and Linzheng Chai and Jiaqi Bai and Junran Peng and Xiaorong Hu and Chao Chen and Dongfeng Zhang and Xu Shi and Tieqiao Zheng and Liangfan Zheng and Bo Zhang and Ke Xu and Zhoujun Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024owl.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=SZOQ9RKYJu},
  publisher = {OpenReview.net},
  title = {OWL: A Large Language Model for IT Operations},
  url = {https://openreview.net/forum?id=SZOQ9RKYJu},
  year = {2024}
}

@inproceedings{guo2024sampleefficient,
  abstract = {Sample-efficiency of learning in Partially Observable Markov Decision Processes (POMDPs) is known to be exponentially hard in the worst case. However, this paper shows that sample-efficient learning is achievable under an enhanced feedback model called multiple observations in hindsight. In this model, after each episode, the learner can collect multiple additional observations from encountered latent states without directly observing the states themselves. We establish sample-efficient learning for two new POMDP subclasses: multi-observation revealing POMDPs and distinguishable POMDPs. Both subclasses generalize and substantially relax revealing POMDPs‚Äîa widely studied subclass for sample-efficient learning. The model resembles practical scenarios like save/load mechanisms in game playing, making it highly relevant for real-world applications.},
  address = {Vienna, Austria},
  author = {Jiacheng Guo and Minshuo Chen and Huan Wang and Caiming Xiong and Mengdi Wang and Yu Bai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024sampleefficient.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=1hsVvgW0rU},
  publisher = {OpenReview.net},
  title = {Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight},
  url = {https://openreview.net/forum?id=1hsVvgW0rU},
  year = {2024}
}

@inproceedings{guo2024lqlora,
  abstract = {We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We also present a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that the low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations.},
  address = {Vienna, Austria},
  author = {Han Guo and Philip Greengard and Eric P. Xing and Yoon Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024lqlora.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xw29VvOMmU},
  publisher = {OpenReview.net},
  title = {LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning},
  url = {https://openreview.net/forum?id=xw29VvOMmU},
  year = {2024}
}

@inproceedings{guo2024generative,
  abstract = {Human motion stylization aims to revise the style of an input motion while keeping its content unaltered. Unlike existing works that operate directly in pose space, we leverage the latent space of pretrained autoencoders as a more expressive and robust representation for motion extraction and infusion. We present a novel generative model that produces diverse stylization results of a single motion (latent) code. During training, a motion code is decomposed into two coding components: a deterministic content code, and a probabilistic style code adhering to a prior distribution; then a generator massages the random combination of content and style codes to reconstruct the corresponding motion codes. In inference, users can opt to stylize a motion using style cues from a reference motion or a label. Even in the absence of explicit style input, our model facilitates novel re-stylization by sampling from the unconditional style prior distribution. Experimental results show that our proposed stylization models, despite their lightweight design, outperform the state-of-the-arts in style reenactment, content preservation, and generalization across various applications and settings.},
  address = {Vienna, Austria},
  author = {Chuan Guo and Yuxuan Mu and Xinxin Zuo and Peng Dai and Youliang Yan and Juwei Lu and Li Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/guo2024generative.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-07},
  pdf = {https://openreview.net/pdf?id=daEqXJ0yZo},
  publisher = {OpenReview.net},
  title = {Generative Human Motion Stylization in Latent Space},
  url = {https://openreview.net/forum?id=daEqXJ0yZo},
  year = {2024}
}

@inproceedings{guo2024lpntk,
  abstract = {Although much research has been done on proposing new models or loss functions to improve the generalisation of artificial neural networks ({ANNs}), less attention has been directed to the impact of the training data on generalisation. In this work, we start from approximating the interaction between samples, i.e. how learning one sample would modify the model's prediction on other samples. Through analysing the terms involved in weight updates in supervised learning, we find that labels influence the interaction between samples. Therefore, we propose the labelled pseudo Neural Tangent Kernel ({lpNTK}) which takes label information into consideration when measuring the interactions between samples. We first prove that {lpNTK} asymptotically converges to the empirical neural tangent kernel in terms of the Frobenius norm under certain assumptions. Secondly, we illustrate how {lpNTK} helps to understand learning phenomena identified in previous work, specifically the learning difficulty of samples and forgetting events during learning. Moreover, we also show that using {lpNTK} to identify and remove poisoning training samples does not hurt the generalisation performance of {ANNs}.},
  author = {Shangmin Guo and Yi Ren and Stefano V. Albrecht and Kenny Smith},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/guo2024lpntk.pdf:pdf},
  note = {{ICLR} 2024 poster presentation. Primary area: Learning Theory. Code available at https://github.com/Shawn-Guo-CN/lpNTK. Authors: Shangmin Guo (University of Edinburgh), Yi Ren (University of British Columbia), Stefano V. Albrecht (University of Edinburgh), Kenny Smith (University of Edinburgh)},
  pdf = {https://openreview.net/pdf?id=8Ju0VmvMCW},
  publisher = {OpenReview.net},
  title = {{lpNTK}: Better Generalisation with Less Data via Sample Interaction During Learning},
  url = {https://openreview.net/forum?id=8Ju0VmvMCW},
  year = {2024}
}

@inproceedings{guo2024beam,
  abstract = {Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memory algorithm, which achieved the new state-of-the-art on the Practical Molecular Optimization benchmark for sample efficiency. The combined algorithm generates more high reward molecules and faster, given a fixed oracle budget. Beam Enumeration shows that improvements to explainability and sample efficiency for molecular design can be made synergistic.},
  author = {Jeff Guo and Philippe Schwaller},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/guo2024beam.pdf:pdf},
  note = {{ICLR} 2024. Primary area: Applications to physical sciences. Molecular design, reinforcement learning, explainability. Also available at arXiv:2309.13957},
  pdf = {https://openreview.net/pdf?id=7UhxsmbdaQ},
  publisher = {OpenReview.net},
  title = {Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design},
  url = {https://openreview.net/forum?id=7UhxsmbdaQ},
  year = {2024}
}

@inproceedings{guo2024outofvariable,
  abstract = {The ability of an agent to do well in new environments is a critical aspect of intelligence. In machine learning, this ability is known as \emph{strong} or \emph{out-of-distribution} generalization. However, merely considering differences in distributions is inadequate for fully capturing differences between learning environments. In the present paper, we investigate \emph{out-of-variable} generalization, which pertains to an agent's generalization capabilities concerning environments with variables that were never jointly observed before. This skill closely reflects the process of animate learning: we, too, explore Nature by probing, observing, and measuring proper \emph{subsets} of variables at any given time. Mathematically, \emph{oov} generalization requires the efficient re-use of past marginal information, i.e., information over subsets of previously observed variables. We study this problem, focusing on prediction tasks across environments that contain overlapping, yet distinct, sets of causes. We show that after fitting a classifier, the residual distribution in one environment reveals the partial derivative of the true generating function with respect to the unobserved causal parent in that environment. We leverage this information and propose a method that exhibits non-trivial out-of-variable generalization performance when facing an overlapping, yet distinct, set of causal predictors.},
  author = {Siyuan Guo and Jonas Bernhard Wildberger and Bernhard Sch{\"o}lkopf},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/guo2024outofvariable.pdf:pdf},
  note = {{ICLR} 2024. Primary area: Transfer learning, meta learning, and lifelong learning. Out-of-distribution generalization, causal inference. Authors: Siyuan Guo, Jonas Bernhard Wildberger, Bernhard Sch{\"o}lkopf (Max Planck Institute for Intelligent Systems)},
  pdf = {https://openreview.net/pdf?id=zwMfg9PfPs},
  publisher = {OpenReview.net},
  title = {Out-of-Variable Generalisation for Discriminative Models},
  url = {https://openreview.net/forum?id=zwMfg9PfPs},
  year = {2024}
}

@inproceedings{guo2024beyond,
  abstract = {Alignment with human preference is a desired property of large language models ({LLMs}). Currently, the main alignment approach is based on reinforcement learning from human feedback ({RLHF}). Despite the effectiveness of {RLHF}, it is intricate to implement and train, thus recent studies explore how to develop alternative alignment approaches based on supervised fine-tuning ({SFT}). A major limitation of {SFT} is that it essentially does imitation learning, which can't fully understand what are the expected behaviors. To address this issue, we propose an improved alignment approach named {FIGA}. Different from prior methods, we incorporate fine-grained (i.e., token or phrase level) quality signals that are derived by contrasting good and bad responses. Our approach has made two major contributions. Firstly, we curate a refined alignment dataset that pairs initial responses and the corresponding revised ones. Secondly, we devise a new loss function can leverage fine-grained quality signals to instruct the learning of {LLMs} for alignment. Extensive experiments have demonstrated the effectiveness of our approaches by comparing a number of competitive baselines.},
  author = {Geyang Guo and Ranchi Zhao and Tianyi Tang and Xin Zhao and Ji-Rong Wen},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/guo2024beyond.pdf:pdf},
  note = {{ICLR} 2024. Primary area: Representation learning for computer vision, audio, language, and other modalities. {LLM} alignment, human preference, {FIGA}. Code available at https://github.com/RUCAIBox/FIGA. Authors from Renmin University of China. Also available at arXiv:2311.04072},
  pdf = {https://openreview.net/pdf?id=LNLjU5C5dK},
  publisher = {OpenReview.net},
  title = {Beyond Imitation: Leveraging Fine-grained Quality Signals for Alignment},
  url = {https://openreview.net/forum?id=LNLjU5C5dK},
  year = {2024}
}

@inproceedings{gupta2024structuring,
  abstract = {Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima force augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, {CARE}: Contrastive Augmentation-induced Rotational Equivariance, leads to improved performance on downstream tasks and ensures sensitivity in embedding space to important variations in data (e.g., color) that standard contrastive methods do not achieve. Code is available at https://github.com/Sharut/CARE},
  author = {Sharut Gupta and Joshua Robinson and Derek Lim and Soledad Villar and Stefanie Jegelka},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/gupta2024structuring.pdf:pdf},
  note = {{ICLR} 2024 poster presentation. Primary area: Unsupervised, self-supervised, semi-supervised, and supervised representation learning. Contrastive learning, equivariance, geometric structure. Code available at https://github.com/Sharut/CARE. Authors: Sharut Gupta ({MIT}), Joshua Robinson ({MIT}), Derek Lim, Soledad Villar, Stefanie Jegelka ({MIT}). Also available at arXiv:2306.13924},
  pdf = {https://openreview.net/pdf?id=lgaFMvZHSJ},
  publisher = {OpenReview.net},
  title = {Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning},
  url = {https://openreview.net/forum?id=lgaFMvZHSJ},
  year = {2024}
}

@inproceedings{gupta2024dualencoders,
  abstract = {Dual-encoder ({DE}) models are widely used in retrieval tasks, most commonly studied on open {QA} benchmarks that are often characterized by multi-class and limited training data. In contrast, their performance in multi-label and data-rich retrieval settings like extreme multi-label classification ({XMC}), remains under-explored. Current empirical evidence indicates that {DE} models fall significantly short on {XMC} benchmarks, where {SOTA} methods linearly scale the number of learnable parameters with the total number of classes (documents in the corpus) by employing per-class classification head. To this end, we first study and highlight that existing multi-label contrastive training losses are not appropriate for training {DE} models on {XMC} tasks. We propose decoupled softmax loss - a simple modification to the {InfoNCE} loss - that overcomes the limitations of existing contrastive losses. We further extend our loss design to a soft top-k operator-based loss which is tailored to optimize top-k prediction performance. When trained with our proposed loss functions, standard {DE} models alone can match or outperform {SOTA} methods by up to 2\% at Precision@1 even on the largest {XMC} datasets while being 20x smaller in terms of the number of trainable parameters. This leads to more parameter-efficient and universally applicable solutions for retrieval tasks.},
  author = {Nilesh Gupta and Devvrit and Ankit Singh Rawat and Srinadh Bhojanapalli and Prateek Jain and Inderjit S. Dhillon},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/gupta2024dualencoders.pdf:pdf},
  note = {{ICLR} 2024. Primary area: Unsupervised, self-supervised, semi-supervised, and supervised representation learning. Extreme multi-label classification, dual-encoders, retrieval. Code available at https://github.com/nilesh2797/dexml. Authors include researchers from Google Research. Also available at arXiv:2310.10636},
  pdf = {https://openreview.net/pdf?id=dNe1T0Ahby},
  publisher = {OpenReview.net},
  title = {Dual-Encoders for Extreme Multi-label Classification},
  url = {https://openreview.net/forum?id=dNe1T0Ahby},
  year = {2024}
}

@inproceedings{gupta2024context,
  abstract = {Two lines of work are taking center stage in {AI} research. On the one hand, increasing efforts are being made to build models that generalize out-of-distribution ({OOD}). Unfortunately, a hard lesson so far is that no proposal convincingly outperforms a simple empirical risk minimization baseline. On the other hand, large language models ({LLMs}) have erupted as algorithms able to learn in-context, generalizing on-the-fly to the eclectic contextual circumstances. We argue that context is environment, and posit that in-context learning holds the key to better domain generalization. Via extensive theory and experiments, we show that paying attention to context---unlabeled examples as they arrive---allows our proposed In-Context Risk Minimization ({ICRM}) algorithm to zoom-in on the test environment risk minimizer, leading to significant {OOD} performance improvements.},
  author = {Sharut Gupta and Stefanie Jegelka and David Lopez-Paz and Kartik Ahuja},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/gupta2024context.pdf:pdf},
  note = {{ICLR} 2024 poster presentation. Domain generalization, in-context learning, {ICRM}. Code available at https://github.com/facebookresearch/ICRM. Authors: Sharut Gupta ({MIT}), Stefanie Jegelka ({MIT}), David Lopez-Paz (Facebook {AI} Research), Kartik Ahuja (Facebook {AI} Research)},
  pdf = {https://openreview.net/pdf?id=8VPWfqtQMX},
  publisher = {OpenReview.net},
  title = {Context is Environment},
  url = {https://openreview.net/forum?id=8VPWfqtQMX},
  year = {2024}
}

@inproceedings{gupta2024mirage,
  abstract = {{GNNs}, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of {GNNs} on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target {GNN} architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing {GNN} decomposes the input graph into a multiset of computation trees. Furthermore, the frequency distribution of computation trees is often skewed in nature, enabling us to condense this data into a concise distilled summary. By compressing the computation data itself, as opposed to emulating gradient flows on the original training set---a prevalent approach to date---Mirage transforms into an unsupervised and architecture-agnostic distillation algorithm. Extensive benchmarking on real-world datasets underscores Mirage's superiority, showcasing enhanced generalization accuracy, data compression, and distillation efficiency when compared to state-of-the-art baselines.},
  author = {Mridul Gupta and Sahil Manchanda and Hariprasad Kodamana and Sayan Ranu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/gupta2024mirage.pdf:pdf},
  note = {{ICLR} 2024. Graph neural networks, distillation, graph classification. Model-agnostic, computation trees. Authors from Indian Institute of Technology, Delhi. Also available at arXiv:2310.09486},
  pdf = {https://openreview.net/pdf?id=78iGZdqxYY},
  publisher = {OpenReview.net},
  title = {Mirage: Model-agnostic Graph Distillation for Graph Classification},
  url = {https://openreview.net/forum?id=78iGZdqxYY},
  year = {2024}
}

@inproceedings{gupta2024language,
  abstract = {Recent advances in language models ({LMs}) have led to significant improvements in quality on complex {NLP} tasks, but at the expense of increased inference costs. A simple strategy to achieve more favorable cost-quality tradeoffs is cascading: here, a small model is invoked for most ``easy'' instances, while a few ``hard'' instances are deferred to the large model. While the principles underpinning effective cascading are well-studied for classification tasks --- with deferral based on predicted class uncertainty favored theoretically and practically --- a similar understanding is lacking for generative {LM} tasks. In this work, we initiate a systematic study of deferral rules for {LM} cascades. We begin by examining the natural extension of predicted class uncertainty to generative {LM} tasks, namely, the predicted sequence uncertainty. We show that this measure suffers from the length bias problem, either over- or under-emphasizing outputs based on their lengths. We propose token-level deferral rules to mitigate this issue, and further extend our approach to take into account cascading beyond two models. Experiments across four tasks --- translation, summarization, question answering, and math word problems --- and model families ranging from {T5} to {GPT-4} demonstrate the effectiveness of our approach.},
  author = {Neha Gupta and Harikrishna Narasimhan and Wittawat Jitkrittum and Ankit Singh Rawat and Aditya Krishna Menon and Sanjiv Kumar},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/gupta2024language.pdf:pdf},
  note = {{ICLR} 2024. Language model cascades, uncertainty estimation, token-level analysis. Cost-quality tradeoffs, deferral rules. Authors from Google Research. Also available at arXiv:2404.10136},
  pdf = {https://openreview.net/pdf?id=KgaBScZ4VI},
  publisher = {OpenReview.net},
  title = {Language Model Cascades: Token-Level Uncertainty And Beyond},
  url = {https://openreview.net/forum?id=KgaBScZ4VI},
  year = {2024}
}

@inproceedings{gupta2024bias,
  abstract = {Recent works have showcased the ability of {LLMs} to embody diverse personas in their responses, exemplified by prompts like `You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of {LLMs} and enables human behavior simulation, its effect on {LLMs}' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of {LLMs} to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 {LLMs}, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that {LLMs} harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. Specifically, we find that assigning personas from certain socio-demographic groups can systematically decrease performance on reasoning tasks, which has potential negative implications in real-world applications. We further investigate the origins of this bias and find correlations between the extent of bias and the socio-economic status of the demographic group. Our findings emphasize the urgent need to investigate and mitigate the potential negative side-effects of persona assignment in {LLMs} before their broad adoption.},
  author = {Shashank Gupta and Vaishnavi Shrivastava and Ameet Deshpande and Ashwin Kalyan and Peter Clark and Ashish Sabharwal and Tushar Khot},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/gupta2024bias.pdf:pdf},
  note = {{ICLR} 2024. {LLM} bias, persona assignment, reasoning tasks. Socio-demographic bias, fairness. Dataset available at https://huggingface.co/datasets/allenai/persona-bias. Code at https://github.com/allenai/persona-bias. Authors from Allen Institute for {AI} and others. Also available at arXiv:2311.04892},
  pdf = {https://openreview.net/pdf?id=kGteeZ18Ir},
  publisher = {OpenReview.net},
  title = {Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned {LLMs}},
  url = {https://openreview.net/forum?id=kGteeZ18Ir},
  year = {2024}
}

@inproceedings{gur2024realworld,
  abstract = {Pre-trained large language models ({LLMs}) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on {HTML}. We introduce {WebAgent}, an {LLM}-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. {WebAgent} plans ahead by decomposing instructions into canonical sub-instructions, summarizes long {HTML} documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design {WebAgent} with Flan-U-{PaLM}, for grounded code generation, and {HTML}-T5, new pre-trained {LLMs} for long {HTML} documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50\%, and that {HTML}-T5 is the best model to solve various {HTML} understanding tasks; achieving 18.7\% higher success rate than the prior method on {MiniWoB} web automation benchmark, and SoTA performance on {Mind2Web}, an offline task planning evaluation.},
  address = {Vienna, Austria},
  author = {Izzeddin Gur and Hiroki Furuta and Austin V. Huang and Mustafa Safdari and Yutaka Matsuo and Douglas Eck and Aleksandra Faust},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gur2024realworld.pdf:pdf},
  month = {5},
  note = {Oral presentation},
  pdf = {https://openreview.net/pdf?id=9JQtrumvg8},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {A Real-World {WebAgent} with Planning, Long Context Understanding, and Program Synthesis},
  url = {https://openreview.net/forum?id=9JQtrumvg8},
  year = {2024}
}

@inproceedings{gurnee2024language,
  abstract = {The capabilities of large language models ({LLMs}) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, {US}, {NYC} places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that {LLMs} learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual 'space neurons' and 'time neurons' that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern {LLMs} learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.},
  address = {Vienna, Austria},
  author = {Wes Gurnee and Max Tegmark},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gurnee2024language.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=jE8xbmvFin},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Language Models Represent Space and Time},
  url = {https://openreview.net/forum?id=jE8xbmvFin},
  year = {2024}
}

@inproceedings{gurugubelli2024sann,
  abstract = {Simplicial neural networks ({SNNs}) are deep models for higher-order graph representation learning. {SNNs} learn low-dimensional embeddings of simplices in a simplicial complex by aggregating features of their respective upper, lower, boundary, and coboundary adjacent simplices. The aggregation in {SNNs} is carried out during training. Since the number of simplices of various orders in a simplicial complex is significantly large, the memory and training-time requirement in {SNNs} is enormous. In this work, we propose a scalable simplicial-aware neural network ({SaNN}) model with a constant run-time and memory requirements independent of the size of the simplicial complex and the density of interactions in it. {SaNN} is based on pre-aggregated simplicial-aware features as inputs to a neural network, so it has a strong simplicial-structural inductive bias. We provide theoretical conditions under which {SaNN} is provably more powerful than the Weisfeiler-Lehman ({WL}) graph isomorphism test and as powerful as the simplicial Weisfeiler-Lehman ({SWL}) test. We also show that {SaNN} is permutation and orientation equivariant and satisfies simplicial-awareness of the highest order in a simplicial complex. We demonstrate via numerical experiments that despite being computationally economical, the proposed model achieves state-of-the-art performance in predicting trajectories, simplicial closures, and classifying graphs.},
  address = {Vienna, Austria},
  author = {Sravanthi Gurugubelli and Sundeep Prabhakar Chepuri},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/gurugubelli2024sann.pdf:pdf},
  month = {5},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=eUgS9Ig8JG},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{SaNN}: Simple Yet Powerful Simplicial-aware Neural Networks},
  url = {https://openreview.net/forum?id=eUgS9Ig8JG},
  year = {2024}
}

@inproceedings{hagemann2024posterior,
  abstract = {We propose conditional flows of the maximum mean discrepancy ({MMD}) with the negative distance kernel for posterior sampling and conditional generative modelling. This {MMD}, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting. We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions. Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional. The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.},
  address = {Vienna, Austria},
  author = {Paul Hagemann and Johannes Hertrich and Fabian Altekr√ºger and Robert Beinert and Jannis Chemseddine and Gabriele Steidl},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hagemann2024posterior.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=YrXHEb2qMb},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Posterior Sampling Based on Gradient Flows of the {MMD} with Negative Distance Kernel},
  url = {https://openreview.net/forum?id=YrXHEb2qMb},
  year = {2024}
}

@inproceedings{haghighat2024pretraining,
  abstract = {Masked Image Modeling ({MIM}) is a powerful self-supervised strategy for visual pre-training without the use of labels. {MIM} applies random crops to input images, processes them with an encoder, and then recovers the masked inputs with a decoder, which encourages the network to capture and learn structural information about objects and scenes. The intermediate feature representations obtained from {MIM} are suitable for fine-tuning on downstream tasks. In this paper, we propose an Image Modeling framework based on random orthogonal projection instead of binary masking as in {MIM}. Our proposed Random Orthogonal Projection Image Modeling ({ROPIM}) reduces spatially-wise token information under guaranteed bound on the noise variance and can be considered as masking entire spatial image area under locally varying masking degrees. Since {ROPIM} uses a random subspace for the projection that realizes the masking step, the readily available complement of the subspace can be used during unmasking to promote recovery of removed information. In this paper, we show that using random orthogonal projection leads to superior performance compared to crop-based masking. We demonstrate state-of-the-art results on several popular benchmarks.},
  address = {Vienna, Austria},
  author = {Maryam Haghighat and Peyman Moghadam and Shaheer Mohamed and Piotr Koniusz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/haghighat2024pretraining.pdf:pdf},
  month = {5},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=z4Hcegjzph},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Pre-training with Random Orthogonal Projection Image Modeling},
  url = {https://openreview.net/forum?id=z4Hcegjzph},
  year = {2024}
}

@inproceedings{halawi2024overthinking,
  abstract = {Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Danny Halawi and Jean-Stanislas Denain and Jacob Steinhardt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2307.09476},
  file = {:/home/b/documents/inproceedings/halawi2024overthinking.pdf:pdf},
  month = {5},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=Tigr1kMDZy},
  primaryclass = {cs.CL},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Overthinking the Truth: Understanding how Language Models Process False Demonstrations},
  url = {https://openreview.net/forum?id=Tigr1kMDZy},
  year = {2024}
}

@inproceedings{hamade2024designing,
  abstract = {Powerful artificial intelligence systems are often used in settings where they must interact with agents that are computationally much weaker, for example when they work alongside humans or operate in complex environments where some tasks are handled by algorithms, heuristics, or other entities of varying computational power. For {AI} agents to successfully interact in these settings, however, achieving superhuman performance alone is not sufficient; they also need to account for suboptimal actions or idiosyncratic style from their less-skilled counterparts. We propose a formal evaluation framework for assessing the compatibility of near-optimal {AI} with interaction partners who may have much lower levels of skill; we use popular collaborative chess variants as model systems to study and develop {AI} agents that can successfully interact with lower-skill entities. Traditional chess engines designed to output near-optimal moves prove to be inadequate partners when paired with engines of various lower skill levels in this domain, as they are not designed to consider the presence of other agents. We contribute three methodologies to explicitly create skill-compatible {AI} agents in complex decision-making settings, and two chess game frameworks designed to foster collaboration between powerful {AI} agents and less-skilled partners. On these frameworks, our agents outperform state-of-the-art chess {AI} (based on {AlphaZero}) despite being weaker in conventional chess, demonstrating that skill-compatibility is a tangible trait that is qualitatively and measurably distinct from raw performance. Our evaluations further explore and clarify the mechanisms by which our agents achieve skill-compatibility.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Karim Hamade and Reid McIlroy-Young and Siddhartha Sen and Jon M. Kleinberg and Ashton Anderson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2405.05066},
  file = {:/home/b/documents/inproceedings/hamade2024designing.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=79rfgv3jw4},
  primaryclass = {cs.AI},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Designing Skill-Compatible {AI}: Methodologies and Frameworks in Chess},
  url = {https://openreview.net/forum?id=79rfgv3jw4},
  year = {2024}
}

@inproceedings{hamidieh2024views,
  abstract = {Supervised learning methods have been found to exhibit inductive biases favoring simpler features. When such features are spuriously correlated with the label, this can result in suboptimal performance on minority subgroups. Despite the growing popularity of methods which learn from unlabeled data, the extent to which these representations rely on spurious features for prediction is unclear. In this work, we explore the impact of spurious features on Self-Supervised Learning ({SSL}) for visual representation learning. We first empirically show that commonly used augmentations in {SSL} can cause undesired invariances in the image space, and illustrate this with a simple example. We further show that classical approaches in combating spurious correlations, such as dataset re-sampling during {SSL}, do not consistently lead to invariant representations. Motivated by these findings, we propose {LateTVG} to remove spurious information from these representations during pretraining, by regularizing later layers of the encoder via pruning. We find that our method produces representations which outperform the baselines on several benchmarks, without the need for group or label information during {SSL}.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Kimia Hamidieh and Haoran Zhang and Swami Sankaranarayanan and Marzyeh Ghassemi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2406.18562},
  file = {:/home/b/documents/inproceedings/hamidieh2024views.pdf:pdf},
  month = {5},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=mutJBk3ILg},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Views Can Be Deceiving: Improved {SSL} Through Feature Space Augmentation},
  url = {https://openreview.net/forum?id=mutJBk3ILg},
  year = {2024}
}

@inproceedings{hamman2024demystifying,
  abstract = {This work presents an information-theoretic perspective to group fairness trade-offs in federated learning ({FL}) with respect to sensitive attributes, such as gender, race, etc. Existing works often focus on either global fairness (overall disparity of the model across all clients) or local fairness (disparity of the model at each client), without always considering their trade-offs. There is a lack of understanding regarding the interplay between global and local fairness in {FL}, particularly under data heterogeneity, and if and when one implies the other. To address this gap, we leverage a body of work in information theory called partial information decomposition ({PID}), which first identifies three sources of unfairness in {FL}, namely, Unique Disparity, Redundant Disparity, and Masked Disparity. We demonstrate how these three disparities contribute to global and local fairness using canonical examples. This decomposition helps us derive fundamental limits on the trade-off between global and local fairness, highlighting where they agree or disagree. We introduce the Accuracy and Global-Local Fairness Optimality Problem ({AGLFOP}), a convex optimization that defines the theoretical limits of accuracy and fairness trade-offs, identifying the best possible performance any {FL} strategy can attain given a dataset and client distribution. We also present experimental results on synthetic datasets and the {ADULT} dataset to support our theoretical findings.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Faisal Hamman and Sanghamitra Dutta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2307.11333},
  file = {:/home/b/documents/inproceedings/hamman2024demystifying.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=SBj2Qdhgew},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Demystifying Local \& Global Fairness Trade-offs in Federated Learning Using Partial Information Decomposition},
  url = {https://openreview.net/forum?id=SBj2Qdhgew},
  year = {2024}
}

@inproceedings{hammouamri2024learning,
  abstract = {Spiking Neural Networks ({SNNs}) are a promising research direction for building power-efficient information processing systems, especially for temporal tasks such as speech recognition. In {SNNs}, delays refer to the time needed for one spike to travel from one neuron to another. These delays matter because they influence the spike arrival times, and it is well-known that spiking neurons respond more strongly to coincident input spikes. More formally, it has been shown theoretically that plastic delays greatly increase the expressivity in {SNNs}. Yet, efficient algorithms to learn these delays have been lacking. Here, we propose a new discrete-time algorithm that addresses this issue in deep feedforward {SNNs} using backpropagation, in an offline manner. To simulate delays between consecutive layers, we use 1{D} convolutions across time. The kernels contain only a few non-zero weights -- one per synapse -- whose positions correspond to the delays. These positions are learned together with the weights using the recently proposed Dilated Convolution with Learnable Spacings ({DCLS}).},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Ilyass Hammouamri and Ismail Khalfaoui Hassani and Timoth√©e Masquelier},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2306.17670},
  file = {:/home/b/documents/inproceedings/hammouamri2024learning.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=4r2ybzJnmN},
  primaryclass = {cs.NE},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Learning Delays in Spiking Neural Networks using Dilated Convolutions with Learnable Spacings},
  url = {https://openreview.net/forum?id=4r2ybzJnmN},
  year = {2024}
}

@inproceedings{han2024ffb,
  abstract = {The rapid development of fairness research in machine learning has led to a plethora of methods for addressing bias and promoting group fairness. However, the lack of a systematic and comprehensive benchmark has hindered meaningful comparison and evaluation of these methods. In this work, we present FFB (Fair Fairness Benchmark), an open-source, extensible, and comprehensive benchmark for evaluating in-processing group fairness methods. FFB addresses key challenges including inconsistent implementations, differing evaluation protocols, and lack of transparency. We conduct extensive experiments across 45,079 configurations with 14,428 GPU hours, comparing 14 fairness methods across 20 datasets. Our results provide valuable insights into the performance-fairness trade-offs of different methods and highlight the importance of proper hyperparameter tuning. FFB aims to facilitate the growth and development of the fairness research community.},
  author = {Xiaotian Han and Jianfeng Chi and Yu Chen and Qifan Wang and Han Zhao and Na Zou and Xia Hu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024ffb.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=TzAJbTClAz},
  publisher = {OpenReview.net},
  title = {FFB: A Fair Fairness Benchmark for In-Processing Group Fairness Methods},
  url = {https://openreview.net/forum?id=TzAJbTClAz},
  year = {2024}
}

@inproceedings{han2024hyperattention,
  abstract = {We present an approximate attention mechanism named HyperAttention to address the computational challenges posed by the growing complexity of long contexts used in Large Language Models (LLMs). Recent work suggests that in the worst-case scenario, the quadratic time is necessary unless the entries of the attention matrix are bounded or the matrix has low stable rank. We introduce two parameters which measure: (1) the max column norm in the normalized attention matrix, and (2) the ratio of row norms in the unnormalized attention matrix after detecting and removing large entries. We use these fine-grained parameters to capture the hardness of the problem. Despite previous lower bounds, we are able to achieve a linear time sampling algorithm even when the matrix has unbounded entries or a large stable rank, provided the above parameters are small. HyperAttention features a modular design that easily accommodates integration of other fast low-level implementations, particularly FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to identify large entries, HyperAttention outperforms existing methods, giving significant speed improvements compared to state-of-the-art solutions like FlashAttention.},
  author = {Insu Han and Rajesh Jayaram and Amin Karbasi and Vahab Mirrokni and David P. Woodruff and Amir Zandieh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024hyperattention.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=Eh0Od2BJIM},
  publisher = {OpenReview.net},
  title = {HyperAttention: Long-context Attention in Near-Linear Time},
  url = {https://openreview.net/forum?id=Eh0Od2BJIM},
  year = {2024}
}

@inproceedings{han2024respect,
  abstract = {The truthfulness of existing explanation methods in authentically elucidating the underlying model's decision-making process has been questioned. Existing methods have deviated from faithfully representing the model, thus susceptible to adversarial attacks. To address this, we propose a novel eXplainable AI (XAI) method called SRD (Sharing Ratio Decomposition), which sincerely reflects the model's inference process, resulting in significantly enhanced robustness in our explanations. Different from the conventional emphasis on the neuronal level, we adopt a vector perspective to consider the intricate nonlinear interactions between filters. We also introduce an interesting observation termed Activation-Pattern-Only Prediction (APOP), letting us emphasize the importance of inactive neurons and redefine relevance encapsulating all relevant information including both active and inactive neurons. Our method, SRD, allows for the recursive decomposition of a Pointwise Feature Vector (PFV), providing a high-resolution Effective Receptive Field (ERF) at any layer.},
  author = {Sangyu Han and Yearim Kim and Nojun Kwak},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024respect.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=U7VW3KBm34},
  publisher = {OpenReview.net},
  title = {Respect the model: Fine-grained and Robust Explanation with Sharing Ratio Decomposition},
  url = {https://openreview.net/forum?id=U7VW3KBm34},
  year = {2024}
}

@inproceedings{han2024structural,
  abstract = {Graph Neural Networks (GNNs) have seen significant achievements in semi-supervised node classification. Yet, their efficacy often hinges on access to high-quality labeled node samples, which may not always be available in real-world scenarios. Active learning presents a strategy to judiciously select a subset of nodes for labeling, but its application to graph data introduces unique challenges due to non-independent and identically distributed (non-i.i.d.) node structures and potential bias from labeled node positioning. We propose SCARCE, a unified optimization framework that leverages graph structure and mitigates structural bias while easily incorporating node features. Extensive experiments demonstrate that the proposed method not only improves the GNNs performance but also paves the way for more fair results.},
  author = {Haoyu Han and Xiaorui Liu and Li Ma and MohamadAli Torkamani and Hui Liu and Jiliang Tang and Makoto Yamada},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024structural.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=bvjcMvMn7B},
  publisher = {OpenReview.net},
  title = {Structural Fairness-aware Active Learning for Graph Neural Networks},
  url = {https://openreview.net/forum?id=bvjcMvMn7B},
  year = {2024}
}

@inproceedings{han2024image,
  abstract = {We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework that seamlessly embeds a condition-flexible diffusion model within the GPT architecture. DVP orchestrates a coherent sequence of visual programs (i.e., computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. The success is attributed to DVP achieving condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Extensive experiments demonstrate DVP's remarkable performance, surpassing concurrent arts.},
  author = {Cheng Han and James Chenhao Liang and Qifan Wang and Majid Rabbani and Sohail A. Dianat and Raghuveer Rao and Ying Nian Wu and Dongfang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024image.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=yozwqhIHXj},
  publisher = {OpenReview.net},
  title = {Image Translation as Diffusion Visual Programmers},
  url = {https://openreview.net/forum?id=yozwqhIHXj},
  year = {2024}
}

@inproceedings{han2024neural,
  abstract = {Diffusion models have emerged as a powerful tool rivaling GANs in generating high-quality samples with improved fidelity, flexibility, and robustness. A key component of these models is to learn the score function through score matching. However, it remains unclear whether gradient-based algorithms can learn the score function with a provable accuracy. This paper establishes a mathematical framework for analyzing score estimation using neural networks trained by gradient descent, covering both the optimization and the generalization aspects of the learning procedure. We propose a parametric form to formulate the denoising score-matching problem as a regression with noisy labels. The score-matching problem introduces distinct challenges, including unbounded input, vector-valued output, and an additional time variable, preventing existing techniques from being applied directly. We show that with proper designs, the evolution of neural networks during training can be accurately modeled by a series of kernel regression tasks. We establish the first generalization error (sample complexity) bounds for learning the score function with neural networks, despite the presence of noise in the observations.},
  author = {Yinbin Han and Meisam Razaviyayn and Renyuan Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024neural.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=h8GeqOxtd4},
  publisher = {OpenReview.net},
  title = {Neural Network-Based Score Estimation in Diffusion Models: Optimization and Generalization},
  url = {https://openreview.net/forum?id=h8GeqOxtd4},
  year = {2024}
}

@inproceedings{han2024trainingfree,
  abstract = {Searching for novel and diverse molecular candidates is a critical undertaking in drug and material discovery. Existing approaches have successfully adapted the diffusion model, the most effective generative model in image generation, to create 1D SMILES strings, 2D chemical graphs, or 3D molecular conformers. However, these methods are not efficient and flexible enough to generate 3D molecules with multiple desired properties, as they require additional training for the models for each new property or even a new combination of existing properties. Moreover, some properties may potentially conflict, making it impossible to find a molecule that satisfies all of them simultaneously. To address these challenges, we present a training-free conditional 3D molecular generation algorithm based on off-the-shelf unconditional diffusion models and property prediction models.},
  author = {Xu Han and Caihua Shan and Yifei Shen and Can Xu and Han Yang and Xiang Li and Dongsheng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024trainingfree.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=X41c4uB4k0},
  publisher = {OpenReview.net},
  title = {Training-free Multi-objective Diffusion Model for 3D Molecule Generation},
  url = {https://openreview.net/forum?id=X41c4uB4k0},
  year = {2024}
}

@inproceedings{han2024facing,
  abstract = {As the scale of vision models continues to grow, the emergence of Visual Prompt Tuning (VPT) as a parameter-efficient transfer learning technique has gained attention due to its superior performance compared to traditional full-finetuning. However, the conditions favoring VPT (the "when") and the underlying rationale (the "why") remain unclear. In this paper, we conduct a comprehensive analysis across 19 distinct datasets and tasks. To understand the "when" aspect, we identify the scenarios where VPT proves favorable by two dimensions: task objectives and data distributions. We find that VPT is preferable when there is 1) a substantial disparity between the original and the downstream task objectives (e.g., transitioning from classification to counting), or 2) a similarity in data distributions between the two tasks. The results indicate VPT's success cannot be attributed solely to overfitting and optimization considerations, and provides insights into VPT's mechanisms, offering guidance for its optimal utilization.},
  author = {Cheng Han and Qifan Wang and Yiming Cui and Wenguan Wang and Lifu Huang and Siyuan Qi and Dongfang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/han2024facing.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=bJx4iOIOxn},
  publisher = {OpenReview.net},
  title = {Facing the Elephant in the Room: Visual Prompt Tuning or Full finetuning?},
  url = {https://openreview.net/forum?id=bJx4iOIOxn},
  year = {2024}
}

@inproceedings{hao2024threaten,
  abstract = {Spiking Neural Networks (SNNs) have gained significant attention for their superior spatio-temporal processing capabilities and energy-efficient characteristics, but their vulnerability under adversarial attacks has become a concern. Drawing inspiration from two mainstream learning algorithms of SNNs, we observe that SNN models preserve both rate and temporal information, and conduct quantitative analysis separately for each type. We propose a hybrid adversarial attack based on rate and temporal information (HART) for Spiking Neural Networks, whose rate and temporal attributes are dynamically adjustable. Through comprehensive experiments, we demonstrate the effectiveness of our approach in threatening SNNs while revealing the unique vulnerabilities of these biologically-inspired neural networks.},
  author = {Zecheng Hao and Tong Bu and Xinyu Shi and Zihan Huang and Zhaofei Yu and Tiejun Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hao2024threaten.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=xv8iGxENyI},
  publisher = {OpenReview.net},
  title = {Threaten Spiking Neural Networks through Combining Rate and Temporal Information},
  url = {https://openreview.net/forum?id=xv8iGxENyI},
  year = {2024}
}

@inproceedings{hao2024bilevel,
  abstract = {Bilevel optimization is an important formulation for many machine learning problems. Current bilevel optimization algorithms assume that the gradient of the upper-level function is Lipschitz. However, recent studies reveal that certain neural networks such as recurrent neural networks (RNNs) and long-short-term memory networks (LSTMs) exhibit potential unbounded smoothness, rendering conventional bilevel optimization algorithms unsuitable. In this paper, we design a new bilevel optimization algorithm, namely BO-REP (Bilevel Optimization with lower-level initialization REfinement and Periodic updates), to address this challenge. The algorithm updates the upper-level variable using normalized momentum to control the effects of stochastic gradient noise and possibly unbounded gradients. When the upper-level problem is nonconvex and unbounded smooth, and the lower-level problem is strongly convex, our algorithm requires O(1/Œµ^4) iterations to find an Œµ-stationary point in the stochastic setting. This result matches the state-of-the-art complexity results under the bounded smoothness setting up to logarithmic factors.},
  author = {Jie Hao and Xiaochuan Gong and Mingrui Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hao2024bilevel.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=LqRGsGWOTX},
  publisher = {OpenReview.net},
  title = {Bilevel Optimization under Unbounded Smoothness: A New Algorithm and Convergence Analysis},
  url = {https://openreview.net/forum?id=LqRGsGWOTX},
  year = {2024}
}

@inproceedings{hao2024a,
  abstract = {Spiking Neural Networks ({SNNs}) have garnered considerable attention due to their energy efficiency and unique biological characteristics. However, the widely adopted Leaky Integrate-and-Fire ({LIF}) model has been revealed to exhibit significant deficiencies in deep-layer gradient calculation and capturing global information on the time dimension. This paper proposes a Learnable Multi-hierarchical ({LM-H}) model to address limitations in current {Spiking} {Neural} {Network} approaches, enabling more effective spatio-temporal back-propagation for training deep spiking networks.},
  address = {Vienna, Austria},
  author = {Zecheng Hao and Xinyu Shi and Zihan Huang and Tong Bu and Zhaofei Yu and Tiejun Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hao2024a.pdf:pdf},
  month = {5},
  note = {Poster presentation. Primary area: applications to neuroscience \& cognitive science},
  pdf = {https://openreview.net/pdf?id=g52tgL8jy6},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{A} {Progressive} {Training} {Framework} for {Spiking} {Neural} {Networks} with {Learnable} {Multi}-hierarchical {Model}},
  url = {https://openreview.net/forum?id=g52tgL8jy6},
  year = {2024}
}

@inproceedings{haramati2024entity,
  abstract = {Manipulating objects is a hallmark of human intelligence, and an important task in domains such as robotics. In principle, Reinforcement Learning ({RL}) offers a general approach to learn object manipulation. In practice, however, domains with more than a few objects are difficult for {RL} agents due to the curse of dimensionality, especially when learning from raw image observations. This paper proposes a structured approach for visual {RL} that can represent multiple objects and their interactions, learn goal-conditioned manipulation, handle goals with object dependencies, and demonstrate generalization from learning with 3 objects to tasks with over 10 objects.},
  address = {Vienna, Austria},
  author = {Dan Haramati and Tal Daniel and Aviv Tamar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/haramati2024entity.pdf:pdf},
  month = {5},
  note = {Spotlight presentation (top 5\% of accepted papers)},
  pdf = {https://openreview.net/pdf?id=uDxeSZ1wdI},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Entity}-{Centric} {Reinforcement} {Learning} for {Object} {Manipulation} from {Pixels}},
  url = {https://openreview.net/forum?id=uDxeSZ1wdI},
  year = {2024}
}

@inproceedings{hardt2024test,
  abstract = {Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small {GPT}-2 and a {GPT}-{Neo} model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.},
  address = {Vienna, Austria},
  author = {Moritz Hardt and Yu Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hardt2024test.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=CNL2bku4ra},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Test}-{Time} {Training} on {Nearest} {Neighbors} for {Large} {Language} {Models}},
  url = {https://openreview.net/forum?id=CNL2bku4ra},
  year = {2024}
}

@inproceedings{harrington2024coco,
  abstract = {Evaluating deep neural networks ({DNNs}) as models of human perception has given rich insights into both human visual processing and representational properties of {DNNs}. We extend this work by analyzing how well {DNNs} perform compared to humans when constrained by peripheral vision -- which limits human performance on a variety of tasks, but also benefits the visual system significantly. We evaluate this by (1) modifying the Texture Tiling Model ({TTM}), a well tested model of peripheral vision to be more flexibly used with {DNNs}, (2) generating a large dataset which we call {COCO}-{Periph} that contains images processed to simulate peripheral vision, and (3) conducting human psychophysics experiments to compare human and machine performance. Training on {COCO}-{Periph} begins to reduce the gap between human and {DNN} performance and leads to small increases in corruption robustness, but {DNNs} still struggle to capture human-like sensitivity to peripheral clutter. Our work brings us closer to accurately modeling human vision, and paves the way for {DNNs} to mimic and sometimes benefit from properties of human visual processing.},
  address = {Vienna, Austria},
  author = {Anne Harrington and Vasha {DuTell} and Mark Hamilton and Ayush Tewari and Simon Stent and William T. Freeman and Ruth Rosenholtz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/harrington2024coco.pdf:pdf},
  month = {5},
  note = {Poster presentation. Primary area: applications to neuroscience \& cognitive science},
  pdf = {https://openreview.net/pdf?id=MiRPBbQNHv},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{COCO}-{Periph}: {Bridging} the {Gap} {Between} {Human} and {Machine} {Perception} in the {Periphery}},
  url = {https://openreview.net/forum?id=MiRPBbQNHv},
  year = {2024}
}

@inproceedings{harrison2024variational,
  abstract = {We introduce a deterministic variational formulation for training {Bayesian} last layer neural networks. This yields a sampling-free, single-pass model and loss that effectively improves uncertainty estimation. The variational {Bayesian} last layer ({VBLL}) can be trained and evaluated with only quadratic complexity in last layer width, and is thus (nearly) computationally free to add to standard architectures. The variational formulation relies on a deterministic lower bound on the marginal likelihood, which enables highly-efficient mini-batch, sampling-free loss computation, and is thus highly scalable. We experimentally investigate {VBLLs}, and show that they improve predictive accuracy, calibration, and out of distribution detection over baselines across both regression and classification.},
  address = {Vienna, Austria},
  author = {James Harrison and John Willes and Jasper Snoek},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/harrison2024variational.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=Sx7BIiPzys},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Variational} {Bayesian} {Last} {Layers}},
  url = {https://openreview.net/forum?id=Sx7BIiPzys},
  year = {2024}
}

@inproceedings{hasegawa2024exploring,
  abstract = {Recognition problems in long-tailed data, in which the sample size per class is heavily skewed, have gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance compared with existing methods devised in various ways. However, there is a lack of understanding as to why this method is effective for long-tailed data. This study analyzes weight balancing by focusing on neural collapse and the cone effect at each training stage and found that it can be decomposed into an increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-balanced loss.},
  address = {Vienna, Austria},
  author = {Naoya Hasegawa and Issei Sato},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hasegawa2024exploring.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=JsnR0YO4Fq},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Exploring} {Weight} {Balancing} on {Long}-{Tailed} {Recognition} {Problem}},
  url = {https://openreview.net/forum?id=JsnR0YO4Fq},
  year = {2024}
}

@inproceedings{hashemizadeh2024balancing,
  abstract = {Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities. Although sparse models achieve performance comparable to that of their dense counterparts at the level of the entire dataset, they exhibit high accuracy drops for some data sub-groups. Existing methods to mitigate this disparate impact induced by pruning (i) rely on surrogate metrics that address the problem indirectly and have limited interpretability; or (ii) scale poorly with the number of protected sub-groups in terms of computational cost. We propose a constrained optimization approach that directly addresses the disparate impact of pruning: our formulation bounds the accuracy change between the dense and sparse models, for each sub-group. This choice of constraints provides an interpretable success criterion to determine if a pruned model achieves acceptable disparity levels. Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups.},
  address = {Vienna, Austria},
  author = {Meraj Hashemizadeh and Juan Ramirez and Rohan Sukumaran and Golnoosh Farnadi and Simon {Lacoste-Julien} and Jose {Gallego-Posada}},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hashemizadeh2024balancing.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=Xz13DtbOVW},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Balancing} {Act}: {Constraining} {Disparate} {Impact} in {Sparse} {Models}},
  url = {https://openreview.net/forum?id=Xz13DtbOVW},
  year = {2024}
}

@inproceedings{hashimoto2024koopman,
  abstract = {We propose a new bound for generalization of neural networks using {Koopman} operators. While most existing works focus on low-rank weight matrices, this work focuses on full-rank weight matrices. Our bound is tighter than existing norm-based bounds when the condition numbers of weight matrices are small, and is completely independent of the width of the network if the weight matrices are orthogonal. Our bound does not contradict existing bounds but is a complement to them, and can be combined with existing bounds to obtain a tighter bound. We use {Koopman} operators to derive the determinant term in our bound, and emphasize that our operator-theoretic approach reveals a new aspect of neural networks.},
  address = {Vienna, Austria},
  author = {Yuka Hashimoto and Sho Sonoda and Isao Ishikawa and Atsushi Nitanda and Taiji Suzuki},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hashimoto2024koopman.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=JN7TcCm9LF},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Koopman}-based generalization bound: {New} aspect for full-rank weights},
  url = {https://openreview.net/forum?id=JN7TcCm9LF},
  year = {2024}
}

@inproceedings{hatamizadeh2024fastervit,
  abstract = {We introduce {FasterViT}, a new family of hybrid {CNN}-{ViT} neural networks with a focus on high image throughput for computer vision ({CV}) applications. The main contribution is Hierarchical Attention ({HAT}), that captures both short and long-range information by learning cross-window carrier tokens and decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. {FasterViT} combines the benefits of fast local representation learning in {CNNs} and global modeling properties in {ViT}. {FasterViT} achieves a {SOTA} Pareto-front in terms of accuracy and image throughput and outperforms existing hybrid models, such as the recent {EfficientFormer} and {MaxViT} models, while being significantly faster than Transformer-based models such as the family of {Swin} Transformers.},
  address = {Vienna, Austria},
  author = {Ali Hatamizadeh and Greg Heinrich and Hongxu Yin and Andrew Tao and Jos M. {√Ålvarez} and Jan Kautz and Pavlo Molchanov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hatamizadeh2024fastervit.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=kB4yBiNmXX},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{FasterViT}: {Fast} {Vision} {Transformers} with {Hierarchical} {Attention}},
  url = {https://openreview.net/forum?id=kB4yBiNmXX},
  year = {2024}
}

@inproceedings{havaldar2024learning,
  abstract = {Learning from Label Proportions ({LLP}) is a learning problem where only aggregate level labels are available for groups of instances, called bags, during training, and the aim is to get the best performance at the instance-level on the test data. This setting arises in domains like advertising and medicine due to privacy considerations. We propose a novel algorithmic framework for this problem that iteratively performs two main steps: (1) Pseudo Labeling: In every iteration, we define a Gibbs distribution over binary instance labels that incorporates covariate information through the constraint that instances with similar covariates should have similar labels and the bag level aggregated label. We then use Belief Propagation ({BP}) to marginalize the Gibbs distribution to obtain pseudo labels. (2) Embedding Refinement: We use the pseudo labels to provide supervision for a learner that yields a better embedding.},
  address = {Vienna, Austria},
  author = {Shreyas Havaldar and Navodita Sharma and Shubhi Sareen and Karthikeyan Shanmugam and Aravindan Raghuveer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/havaldar2024learning.pdf:pdf},
  month = {5},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=KQe9tHd0k8},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Learning} from {Label} {Proportions}: {Bootstrapping} {Supervised} {Learners} via {Belief} {Propagation}},
  url = {https://openreview.net/forum?id=KQe9tHd0k8},
  year = {2024}
}

@inproceedings{hay2024dynamic,
  abstract = {In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ {R}einforcement {L}earning to dynamically select layers during training and tie them together. Every few iterations, the {RL} agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.},
  address = {Vienna, Austria},
  author = {Tamir David Hay and Lior Wolf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hay2024dynamic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=d4uL2MSe0z},
  publisher = {OpenReview.net},
  title = {{Dynamic Layer Tying for Parameter-Efficient Transformers}},
  url = {https://openreview.net/forum?id=d4uL2MSe0z},
  year = {2024}
}

@inproceedings{he2024adaptive,
  abstract = {Representation rank is an important concept for understanding the role of {N}eural {N}etworks ({NN}s) in {D}eep {R}einforcement learning ({DRL}), which measures the expressive capacity of value networks. Existing studies focus on unboundedly maximizing this rank; nevertheless, that approach would introduce overly complex models in the learning, thus undermining performance. Hence, fine-tuning representation rank presents a challenging and crucial optimization problem. To address this issue, we find a guiding principle for adaptive control of the representation rank. We employ the {B}ellman equation as a theoretical foundation and derive an upper bound on the cosine similarity of consecutive state-action pairs representations of value networks. We then leverage this upper bound to propose a novel regularizer, namely {BE}llman {E}quation-based automatic rank {R}egularizer ({BEER}). This regularizer adaptively regularizes the representation rank, thus improving the {DRL} agent's performance. We first validate the effectiveness of automatic control of rank on illustrative experiments. Then, we scale up {BEER} to complex continuous control tasks by combining it with the deterministic policy gradient method. Among 12 challenging {D}eep{M}ind control tasks, {BEER} outperforms the baselines by a large margin. Besides, {BEER} demonstrates significant advantages in {Q}-value approximation. Our code is available at https://github.com/sweetice/BEER-ICLR2024.},
  address = {Vienna, Austria},
  author = {Qiang He and Tianyi Zhou and Meng Fang and Setareh Maghsudi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024adaptive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=apXtolxDaJ},
  publisher = {OpenReview.net},
  title = {{Adaptive Regularization of Representation Rank as an Implicit Constraint of Bellman Equation}},
  url = {https://openreview.net/forum?id=apXtolxDaJ},
  year = {2024}
}

@inproceedings{he2024sharpnessaware,
  abstract = {Recent research has highlighted the vulnerability of {D}eep {N}eural {N}etworks ({DNN}s) against data poisoning attacks. These attacks aim to inject poisoning samples into the models' training dataset such that the trained models have inference failures. While previous studies have executed different types of attacks, one major challenge that greatly limits their effectiveness is the uncertainty of the re-training process after the injection of poisoning samples. It includes the uncertainty of training initialization, algorithm and model architecture. To address this challenge, we propose a new strategy called {S}harpness-{A}ware {D}ata {P}oisoning {A}ttack ({SAPA}). In particular, it leverages the concept of {DNN}s' loss landscape sharpness to optimize the poisoning effect on the (approximately) worst re-trained model. Extensive experiments demonstrate that {SAPA} offers a general and principled strategy that significantly enhances various types of poisoning attacks against various types of re-training uncertainty.},
  address = {Vienna, Austria},
  author = {Pengfei He and Han Xu and Jie Ren and Yingqian Cui and Shenglai Zeng and Hui Liu and Charu C. Aggarwal and Jiliang Tang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024sharpnessaware.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-16},
  pdf = {https://openreview.net/pdf?id=bxITGFPVWh},
  publisher = {OpenReview.net},
  title = {{Sharpness-Aware Data Poisoning Attack}},
  url = {https://openreview.net/forum?id=bxITGFPVWh},
  year = {2024}
}

@inproceedings{he2024harnessing,
  abstract = {Representation learning on text-attributed graphs ({TAG}s) has become a critical research problem in recent years. A typical example of a {TAG} is a paper citation graph, where the text of each paper serves as node attributes. Initial graph neural network ({GNN}) pipelines handled these text attributes by transforming them into shallow or hand-crafted features, such as skip-gram or bag-of-words features. Recent efforts have focused on enhancing these pipelines with language models ({LM}s), which typically demand intricate designs and substantial computational resources. With the advent of powerful large language models ({LLM}s) such as {GPT} or {L}lama2, which demonstrate an ability to reason and to utilize general knowledge, there is a growing need for techniques which combine the textual modelling abilities of {LLM}s with the structural learning capabilities of {GNN}s. Hence, in this work, we focus on leveraging {LLM}s to capture textual information as features, which can be used to boost {GNN} performance on downstream tasks. A key innovation is our use of explanations as features: we prompt an {LLM} to perform zero-shot classification, request textual explanations for its decision-making process, and design an {LLM}-to-{LM} interpreter to translate these explanations into informative features for downstream {GNN}s. Our experiments demonstrate that our method achieves state-of-the-art results on well-established {TAG} datasets, including {C}ora, {P}ub{M}ed, ogbn-arxiv, as well as our newly introduced dataset, tape-arxiv23. Furthermore, our method significantly speeds up training, achieving a 2.88 times improvement over the closest baseline on ogbn-arxiv. Lastly, we believe the versatility of the proposed method extends beyond {TAG}s and holds the potential to enhance other tasks involving graph-text data.},
  address = {Vienna, Austria},
  author = {Xiaoxin He and Xavier Bresson and Thomas Laurent and Adam Perold and Yann LeCun and Bryan Hooi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024harnessing.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RXFVcynVe1},
  publisher = {OpenReview.net},
  title = {{Harnessing Explanations: {LLM}-to-{LM} Interpreter for Enhanced Text-Attributed Graph Representation Learning}},
  url = {https://openreview.net/forum?id=RXFVcynVe1},
  year = {2024}
}

@inproceedings{he2024differentially,
  abstract = {We study differentially private ({DP}) algorithms for recovering clusters in well-clustered graphs, which are graphs whose vertex set can be partitioned into a small number of sets, each inducing a subgraph of high inner conductance and small outer conductance. Such graphs have widespread application as a benchmark in the theoretical analysis of spectral clustering. We provide an efficient ($\varepsilon$,$\delta$)-{DP} algorithm tailored specifically for such graphs. Our algorithm draws inspiration from the recent work of {C}hen et al., who developed {DP} algorithms for recovery of stochastic block models in cases where the graph comprises exactly two nearly-balanced clusters. Our algorithm works for well-clustered graphs with $k$ nearly-balanced clusters, and the misclassification ratio almost matches the one of the best-known non-private algorithms. We conduct experimental evaluations on datasets with known ground truth clusters to substantiate the prowess of our algorithm. We also show that any (pure) $\varepsilon$-{DP} algorithm would result in substantial error.},
  address = {Vienna, Austria},
  author = {Weiqiang He and Hendrik Fichtenberger and Pan Peng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024differentially.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hkSjjs4o5d},
  publisher = {OpenReview.net},
  title = {{A Differentially Private Clustering Algorithm for Well-Clustered Graphs}},
  url = {https://openreview.net/forum?id=hkSjjs4o5d},
  year = {2024}
}

@inproceedings{he2024gaia,
  abstract = {Zero-shot talking avatar generation aims at synthesizing natural talking videos from speech and a single portrait image. Previous methods have relied on domain-specific heuristics such as warping-based motion representation and 3{D} {M}orphable {M}odels, which limit the naturalness and diversity of the generated avatars. In this work, we introduce {GAIA} ({G}enerative {AI} for {A}vatar), which eliminates the domain priors in talking avatar generation. In light of the observation that the speech only drives the motion of the avatar while the appearance of the avatar and the background typically remain the same throughout the entire video, we divide our approach into two stages: 1) disentangling each frame into motion and appearance representations; 2) generating motion sequences conditioned on the speech and reference portrait image. We collect a large-scale high-quality talking avatar dataset and train the model on it with different scales (up to 2{B} parameters).},
  address = {Vienna, Austria},
  author = {Tianyu He and Junliang Guo and Runyi Yu and Yuchi Wang and Jialiang Zhu and Kaikai An and Leyi Li and Xu Tan and Chunyu Wang and Han Hu and HsiangTao Wu and Sheng Zhao and Jiang Bian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024gaia.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-05-22},
  pdf = {https://openreview.net/pdf?id=ATEawsFUj4},
  publisher = {OpenReview.net},
  title = {{{GAIA}: Zero-shot Talking Avatar Generation}},
  url = {https://openreview.net/forum?id=ATEawsFUj4},
  year = {2024}
}

@inproceedings{he2024simplifying,
  abstract = {A simple design recipe for deep {T}ransformers is to compose identical building blocks. But standard transformer blocks are far from simple, interweaving attention and {MLP} sub-blocks with skip connections \& normalisation layers in precise arrangements. This complexity leads to brittle architectures, where seemingly minor changes can significantly reduce training speed, or render models untrainable. In this work, we ask to what extent the standard transformer block can be simplified? Combining signal propagation theory and empirical observations, we motivate modifications that allow many block components to be removed with no loss of training speed, including skip connections, projection or value parameters, sequential sub-blocks and normalisation layers. In experiments on both autoregressive decoder-only and {BERT} encoder-only models, our simplified transformers emulate the per-update training speed and performance of standard transformers, while enjoying 15\% faster training throughput.},
  address = {Vienna, Austria},
  author = {Bobby He and Thomas Hofmann},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024simplifying.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RtDok9eS3s},
  publisher = {OpenReview.net},
  title = {{Simplifying Transformer Blocks}},
  url = {https://openreview.net/forum?id=RtDok9eS3s},
  year = {2024}
}

@inproceedings{he2024flexible,
  abstract = {The paper presents {FLEXGEN}-{EHR}, a versatile diffusion model tailored for heterogeneous tabular {E}lectronic {H}ealth {R}ecords ({EHR}s), equipped with the capability of handling missing modalities in an integrative learning framework. Realistic synthetic electronic health records can be leveraged to accelerate methodological developments for research purposes while mitigating privacy concerns, though training of {G}enerative {A}dversarial {N}etworks remains challenging, often resulting in issues like mode collapse. While diffusion models have demonstrated progress in generating quality synthetic samples for tabular {EHR}s given ample denoising steps, their performance wanes when confronted with missing modalities in heterogeneous tabular {EHR}s data -- for example, some {EHR}s contain solely static measurements, and some contain only temporal measurements, or a blend of both data types. The authors define an optimal transport module to align and accentuate the common feature space of heterogeneity of {EHR}s. The model consistently outperforms existing state-of-the-art synthetic {EHR} generation methods both in fidelity by up to 3.10\% and utility by up to 7.16\%.},
  address = {Vienna, Austria},
  author = {Huan He and William Hao and Yuanzhe Xi and Yong Chen and Bradley A. Malin and Joyce C. Ho},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024flexible.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=W2tCmRrj7H},
  publisher = {OpenReview.net},
  title = {{A Flexible Generative Model for Heterogeneous Tabular {EHR} with Missing Modality}},
  url = {https://openreview.net/forum?id=W2tCmRrj7H},
  year = {2024}
}

@inproceedings{he2024efficientdm,
  abstract = {Diffusion models have demonstrated remarkable capabilities in image synthesis and related generative tasks. Nevertheless, their practicality for low-latency real-world applications is constrained by substantial computational costs and latency issues. Quantization is a dominant way to compress and accelerate diffusion models, where post-training quantization ({PTQ}) and quantization-aware training ({QAT}) are two main approaches, each bearing its own properties. While {PTQ} exhibits efficiency in terms of both time and data usage, it may lead to diminished performance in low bit-width settings. On the other hand, {QAT} can help alleviate performance degradation but comes with substantial demands on computational and data resources. To further enhance performance, we introduce scale-aware optimization to address ineffective learning of {QALoRA} due to variations in weight quantization scales across different layers. We also employ temporal learned step-size quantization to handle notable variations in activation distributions across denoising steps. Extensive experimental results demonstrate that our method significantly outperforms previous {PTQ}-based diffusion models while maintaining similar time and data efficiency. Specifically, there is only a marginal 0.05 sFID increase when quantizing both weights and activations of {LDM}-4 to 4-bit on {I}mage{N}et $256\times256$. Compared to {QAT}-based methods, our {EfficientDM} also boasts a $16.2\times$ faster quantization speed with comparable generation quality.},
  address = {Vienna, Austria},
  author = {Yefei He and Jing Liu and Weijia Wu and Hong Zhou and Bohan Zhuang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024efficientdm.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-11-14},
  pdf = {https://openreview.net/pdf?id=UmMa3UNDAz},
  publisher = {OpenReview.net},
  title = {{{EfficientDM}: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models}},
  url = {https://openreview.net/forum?id=UmMa3UNDAz},
  year = {2024}
}

@inproceedings{he2024mgno,
  abstract = {The paper proposes a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, they define the neural operator where the output of the $i$-th neuron in a nonlinear operator layer is defined by $O_i(u) = \sigma(\sum_j W_{ij} u + B_{ij})$. Here, $W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons ({B}anach spaces) plays a critical role. As a result, they introduce {MgNO}, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, {MgNO} obviates the need for conventional lifting and projecting operators typically required in previous neural operators. Their empirical observations reveal that {MgNO} exhibits superior ease of training compared to other {CNN}-based models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators. They demonstrate the efficiency and accuracy of their method with consistently state-of-the-art performance on different types of partial differential equations ({PDE}s).},
  address = {Vienna, Austria},
  author = {Juncai He and Xinliang Liu and Jinchao Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024mgno.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8OxL034uEr},
  publisher = {OpenReview.net},
  title = {{{MgNO}: Efficient Parameterization of Linear Operators via Multigrid}},
  url = {https://openreview.net/forum?id=8OxL034uEr},
  year = {2024}
}

@inproceedings{he2024strategic,
  abstract = {Camouflaged object detection (COD) is the challenging task of identifying camouflaged objects visually blended into surroundings. Albeit achieving remarkable success, existing COD detectors still struggle to obtain precise results in some challenging cases. To handle this problem, we draw inspiration from the prey-vs-predator game that leads preys to develop better camouflage and predators to acquire more acute vision systems and develop algorithms from both the prey side and the predator side. On the prey side, we propose an adversarial training framework, Camouflageator, which introduces an auxiliary generator to generate more camouflaged objects that are harder for a COD method to detect. Camouflageator trains the generator and detector in an adversarial way such that the enhanced auxiliary generator helps produce a stronger detector. On the predator side, we introduce a novel COD method, called Internal Coherence and Edge Guidance (ICEG), which introduces a camouflaged feature coherence module to excavate the internal coherence of camouflaged objects, striving to obtain more complete segmentation results. Additionally, ICEG proposes a novel edge-guided separated calibration module to remove false predictions to avoid obtaining ambiguous boundaries. Extensive experiments show that ICEG outperforms existing COD detectors and Camouflageator is flexible to improve various COD detectors, including ICEG, which brings state-of-the-art COD performance.},
  author = {Chunming He and Kai Li and Yachao Zhang and Yulun Zhang and Chenyu You and Zhenhua Guo and Xiu Li and Martin Danelljan and Fisher Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024strategic.pdf:pdf},
  note = {DBLP last modified: 2024-10-25},
  pdf = {https://openreview.net/pdf?id=hywpSoHwgX},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Strategic Preys Make Acute Predators: Enhancing Camouflaged Object Detectors by Generating Camouflaged Objects},
  url = {https://openreview.net/forum?id=hywpSoHwgX},
  year = {2024}
}

@inproceedings{he2024manifold,
  abstract = {Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.},
  author = {Yutong He and Naoki Murata and Chieh-Hsin Lai and Yuhta Takida and Toshimitsu Uesaka and Dongjun Kim and Wei-Hsiang Liao and Yuki Mitsufuji and J. Zico Kolter and Ruslan Salakhutdinov and Stefano Ermon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024manifold.pdf:pdf},
  note = {DBLP last modified: 2025-06-10},
  pdf = {https://openreview.net/pdf?id=o3BxOLoxm1},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Manifold Preserving Guided Diffusion},
  url = {https://openreview.net/forum?id=o3BxOLoxm1},
  year = {2024}
}

@inproceedings{he2024robust,
  abstract = {The angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example, sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting (dubbed $k$-synchronization) is to estimate $k$ groups of angles simultaneously, given noisy observations (with unknown group assignment) from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem, and its heterogeneous extension, by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchronization objectives. Experimental results on extensive data sets demonstrate that GNNSync attains competitive, and often superior, performance against a comprehensive set of baselines for the angular synchronization problem and its extension, validating the robustness of GNNSync even at high noise levels.},
  author = {Yixuan He and Gesine Reinert and David Wipf and Mihai Cucuringu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024robust.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5sjxMwWmk8},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Robust Angular Synchronization via Directed Graph Neural Networks},
  url = {https://openreview.net/forum?id=5sjxMwWmk8},
  year = {2024}
}

@inproceedings{he2024robustifying,
  abstract = {Neural architecture search (NAS) has become a key component of AutoML and a standard tool to automate the design of deep neural networks. Recently, training-free NAS as an emerging paradigm has successfully reduced the search costs of standard training-based NAS by estimating the true architecture performance with only training-free metrics. Nevertheless, the estimation ability of these metrics typically varies across different tasks, making it challenging to achieve robust and consistently good search performance on diverse tasks with only a single training-free metric. Meanwhile, the estimation gap between training-free metrics and the true architecture performances limits training-free NAS to achieve superior performance. To address these challenges, we propose the robustifying and boosting training-free NAS (RoBoT) algorithm which (a) employs the optimized combination of existing training-free metrics explored from Bayesian optimization to develop a robust and consistently better-performing metric on diverse tasks, and (b) applies greedy search, i.e., the exploitation, on the newly developed metric to bridge the aforementioned gap and consequently to boost the search performance of standard training-free NAS further. Remarkably, the expected performance of our RoBoT can be theoretically guaranteed, which improves over the existing training-free NAS under mild conditions with additional interesting insights. Our extensive experiments on various NAS benchmark tasks yield substantial empirical evidence to support our theoretical results.},
  author = {Zhenfeng He and Yao Shu and Zhongxiang Dai and Bryan Kian Hsiang Low},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024robustifying.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qPloNoDJZn},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Robustifying and Boosting Training-Free Neural Architecture Search},
  url = {https://openreview.net/forum?id=qPloNoDJZn},
  year = {2024}
}

@inproceedings{he2024scalecrafter,
  abstract = {In this work, we investigate the capability of generating images from pre-trained diffusion models at much higher resolutions than the training image sizes. In addition, the generated images should have arbitrary image aspect ratios. When generating images directly at a higher resolution, 1024 x 1024, with the pre-trained Stable Diffusion using training images of resolution 512 x 512, we observe persistent problems of object repetition and unreasonable object structures. Existing works for higher-resolution generation, such as attention-based and joint-diffusion approaches, cannot well address these issues. As a new perspective, we examine the structural components of the U-Net in diffusion models and identify the crucial cause as the limited perception field of convolutional kernels. Based on this key observation, we propose a simple yet effective re-dilation that can dynamically adjust the convolutional perception field during inference. We further propose the dispersed convolution and noise-damped classifier-free guidance, which can enable ultra-high-resolution image generation (e.g., 4096 x 4096). Notably, our approach does not require any training or optimization. Extensive experiments demonstrate that our approach can address the repetition issue well and achieve state-of-the-art performance on higher-resolution image synthesis, especially in texture details. Our work also suggests that a pre-trained diffusion model trained on low-resolution images can be directly used for high-resolution visual generation without further tuning, which may provide insights for future research on ultra-high-resolution image and video synthesis.},
  author = {Yingqing He and Shaoshu Yang and Haoxin Chen and Xiaodong Cun and Menghan Xia and Yong Zhang and Xintao Wang and Ran He and Qifeng Chen and Ying Shan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024scalecrafter.pdf:pdf},
  note = {DBLP last modified: 2025-05-12},
  pdf = {https://openreview.net/pdf?id=u48tHG5f66},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {ScaleCrafter: Tuning-free Higher-Resolution Visual Generation with Diffusion Models},
  url = {https://openreview.net/forum?id=u48tHG5f66},
  year = {2024}
}

@inproceedings{he2024sampleefficient,
  abstract = {We study infinite-horizon average-reward Markov decision processes (AMDPs) in the context of general function approximation. Specifically, we propose a novel algorithmic framework named Local-fitted Optimization with OPtimism (LOOP), which incorporates both model-based and value-based incarnations. In particular, LOOP features a novel construction of confidence sets and a low-switching policy updating scheme, which are tailored to the average-reward and function approximation setting. Moreover, for AMDPs, we propose a novel complexity measure -- average-reward generalized eluder coefficient (AGEC) -- which captures the challenge of exploration in AMDPs with general function approximation. Such a complexity measure encompasses almost all previously known tractable AMDP models, such as linear AMDPs and linear mixture AMDPs, and also includes newly identified cases such as kernel AMDPs and AMDPs with Bellman eluder dimensions. Using AGEC, we prove that LOOP achieves a sublinear $\tilde{\mathcal{O}}(\mathrm{poly}(d, \mathrm{sp}(V^*)) \sqrt{T\beta} )$ regret, where $d$ and $\beta$ correspond to AGEC and log-covering number of the hypothesis class respectively, $\mathrm{sp}(V^*)$ is the span of the optimal state bias function, $T$ denotes the number of steps, and $\tilde{\mathcal{O}} (\cdot) $ omits logarithmic factors. When specialized to concrete AMDP models, our regret bounds are comparable to those established by the existing algorithms designed specifically for these special cases. To the best of our knowledge, this paper presents the first comprehensive theoretical framework capable of handling nearly all AMDPs.},
  author = {Jianliang He and Han Zhong and Zhuoran Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/he2024sampleefficient.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fq1wNrC2ai},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Sample-efficient Learning of Infinite-horizon Average-reward MDPs with General Function Approximation},
  url = {https://openreview.net/forum?id=fq1wNrC2ai},
  year = {2024}
}

@inproceedings{hejna2024contrastive,
  abstract = {Reinforcement Learning from Human Feedback (RLHF) has emerged as a popular paradigm for aligning models with human intent. Typically RLHF algorithms operate in two phases: first, use human preferences to learn a reward function and second, align the model by optimizing the learned reward via reinforcement learning (RL). This paradigm assumes that human preferences are distributed according to reward, but recent work suggests that they instead follow the regret under the user's optimal policy. Thus, learning a reward function from feedback is not only based on a flawed assumption of human preference, but also leads to unwieldy optimization challenges that stem from policy gradients or bootstrapping in the RL phase. We introduce Contrastive Preference Learning (CPL), an algorithm for learning optimal policies from preferences without learning reward functions. CPL learns policies via a simple contrastive objective that can be applied to arbitrary MDPs. This approach circumvents the need for RL entirely, avoiding the associated challenges while being fully off-policy. Our experiments demonstrate that CPL is competitive with or outperforms existing RLHF methods across a variety of tasks including robotics, language modeling, and image generation, while being significantly simpler to implement and tune.},
  author = {Joey Hejna and Rafael Rafailov and Harshit Sikchi and Chelsea Finn and Scott Niekum and W. Bradley Knox and Dorsa Sadigh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hejna2024contrastive.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iX1RjVQODj},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Contrastive Preference Learning: Learning from Human Feedback without Reinforcement Learning},
  url = {https://openreview.net/forum?id=iX1RjVQODj},
  year = {2024}
}

@inproceedings{hendawy2024multitask,
  abstract = {Multi-Task Reinforcement Learning (MTRL) tackles the long-standing problem of endowing agents with skills that generalize across a variety of problems. To this end, sharing representations plays a fundamental role in capturing both unique and common characteristics of the tasks. Tasks may exhibit similarities in terms of skills, objects, or physical properties while leveraging their representations eases the achievement of a universal policy. Nevertheless, the pursuit of learning a shared set of diverse representations is still an open challenge. In this paper, we introduce a novel approach for representation learning in MTRL that encapsulates common structures among the tasks using orthogonal representations to promote diversity. Our method, named Mixture Of Orthogonal Experts (MOORE), leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts. When task-specific information is provided, MOORE generates relevant representations from this shared subspace. We assess the effectiveness of our approach on two MTRL benchmarks, namely MiniGrid and MetaWorld, showing that MOORE surpasses related baselines and establishes a new state-of-the-art result on MetaWorld.},
  author = {Ahmed Hendawy and Jan Peters and Carlo D'Eramo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hendawy2024multitask.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=aZH1dM3GOX},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts},
  url = {https://openreview.net/forum?id=aZH1dM3GOX},
  year = {2024}
}

@inproceedings{heo2024rethinking,
  abstract = {Large Language Models (LLMs) have recently demonstrated remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to the large memory bottleneck, specifically in small batch inference settings. Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. With per-IC quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions (AdaDim), a versatile quantization framework that can adapt to various weight sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting prior methods such as Round-To-Nearest and GPTQ, showing significant improvements across various language modeling benchmarks for both base (up to +4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs.},
  author = {Jung Hwan Heo and Jeonghoon Kim and Beomseok Kwon and Byeongwook Kim and Se Jung Kwon and Dongsoo Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/heo2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JzG7kSpjJk},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models},
  url = {https://openreview.net/forum?id=JzG7kSpjJk},
  year = {2024}
}

@inproceedings{hermann2024foundations,
  abstract = {Deep-learning models can extract a rich assortment of features from data. Which features a model uses depends not only on predictivity -- how reliably a feature indicates training-set labels -- but also on availability -- how easily the feature can be extracted from inputs. The literature on shortcut learning has noted examples in which models privilege one feature over another, for example texture over shape and image backgrounds over foreground objects. Here, we test hypotheses about which input properties are more available to a model, and systematically study how predictivity and availability interact to shape models' feature use. We construct a minimal, explicit generative framework for synthesis of classification datasets with two latent features that can be varied independently and along interpretable dimensions including predictivity and availability. We train linear models and deep neural networks on these synthetic datasets, and identify predictivity and availability through both theory and experiments. Our results show that linear models are well-described by a natural formalization of the predictivity--availability trade-off, and that in neural networks, this trade-off is further modulated by feature interactions.},
  author = {Katherine L. Hermann and Hossein Mobahi and Thomas Fel and Michael Curtis Mozer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hermann2024foundations.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Tj3xLVuE9f},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {On the Foundations of Shortcut Learning},
  url = {https://openreview.net/forum?id=Tj3xLVuE9f},
  year = {2024}
}

@inproceedings{hernandez2024linearity,
  abstract = {Much of the knowledge encoded in transformer language models ({LM}s) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a subspace from the difference vectors of known relation pairs. Using linear algebra, we show that relation computations in language models are equivalent to an implicit linear map on subject representations. We validate this claim empirically across a range of factual and commonsense relations, demonstrating strong performance correlations between linear maps and language model relation decoding. We also provide evidence that the implicit relation representations are robust to model size and intervention techniques.},
  address = {Vienna, Austria},
  author = {Evan Hernandez and Arnab Sen Sharma and Tal Haklay and Kevin Meng and Martin Wattenberg and Jacob Andreas and Yonatan Belinkov and David Bau},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hernandez2024linearity.pdf:pdf},
  month = {5},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=w7LU2s14kE},
  publisher = {OpenReview.net},
  title = {Linearity of Relation Decoding in Transformer Language Models},
  url = {https://openreview.net/forum?id=w7LU2s14kE},
  year = {2024}
}

@inproceedings{hertrich2024generative,
  abstract = {Maximum mean discrepancy (MMD) flows suffer from high computational costs in large scale computations. In this paper, we show that MMD flows with Riesz kernels K(x,y) = - ||x-y||^r, r ‚àà (0,2) have exceptional properties which allow their efficient computation.},
  address = {Vienna, Austria},
  author = {Johannes Hertrich and Christian Wald and Fabian Altekr{\"u}ger and Paul Hagemann},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hertrich2024generative.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=VdkGRV1vcf},
  publisher = {OpenReview.net},
  title = {Generative Sliced {MMD} Flows with {R}iesz Kernels},
  url = {https://openreview.net/forum?id=VdkGRV1vcf},
  year = {2024}
}

@inproceedings{hess2024bayesian,
  abstract = {Treatment effect estimation in continuous time is crucial for personalized medicine. However, existing methods for this task are limited to point estimates of the potential outcomes, whereas uncertainty estimates have been ignored. Needless to say, uncertainty quantification is crucial for reliable decision-making in medical applications. To fill this gap, we propose a novel Bayesian neural controlled differential equation (BNCDE) for treatment effect estimation in continuous time.},
  address = {Vienna, Austria},
  author = {Konstantin Hess and Valentyn Melnychuk and Dennis Frauen and Stefan Feuerriegel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hess2024bayesian.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=uwO71a8wET},
  publisher = {OpenReview.net},
  title = {Bayesian Neural Controlled Differential Equations for Treatment Effect Estimation},
  url = {https://openreview.net/forum?id=uwO71a8wET},
  year = {2024}
}

@inproceedings{hettige2024airphynet,
  abstract = {Air quality prediction and modelling plays a pivotal role in public health and environment management, for individuals and authorities to make informed decisions. Although traditional data-driven models have shown promise in this domain, their long-term prediction accuracy can be limited, especially in scenarios with sparse or incomplete data and they often rely on black-box deep learning structures that lack solid physical foundation leading to reduced transparency and interpretability in predictions.},
  address = {Vienna, Austria},
  author = {Kethmi Hirushini Hettige and Jiahao Ji and Shili Xiang and Cheng Long and Gao Cong and Jingyuan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hettige2024airphynet.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=JW3jTjaaAB},
  publisher = {OpenReview.net},
  title = {{AirPhyNet}: Harnessing Physics-Guided Neural Networks for Air Quality Prediction},
  url = {https://openreview.net/forum?id=JW3jTjaaAB},
  year = {2024}
}

@inproceedings{hoang2024efficient,
  abstract = {We tackle the problem of meta-learning across heterogenous tasks. This problem seeks to extract and generalize transferable meta-knowledge through streaming task sets from a multi-modal task distribution. The extracted meta-knowledge can be used to create predictors for new tasks using a small number of labeled samples. Most meta-learning methods assume a homogeneous task distribution, thus limiting their generalization capacity when handling multi-modal task distributions.},
  address = {Vienna, Austria},
  author = {Minh Hoang and Carl Kingsford},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hoang2024efficient.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=QiJuMJl0QS},
  publisher = {OpenReview.net},
  title = {Efficient Heterogeneous Meta-Learning via Channel Shuffling Modulation},
  url = {https://openreview.net/forum?id=QiJuMJl0QS},
  year = {2024}
}

@inproceedings{holt2024l2mac,
  abstract = {Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and coherent outputs. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long output generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains.},
  address = {Vienna, Austria},
  author = {Samuel Holt and Max Ruiz Luyten and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/holt2024l2mac.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=EhrzQwsV4K},
  publisher = {OpenReview.net},
  title = {{L2MAC}: Large Language Model Automatic Computer for Extensive Code Generation},
  url = {https://openreview.net/forum?id=EhrzQwsV4K},
  year = {2024}
}

@inproceedings{hong2024lrm,
  abstract = {We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image.},
  address = {Vienna, Austria},
  author = {Yicong Hong and Kai Zhang and Jiuxiang Gu and Sai Bi and Yang Zhou and Difan Liu and Feng Liu and Kalyan Sunkavalli and Trung Bui and Hao Tan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024lrm.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=sllU8vvsFF},
  publisher = {OpenReview.net},
  title = {{LRM}: Large Reconstruction Model for Single Image to {3D}},
  url = {https://openreview.net/forum?id=sllU8vvsFF},
  year = {2024}
}

@inproceedings{hong2024unifying,
  abstract = {This paper introduces a Transformer-based integrative feature and cost aggregation network designed for dense matching tasks. In the context of dense matching, many works benefit from one of two forms of aggregation: feature aggregation, which pertains to the alignment of similar features, or cost aggregation, a procedure aimed at instilling coherence in the flow estimates across neighboring pixels.},
  address = {Vienna, Austria},
  author = {Sunghwan Hong and Seokju Cho and Seungryong Kim and Stephen Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024unifying.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=fQHb1uZzl7},
  publisher = {OpenReview.net},
  title = {Unifying Feature and Cost Aggregation with Transformers for Semantic and Visual Correspondence},
  url = {https://openreview.net/forum?id=fQHb1uZzl7},
  year = {2024}
}

@inproceedings{hong2024cas,
  abstract = {Recent conditional diffusion models have shown remarkable advancements and have been widely applied in fascinating real-world applications. However, samples generated by these models often do not strictly comply with user-provided conditions. Due to this, there have been few attempts to evaluate this alignment via pre-trained scoring models to select well-generated samples.},
  address = {Vienna, Austria},
  author = {Chunsan Hong and Byunghee Cha and Tae-Hyun Oh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024cas.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=E78OaH2s3f},
  publisher = {OpenReview.net},
  title = {{CAS}: A Probability-Based Approach for Universal Condition Alignment Score},
  url = {https://openreview.net/forum?id=E78OaH2s3f},
  year = {2024}
}

@inproceedings{hong2024offline,
  abstract = {Offline reinforcement learning (RL) can in principle synthesize more optimal behavior from a dataset consisting only of suboptimal trials. One way that this can happen is by "stitching" together the best parts of otherwise suboptimal trajectories that overlap on similar states, to create new behaviors where each individual state is in-distribution, but the overall returns are higher.},
  address = {Vienna, Austria},
  author = {Joey Hong and Anca D. Dragan and Sergey Levine},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024offline.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=GnOLWS4Llt},
  publisher = {OpenReview.net},
  title = {Offline {RL} with Observation Histories: Analyzing and Improving Sample Complexity},
  url = {https://openreview.net/forum?id=GnOLWS4Llt},
  year = {2024}
}

@inproceedings{hong2024policy,
  abstract = {In this paper, we propose a policy gradient method for confounded partially observable Markov decision processes (POMDPs) with continuous state and observation spaces in the offline setting. We first establish a novel identification result to non-parametrically estimate any history-dependent policy gradient under POMDPs using the offline data. The identification enables us to solve a sequence of conditional moment restrictions and adopt the min-max learning procedure with general function approximation for estimating the policy gradient. We then provide a finite-sample non-asymptotic bound for estimating the gradient uniformly over a pre-specified policy class in terms of the sample size, length of horizon, concentratability coefficient and the measure of ill-posedness in solving the conditional moment restrictions. Lastly, by deploying the proposed gradient estimation in the gradient ascent algorithm, we show the global convergence of the proposed algorithm in finding the history-dependent optimal policy under some technical conditions. To the best of our knowledge, this is the first work studying the policy gradient method for POMDPs under the offline setting.},
  address = {Vienna, Austria},
  author = {Mao Hong and Zhengling Qi and Yanxun Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024policy.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8BAkNCqpGW},
  publisher = {OpenReview.net},
  title = {A Policy Gradient Method for Confounded {POMDP}s},
  url = {https://openreview.net/forum?id=8BAkNCqpGW},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hong2024curiositydriven,
  abstract = {Large language models ({LLM}s) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an {LLM} generates unwanted content, the current paradigm is to recruit a red team of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from {LLM}s. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team {LLM} with reinforcement learning ({RL}) to generate test cases that maximize the chance of eliciting undesirable responses from the target {LLM}. However, current {RL} methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target {LLM}. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming ({CRT}) achieves greater coverage of test cases while maintaining or increasing their effectiveness compared to existing methods. Our method, {CRT} successfully provokes toxic responses from {LLaMA}2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs.},
  address = {Vienna, Austria},
  author = {Zhang-Wei Hong and Idan Shenfeld and Tsun-Hsuan Wang and Yung-Sung Chuang and Aldo Pareja and James R. Glass and Akash Srivastava and Pulkit Agrawal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024curiositydriven.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07; Code available at https://github.com/Improbable-AI/curiosity_redteam},
  pdf = {https://openreview.net/pdf?id=4KqkizXgXU},
  publisher = {OpenReview.net},
  title = {Curiosity-driven Red-teaming for Large Language Models},
  url = {https://openreview.net/forum?id=4KqkizXgXU},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hong2024improving,
  abstract = {Non-transferable learning ({NTL}) aims to restrict the generalization of models toward the target domain(s). To this end, existing works learn non-transferable representations by reducing statistical dependence between the source and target domain. However, such statistical methods essentially neglect to distinguish between styles and contents, leading them to inadvertently fit (i) spurious correlation between styles and labels, and (ii) fake independence between contents and labels. Consequently, their performance will be limited when natural distribution shifts occur or malicious intervention is imposed. In this paper, we propose a novel method (dubbed as {H}-{NTL}) to understand and advance the {NTL} problem by introducing a causal model to separately model content and style as two latent factors, based on which we disentangle and harness them as guidances for learning non-transferable representations with intrinsically causal relationships. Specifically, to avoid fitting spurious correlation and fake independence, we propose a variational inference framework to disentangle the naturally mixed content factors and style factors under our causal model. Subsequently, based on dual-path knowledge distillation, we harness the disentangled two factors as guidances for non-transferable representation learning: (i) we constraint the source domain representations to fit content factors (which are the intrinsic cause of labels), and (ii) we enforce that the target domain representations fit style factors which barely can predict labels. As a result, the learned feature representations follow optimal untransferability toward the target domain and minimal negative influence on the source domain, thus enabling better {NTL} performance. Empirically, the proposed {H}-{NTL} significantly outperforms competing methods by a large margin.},
  address = {Vienna, Austria},
  author = {Ziming Hong and Zhenyi Wang and Li Shen and Yu Yao and Zhuo Huang and Shiming Chen and Chuanwu Yang and Mingming Gong and Tongliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024improving.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-10; Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=FYKVPOHCpE},
  publisher = {OpenReview.net},
  title = {Improving Non-Transferable Representation Learning by Harnessing Content and Style},
  url = {https://openreview.net/forum?id=FYKVPOHCpE},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hong2024dp,
  abstract = {Large Language Models ({LLM}s) have emerged as dominant tools for various tasks, particularly when tailored for a specific target by prompt tuning. Nevertheless, concerns surrounding data privacy present obstacles due to the tuned prompts' dependency on sensitive private information. A practical solution is to host a local {LLM} and optimize a soft prompt privately using data. Yet, hosting a local model becomes problematic when model ownership is protected. Alternative methods, like sending data to the model's provider for training, intensify these privacy issues facing an untrusted provider. In this paper, we present a novel solution called Differentially-Private Offsite Prompt Tuning ({DP}-{OPT}) to address this challenge. Our approach involves tuning a discrete prompt on the client side and then applying it to the desired cloud models. We demonstrate that prompts suggested by {LLM}s themselves can be transferred without compromising performance significantly. To ensure that the prompts do not leak private information, we introduce the first private prompt generation mechanism, by a differentially-private ({DP}) ensemble of in-context learning with private demonstrations. With {DP}-{OPT}, generating privacy-preserving prompts by Vicuna-7b can yield competitive performance compared to non-private in-context learning on {GPT}3.5 or local private prompt tuning.},
  address = {Vienna, Austria},
  author = {Junyuan Hong and Jiachen T. Wang and Chenhui Zhang and Zhangheng Li and Bo Li and Zhangyang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024dp.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29; Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=Ifz3IgsEPX},
  publisher = {OpenReview.net},
  title = {{DP}-{OPT}: Make Large Language Model Your Privacy-Preserving Prompt Engineer},
  url = {https://openreview.net/forum?id=Ifz3IgsEPX},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hong2024metagpt,
  abstract = {Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models ({LLM}s). Previous {LLM}-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining {LLM}s. Here we introduce {MetaGPT}, an innovative meta-programming framework incorporating efficient human workflows into {LLM}-based multi-agent collaborations. {MetaGPT} encodes Standardized Operating Procedures ({SOP}s) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors. {MetaGPT} utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together. On collaborative software engineering benchmarks, {MetaGPT} generates more coherent solutions than previous chat-based multi-agent systems.},
  address = {Vienna, Austria},
  author = {Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Jinlin Wang and Ceyao Zhang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hong2024metagpt.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29; Oral presentation},
  pdf = {https://openreview.net/pdf?id=VtmBAGCN7o},
  publisher = {OpenReview.net},
  title = {{MetaGPT}: Meta Programming for A Multi-Agent Collaborative Framework},
  url = {https://openreview.net/forum?id=VtmBAGCN7o},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{horvat2024gauge,
  abstract = {Diffusion models are generative models that have recently demonstrated impressive performances in terms of sampling quality and density estimation in high dimensions. They rely on a forward continuous diffusion process and a backward continuous denoising process, which can be described by a time-dependent vector field and is used as a generative model. In the original formulation of the diffusion model, this vector field is assumed to be the score function (i.e. it is the gradient of the log-probability at a given time in the diffusion process). Curiously, on the practical side, most studies on diffusion models implement this vector field as a neural network function and do not constrain it be the gradient of some energy function (that is, most studies do not constrain the vector field to be conservative). Even though some studies investigated empirically whether such a constraint will lead to a performance gain, they lead to contradicting results and failed to provide analytical results. Here, we provide three analytical results regarding the extent of the modeling freedom of this vector field. Firstly, we propose a novel decomposition of vector fields into a conservative component and an orthogonal component which satisfies a given (gauge) freedom. Secondly, from this orthogonal decomposition, we show that exact density estimation and exact sampling is achieved when the conservative component is exactly equals to the true score and therefore conservativity is neither necessary nor sufficient to obtain exact density estimation and exact sampling. Finally, we show that when it comes to inferring local information of the data manifold, constraining the vector field to be conservative is desirable.},
  address = {Vienna, Austria},
  author = {Christian Horvat and Jean-Pascal Pfister},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/horvat2024gauge.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=92KV9xAMhF},
  publisher = {OpenReview.net},
  title = {On gauge freedom, conservativity and intrinsic dimensionality estimation in diffusion models},
  url = {https://openreview.net/forum?id=92KV9xAMhF},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hosking2024human,
  abstract = {Human feedback has become the de facto standard for evaluating the performance of Large Language Models, and is increasingly being used as a training objective. However, it is not clear which properties of a generated output this single 'preference' score captures. We hypothesise that preference scores are subjective and open to undesirable biases. We critically analyse the use of human feedback for both training and evaluation, to verify whether it fully captures a range of crucial error criteria. We find that while preference scores have fairly good coverage, they under-represent important aspects like factuality. We further hypothesise that both preference scores and error annotation may be affected by confounders, and leverage instruction-tuned models to generate outputs that vary along two possible confounding dimensions: assertiveness and complexity. We find that the assertiveness of an output skews the perceived rate of factuality errors, indicating that human annotations are not a fully reliable evaluation metric or training objective. Finally, we offer preliminary evidence that using human feedback as a training objective disproportionately increases the assertiveness of model outputs. We encourage future work to carefully consider whether preference scores are well aligned with the desired objective.},
  address = {Vienna, Austria},
  author = {Tom Hosking and Phil Blunsom and Max Bartolo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hosking2024human.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7W3GLNImfS},
  publisher = {OpenReview.net},
  title = {Human Feedback is not Gold Standard},
  url = {https://openreview.net/forum?id=7W3GLNImfS},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hosoya2024cognitive,
  abstract = {Motivated by a recent neuroscientific hypothesis, some theoretical studies have accounted for neural cognitive maps in the rodent hippocampal formation as a representation of the general relational structure across task environments. However, despite their remarkable results, it is unclear whether their account can be extended to more general settings beyond spatial random-walk tasks in {2D} environments. To address this question, we construct a novel cognitive model that performs memory-based relational decision-making tasks, inspired by previous human studies, for learning abstract structures in non-spatial relations. Building on previous approaches of modular architecture, we develop a learning algorithm that performs reward-guided search for representation of abstract relations, while dynamically maintaining their binding to concrete entities using our specific memory mechanism enabling content replacement. Our experiments show (i) the capability of our model to capture relational structures that can generalize over new domains with unseen entities, (ii) the difficulty of our task that leads previous models, including Neural Turing Machine and vanilla Transformer, to complete failure, and (iii) the similarity of performance and internal representations of our model to recent human behavioral and {fMRI} experimental data in the human hippocampal formation.},
  address = {Vienna, Austria},
  author = {Haruo Hosoya},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hosoya2024cognitive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KC58bVmxyN},
  publisher = {OpenReview.net},
  title = {A Cognitive Model for Learning Abstract Relational Structures from Memory-based Decision-Making Tasks},
  url = {https://openreview.net/forum?id=KC58bVmxyN},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hsu2024diagnosing,
  abstract = {Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce {SUFO}, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. {SUFO} utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability (e.g. model suitability for a task, feature space evolution during fine-tuning, and interpretation of fine-tuned features and failure modes). We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on {MedNLI}. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain ({BERT}, {TNLR}), mixed-domain ({BioBERT}, Clinical {BioBERT}), and domain-specific ({PubMedBERT}) groups. Our {SUFO} analyses reveal that: (1) while {PubMedBERT}, the domain-specific model, contains valuable information for fine-tuning, it can overfit to minority classes when class imbalances exist. In contrast, mixed-domain models exhibit greater resistance to overfitting, suggesting potential improvements in domain-specific model robustness; (2) in-domain pre-training accelerates feature disambiguation during fine-tuning; and (3) feature spaces undergo significant sparsification during this process, enabling clinicians to identify common outlier modes among fine-tuned models as demonstrated in this paper. These findings showcase the utility of {SUFO} in enhancing trust and safety when using transformers in medicine, and we believe {SUFO} can aid practitioners in evaluating fine-tuned language models ({LM}s) for other applications in medicine and in more critical domains.},
  address = {Vienna, Austria},
  author = {Aliyah R. Hsu and Yeshwanth Cherapanamjeri and Briton Park and Tristan Naumann and Anobel Y. Odisho and Bin Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hsu2024diagnosing.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=k581sTMyPt},
  publisher = {OpenReview.net},
  title = {Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making},
  url = {https://openreview.net/forum?id=k581sTMyPt},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hsu2024dropoutbased,
  abstract = {Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to 20$\times$ to 5000$\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection.},
  address = {Vienna, Austria},
  author = {Hsiang Hsu and Guihong Li and Shaohan Hu and Chun-Fu Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hsu2024dropoutbased.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Sf2A2PUXO3},
  publisher = {OpenReview.net},
  title = {Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation},
  url = {https://openreview.net/forum?id=Sf2A2PUXO3},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{hu2024learning,
  abstract = {In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, {CommFormer}, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.},
  address = {Vienna, Austria},
  author = {Shengchao Hu and Li Shen and Ya Zhang and Dacheng Tao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024learning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Qox9rO0kN0},
  publisher = {OpenReview.net},
  title = {Learning Multi-Agent Communication from Graph Modeling Perspective},
  url = {https://openreview.net/forum?id=Qox9rO0kN0},
  year = {2024}
}

@inproceedings{hu2024better,
  abstract = {Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning techniques to create adaptive and dynamic meshes, due to the dynamic nature of these systems. However, these approaches face two primary challenges: (1) the need for expensive optimal mesh data, and (2) the change of the solution space's degree of freedom and topology during mesh refinement. To address these challenges, this paper proposes a neural PDE solver with a neural mesh adapter. To begin with, we introduce a novel data-free neural mesh adaptor, called Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an operator that maps the solution to adaptive meshes and is trained using the Monge-Amp√®re equation without optimal mesh data. Secondly, it dynamically changes the mesh by moving existing nodes rather than adding or deleting nodes and edges. Theoretical analysis shows that meshes generated by DMM have the lowest interpolation error bound. Based on DMM, to efficiently and accurately model dynamic systems, we develop a moving mesh based neural PDE solver (MM-PDE) that embeds the moving mesh with a two-branch architecture and a learnable interpolation framework to preserve information within the data. Empirical experiments demonstrate that our method generates suitable meshes and considerably enhances accuracy when modeling widely considered PDE systems. The code can be found at: https://github.com/Peiyannn/MM-PDE.git.},
  address = {Vienna, Austria},
  author = {Peiyan Hu and Yue Wang and Zhi-Ming Ma},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024better.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hj9ZuNimRl},
  publisher = {OpenReview.net},
  title = {Better Neural {PDE} Solvers Through Data-Free Mesh Movers},
  url = {https://openreview.net/forum?id=hj9ZuNimRl},
  year = {2024}
}

@inproceedings{hu2024scaleadaptive,
  abstract = {While diffusion models have revolutionized generative AI, their application to human sketch generation, especially in the creation of complex yet concise and recognizable sketches, remains largely unexplored. Existing efforts have primarily focused on vector-based sketches, limiting their ability to handle intricate sketch data. This paper introduces an innovative extension of diffusion models to pixel-level sketch generation, addressing the challenge of dynamically optimizing the guidance scale for classifier-guided diffusion. Our approach achieves a delicate balance between recognizability and complexity in generated sketches through scale-adaptive classifier-guided diffusion models, a scaling indicator, and the concept of a residual sketch. We also propose a three-phase sampling strategy to enhance sketch diversity and quality. Experiments on the QuickDraw dataset showcase the potential of diffusion models to push the boundaries of sketch generation, particularly in complex scenarios unattainable by vector-based methods.},
  address = {Vienna, Austria},
  author = {Jijin Hu and Ke Li and Yonggang Qi and Yi-Zhe Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024scaleadaptive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07; Poster Track},
  pdf = {https://openreview.net/pdf?id=5xadJmgwix},
  publisher = {OpenReview.net},
  title = {Scale-Adaptive Diffusion Model for Complex Sketch Synthesis},
  url = {https://openreview.net/forum?id=5xadJmgwix},
  year = {2024}
}

@inproceedings{hu2024large,
  abstract = {Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a (quasi)-zero-shot manner, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MPM, we build large multimodal models VisCPM in image-to-text and text-to-image generation, which achieve state-of-the-art (open-source) performance in Chinese. To facilitate future research, we open-source codes and model weights at https://github.com/OpenBMB/VisCPM.},
  address = {Vienna, Austria},
  author = {Jinyi Hu and Yuan Yao and Chongyi Wang and Shan Wang and Yinxu Pan and Qianyu Chen and Tianyu Yu and Hanghao Wu and Yue Zhao and Haoye Zhang and Xu Han and Yankai Lin and Jiao Xue and Dahai Li and Zhiyuan Liu and Maosong Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024large.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-01-15; Spotlight},
  pdf = {https://openreview.net/pdf?id=Kuh5qgCGCp},
  publisher = {OpenReview.net},
  title = {Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages},
  url = {https://openreview.net/forum?id=Kuh5qgCGCp},
  year = {2024}
}

@inproceedings{hu2024large,
  abstract = {Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with "HyPoradise" dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech. Experiments on various latest LLMs show the proposed approach achieves a new breakthrough on RobustHP with up to 53.9\% GER improvement in terms of word error rate (WER). Analysis verifies the effectiveness of our proposed language-space embedding to represent audio noise, under which LLMs show strong ability of language-space denoising.},
  address = {Vienna, Austria},
  author = {Yuchen Hu and Chen Chen and Chao-Han Huck Yang and Ruizhe Li and Chao Zhang and Pin-Yu Chen and Engsiong Chng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024large.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29; Spotlight top 5\%},
  pdf = {https://openreview.net/pdf?id=ceATjGPTUD},
  publisher = {OpenReview.net},
  title = {Large Language Models are Efficient Learners of Noise-Robust Speech Recognition},
  url = {https://openreview.net/forum?id=ceATjGPTUD},
  year = {2024}
}

@inproceedings{hu2024towards,
  abstract = {Large language models (LLMs) have recently driven striking performance improvements across a range of natural language processing tasks. The factual knowledge acquired during pretraining and instruction tuning can be useful in various downstream tasks, such as question answering, and language generation. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, LLMs implicitly store facts in their parameters. Content generated by the LLMs can often exhibit inaccuracies or deviations from the truth, due to facts that can be incorrectly induced or become obsolete over time. To this end, we aim to explore the extent and scope of factual knowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. Furthermore, we investigate whether LLMs can compose multiple facts, update factual knowledge temporally, reason over multiple pieces of facts, identify subtle factual differences, and resist adversarial examples. Extensive experiments on different sizes and types of LLMs show that existing LLMs still lack factual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing trustworthy artificial intelligence. The dataset Pinocchio and our codes are publicly available at: https://github.com/THU-BPM/Pinocchio.},
  address = {Vienna, Austria},
  author = {Xuming Hu and Junzhe Chen and Xiaochuan Li and Yufei Guo and Lijie Wen and Philip S. Yu and Zhijiang Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024towards.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-04-15; Spotlight},
  pdf = {https://openreview.net/pdf?id=9OevMUdods},
  publisher = {OpenReview.net},
  title = {Towards Understanding Factual Knowledge of Large Language Models},
  url = {https://openreview.net/forum?id=9OevMUdods},
  year = {2024}
}

@inproceedings{hu2024scalable,
  abstract = {In this paper, we propose a novel modular network framework, called Scalable Modular Network (SMN), which enables adaptive learning capability and supports integration of new modules after pre-training for better adaptation. This adaptive capability comes from a novel design of router within SMN, named agreement router, which selects and composes different specialist modules through an iterative message passing process. The agreement router iteratively computes the agreements among a set of input and outputs of all modules to allocate inputs to specific module. During the iterative routing, messages of modules are passed to each other, which improves the module selection process with consideration of both local interactions (between a single module and input) and global interactions involving multiple other modules. To validate our contributions, we conduct experiments on two problems: a toy min-max game and few-shot image classification task. Our experimental results demonstrate that SMN can generalize to new distributions and exhibit sample-efficient adaptation to new tasks. Furthermore, SMN can achieve a better adaptation capability when new modules are introduced after pre-training.},
  address = {Vienna, Austria},
  author = {Minyang Hu and Hong Chang and Bingpeng Ma and Shiguang Shan and Xilin Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024scalable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-25; Poster},
  pdf = {https://openreview.net/pdf?id=pEKJl5sflp},
  publisher = {OpenReview.net},
  title = {Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing},
  url = {https://openreview.net/forum?id=pEKJl5sflp},
  year = {2024}
}

@inproceedings{hu2024unbiased,
  abstract = {The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an unbiased watermark. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.},
  address = {Vienna, Austria},
  author = {Zhengmian Hu and Lichang Chen and Xidong Wu and Yihan Wu and Hongyang Zhang and Heng Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024unbiased.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-13; Spotlight},
  pdf = {https://openreview.net/pdf?id=uWVC5FVidc},
  publisher = {OpenReview.net},
  title = {Unbiased Watermark for Large Language Models},
  url = {https://openreview.net/forum?id=uWVC5FVidc},
  year = {2024}
}

@inproceedings{hu2024accelerating,
  abstract = {We study a family of distributed stochastic optimization algorithms where gradients are sampled by a token traversing a network of agents in random-walk fashion. Typically, these random-walks are chosen to be Markov chains that asymptotically sample from a desired target distribution, and play a critical role in the convergence of the optimization iterates. In this paper, we take a novel approach by replacing the standard linear Markovian token by one which follows a nonlinear Markov chain - namely the Self-Repellent Random Walk (SRRW). Defined for any given 'base' Markov chain, the SRRW, parameterized by a positive scalar Œ±, is less likely to transition to states that were highly visited in the past, thus the name. In the context of MCMC sampling on a graph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW achieves O(1/Œ±) decrease in the asymptotic variance for sampling. We propose the use of a 'generalized' version of the SRRW to drive token algorithms for distributed stochastic optimization in the form of stochastic approximation, termed SA-SRRW. We prove that the optimization iterate errors of the resulting SA-SRRW converge to zero almost surely and prove a central limit theorem, deriving the explicit form of the resulting asymptotic covariance matrix corresponding to iterate errors. This asymptotic covariance is always smaller than that of an algorithm driven by the base Markov chain and decreases at rate O(1/Œ±¬≤) - the performance benefit of using SRRW thereby amplified in the stochastic optimization context. Empirical results support our theoretical findings.},
  address = {Vienna, Austria},
  author = {Jie Hu and Vishwaraj Doshi and Do Young Eun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024accelerating.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29; Oral Presentation},
  pdf = {https://openreview.net/pdf?id=BV1PHbTJzd},
  publisher = {OpenReview.net},
  title = {Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks},
  url = {https://openreview.net/forum?id=BV1PHbTJzd},
  year = {2024}
}

@inproceedings{hu2024restoration,
  abstract = {Image denoisers have been shown to be powerful priors for solving inverse problems in imaging. In this work, we introduce a generalization of these methods that allows any image restoration network to be used as an implicit prior. The proposed method uses priors specified by deep neural networks pre-trained as general restoration operators. The method provides a principled approach for adapting state-of-the-art restoration models for other inverse problems. Our theoretical result analyzes its convergence to a stationary point of a global functional associated with the restoration operator. Numerical results show that the method using a super-resolution prior achieves state-of-the-art performance both quantitatively and qualitatively. Overall, this work offers a step forward for solving inverse problems by enabling the use of powerful pre-trained restoration models as priors.},
  address = {Vienna, Austria},
  author = {Yuyang Hu and Mauricio Delbracio and Peyman Milanfar and Ulugbek Kamilov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024restoration.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07; Poster},
  pdf = {https://openreview.net/pdf?id=x7d1qXEn1e},
  publisher = {OpenReview.net},
  title = {A Restoration Network as an Implicit Prior},
  url = {https://openreview.net/forum?id=x7d1qXEn1e},
  year = {2024}
}

@inproceedings{hu2024plugandplay,
  abstract = {Deformable image registration (DIR) is an active research topic in biomedical imaging. There is a growing interest in developing DIR methods based on deep learning (DL). A traditional DL approach to DIR is based on training a convolutional neural network (CNN) to estimate the registration field between two input images. While conceptually simple, this approach comes with a limitation that it exclusively relies on a pre-trained CNN without explicitly enforcing fidelity between the registered image and the reference. We present plug-and-play image registration network (PIRATE) as a new DIR method that addresses this issue by integrating an explicit data-fidelity penalty and a CNN prior. PIRATE pre-trains a CNN denoiser on the registration field and 'plugs' it into an iterative method as a regularizer. We additionally present PIRATE+ that fine-tunes the CNN prior in PIRATE using deep equilibrium models (DEQ). PIRATE+ interprets the fixed-point iteration of PIRATE as a network with effectively infinite layers and then trains the resulting network end-to-end, enabling it to learn more task-specific information and boosting its performance. Our numerical results on OASIS and CANDI datasets show that our methods achieve state-of-the-art performance on DIR.},
  author = {Junhao Hu and Weijie Gan and Zhixin Sun and Hongyu An and Ulugbek Kamilov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024plugandplay.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DGez4B2a6Y},
  publisher = {OpenReview.net},
  title = {A Plug-and-Play Image Registration Network},
  url = {https://openreview.net/forum?id=DGez4B2a6Y},
  year = {2024}
}

@inproceedings{hu2024amortizing,
  abstract = {Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.},
  author = {Edward J. Hu and Moksh Jain and Eric Elmoznino and Younesse Kaddar and Guillaume Lajoie and Yoshua Bengio and Nikolay Malkin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024amortizing.pdf:pdf},
  note = {ICLR 2024 oral presentation, DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Ouj6p4ca60},
  publisher = {OpenReview.net},
  title = {Amortizing intractable inference in large language models},
  url = {https://openreview.net/forum?id=Ouj6p4ca60},
  year = {2024}
}

@inproceedings{hu2024revisiting,
  abstract = {Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop, previously proposed in computer vision, but whose adaptation to DRL is novel to the best of our knowledge. We evaluate our proposition and validate our analysis in several domains. Compared to different relevant baselines, we demonstrate that it achieves state-of-the-art performance in most environments and shows higher sample efficiency and better generalization ability in some complex environments.},
  author = {Jianshu Hu and Yunpeng Jiang and Paul Weng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024revisiting.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=EGQBpkIEuu},
  publisher = {OpenReview.net},
  title = {Revisiting Data Augmentation in Deep Reinforcement Learning},
  url = {https://openreview.net/forum?id=EGQBpkIEuu},
  year = {2024}
}

@inproceedings{hu2024predicting,
  abstract = {The scientific scale-up of large language models (LLMs) necessitates a comprehensive understanding of their scaling properties. However, the existing literature on the scaling properties only yields an incomplete answer: optimization loss decreases predictably as the model size increases, in line with established scaling law; yet no scaling law for task has been established and the task performances are far from predictable during scaling. Task performances typically show minor gains on small models until they improve dramatically once models exceed a size threshold, exemplifying the ''emergent abilities''. In this study, we discover that small models, although they exhibit minor performance, demonstrate critical and consistent task performance improvements that are not captured by conventional evaluation strategies due to insufficient measurement resolution. To measure such improvements, we introduce PassUntil, an evaluation strategy with theoretically infinite resolution, through massive sampling in the decoding phase. With PassUntil, we conduct a quantitative investigation into the scaling law of task performance. The investigation contains two parts. Firstly, a strict task scaling law that is not conventionally known to exist, is identified, enhancing the predictability of task performances. Remarkably, we are able to predict the performance of the 2.4B model on code generation with merely 0.05\% deviation before training starts, which is the first systematic attempt to verify predictable scaling proposed by GPT-4's report. Secondly, underpinned by PassUntil, we are able to study emergent abilities quantitatively. We identify a kind of accelerated emergence whose scaling curve cannot be fitted by standard scaling law function and has a increasing speed. We then examine two hypothesis and imply that the ``multiple circuits hypothesis'' might be responsible for the accelerated emergence.},
  author = {Shengding Hu and Xin Liu and Xu Han and Xinrong Zhang and Chaoqun He and Weilin Zhao and Yankai Lin and Ning Ding and Zebin Ou and Guoyang Zeng and Zhiyuan Liu and Maosong Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024predicting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lDbjooxLkD},
  publisher = {OpenReview.net},
  title = {Predicting Emergent Abilities with Infinite Resolution Evaluation},
  url = {https://openreview.net/forum?id=lDbjooxLkD},
  year = {2024}
}

@inproceedings{hu2024recipe,
  abstract = {Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards underfitting than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art VRA for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large 'Cholesky-orthogonalized residual dense' layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points.},
  author = {Kai Hu and Klas Leino and Zifan Wang and Matt Fredrikson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024recipe.pdf:pdf},
  note = {DBLP last modified: 2024-08-19},
  pdf = {https://openreview.net/pdf?id=qz3mcn99cu},
  publisher = {OpenReview.net},
  title = {A Recipe for Improved Certifiable Robustness},
  url = {https://openreview.net/forum?id=qz3mcn99cu},
  year = {2024}
}

@inproceedings{hu2024querypolicy,
  abstract = {Preference-based reinforcement learning (PbRL) provides a natural way to align RL agents' behavior with human desired outcomes, but is often restrained by costly human feedback. The key to addressing this issue lies in how to select queries such that the preference oracle gives the most useful information about the reward function. Previous methods have focused on designing better query selection schemes, however, they do not consider the underlying learning dynamics, and thus may not be compatible with how policies are optimized. In this work, we identify a problem called query-policy misalignment: queries selected to improve the reward model may not actually benefit policy learning. We show both theoretically and empirically that this phenomenon can seriously impact the learning outcome. To this end, we propose a new method called policy-aligned query selection which not only improves the reward model but also takes into account policy learning dynamics. We design a policy-aligned query and a specially designed hybrid experience replay to address the query-policy misalignment issue. Our method demonstrates significant improvements over existing approaches on a diverse set of tasks and empirically validates the importance of considering policy learning dynamics in query selection.},
  author = {Xiao Hu and Jianxiong Li and Xianyuan Zhan and Qing-Shan Jia and Ya-Qin Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024querypolicy.pdf:pdf},
  note = {ICLR 2024 spotlight presentation, DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=UoBymIwPJR},
  publisher = {OpenReview.net},
  title = {Query-Policy Misalignment in Preference-Based Reinforcement Learning},
  url = {https://openreview.net/forum?id=UoBymIwPJR},
  year = {2024}
}

@inproceedings{hu2024treeplanner,
  abstract = {This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2\% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5\% decrease in error corrections.},
  author = {Mengkang Hu and Yao Mu and Xinmiao Yu and Mingyu Ding and Shiguang Wu and Wenqi Shao and Qiguang Chen and Bin Wang and Yu Qiao and Ping Luo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024treeplanner.pdf:pdf},
  note = {DBLP last modified: 2024-08-30},
  pdf = {https://openreview.net/pdf?id=Glcsog6zOe},
  publisher = {OpenReview.net},
  title = {Tree-Planner: Efficient Close-loop Task Planning with Large Language Models},
  url = {https://openreview.net/forum?id=Glcsog6zOe},
  year = {2024}
}

@inproceedings{hu2024privileged,
  abstract = {We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon 'sensory scaffolding': observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose 'Scaffolder', a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new 'S3' suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors.},
  author = {Edward S. Hu and James Springer and Oleh Rybkin and Dinesh Jayaraman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024privileged.pdf:pdf},
  note = {ICLR 2024 spotlight presentation, DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=EpVe8jAjdx},
  publisher = {OpenReview.net},
  title = {Privileged Sensing Scaffolds Reinforcement Learning},
  url = {https://openreview.net/forum?id=EpVe8jAjdx},
  year = {2024}
}

@inproceedings{hu2024evoke,
  abstract = {Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently. We propose Evoke, an automatic prompt refinement framework. In Evoke, there are two instances of a same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the other as an author (LLM-Author), it edits the prompt by considering the edit history and the reviewer's feedback. Such an author-reviewer feedback loop ensures that the prompt is refined in each iteration. We further aggregate a data selection approach to Evoke, where only the hard samples are exposed to the LLM. The hard samples are more important because the LLM can develop deeper understanding of the tasks out of them, while the model may already know how to solve the easier cases. Experimental results show that Evoke significantly outperforms existing methods. For instance, in the challenging task of logical fallacy detection, Evoke scores above 80, while all other baseline methods struggle to reach 20.},
  author = {Xinyu Hu and Pengfei Tang and Simiao Zuo and Zihan Wang and Bowen Song and Qiang Lou and Jian Jiao 0007 and Denis Charles},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024evoke.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OXv0zQ1umU},
  publisher = {OpenReview.net},
  title = {Evoke: Evoking Critical Thinking Abilities in {LLM}s via Reviewer-Author Prompt Editing},
  url = {https://openreview.net/forum?id=OXv0zQ1umU},
  year = {2024}
}

@inproceedings{hu2024less,
  abstract = {Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction, teacher prediction, and ground truth. To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher's global average prediction for samples within the same class. A simple neural network then learns the implicit mapping from the intra- and inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a bilevel-optimization manner. Our approach provides a simple, practical, and adaptable solution for knowledge distillation that can be employed across various architectures and model sizes. Extensive experiments demonstrate consistent improvements over other loss re-weighting methods on image classification, attack detection, and click-through rate prediction.},
  author = {Chengming Hu and Haolun Wu and Xuan Li and Chen Ma 0001 and Xi Chen 0009 and Boyu Wang and Jun Yan 0007 and Xue Liu 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024less.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OZitfSXpdT},
  publisher = {OpenReview.net},
  title = {Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation},
  url = {https://openreview.net/forum?id=OZitfSXpdT},
  year = {2024}
}

@inproceedings{hu2024attentionguided,
  abstract = {Real-world multi-agent tasks usually involve dynamic team composition with the emergence of roles, which should also be a key to efficient cooperation in multi-agent reinforcement learning (MARL). Drawing inspiration from the correlation between roles and agent's behavior patterns, we propose a novel framework of Attention-guided COntrastive Role representation learning for MARL (ACORM) to promote behavior heterogeneity, knowledge transfer, and skillful coordination across agents. First, we introduce mutual information maximization to formalize role representation learning, derive a contrastive learning objective, and concisely approximate the distribution of negative pairs. Second, we leverage an attention mechanism to prompt the global state to attend to learned role representations in value decomposition, implicitly guiding agent coordination in a skillful role space to yield more expressive credit assignment. Experiments on challenging StarCraft II micromanagement and Google research football tasks demonstrate the state-of-the-art performance of our method and its advantages over existing approaches.},
  address = {Vienna, Austria},
  author = {Zican Hu and Zongzhang Zhang and Huaxiong Li and Chunlin Chen and Hongyu Ding and Zhi Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024attentionguided.pdf:pdf},
  keywords = {Multi-agent reinforcement learning, contrastive learning, attention mechanism},
  month = {5},
  pdf = {https://openreview.net/pdf?id=LWmuPfEYhH},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Attention-Guided Contrastive Role Representations for Multi-agent Reinforcement Learning},
  url = {https://openreview.net/forum?id=LWmuPfEYhH},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{hu2024augmenting,
  abstract = {We present ReCAT, a recursive composition augmented Transformer that is able to explicitly model hierarchical syntactic structures of raw texts without relying on gold trees during both learning and inference. Existing research along this line restricts data to follow a hierarchical tree structure and thus lacks inter-span communications. To overcome the problem, we propose a novel contextual inside-outside (CIO) layer that learns contextualized representations of spans through bottom-up and top-down passes, where a bottom-up pass forms representations of high-level spans by composing low-level spans, while a top-down pass combines information inside and outside a span. By stacking several CIO layers between the embedding layer and the attention layers in Transformer, ReCAT can perform both deep intra-span and deep inter-span interactions, and thus generate multi-grained representations fully contextualized with other spans. The CIO layers can be jointly pre-trained with Transformers, making ReCAT enjoy scaling ability, strong performance, and interpretability at the same time. Evaluation results indicate that ReCAT can significantly outperform vanilla Transformer models on all span-level tasks and baselines that combine recursive networks with Transformers on natural language inference tasks. The hierarchical structures induced by ReCAT exhibit strong consistency with human-annotated syntactic trees, indicating good interpretability brought by the CIO layers.},
  address = {Vienna, Austria},
  author = {Xiang Hu and Qingyang Zhu and Kewei Tu and Wei Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024augmenting.pdf:pdf},
  keywords = {NLP, recursive neural network, multi-grain representation, compositional representation, span labeling, relation extraction, grammar induction, language understanding},
  month = {5},
  pdf = {https://openreview.net/pdf?id=u859gX7ADC},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Augmenting Transformers with Recursively Composed Multi-grained Representations},
  url = {https://openreview.net/forum?id=u859gX7ADC},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024understanding,
  abstract = {Federated Learning ({FL}) has attracted significant attention as an efficient privacy-preserving approach to distributed learning across multiple clients. Despite extensive empirical research and practical applications, a systematic way to theoretically understand the convergence and generalization properties in {FL} remains limited. This work aims to establish a unified theoretical foundation for understanding {FL} through feature learning theory. We focus on a scenario where each client employs a two-layer convolutional neural network ({CNN}) for local training on their own data. Many existing works analyze the convergence of Federated Averaging ({FedAvg}) under lazy training with linearizing assumptions in weight space. In contrast, our approach tracks the trajectory of signal learning and noise memorization in {FL}, eliminating the need for these assumptions. We further show that {FedAvg} can achieve near-zero test error by effectively increasing signal-to-noise ratio ({SNR}) in feature learning, while local training without communication achieves a large constant test error. This finding highlights the benefits of communication for generalization in {FL}. Moreover, our theoretical results suggest that a weighted {FedAvg} method, based on the similarity of input features across clients, can effectively tackle data heterogeneity issues in {FL}. Experimental results on both synthetic and real-world datasets verify our theoretical conclusions and emphasize the effectiveness of the weighted {FedAvg} approach.},
  address = {Vienna, Austria},
  author = {Wei Huang and Ye Shi and Zhongyi Cai and Taiji Suzuki},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024understanding.pdf:pdf},
  keywords = {Federated learning, feature learning theory, convergence analysis, generalization, convolutional neural networks},
  month = {5},
  pdf = {https://openreview.net/pdf?id=EcetCr4trp},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Understanding Convergence and Generalization in Federated Learning through Feature Learning Theory},
  url = {https://openreview.net/forum?id=EcetCr4trp},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024dittogym,
  abstract = {Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables discovery of policies that accomplish fine-grained control. We introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to accomplish tasks. We evaluate our proposed coarse-to-fine algorithm on DittoGym, demonstrating robots that learn to change their morphology several times within a sequence.},
  address = {Vienna, Austria},
  author = {Suning Huang and Boyuan Chen and Huazhe Xu and Vincent Sitzmann},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024dittogym.pdf:pdf},
  keywords = {Reinforcement learning, soft robotics, robot co-design, morphology control, reconfigurable robots},
  month = {5},
  pdf = {https://openreview.net/pdf?id=MpyFAhH9CK},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{DittoGym}: Learning to Control Soft Shape-Shifting Robots},
  url = {https://openreview.net/forum?id=MpyFAhH9CK},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024proteinligand,
  abstract = {Generating 3D ligand molecules that bind to specific protein targets via diffusion models has shown great promise for structure-based drug design. Existing diffusion models primarily focus on incorporating protein-ligand interaction information solely in the reverse process, and neglect the interactions in the forward process. The inconsistency between forward and reverse processes may impair the binding affinity of generated molecules towards target protein. To bridge this gap, we propose IPDiff, a novel approach that integrates protein-ligand interaction prior into both diffusion and sampling process. We begin by pretraining a protein-ligand interaction prior network (IPNet) by utilizing the binding affinity signals as supervision. We then leverage the pretrained prior network to (1) integrate interactions between the target protein and the molecular ligand into the forward process for adapting the molecule diffusion trajectories (prior-shifting), and (2) enhance the binding-aware molecule sampling process (prior-conditioning). Empirical studies on CrossDocked2020 dataset show IPDiff can generate molecules with more realistic 3D structures and state-of-the-art binding affinities towards the protein targets, with up to -6.42 Avg. Vina Score, while maintaining proper molecular properties.},
  address = {Vienna, Austria},
  author = {Zhilin Huang and Ling Yang and Xiangxin Zhou and Zhilong Zhang and Wentao Zhang and Xiawu Zheng and Jie Chen and Yu Wang and Bin Cui and Wenming Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024proteinligand.pdf:pdf},
  keywords = {Diffusion models, protein-ligand interaction, structure-based drug design, molecular generation, binding affinity},
  month = {5},
  pdf = {https://openreview.net/pdf?id=qH9nrMNTIW},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Protein-Ligand Interaction Prior for Binding-aware {3D} Molecule Diffusion Models},
  url = {https://openreview.net/forum?id=qH9nrMNTIW},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024ldreg,
  abstract = {Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse, also known as the underfilling phenomenon, is one of the major causes of degraded performance on downstream tasks. While previous work has investigated the dimensional collapse problem of SSL at a global level, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called local dimensionality regularization (LDReg). LDReg introduces a novel approach by regularizing the local intrinsic dimensionality (LID) of representations. By maximizing the distance between local distance distributions and a target distribution, LDReg aims to prevent representations from collapsing locally to lower-dimensional subspaces. Our experiments demonstrate that LDReg improves the representation quality of SSL methods and achieves superior performance on downstream tasks.},
  address = {Vienna, Austria},
  author = {Hanxun Huang and Ricardo J. G. B. Campello and Sarah Monazam Erfani and Xingjun Ma and Michael E. Houle and James Bailey},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024ldreg.pdf:pdf},
  keywords = {Self-supervised learning, dimensional collapse, local intrinsic dimensionality, representation learning, underfilling phenomenon},
  month = {5},
  pdf = {https://openreview.net/pdf?id=oZyAqjAjJW},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{LDReg}: Local Dimensionality Regularized Self-Supervised Learning},
  url = {https://openreview.net/forum?id=oZyAqjAjJW},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024ovor,
  abstract = {Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost. Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks.},
  address = {Vienna, Austria},
  author = {Wei-Cheng Huang and Chun-Fu Richard Chen and Hsiang Hsu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024ovor.pdf:pdf},
  keywords = {Class-incremental learning, rehearsal-free learning, virtual outlier regularization, prompt-based learning, continual learning},
  month = {5},
  pdf = {https://openreview.net/pdf?id=FbuyDzZTPt},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{OVOR}: {OnePrompt} with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning},
  url = {https://openreview.net/forum?id=FbuyDzZTPt},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024generative,
  abstract = {Limited data availability poses a major obstacle in training deep learning models for financial applications. Synthesizing financial time series to augment real-world data is challenging due to the irregular and scale-invariant patterns uniquely associated with financial time series ‚Äì temporal dynamics that repeat with varying duration and magnitude. To tackle this challenge, we develop a novel generative framework called FTS-Diffusion to model irregular and scale-invariant patterns that consists of three modules. First, we develop a scale-invariant pattern recognition algorithm to extract recurring patterns that vary in duration and magnitude. Second, we construct a diffusion-based generative network to synthesize segments of patterns. Third, we model the temporal transition of patterns in order to aggregate the generated segments. Extensive experiments show that FTS-Diffusion generates synthetic financial time series highly resembling observed data, outperforming state-of-the-art alternatives. Two downstream experiments demonstrate that augmenting real-world data with synthetic data generated by FTS-Diffusion reduces the error of stock market prediction by up to 17.9 percent. To the best of our knowledge, this is the first work on generating intricate time series with irregular and scale-invariant patterns, addressing data limitation issues in finance.},
  address = {Vienna, Austria},
  author = {Hongbin Huang and Minghua Chen and Xiao Qiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024generative.pdf:pdf},
  keywords = {Financial time series, generative learning, diffusion models, scale-invariant patterns, synthetic data generation},
  month = {5},
  pdf = {https://openreview.net/pdf?id=CdjnzWsQax},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Generative Learning for Financial Time Series with Irregular and Scale-Invariant Patterns},
  url = {https://openreview.net/forum?id=CdjnzWsQax},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024provable,
  abstract = {Multi-task reinforcement learning (RL) under Markov decision processes (MDPs) has shown significant benefits to sample efficiency compared to single-task RL, where shared latent structures among multiple MDPs can be exploited. We investigate whether such benefits can extend to more general sequential decision making problems such as partially observable MDPs (POMDPs) and more general predictive state representations (PSRs). The main challenge is that the large and complex model space makes it hard to identify what types of common latent structure of multi-task PSRs can reduce model complexity and improve sample efficiency. We propose using a joint model class for tasks and use the notion of Œ∑-bracketing number to quantify its complexity, which also serves as a general metric to capture task similarity and thus determines the benefit of multi-task over single-task RL. Our theoretical analysis reveals that under certain conditions on task similarity, multi-task RL achieves improved sample complexity compared to single-task approaches, providing the first theoretical guarantees for multi-task learning benefits in non-Markovian settings.},
  address = {Vienna, Austria},
  author = {Ruiquan Huang and Yuan Cheng and Jing Yang and Vincent Tan and Yingbin Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024provable.pdf:pdf},
  keywords = {Multi-task reinforcement learning, non-Markovian decision processes, POMDPs, predictive state representations, sample complexity, theoretical analysis},
  month = {5},
  pdf = {https://openreview.net/pdf?id=U6Qulbv2qT},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Provable Benefits of Multi-task {RL} under Non-{Markovian} Decision Making Processes},
  url = {https://openreview.net/forum?id=U6Qulbv2qT},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024reverse,
  abstract = {We propose a Monte Carlo sampling algorithm from the reverse diffusion process. Unlike typical implementations of diffusion models, where score functions are learned using neural networks, we transform the score matching problem into a mean estimation problem. By estimating the means of regularized posterior distributions, we derive a novel Monte Carlo sampling algorithm called reverse diffusion Monte Carlo (rdMC), which is distinct from Markov chain Monte Carlo (MCMC) methods. We determine the sample size from the error tolerance and properties of the posterior distribution to yield an algorithm that can approximately sample the target distribution with any desired accuracy. We demonstrate and prove under suitable conditions that sampling with rdMC can be significantly faster than MCMC methods. For multi-modal target distributions such as Gaussian mixture models, rdMC greatly improves over Langevin-style MCMC sampling methods both theoretically and in practice.},
  address = {Vienna, Austria},
  author = {Xunpeng Huang and Hanze Dong and Yifan Hao and Yian Ma and Tong Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024reverse.pdf:pdf},
  keywords = {Monte Carlo methods, reverse diffusion, score matching, sampling algorithms, Bayesian inference, multi-modal distributions},
  month = {5},
  pdf = {https://openreview.net/pdf?id=kIPEyMSdFV},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Reverse Diffusion Monte Carlo},
  url = {https://openreview.net/forum?id=kIPEyMSdFV},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{huang2024catastrophic,
  abstract = {The rapid progress in open-source large language models (LLMs) is significantly advancing AI development. Extensive efforts have been made before model release to align their behavior with human values, with the primary goal of ensuring their helpfulness and harmlessness. However, even carefully aligned models can be manipulated maliciously, leading to unintended behaviors, known as "jailbreaks". These jailbreaks are typically triggered by specific text inputs, often referred to as adversarial prompts. In this work, we propose the generation exploitation attack, an extremely simple approach that disrupts model alignment by only manipulating variations of decoding methods. By exploiting different generation strategies, including varying decoding hyper-parameters and sampling methods, we increase the attack success rate from 0\% to more than 95\% across 11 language models including LLaMA2, Vicuna, Falcon, and MPT families, outperforming state-of-the-art attacks. Finally, we propose an effective alignment method that explores diverse generation strategies, which can reasonably reduce the attack success rate under our attack. Altogether, our study underscores a major failure in current safety evaluation and alignment procedures for open-source LLMs, strongly advocating for more comprehensive red teaming and better alignment before releasing such models.},
  address = {Vienna, Austria},
  author = {Yangsibo Huang and Samyak Gupta and Mengzhou Xia and Kai Li and Danqi Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024catastrophic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-13},
  pdf = {https://openreview.net/pdf?id=r42tSSCHPh},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Catastrophic Jailbreak of Open-source {LLMs} via Exploiting Generation},
  url = {https://openreview.net/forum?id=r42tSSCHPh},
  year = {2024}
}

@inproceedings{huang2024fourier,
  abstract = {Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. We propose Fourier Transporter (FourTran), which leverages the two-fold SE(d)√óSE(d) symmetry in the pick-place problem to achieve much higher sample efficiency. FourTran is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new environments. FourTran is constrained to incorporate symmetries of the pick and place actions independently. The method utilizes a fiber space Fourier transformation that allows for memory-efficient construction. Tests on the RLbench benchmark achieve state-of-the-art results across various tasks. More specifically, FourTran outperforms baselines by a margin of between six percent (Stack-Wine) and two-hundred percent (Stack-Cups).},
  address = {Vienna, Austria},
  author = {Haojie Huang and Owen Lewis Howell and Dian Wang and Xupeng Zhu and Robert Platt and Robin Walters},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024fourier.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UulwvAU1W0},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Fourier Transporter: Bi-Equivariant Robotic Manipulation in {3D}},
  url = {https://openreview.net/forum?id=UulwvAU1W0},
  year = {2024}
}

@inproceedings{huang2024exposing,
  abstract = {In the battle against widespread online misinformation, a growing problem is text-image inconsistency, where images are misleadingly paired with texts with different intent or meaning. Existing classification-based methods for text-image inconsistency can identify contextual inconsistencies but fail to provide explainable justifications for their decisions that humans can understand. Although more nuanced, human evaluation is impractical at scale and susceptible to errors. Instead of employing a binary classification model, we introduce D-TIIL (Diffusion-based Text-Image Inconsistency Localization), which leverages text-to-image diffusion models to locate semantic inconsistencies in text and image pairs. D-TIIL offers interpretable evidence by localizing word- and pixel-level inconsistencies and quantifying them through a consistency score. The method uses text embeddings and modified image regions to visualize these inconsistencies, treating diffusion models trained on large-scale datasets as "omniscient" agents with extensive background knowledge. To evaluate D-TIIL's efficacy, we introduce a new TIIL dataset containing 14K consistent and inconsistent text-image pairs. Unlike existing datasets, TIIL enables assessment at the level of individual words and image regions and is carefully designed to represent various inconsistencies.},
  address = {Vienna, Austria},
  author = {Mingzhen Huang and Shan Jia and Zhou Zhou and Yan Ju and Jialing Cai and Siwei Lyu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024exposing.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Ny150AblPu},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Exposing Text-Image Inconsistency Using Diffusion Models},
  url = {https://openreview.net/forum?id=Ny150AblPu},
  year = {2024}
}

@inproceedings{huang2024provably,
  abstract = {The general sequential decision-making problem, which includes Markov decision processes (MDPs) and partially observable MDPs (POMDPs) as special cases, aims at maximizing a cumulative reward by making a sequence of decisions based on a history of observations and actions over time. Recent studies have shown that the sequential decision-making problem is statistically learnable if it admits a low-rank structure modeled by predictive state representations (PSRs). Despite these advancements, existing approaches typically involve oracles or steps that are computationally intractable. On the other hand, the upper confidence bound (UCB) based approaches, which have served successfully as computationally efficient methods in bandits and MDPs, have not been investigated for more general PSRs. We develop a provably efficient upper confidence bound algorithm for online and offline low rank decision-making problem, which is modeled by predictive state representation. The theoretical analysis only requires the smoothness of local objectives and bounded variance of stochastic gradients, without any additional assumptions on data heterogeneity or compression errors, as opposed to all prior related works.},
  address = {Vienna, Austria},
  author = {Ruiquan Huang and Yingbin Liang and Jing Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024provably.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jId5PXbBbX},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Provably Efficient {UCB}-type Algorithms For Learning Predictive State Representations},
  url = {https://openreview.net/forum?id=jId5PXbBbX},
  year = {2024}
}

@inproceedings{huang2024stability,
  abstract = {Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) Non-uniqueness: there are many different eigendecompositions of the same Laplacian, and (2) Instability: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding. Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally expressive for basis invariant functions whilst respecting all symmetries of eigenvectors. Besides guaranteed stability, we prove that SPE is at least as expressive as existing methods, and highly capable of counting graph structures. Finally, we evaluate the effectiveness of our method on molecular property prediction, and out-of-distribution generalization tasks, finding improved generalization compared to existing positional encoding methods.},
  address = {Vienna, Austria},
  author = {Yinan Huang and William Lu and Joshua Robinson and Yu Yang and Muhan Zhang and Stefanie Jegelka and Pan Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024stability.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xAqcJ9XoTf},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {On the Stability of Expressive Positional Encodings for Graphs},
  url = {https://openreview.net/forum?id=xAqcJ9XoTf},
  year = {2024}
}

@inproceedings{huang2024training,
  abstract = {Recent studies have shown that Graph Transformers (GTs) can be effective for specific graph-level tasks. However, when it comes to node classification, training GTs remains challenging, especially in semi-supervised settings with a severe scarcity of labeled data. Our paper aims to address this research gap by focusing on semi-supervised node classification. To accomplish this, we develop a curriculum-enhanced attention distillation method that involves utilizing a Local GT teacher and a Global GT student. Additionally, we introduce the concepts of in-class and out-of-class and then propose two improvements, out-of-class entropy and top-k pruning, to facilitate the student's out-of-class exploration under the teacher's in-class guidance. Taking inspiration from human learning, our method involves a curriculum mechanism for distillation that initially provides strict guidance to the student and gradually allows for more out-of-class exploration by a dynamic balance. Experiments demonstrate that our curriculum-enhanced attention distillation method for semi-supervised node classification, leveraging Local and Global Graph Transformers, outperforms state-of-the-art approaches on multiple benchmarks.},
  address = {Vienna, Austria},
  author = {Yisong Huang and Jin Li and Xinlong Chen and Yang-Geng Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024training.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=j4VMrwgn1M},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Training Graph Transformers via Curriculum-Enhanced Attention Distillation},
  url = {https://openreview.net/forum?id=j4VMrwgn1M},
  year = {2024}
}

@inproceedings{huang2024stochastic,
  abstract = {Communication compression is important for alleviating communication overhead in Federated Learning (FL). However, compression brings new challenges due to the interplay of compression-incurred information distortion and inherent FL characteristics like partial participation and data heterogeneity. Existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression. We revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. We introduce SCALLION and SCAFCOM algorithms that reach state-of-the-art performance among compressed FL algorithms. The keys to this significant improvement are our new formulation of stochastic controlled averaging and the introduction of momentum. Our theoretical analysis only requires the smoothness of local objectives and bounded variance of stochastic gradients, without any additional assumptions on data heterogeneity or compression errors, as opposed to all prior related works. Experiments demonstrate superior convergence and communication efficiency compared to existing compressed FL methods.},
  address = {Vienna, Austria},
  author = {Xinmeng Huang and Ping Li and Xiaoyun Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024stochastic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-31},
  pdf = {https://openreview.net/pdf?id=jj5ZjZsWJe},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Stochastic Controlled Averaging for Federated Learning with Communication Compression},
  url = {https://openreview.net/forum?id=jj5ZjZsWJe},
  year = {2024}
}

@inproceedings{huang2024mustard,
  abstract = {MUSTARD is a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. The challenge of generating high-quality mathematical training data with step-by-step solutions has been limited by the heavy labor required for manual annotation. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-wise formal solutions. Lastly, the framework utilizes a proof assistant (e.g., Lean Prover) to filter the valid proofs. With the proposed MUSTARD, we present a theorem-and-proof benchmark MUSTARDSAUCE with 5,866 valid data points. Each data point contains an informal statement, an informal proof, and a translated formal proof that passes the prover validation. The fine-tuned Llama 2-7B achieves a 15.41\% average relative performance gain in automated theorem proving, and 8.18\% in math word problems. Additionally, the fine-tuned Llama 2-7B achieves improvements by 20.9\% on zero-shot inference on GSM8K and achieves 8.7 of pass@1 on mathlib.},
  address = {Vienna, Austria},
  author = {Yinya Huang and Xiaohan Lin and Zhengying Liu and Qingxing Cao and Huajian Xin and Haiming Wang and Zhenguo Li and Linqi Song and Xiaodan Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024mustard.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=8xliOUg9EW},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {{MUSTARD}: Mastering Uniform Synthesis of Theorem and Proof Data},
  url = {https://openreview.net/forum?id=8xliOUg9EW},
  year = {2024}
}

@inproceedings{huang2024oneshot,
  abstract = {Active learning (AL) for multiple target models aims to reduce labeled data querying while effectively training multiple models concurrently. Existing AL algorithms often rely on iterative model training, which can be computationally expensive, particularly for deep models. This work proposes a one-shot AL method to address this challenge, which performs all label queries without repeated model training. We extract different representations of the same dataset using distinct network backbones, and actively learn the linear prediction layer on each representation via an ‚Ñìp-regression formulation. The regression problems are solved approximately by sampling and reweighting the unlabeled instances based on their maximum Lewis weights across the representations. An upper bound on the number of samples needed is provided with a rigorous analysis for p‚àà[1,+‚àû). Experimental results on 11 benchmarks show that our one-shot approach achieves competitive performances with the state-of-the-art AL methods for multiple target models while eliminating the need for repeated model training during the active learning process.},
  address = {Vienna, Austria},
  author = {Sheng-Jun Huang and Yi Li and Yiming Sun and Ying-Peng Tang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024oneshot.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EDXkkUAIFW},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models},
  url = {https://openreview.net/forum?id=EDXkkUAIFW},
  year = {2024}
}

@inproceedings{huang2024metatool,
  abstract = {MetaTool is a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. In scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. The ToolE dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-tool scenarios. The ToolE dataset includes 21.1k diverse user queries related to tool usage. The benchmark sets tasks for both tool usage awareness and tool selection, defining four subtasks from different perspectives in tool selection, including tool selection with similar choices, tool selection in specific scenarios, tool selection with possible reliability issues, and multi-tool selection. Experiments involving nine popular LLMs found that the majority of them still struggle to effectively select tools, highlighting the existing gaps between LLMs and genuine intelligent agents.},
  address = {Vienna, Austria},
  author = {Yue Huang and Jiawen Shi and Yuan Li and Chenrui Fan and Siyuan Wu and Qihui Zhang and Yixin Liu and Pan Zhou and Yao Wan and Neil Zhenqiang Gong and Lichao Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024metatool.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=R0c2qtalgG},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {{MetaTool} Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use},
  url = {https://openreview.net/forum?id=R0c2qtalgG},
  year = {2024}
}

@inproceedings{huang2024enhanced,
  abstract = {The current face recognition (FR) algorithms has achieved a high level of accuracy, making further improvements increasingly challenging. While existing FR algorithms primarily focus on optimizing margins and loss functions, limited attention has been given to exploring the feature representation space. The paper aims to improve FR performance by analyzing two FR models with different performance levels, implementing orthogonal decomposition of features, discovering that sub-features can have face distinguishability, and introducing an intra-class incoherence constraint (IIC). Experiments demonstrate potential for improved facial feature representation and performance enhancement across FR benchmarks.},
  author = {Yuanqing Huang and Yinggui Wang and Le Yang and Lei Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024enhanced.pdf:pdf},
  keywords = {representation learning, computer vision, face recognition, intra-class incoherence constraint},
  month = {5},
  note = {ICLR 2024 spotlight paper},
  pdf = {https://openreview.net/pdf?id=uELjxVbrqG},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Enhanced Face Recognition using Intra-class Incoherence Constraint}},
  url = {https://openreview.net/forum?id=uELjxVbrqG},
  year = {2024}
}

@inproceedings{huang2024cleanba,
  abstract = {Distributed Deep Reinforcement Learning (DRL) aims to leverage more computational resources to train autonomous agents with less training time. Despite recent progress in the field, reproducibility issues have not been sufficiently explored. This paper first shows that the typical actor-learner framework can have reproducibility issues even if hyperparameters are controlled. We then introduce Cleanba, a new open-source platform for distributed DRL that proposes a highly reproducible architecture. Cleanba implements highly optimized distributed variants of PPO and IMPALA. Our Atari experiments show that these variants can obtain equivalent or higher scores than strong IMPALA baselines in moolib and torchbeast and PPO baseline in CleanRL. However, Cleanba variants present 1) shorter training time and 2) more reproducible learning curves in different hardware settings.},
  author = {Shengyi Huang and Jiayi Weng and Rujikorn Charakorn and Min Lin and Zhongwen Xu and Santiago Ontan{\'o}n},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024cleanba.pdf:pdf},
  keywords = {distributed reinforcement learning, reproducibility, PPO, IMPALA},
  month = {5},
  pdf = {https://openreview.net/pdf?id=Diq6urt3lS},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Cleanba: A Reproducible and Efficient Distributed Reinforcement Learning Platform}},
  url = {https://openreview.net/forum?id=Diq6urt3lS},
  year = {2024}
}

@inproceedings{huang2024efficient,
  abstract = {Score matching methods -- estimate probability densities without computing the normalization constant -- are particularly useful in deep learning. However, computational and memory costs of score matching methods can be prohibitive for high-dimensional data or complex models, particularly due to the derivatives or Hessians of the log density function appearing in the objective function. Some existing approaches modify the objective function to reduce the quadratic computational complexity for Hessian computation. However, the memory bottleneck of score matching methods remains for deep learning. This study improves the memory efficiency of score matching by leveraging deep equilibrium models. We provide a theoretical analysis of deep equilibrium models for scoring matching and applying implicit differentiation to higher-order derivatives. Empirical evaluations demonstrate that our approach enables the development of deep and expressive models with improved performance and comparable computational and memory costs over shallow architectures.},
  author = {Yuhao Huang and Qingsong Wang and Akwum Onwunta and Bao Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024efficient.pdf:pdf},
  keywords = {score matching, deep equilibrium model, density estimation},
  month = {5},
  pdf = {https://openreview.net/pdf?id=J1djqLAa6N},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Efficient Score Matching with Deep Equilibrium Layers}},
  url = {https://openreview.net/forum?id=J1djqLAa6N},
  year = {2024}
}

@inproceedings{huang2024dreamtime,
  abstract = {Text-to-image diffusion models pre-trained on billions of image-text pairs have recently enabled 3D content creation by optimizing a randomly initialized differentiable 3D representation with score distillation. However, the optimization process suffers slow convergence and the resultant 3D models often exhibit two limitations: (a) quality concerns such as missing attributes and distorted shape and texture; (b) extremely low diversity comparing to text-guided image synthesis. In this paper, we show that the conflict between the 3D optimization process and uniform timestep sampling in score distillation is the main reason for these limitations. To resolve this conflict, we propose to prioritize timestep sampling with monotonically non-increasing functions, which aligns the 3D optimization process with the sampling process of diffusion model. Extensive experiments show that our simple redesign significantly improves 3D content creation with faster convergence, better quality and diversity.},
  author = {Yukun Huang and Jianan Wang and Yukai Shi and Boshi Tang and Xianbiao Qi and Lei Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024dreamtime.pdf:pdf},
  keywords = {score distillation, 3D content creation, diffusion model},
  month = {5},
  pdf = {https://openreview.net/pdf?id=1bAUywYJTU},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{DreamTime: An Improved Optimization Strategy for Diffusion-Guided 3D Generation}},
  url = {https://openreview.net/forum?id=1bAUywYJTU},
  year = {2024}
}

@inproceedings{huang2024branching,
  abstract = {Generating a set of text is a common challenge for many NLP applications, for example, automatically providing multiple keyphrases for a document to facilitate user reading. Existing generative models use a sequential decoder that generates a single sequence successively, and the set generation problem is converted to sequence generation via concatenating multiple text into a long text sequence. However, the elements of a set are unordered, which makes this scheme suffer from biased or conflicting training signals. In this paper, we propose a branching decoder, which can generate a dynamic number of tokens at each time-step and branch multiple generation paths. In particular, paths are generated individually so that no order dependence is required. Moreover, multiple paths can be generated in parallel which greatly reduces the inference time. Experiments on several keyphrase generation datasets demonstrate that the branching decoder is more effective and efficient than the existing sequential decoder.},
  author = {Zixian Huang and Gengyang Xiao and Yu Gu and Gong Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024branching.pdf:pdf},
  keywords = {set generation, keyphrase generation},
  month = {5},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=riNuqYiD66},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{A Branching Decoder for Set Generation}},
  url = {https://openreview.net/forum?id=riNuqYiD66},
  year = {2024}
}

@inproceedings{huang2024textfield3d,
  abstract = {Recent works learn 3D representation explicitly under text-3D guidance. However, limited text-3D data restricts the vocabulary scale and text control of generations. Generators may easily fall into a stereotype concept for certain text prompts, thus losing open-vocabulary generation ability. To tackle this issue, we introduce a conditional 3D generative model, namely TextField3D. The approach injects dynamic noise into the latent space of given text prompts through Noisy Text Fields (NTFs), with the goal of expanding the textual latent space and improving 3D generation capabilities. The method includes an NTFGen module to model general text latent code in noisy fields, an NTFBind module to align view-invariant image latent code to noisy fields for image-conditional generation, and multi-modal discrimination with text-3D and text-2.5D discriminators. Compared to previous methods, TextField3D demonstrates large vocabulary, text consistency, and low latency.},
  author = {Tianyu Huang and Yihan Zeng and Bowen Dong and Hang Xu and Songcen Xu and Rynson W. H. Lau and Wangmeng Zuo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024textfield3d.pdf:pdf},
  keywords = {3D generation, open-vocabulary generation, noisy text fields, text-to-3D},
  month = {5},
  pdf = {https://openreview.net/pdf?id=WOiOzHG2zD},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields}},
  url = {https://openreview.net/forum?id=WOiOzHG2zD},
  year = {2024}
}

@inproceedings{huang2024tag2text,
  abstract = {This paper presents Tag2Text, a vision language pre-training framework that introduces image tagging into vision-language models to guide the learning of visual-linguistic features. In contrast to prior works using limited object tags, this approach utilizes tags parsed from paired text to learn an image tagger and provide model guidance. The method enables large-scale annotation-free image tagging, provides more diverse tag categories beyond objects, demonstrates superior zero-shot performance, and enhances vision-language model performance across tasks. Tag2Text showcases superior zero-shot performance comparable to full supervision manner and achieves image tag recognition ability of 3,429 commonly human-used categories without manual annotations. Across a wide range of downstream benchmarks, Tag2Text achieves state-of-the-art results with similar model sizes and data scales.},
  author = {Xinyu Huang and Youcai Zhang and Jinyu Ma and Weiwei Tian and Rui Feng and Yuejie Zhang and Yaqian Li and Yandong Guo and Lei Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024tag2text.pdf:pdf},
  keywords = {vision-language model, image tagging, zero-shot learning, multimodal pre-training},
  month = {5},
  note = {Project page: https://tag2text.github.io/},
  pdf = {https://openreview.net/pdf?id=x6u2BQ7xcq},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Tag2Text: Guiding Vision-Language Model via Image Tagging}},
  url = {https://openreview.net/forum?id=x6u2BQ7xcq},
  year = {2024}
}

@inproceedings{huang2024froster,
  abstract = {The paper introduces FROSTER, a framework for open-vocabulary action recognition that uses a residual feature distillation approach to adapt the CLIP model to action recognition while maintaining its generalization capabilities. The method aims to bridge the gap between image and video features by treating the frozen CLIP model as a teacher. FROSTER employs residual feature distillation that treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features. The framework uses a residual sub-network for feature distillation to reach a balance between learning generalizable and video-specific features, consistently achieving state-of-the-art performance on all datasets across both base-to-novel and cross-dataset settings.},
  author = {Xiaohu Huang and Hao Zhou and Kun Yao and Kai Han},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huang2024froster.pdf:pdf},
  keywords = {open-vocabulary action recognition, CLIP, feature distillation, video understanding},
  month = {5},
  note = {Project page: https://visual-ai.github.io/froster},
  pdf = {https://openreview.net/pdf?id=zYXFMeHRtO},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{FROSTER: Frozen CLIP is A Strong Teacher for Open-Vocabulary Action Recognition}},
  url = {https://openreview.net/forum?id=zYXFMeHRtO},
  year = {2024}
}

@inproceedings{huben2024sparse,
  abstract = {One of the roadblocks to a better understanding of neural networks' internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. The paper explores using sparse autoencoders to identify more interpretable features in language models by addressing the problem of superposition, where neural networks represent more features than they have neurons. We use a scalable and unsupervised method called Sparse Autoencoders to find interpretable, monosemantic features in the residual streams of real LLMs (Pythia-70M/410M). The authors demonstrate the ability to pinpoint causally responsible features for counterfactual behavior on the indirect object identification task with finer granularity than previous decompositions. This work indicates it's possible to resolve superposition in language models using a scalable, unsupervised method, potentially serving as a foundation for future mechanistic interpretability work to enable greater model transparency and steerability.},
  author = {Robert Huben and Hoagy Cunningham and Logan Riggs Smith and Aidan Ewart and Lee Sharkey},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/huben2024sparse.pdf:pdf},
  keywords = {mechanistic interpretability, sparse autoencoders, polysemanticity, superposition, language models},
  month = {5},
  pdf = {https://openreview.net/pdf?id=F76bwRSLeK},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Sparse Autoencoders Find Highly Interpretable Features in Language Models}},
  url = {https://openreview.net/forum?id=F76bwRSLeK},
  year = {2024}
}

@inproceedings{hungquang2024understanding,
  abstract = {Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model's output, an attacker can employ black-box attacks to generate such adversarial examples. The paper proposes a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Their theoretical analysis confirms that this method effectively enhances the model's resilience against both score-based and decision-based black-box attacks. The defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Their analysis reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function.},
  author = {Nguyen Hung-Quang and Yingjie Lao and Tung Pham and Kok-Seng Wong and Khoa D. Doan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hungquang2024understanding.pdf:pdf},
  keywords = {adversarial attacks, adversarial defense, black-box attacks},
  month = {5},
  pdf = {https://openreview.net/pdf?id=vZ6r9GMT1n},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Understanding the Robustness of Randomized Feature Defense Against Query-Based Adversarial Attacks}},
  url = {https://openreview.net/forum?id=vZ6r9GMT1n},
  year = {2024}
}

@inproceedings{hu2024defining,
  abstract = {Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and 'expertise' is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise---particularly the type of expertise the decision-makers of a domain are likely to have---can be informative in designing and selecting methods for treatment effect estimation. We formally define two types of expertise, predictive and prognostic, and demonstrate empirically that: (i) the prominent type of expertise in a domain significantly influences the performance of different methods in treatment effect estimation, and (ii) it is possible to predict the type of expertise present in a dataset, which can provide a quantitative basis for model selection.},
  author = {Alihan H{\"u}y{\"u}k and Qiyao Wei and Alicia Curth and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hu2024defining.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=1YPfmglNRU},
  publisher = {OpenReview.net},
  title = {Defining Expertise: Applications to Treatment Effect Estimation},
  url = {https://openreview.net/forum?id=1YPfmglNRU},
  year = {2024}
}

@inproceedings{hvarfner2024general,
  abstract = {The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the underlying function dynamics. However, the ability of Bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight budgets. To allow domain experts to customize the optimization routine, we propose {ColaBO}, the first Bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. The generality of {ColaBO} makes it applicable across different Monte Carlo acquisition functions and types of user beliefs. We empirically demonstrate {ColaBO}'s ability to substantially accelerate optimization when the prior information is accurate, and to retain approximately default performance when it is misleading.},
  author = {Carl Hvarfner and Frank Hutter and Luigi Nardi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hvarfner2024general.pdf:pdf},
  note = {ICLR 2024 spotlight},
  pdf = {https://openreview.net/pdf?id=NjU0jtXcYn},
  publisher = {OpenReview.net},
  title = {A General Framework for User-Guided Bayesian Optimization},
  url = {https://openreview.net/forum?id=NjU0jtXcYn},
  year = {2024}
}

@inproceedings{hwang2024sf,
  abstract = {In the face of the deep learning model's vulnerability to domain shift, source-free domain adaptation ({SFDA}) methods have been proposed to adapt models to new, unseen target domains without requiring access to source domain data. Although the potential benefits of applying data augmentation to {SFDA} are attractive, several challenges arise such as the dependence on prior knowledge of class-preserving transformations and the increase in memory and computational requirements. In this paper, we propose Source-free Domain Adaptation Through the Lens of Data Augmentation ({SF}({DA})$^2$), a novel approach that leverages the benefits of data augmentation without suffering from these challenges. We construct an augmentation graph in the feature space of the pretrained model using the neighbor relationships between target features and propose spectral neighborhood clustering to identify partitions in the prediction space. Furthermore, we propose implicit feature augmentation and feature disentanglement as regularization loss functions that effectively utilize class semantic information within the feature space. These regularizers simulate the inclusion of an unlimited number of augmented target features into the augmentation graph while minimizing computational and memory demands. Our method shows superior adaptation performance in {SFDA} scenarios, including 2D image and 3D point cloud datasets and a highly imbalanced dataset.},
  author = {Uiwon Hwang and Jonghyun Lee and Juhyeon Shin and Sungroh Yoon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/hwang2024sf.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=kUCgHbmO11},
  publisher = {OpenReview.net},
  title = {{SF}({DA})$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation},
  url = {https://openreview.net/forum?id=kUCgHbmO11},
  year = {2024}
}

@inproceedings{igashov2024retrobridge,
  abstract = {Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing multi-step reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis as a distribution learning problem in a discrete state space. First, we introduce the {M}arkov {B}ridge {M}odel, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a {M}arkov bridge, a {M}arkov process pinned at its endpoints. Unlike diffusion-based methods, our {M}arkov {B}ridge {M}odel does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior distribution. We then address the retrosynthesis planning problem with our novel framework and introduce {RetroBridge}, a template-free retrosynthesis modeling approach that achieves state-of-the-art results on standard evaluation benchmarks.},
  author = {Ilia Igashov and Arne Schneuing and Marwin Segler and Michael M. Bronstein and Bruno Correia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/igashov2024retrobridge.pdf:pdf},
  note = {ICLR 2024 spotlight},
  pdf = {https://openreview.net/pdf?id=770DetV8He},
  publisher = {OpenReview.net},
  title = {{RetroBridge}: Modeling Retrosynthesis with {M}arkov Bridges},
  url = {https://openreview.net/forum?id=770DetV8He},
  year = {2024}
}

@inproceedings{imfeld2024transformer,
  abstract = {Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. This paper presents a systematic approach for fusing two or more transformer-based networks exploiting {O}ptimal {T}ransport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures -- in principle -- and we apply this to the key ingredients of {T}ransformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way to compress {T}ransformers. The proposed approach is evaluated on both image classification tasks via {V}ision {T}ransformer and natural language modeling tasks using {BERT}. Our approach consistently outperforms vanilla fusion, and, after a surprisingly short finetuning, also outperforms the individual converged parent models. In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of {T}ransformers. Our results showcase the potential of fusing multiple {T}ransformers, thus compounding their expertise, in the budding paradigm of model fusion and recombination.},
  author = {Moritz Imfeld and Jacopo Graldi and Marco Giordano and Thomas Hofmann and Sotiris Anagnostidis and Sidak Pal Singh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/imfeld2024transformer.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=LjeqMvQpen},
  publisher = {OpenReview.net},
  title = {Transformer Fusion with Optimal Transport},
  url = {https://openreview.net/forum?id=LjeqMvQpen},
  year = {2024}
}

@inproceedings{ireland2024revalued,
  abstract = {Discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. A recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. This study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to Q-learning algorithms, it amplifies target variance. To counteract this, we present an ensemble of critics to mitigate target variance. Moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. Our novel algorithm, {REValueD}, tested on discretised versions of the {DeepMind Control Suite} tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. We further dissect the factors influencing {REValueD}'s performance, evaluating the significance of the regularisation loss and the scalability of {REValueD} with increasing sub-actions per dimension.},
  author = {David Ireland and Giovanni Montana},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ireland2024revalued.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=Gf15GsnfTy},
  publisher = {OpenReview.net},
  title = {{REValueD}: Regularised Ensemble Value-Decomposition for Factorisable {M}arkov Decision Processes},
  url = {https://openreview.net/forum?id=Gf15GsnfTy},
  year = {2024}
}

@inproceedings{irie2024exploring,
  abstract = {Real-time recurrent learning ({RTRL}) for sequence-processing recurrent neural networks ({RNNs}) offers certain conceptual advantages over backpropagation through time ({BPTT}). {RTRL} requires neither caching past activations nor truncating context, and enables online learning. However, {RTRL}'s time and space complexity make it impractical. To overcome this problem, most recent work on {RTRL} focuses on approximation theories, while experiments are often limited to diagnostic settings. Here we explore the practical promise of {RTRL} in more realistic settings. We study actor-critic methods that combine {RTRL} and policy gradients, and test them in several subsets of {DMLab}-30, {ProcGen}, and {Atari}-2600 environments. On {DMLab} memory tasks, our system trained on fewer than 1.2B environmental frames is competitive with or outperforms well-known {IMPALA} and {R2D2} baselines trained on 10B frames. To scale to such challenging tasks, we focus on certain well-known neural architectures with element-wise recurrence, allowing for tractable {RTRL} without approximation. Importantly, we also discuss rarely addressed limitations of {RTRL} in real-world applications, such as its complexity in the multi-layer case.},
  author = {Kazuki Irie and Anand Gopalakrishnan and J{\"u}rgen Schmidhuber},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/irie2024exploring.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=V2cBKtdC3a},
  publisher = {OpenReview.net},
  title = {Exploring the Promise and Limits of Real-Time Recurrent Learning},
  url = {https://openreview.net/forum?id=V2cBKtdC3a},
  year = {2024}
}

@inproceedings{isajanyan2024social,
  abstract = {Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to actively engage and contribute with content to accumulate peers approval. In the realm of text-conditioned image synthesis, the recent surge in progress has ushered in a collaborative era where users and {AI} systems coalesce to refine visual creations. This co-creative process in the landscape of online social networks empowers users to craft original visual artworks seeking for community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Existing evaluation methods predominantly center on limited size user studies guided by image quality and alignment with prompts. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from {P}icsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named {P}icsart Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for {AI}-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art.},
  author = {Arman Isajanyan and Artur Shatveryan and David Kocharian and Zhangyang Wang and Humphrey Shi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/isajanyan2024social.pdf:pdf},
  note = {ICLR 2024 spotlight},
  pdf = {https://openreview.net/pdf?id=tjn2YZSHUv},
  publisher = {OpenReview.net},
  title = {Social Reward: Evaluating and Enhancing Generative {AI} through Million-User Feedback from an Online Creative Community},
  url = {https://openreview.net/forum?id=tjn2YZSHUv},
  year = {2024}
}

@inproceedings{iscen2024retrievalenhanced,
  abstract = {Contrastive image-text models such as {CLIP} form the building blocks of many state-of-the-art systems. While they excel at recognizing common generic concepts, they still struggle on fine-grained entities which are rare, or even absent from the pre-training dataset. Hence, a key ingredient to their success has been the use of large-scale curated pre-training data aiming at expanding the set of concepts that they can memorize during the pre-training stage. In this work, we explore an alternative to encoding fine-grained knowledge directly into the model's parameters: we instead train the model to retrieve this knowledge from an external memory. Specifically, we propose to equip existing vision-text models with the ability to refine their embedding with cross-modal retrieved information from a memory at inference time, which greatly improves their zero-shot predictions. Remarkably, we show that this can be done with a light-weight, single-layer, fusion transformer on top of a frozen {CLIP}. Our experiments validate that our retrieval-enhanced contrastive ({RECO}) training improves {CLIP} performance substantially on several challenging fine-grained tasks: for example +10.9 on {S}tanford Cars, +10.2 on {CUB}-2011 and +7.3 on the recent {OVEN} benchmark, where we even outperform the fine-tuned models on unseen classes.},
  author = {Ahmet Iscen and Mathilde Caron and Alireza Fathi and Cordelia Schmid},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/iscen2024retrievalenhanced.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=b2UlHeyyC0},
  publisher = {OpenReview.net},
  title = {Retrieval-Enhanced Contrastive Vision-Text Models},
  url = {https://openreview.net/forum?id=b2UlHeyyC0},
  year = {2024}
}

@inproceedings{ishfaq2024provable,
  abstract = {We present a scalable and effective exploration strategy based on {T}hompson sampling for reinforcement learning ({RL}). One of the key shortcomings of existing {T}hompson sampling algorithms is the need to perform a {G}aussian approximation of the posterior distribution, which is not a good surrogate in most practical settings. We instead directly sample the Q function from its posterior distribution, by using {L}angevin {M}onte {C}arlo, an efficient type of {M}arkov {C}hain {M}onte {C}arlo ({MCMC}) method. Our method only needs to perform noisy gradient descent updates to learn the exact posterior distribution of the Q function, which makes our approach easy to deploy in deep {RL}. We provide a rigorous theoretical analysis for the proposed method and demonstrate that, in the linear {M}arkov decision process (linear {MDP}) setting, it has a regret bound of $\tilde{O}(d^{3/2}H^{3/2}\sqrt{T})$, where $d$ is the dimension of the feature mapping, $H$ is the planning horizon, and $T$ is the total number of steps. We apply this approach to deep {RL}, by using {A}dam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep {RL} algorithms on several challenging exploration tasks from the {A}tari57 suite.},
  author = {Haque Ishfaq and Qingfeng Lan and Pan Xu and A. Rupam Mahmood and Doina Precup and Anima Anandkumar and Kamyar Azizzadenesheli},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ishfaq2024provable.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=nfIAEJFiBZ},
  publisher = {OpenReview.net},
  title = {Provable and Practical: Efficient Exploration in Reinforcement Learning via {L}angevin {M}onte {C}arlo},
  url = {https://openreview.net/forum?id=nfIAEJFiBZ},
  year = {2024}
}

@inproceedings{ishikawa2024parameterization,
  abstract = {Second-order optimization has been developed to accelerate the training of deep neural networks and it is being applied to increasingly larger-scale models. In this study, towards training on further larger scales, we identify a specific parameterization for second-order optimization that promotes feature learning in a stable manner even if the network width increases significantly. Inspired by a maximal update parametrization, we consider a one-step update of the gradient and reveal the appropriate scales of hyperparameters including random initialization, learning rates, and damping terms. Our approach covers two major second-order optimization algorithms, K-FAC and Shampoo, and we demonstrate that our parametrization achieves higher generalization performance in feature learning. In particular, it enables us to transfer the hyperparameters across models with different widths.},
  address = {Vienna, Austria},
  author = {Satoki Ishikawa and Ryo Karakida},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ishikawa2024parameterization.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=g8sGBSQjYk},
  publisher = {OpenReview.net},
  title = {On the Parameterization of Second-Order Optimization Effective towards the Infinite Width},
  url = {https://openreview.net/forum?id=g8sGBSQjYk},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{islam2024eqamx,
  abstract = {Humans predominantly use verbal utterances and nonverbal gestures (e.g., eye gaze and pointing gestures) in their natural interactions. For instance, pointing gestures and verbal information is often required to comprehend questions such as 'what object is that?' Thus, this question-answering (QA) task involves complex reasoning of multimodal expressions (verbal utterances and nonverbal gestures). However, prior works have explored QA tasks in non-embodied settings, where questions solely contain verbal utterances from a single verbal and visual perspective. In this paper, we have introduced 8 novel embodied question answering (EQA) tasks to develop learning models to comprehend embodied questions with multimodal expressions. We have developed a novel large-scale dataset, EQA-MX, with over 8 million diverse embodied QA data samples involving multimodal expressions from multiple visual and verbal perspectives. To learn salient multimodal representations from discrete verbal embeddings and continuous wrapping of multiview visual representations, we propose a vector-quantization (VQ) based multimodal representation learning model, VQ-Fusion, for the EQA tasks. Our extensive experimental results suggest that VQ-Fusion can improve the performance of existing state-of-the-art visual-language models up to 13% across EQA tasks.},
  address = {Vienna, Austria},
  author = {Md Mofijul Islam and Alexi Gladstone and Riashat Islam and Tariq Iqbal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/islam2024eqamx.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=7gUrYE50Rb},
  publisher = {OpenReview.net},
  title = {{EQA-MX}: Embodied Question Answering using Multimodal Expression},
  url = {https://openreview.net/forum?id=7gUrYE50Rb},
  venue = {ICLR 2024 spotlight},
  year = {2024}
}

@inproceedings{ismail2024concept,
  abstract = {We introduce a generative model with an intrinsically interpretable layer---a concept bottleneck layer---that constrains the model to encode human-understandable concepts. The concept bottleneck layer partitions the generative model into three parts: the pre-concept bottleneck portion, the CB layer, and the post-concept bottleneck portion. To train CB generative models, we complement the traditional task-based loss function for training generative models with a concept loss and an orthogonality loss. The CB layer and these loss terms are model agnostic, which we demonstrate by applying the CB layer to three different families of generative models: generative adversarial networks, variational autoencoders, and diffusion models. On multiple datasets across different types of generative models, steering a generative model, with the CB layer, outperforms all baselines---in some cases, it is 10 times more effective. In addition, we show how the CB layer can be used to interpret the output of the generative model and debug the model during or post training.},
  address = {Vienna, Austria},
  author = {Aya Abdelsalam Ismail and Julius Adebayo and H{\'e}ctor Corrada Bravo and Stephen Ra and Kyunghyun Cho},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ismail2024concept.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=L9U5MJJleF},
  publisher = {OpenReview.net},
  title = {Concept Bottleneck Generative Models},
  url = {https://openreview.net/forum?id=L9U5MJJleF},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{ito2024generalization,
  abstract = {The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three types of out-of-distribution generalization performance, finding that cross-attention mechanisms like the Perceiver showed better generalization compared to standard architectures.},
  address = {Vienna, Austria},
  author = {Takuya Ito and Soham Dan and Mattia Rigotti and James R. Kozloski and Murray Campbell},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ito2024generalization.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=zyBJodMrn5},
  publisher = {OpenReview.net},
  title = {On the generalization capacity of neural networks during generic multimodal reasoning},
  url = {https://openreview.net/forum?id=zyBJodMrn5},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jabri2024dorsal,
  abstract = {Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on frozen object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View dataset, we show that DORSal enables scalable neural rendering of 3D scenes with object-level editing and improves upon existing approaches.},
  address = {Vienna, Austria},
  author = {Allan Jabri and Sjoerd van Steenkiste and Emiel Hoogeboom and Mehdi S. M. Sajjadi and Thomas Kipf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jabri2024dorsal.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=3zvB14IF6D},
  publisher = {OpenReview.net},
  title = {{DORSal}: Diffusion for Object-centric Representations of Scenes},
  url = {https://openreview.net/forum?id=3zvB14IF6D},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jackson2024discovering,
  abstract = {Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or 'training horizon'. In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends that ignoring the optimization time horizon significantly restricts the expressive potential of discovered learning algorithms. We propose a simple augmentation to two existing objective discovery approaches that allows the discovered algorithm to dynamically update its objective function throughout the agent's training procedure, resulting in expressive schedules and increased generalization across different training horizons. In the process, we find that commonly used meta-gradient approaches fail to discover such adaptive objective functions while evolution strategies discover highly dynamic learning rules. We demonstrate the effectiveness of our approach on a wide range of tasks and analyze the resulting learned algorithms, which we find effectively balance exploration and exploitation by modifying the structure of their learning rules throughout the agent's lifetime.},
  address = {Vienna, Austria},
  author = {Matthew Thomas Jackson and Chris Lu and Louis Kirsch and Robert Tjarko Lange and Shimon Whiteson and Jakob Nicolaus Foerster},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jackson2024discovering.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=MJJcs3zbmi},
  publisher = {OpenReview.net},
  title = {Discovering Temporally-Aware Reinforcement Learning Algorithms},
  url = {https://openreview.net/forum?id=MJJcs3zbmi},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jacob2024modeling,
  abstract = {We study the problem of modeling a population of agents pursuing unknown goals subject to unknown computational constraints. In standard models of bounded rationality, sub-optimal decision-making is simulated by adding homoscedastic noise to optimal decisions rather than actually simulating constrained inference. In this work, we introduce a latent inference budget model (L-IBM) that models these constraints explicitly, via a latent variable (inferred jointly with a model of agents' goals) that controls the runtime of an iterative inference algorithm. L-IBMs make it possible to learn agent models using data from diverse populations of suboptimal actors. In three modeling tasks---inferring navigation goals from routes, inferring communicative intents from human utterances, and predicting next moves in human chess games---we show that L-IBMs match or outperforms Boltzmann models of decision-making under uncertainty. Moreover, the inferred inference budgets are themselves meaningful, efficient to compute, and correlated with measures of player skill, partner skill and task difficulty.},
  address = {Vienna, Austria},
  author = {Athul Paul Jacob and Abhishek Gupta and Jacob Andreas},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jacob2024modeling.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=W3VsHuga3j},
  publisher = {OpenReview.net},
  title = {Modeling Boundedly Rational Agents with Latent Inference Budgets},
  url = {https://openreview.net/forum?id=W3VsHuga3j},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jacob2024consensus,
  abstract = {When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate answers). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game---which we term the consensus game---in which a generator seeks to communicate an abstract correctness parameter using natural language sentences to a discriminator. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call equilibrium-ranking. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and assistive dialog), equilibrium-ranking consistently improves performance over existing LM decoding procedures. These improvements are sometimes substantial---on multiple benchmarks, we observe that applying equilibrium-ranking to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models.},
  address = {Vienna, Austria},
  author = {Athul Paul Jacob and Yikang Shen and Gabriele Farina and Jacob Andreas},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jacob2024consensus.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=n9xeGcI4Yg},
  publisher = {OpenReview.net},
  title = {The Consensus Game: Language Model Generation via Equilibrium Search},
  url = {https://openreview.net/forum?id=n9xeGcI4Yg},
  venue = {ICLR 2024 spotlight},
  year = {2024}
}

@inproceedings{jain2024neftune,
  abstract = {We show that language model finetuning can be improved, sometimes dramatically, with a simple augmentation. NEFTune adds noise to the embedding vectors during training. Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79\% on AlpacaEval, which rises to 64.69\% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10\% improvement, with ShareGPT an 8\% improvement, and with OpenPlatypus an 8\% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune. Particularly, we see these improvements on the conversational abilities of the instruction model and not on traditional tasks like those on the OpenLLM Leaderboard, where performance is the same.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Neel Jain and Ping-yeh Chiang and Yuxin Wen and John Kirchenbauer and Hong-Min Chu and Gowthami Somepalli and Brian R. Bartoldson and Bhavya Kailkhura and Avi Schwarzschild and Aniruddha Saha and Micah Goldblum and Jonas Geiping and Tom Goldstein},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.05914},
  file = {:/home/b/documents/inproceedings/jain2024neftune.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=0bMmZ3fkCk},
  publisher = {OpenReview.net},
  title = {{NEFTune}: Noisy Embeddings Improve Instruction Finetuning},
  url = {https://openreview.net/forum?id=0bMmZ3fkCk},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jain2024mechanistically,
  abstract = {Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a 'wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such 'wrapped capabilities' are relevant leads to sample-efficient revival of the capability, i.e., the model begins reusing these capabilities after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.},
  address = {Vienna, Austria},
  author = {Samyak Jain and Robert Kirk and Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka and Tim Rockt{\"a}schel and Edward Grefenstette and David Scott Krueger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jain2024mechanistically.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=A0HKeKl4Nl},
  publisher = {OpenReview.net},
  title = {Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks},
  url = {https://openreview.net/forum?id=A0HKeKl4Nl},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jain2024learning,
  abstract = {Predictive uncertainty--a model's self-awareness regarding its accuracy on an input--is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditional reweighting approach that captures predictive uncertainty using an auxiliary network, and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing dropout variance, an approximation of Bayesian predictive uncertainty. We show in controlled experiments that we effectively capture diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings--selective classification, label noise, domain adaptation, calibration--and across datasets--Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs, Imagenet-C,-A,-R, Clothing-1.6M, etc. For Diabetic Retinopathy, we see upto 3.4\%/3.3\% accuracy \& AUC gains over SOTA in selective classification. We also improve upon large-scale pretrained models such as PLEX.},
  author = {Nishant Jain and Karthikeyan Shanmugam and Pradeep Shenoy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jain2024learning.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=bDWXhzZT40},
  publisher = {OpenReview.net},
  title = {Learning Model Uncertainty as Variance-Minimizing Instance Weights},
  url = {https://openreview.net/forum?id=bDWXhzZT40},
  year = {2024}
}

@inproceedings{jain2024llm,
  abstract = {Natural language to code generation is an important application area of {LLM}s and has received wide attention from the community. The majority of relevant studies have exclusively concentrated on increasing the quantity and functional correctness of training sets while disregarding other stylistic elements of programs. More recently, data quality has garnered a lot of interest and multiple works have showcased its importance for improving performance. In this work, we investigate data quality for code and find that making the code more structured and readable leads to improved code generation performance of the system. We build a novel data-cleaning pipeline that uses these principles to transform existing programs by 1.) renaming variables, 2.) modularizing and decomposing complex code into smaller helper sub-functions, and 3.) inserting natural-language based plans via {LLM} based transformations. We evaluate our approach on two challenging algorithmic code generation benchmarks and find that fine-tuning {C}ode{LL}a{M}a-7B on our transformed modularized programs improves the performance by up to 30\% compared to fine-tuning on the original dataset. Additionally, we demonstrate improved performance from using a smaller amount of higher-quality data, finding that a model fine-tuned on the entire original dataset is outperformed by a model trained on 15\% of our cleaned dataset. Even in comparison to closed-source models, our models outperform the much larger {A}lpha{C}oder models.},
  author = {Naman Jain and Tianjun Zhang and Wei-Lin Chiang and Joseph E. Gonzalez and Koushik Sen and Ion Stoica},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jain2024llm.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=maRYffiUpI},
  publisher = {OpenReview.net},
  title = {{LLM}-Assisted Code Cleaning For Training Accurate Code Generators},
  url = {https://openreview.net/forum?id=maRYffiUpI},
  year = {2024}
}

@inproceedings{jaini2024intriguing,
  abstract = {What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99\% for {I}magen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.},
  author = {Priyank Jaini and Kevin Clark and Robert Geirhos},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jaini2024intriguing.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=rmg0qMKYRQ},
  publisher = {OpenReview.net},
  title = {Intriguing Properties of Generative Classifiers},
  url = {https://openreview.net/forum?id=rmg0qMKYRQ},
  year = {2024}
}

@inproceedings{jaiswal2024compressing,
  abstract = {Despite their remarkable achievements, modern Large Language Models ({LLM}s) face exorbitant computational and memory footprints. Recently, several works have shown significant success in training-free and data-free compression (pruning and quantization) of {LLM}s that achieve 50--60\% sparsity and reduce the bit width to 3 or 4 bits per weight, with negligible degradation of perplexity over the uncompressed baseline. As recent research efforts are focused on developing increasingly sophisticated compression methods, our work takes a step back and re-evaluates the effectiveness of existing {SOTA} compression methods, which rely on a fairly simple and widely questioned metric, perplexity (even for dense {LLM}s). We introduce Knowledge-Intensive Compressed {LLM} {B}ench{M}ar{K} ({LLM}-{KICK}), a collection of carefully curated tasks to redefine the evaluation protocol for compressed {LLM}s, which have significant alignment with their dense counterparts and perplexity fail to capture subtle change in their true capabilities. {LLM}-{KICK} unveils many favorable merits and unfortunate plights of current {SOTA} compression methods: all pruning methods suffer significant performance degradation, sometimes at trivial sparsity ratios (e.g., 25--30\%), and fail for {N:M} sparsity in knowledge-intensive tasks; current quantization methods are more successful than pruning; yet, pruned {LLM}s even at $\geq$ 50\% sparsity are robust in-context retrieval and summarization systems; among others. {LLM}-{KICK} is designed to holistically access compressed {LLM}s' ability for language understanding, reasoning, generation, in-context retrieval, in-context summarization, etc. We hope our study can foster the development of better {LLM} compression methods.},
  author = {Ajay Kumar Jaiswal and Zhe Gan and Xianzhi Du and Bowen Zhang and Zhangyang Wang and Yinfei Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jaiswal2024compressing.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=B9klVS7Ddk},
  publisher = {OpenReview.net},
  title = {Compressing {LLM}s: The Truth is Rarely Pure and Never Simple},
  url = {https://openreview.net/forum?id=B9klVS7Ddk},
  year = {2024}
}

@inproceedings{jamasb2024evaluating,
  abstract = {We introduce {P}rotein{W}orkshop, a comprehensive benchmark suite for representation learning on protein structures with Geometric Graph Neural Networks. We consider large-scale pre-training and downstream tasks on both experimental and predicted structures to enable the systematic evaluation of the quality of the learned structural representation and their usefulness in capturing functional relationships for downstream tasks. We find that: (1) large-scale pretraining on {A}lpha{F}old structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant {GNN}s, and (2) more expressive equivariant {GNN}s benefit from pretraining to a greater extent compared to invariant models. We aim to establish a common ground for the machine learning and computational biology communities to rigorously compare and advance protein structure representation learning. Our open-source codebase reduces the barrier to entry for working with large protein structure datasets by providing: (1) storage-efficient dataloaders for large-scale structural databases including {A}lpha{F}old{DB} and {ESM} {A}tlas, as well as (2) utilities for constructing new tasks from the entire {PDB}. {P}rotein{W}orkshop is available at: github.com/a-r-j/{P}rotein{W}orkshop.},
  author = {Arian Rokkum Jamasb and Alex Morehead and Chaitanya K. Joshi and Zuobai Zhang and Kieran Didi and Simon V. Mathis and Charles Harris and Jian Tang and Jianlin Cheng and Pietro Li{\`o} and Tom L. Blundell},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jamasb2024evaluating.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=sTYuRVrdK3},
  publisher = {OpenReview.net},
  title = {Evaluating Representation Learning on the Protein Structure Universe},
  url = {https://openreview.net/forum?id=sTYuRVrdK3},
  year = {2024}
}

@inproceedings{jang2024learning,
  abstract = {This paper studies generative flow networks ({GF}low{N}ets) to sample objects from the {B}oltzmann energy distribution via a sequence of actions. In particular, we focus on improving {GF}low{N}et with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To this end, the recently developed forward-looking {GF}low{N}et reparameterizes the flow functions based on evaluating the energy of intermediate states. However, such an evaluation of intermediate energies may (i) be too expensive or impossible to evaluate and (ii) even provide misleading training signals under large energy fluctuations along the sequence of actions. To resolve this issue, we propose learning energy decompositions for {GF}low{N}ets ({LED}-{GFN}). Our main idea is to (i) decompose the energy of an object into learnable potential functions defined on state transitions and (ii) reparameterize the flow functions using the potential functions. In particular, to produce informative local credits, we propose to regularize the potential to change smoothly over the sequence of actions. It is also noteworthy that training {GF}low{N}et with our learned potential can preserve the optimal policy. We empirically verify the superiority of {LED}-{GFN} in five problems including the generation of unstructured and maximum independent sets, molecular graphs, and {RNA} sequences.},
  author = {Hyosoon Jang and Minsu Kim and Sungsoo Ahn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jang2024learning.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=P15CHILQlg},
  publisher = {OpenReview.net},
  title = {Learning Energy Decompositions for Partial Inference in {GF}low{N}ets},
  url = {https://openreview.net/forum?id=P15CHILQlg},
  year = {2024}
}

@inproceedings{jang2024graph,
  abstract = {Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation, originally designed for lossless graph compression. The $K^2$-tree representation encompasses inherent hierarchy while enabling compact graph generation. In addition, we make contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.},
  author = {Yunhui Jang and Dongwoo Kim and Sungsoo Ahn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jang2024graph.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=RIEW6M9YoV},
  publisher = {OpenReview.net},
  title = {Graph Generation with $K^2$-trees},
  url = {https://openreview.net/forum?id=RIEW6M9YoV},
  year = {2024}
}

@inproceedings{jang2024simple,
  abstract = {Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list ({GEEL}) that has a small representation size that aligns with the number of edges. In addition, {GEEL} significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. {GEEL} can be autoregressively generated with the incorporation of node positional encoding, and we further extend {GEEL} to deal with attributed graphs by designing a new grammar.},
  author = {Yunhui Jang and Seul Lee and Sungsoo Ahn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jang2024simple.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=nO344avRib},
  publisher = {OpenReview.net},
  title = {A Simple and Scalable Representation for Graph Generation},
  url = {https://openreview.net/forum?id=nO344avRib},
  year = {2024}
}

@inproceedings{janny2024space,
  abstract = {Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on computational fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial condition. Our practical implementation involves recurrent {GNN}s and a spatio-temporal attention observer capable of interpolating the solution at arbitrary locations. Our model not only generalizes to new initial conditions (as standard auto-regressive models do) but also performs evaluation at arbitrary space and time locations. We evaluate on three standard datasets in fluid dynamics and compare to strong baselines, which are outperformed in classical settings and the extended new task requiring continuous predictions.},
  author = {Steeven Janny and Madiha Nadri and Julie Digne and Christian Wolf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/janny2024space.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=4yaFQ7181M},
  publisher = {OpenReview.net},
  title = {Space and Time Continuous Physics Simulation from Partial Observations},
  url = {https://openreview.net/forum?id=4yaFQ7181M},
  year = {2024}
}

@inproceedings{javanmard2024learning,
  abstract = {Due to the rise of privacy concerns, in many practical applications the training data is aggregated before being shared with the learner, in order to protect privacy of users' sensitive responses. In an aggregate learning framework, the dataset is grouped into bags of samples, where each bag is available only with an aggregate response, providing a summary of individuals' responses in that bag. In this paper, we study two natural loss functions for learning from aggregate responses: bag-level loss and the instance-level loss. In the former, the model is learnt by minimizing a loss between aggregate responses and aggregate model predictions, while in the latter the model aims to fit individual predictions to the aggregate responses. In this work, we show that the instance-level loss can be perceived as a regularized form of the bag-level loss. This observation lets us compare the two approaches with respect to bias and variance of the resulting estimators, and introduce a novel interpolating estimator which combines the two approaches. For linear regression tasks, we provide a precise characterization of the risk of the interpolating estimator in an asymptotic regime where the size of the training set grows in proportion to the features dimension. Our analysis allows us to theoretically understand the effect of different factors, such as bag size on the model prediction risk. In addition, we propose a mechanism for differentially private learning from aggregate responses and derive the optimal bag size in terms of prediction risk-privacy trade-off. We also carry out thorough experiments to corroborate our theory and show the efficacy of the interpolating estimator.},
  author = {Adel Javanmard and Lin Chen and Vahab Mirrokni and Ashwinkumar Badanidiyuru and Gang Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/javanmard2024learning.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=E60SIDItyT},
  publisher = {OpenReview.net},
  title = {Learning from Aggregate Responses: Instance Level versus Bag Level Loss Functions},
  url = {https://openreview.net/forum?id=E60SIDItyT},
  year = {2024}
}

@inproceedings{jeleni2024outofdistribution,
  abstract = {Effective out-of-distribution ({OOD}) detection is crucial for reliable machine learning models, yet most current methods are limited in practical use due to requirements like access to training data or intervention in training. We present a novel method for detecting {OOD} data in {Transformers} based on transformation smoothness between intermediate layers of a network ({BLOOD}), which is applicable to pre-trained models without access to training data. {BLOOD} utilizes the tendency of between-layer representation transformations of in-distribution ({ID}) data to be smoother than the corresponding transformations of {OOD} data, a property that we also demonstrate empirically.},
  address = {Vienna, Austria},
  arxiv = {2310.02832},
  author = {Fran Jeleniƒá and Josip Jukiƒá and Martin Tutek and Mate Puljiz and Jan ≈†najder},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jeleni2024outofdistribution.pdf:pdf},
  keywords = {out-of-distribution detection, transformers, representation learning},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AcRfzLS6se},
  publisher = {OpenReview.net},
  title = {{Out-of-Distribution} Detection by Leveraging {Between-Layer} Transformation Smoothness},
  url = {https://openreview.net/forum?id=AcRfzLS6se},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{jenssen2024map,
  abstract = {{MAP IT} visualizes representations by taking a fundamentally different approach to dimensionality reduction. {MAP IT} aligns distributions over discrete marginal probabilities in the input space versus the target space, thus capturing information in local regions, as opposed to current methods which align based on individual probabilities between pairs of data points (states) only. The {MAP IT} theory reveals that alignment based on a projective divergence avoids normalization of weights (to obtain true probabilities) entirely, and further reveals a dual viewpoint via continuous densities and kernel smoothing. {MAP IT} is shown to produce visualizations which capture class structure better than the current state of the art while being inherently scalable.},
  address = {Vienna, Austria},
  affiliation = {University of Troms√∏, Norway},
  author = {Robert Jenssen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jenssen2024map.pdf:pdf},
  keywords = {dimensionality reduction, visualization, representation learning, projective divergence},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OKf6JtXtoy},
  publisher = {OpenReview.net},
  title = {{MAP IT} to Visualize Representations},
  url = {https://openreview.net/forum?id=OKf6JtXtoy},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{jeon2024pacfno,
  abstract = {A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. We propose {PAC-FNO} (Parallel-Structured and All-Component {Fourier} Neural Operator), a novel neural network model that operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage training algorithm that requires minimal modification to the original downstream model. When extensively evaluated on seven image recognition benchmarks, the proposed {PAC-FNO} improves the performance of existing baseline models on images with various resolutions by up to 77.1\% and various types of natural variations in the images at inference.},
  address = {Vienna, Austria},
  arxiv = {2402.12721},
  author = {Jinsung Jeon and Hyundong Jin and Jonghyun Choi and Sanghyun Hong and Dongeun Lee and Kookjin Lee and Noseong Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jeon2024pacfno.pdf:pdf},
  keywords = {fourier neural operators, image recognition, multi-resolution, frequency domain},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Cf4FJGmHRQ},
  publisher = {OpenReview.net},
  title = {{PAC-FNO}: {Parallel-Structured} {All-Component} {Fourier} Neural Operators for Recognizing {Low-Quality} Images},
  url = {https://openreview.net/forum?id=Cf4FJGmHRQ},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{jeong2024igraphmix,
  abstract = {Input Mixup is a promising method to alleviate the over-fitting problem across various domains including image classification and natural language processing because of its ability to generate a variety of virtual samples, and ease of usability and versatility. However, designing Input Mixup for node classification is challenging due to the irregularity issue that each node contains a different number of neighboring nodes for input and the alignment issue that how to align and interpolate two sets of neighboring nodes is not well-defined when two nodes are interpolated. We propose {iGraphMix}, a novel mixup method that generates virtual nodes and their edges by interpolating input features and labels, and attaching sampled neighboring nodes. {iGraphMix} focuses on mixing the features and connections of labeled nodes on the input graph to improve the model's classification ability and robustness.},
  address = {Vienna, Austria},
  author = {Jongwon Jeong and Hoyeop Lee and Hyui Geon Yoon and Beomyoung Lee and Junhee Heo and Geonsoo Kim and Kim Jin Seon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jeong2024igraphmix.pdf:pdf},
  keywords = {graph neural networks, mixup, node classification, semi-supervised learning},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=a2ljjXeDcE},
  publisher = {OpenReview.net},
  title = {{iGraphMix}: Input Graph Mixup Method for Node Classification},
  url = {https://openreview.net/forum?id=a2ljjXeDcE},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{jeong2024groundavideo,
  abstract = {Recent endeavors in video editing have showcased promising results in single-attribute editing or style transfer tasks, either by training text-to-video ({T2V}) models on text-video data or adopting training-free methods. However, when confronted with the complexities of multi-attribute editing scenarios, they exhibit shortcomings such as omitting or overlooking intended attribute changes, modifying the wrong elements of the input video, and failing to preserve regions of the input video that should remain intact. We introduce {Ground-A-Video}, a novel grounding-guided video-to-video translation framework for multi-attribute video editing. Central to the method is the introduction of cross-frame gated attention which incorporates groundings information into the latent representations in a temporally consistent fashion, along with Modulated Cross-Attention and optical flow guided inverted latents smoothing. Extensive experiments and applications demonstrate that {Ground-A-Video}'s zero-shot capacity outperforms other baseline methods in terms of edit-accuracy and frame consistency.},
  address = {Vienna, Austria},
  affiliation = {{KAIST} Graduate School of {AI}, South Korea},
  arxiv = {2310.01107},
  author = {Hyeonho Jeong and Jong Chul Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jeong2024groundavideo.pdf:pdf},
  keywords = {video editing, diffusion models, zero-shot learning, multi-attribute editing},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=28L2FCtMWq},
  publisher = {OpenReview.net},
  title = {{Ground-A-Video}: {Zero-shot} Grounded Video Editing using {Text-to-image} Diffusion Models},
  url = {https://openreview.net/forum?id=28L2FCtMWq},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{ji2024large,
  abstract = {With the advancements in Large Language Models ({LLMs}), Vision-Language Models ({VLMs}) have reached a new level of sophistication, showing notable competence in executing intricate cognition and reasoning tasks. However, existing evaluation benchmarks primarily rely on rigid, hand-crafted datasets to measure task-specific performance, facing significant limitations in assessing the alignment of increasingly anthropomorphic {VLMs} with human intelligence. We introduce {Auto-Bench}, a framework that uses {LLMs} as automated aligners to evaluate {VLMs}. Through this pipeline, a total of 28.5K human-verified and 3,504K unfiltered question-answer-reasoning triplets have been curated, covering 4 primary abilities and 16 sub-abilities. Validation results reveal that {LLMs} are proficient in both evaluation data curation and model assessment, achieving an average agreement rate of 85\%.},
  address = {Vienna, Austria},
  arxiv = {2311.14580},
  author = {Yuanfeng Ji and Chongjian Ge and Weikai Kong and Enze Xie and Zhengying Liu and Zhenguo Li and Ping Luo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ji2024large.pdf:pdf},
  keywords = {vision-language models, evaluation benchmark, large language models, automated assessment},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kZEXgtMNNo},
  publisher = {OpenReview.net},
  title = {Large Language Models as Automated Aligners for Benchmarking {Vision-Language} Models},
  url = {https://openreview.net/forum?id=kZEXgtMNNo},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{ji2024language,
  abstract = {Despite the remarkable advances in language modeling, current mainstream decoding methods still struggle to generate texts that align with human texts across different aspects. In particular, sampling-based methods produce less-repetitive texts which are often disjunctive in discourse, while search-based methods maintain topic coherence at the cost of increased repetition. Overall, these methods fall short in achieving holistic alignment across a broad range of aspects. We frame decoding from a language model as an optimization problem with the goal of strictly matching the expected performance with human texts measured by multiple metrics of desired aspects simultaneously. We introduce a novel decoding framework called {Daemon} that treats language model decoding as an optimization problem, aiming to strictly match the expected performance of generations with human texts measured by metrics of desired aspects simultaneously.},
  address = {Vienna, Austria},
  affiliation = {Tsinghua University, Beijing, China},
  arxiv = {2310.01041},
  author = {Haozhe Ji and Pei Ke and Hongning Wang and Minlie Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ji2024language.pdf:pdf},
  keywords = {language model decoding, text generation, optimization, coherence, fluency},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=488A64eOf6},
  publisher = {OpenReview.net},
  title = {Language Model Decoding as Direct Metrics Optimization},
  url = {https://openreview.net/forum?id=488A64eOf6},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{ji2024horizonfree,
  abstract = {Recent studies have shown that episodic reinforcement learning ({RL}) is no harder than bandits when the total reward is bounded by $1$, and proved regret bounds that have a polylogarithmic dependence on the planning horizon $H$. However, it remains an open question that if such results can be carried over to adversarial {RL}, where the reward is adversarially chosen at each episode. In this paper, we answer this question affirmatively by proposing the first horizon-free policy search algorithm called {HF-O¬≤PS} (Horizon-Free Occupancy-Measure Guided Optimistic Policy Search). To tackle the challenges caused by exploration and adversarially chosen reward, our algorithm employs (1) a variance-uncertainty-aware weighted least square estimator for the transition kernel; and (2) an occupancy measure-based technique for the online search of a stochastic policy. We show that our algorithm achieves an $\tilde{O}((d+\log(|\mathcal{S}|^2 |\mathcal{A}|))\sqrt{K})$ regret with full-information feedback.},
  address = {Vienna, Austria},
  arxiv = {2305.08359},
  author = {Kaixuan Ji and Qingyue Zhao and Jiafan He and Weitong Zhang and Quanquan Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ji2024horizonfree.pdf:pdf},
  keywords = {reinforcement learning, adversarial learning, horizon-free, policy search, regret bounds},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=aPNwsJgnZJ},
  publisher = {OpenReview.net},
  title = {Horizon-free Reinforcement Learning in Adversarial Linear Mixture {MDPs}},
  url = {https://openreview.net/forum?id=aPNwsJgnZJ},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{jia2024towards,
  abstract = {Imitation learning with human data has demonstrated remarkable success in teaching robots in a wide range of skills. However, the inherent diversity in human behavior leads to the emergence of multi-modal data distributions, thereby presenting a formidable challenge for existing imitation learning algorithms. Quantifying a model's capacity to capture and replicate this diversity effectively is still an open problem. We introduce {D3IL}, a benchmark that explicitly evaluates a model's ability to learn multi-modal behavior. Our benchmark comprises simulation environments and the corresponding Datasets with Diverse human Demonstrations for Imitation Learning ({D3IL}), specifically designed to assess a model's ability to learn diverse behaviors. The {D3IL} Benchmark comprises 7 robot learning tasks: Avoiding, Pushing, Aligning, Sorting, Stacking, Inserting, and Arranging, all implemented using Mujoco and Gym. We provide tractable metrics that provide valuable insights into a model's ability to acquire and reproduce diverse behaviors.},
  address = {Vienna, Austria},
  affiliation = {Karlsruhe Institute of Technology, Germany},
  arxiv = {2402.14606},
  author = {Xiaogang Jia and Denis Blessing and Xinkai Jiang and Moritz Reuss and Atalay Donat and Rudolf Lioutikov and Gerhard Neumann},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jia2024towards.pdf:pdf},
  keywords = {imitation learning, robotics, multi-modal behavior, benchmark, human demonstrations},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6pPYRXKPpw},
  projectpage = {https://alrhub.github.io/d3il-website/},
  publisher = {OpenReview.net},
  title = {Towards Diverse Behaviors: A Benchmark for Imitation Learning with Human Demonstrations},
  url = {https://openreview.net/forum?id=6pPYRXKPpw},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{jia2024realistic,
  abstract = {Semi-supervised learning ({SSL}) is a powerful paradigm for leveraging unlabeled data and has been proven to be successful across various tasks. Conventional {SSL} studies typically assume close environment scenarios where labeled and unlabeled examples are independently sampled from the same distribution. However, real-world tasks often involve open environment scenarios where the data distribution, label space, and feature space could differ between labeled and unlabeled data. We propose several robustness metrics for {SSL} based on the Robustness Analysis Curve ({RAC}), establish a theoretical framework for studying the generalization performance and robustness of {SSL} algorithms in open environments, and re-implement widely adopted {SSL} algorithms within a unified {SSL} toolkit called {LAMDA-SSL}.},
  address = {Vienna, Austria},
  affiliation = {Nanjing University, China},
  author = {Lin-Han Jia and Lan-Zhe Guo and Zhi Zhou and Yu-Feng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jia2024realistic.pdf:pdf},
  keywords = {semi-supervised learning, open environments, robustness analysis, evaluation benchmark},
  month = {5},
  note = {{DBLP} last modified: 2024-11-25},
  pdf = {https://openreview.net/pdf?id=RvUVMjfp8i},
  publisher = {OpenReview.net},
  title = {Realistic Evaluation of {Semi-supervised} Learning Algorithms in Open Environments},
  url = {https://openreview.net/forum?id=RvUVMjfp8i},
  venue = {{ICLR} 2024},
  year = {2024}
}

@inproceedings{jia2024policy,
  abstract = {Human beings can make adaptive decisions in a preparatory manner, i.e., by making preparations in advance, which offers significant advantages in scenarios where both online and offline experiences are expensive and limited. Meanwhile, current reinforcement learning methods commonly rely on numerous environment interactions but hardly obtain generalizable policies. In this paper, we introduce the idea of rehearsal into policy optimization, where the agent plans for all possible outcomes in mind and acts adaptively according to actual responses from the environment. To effectively rehearse, we propose {ReDM}, an algorithm that generates a diverse and eligible set of dynamics models and then rehearse the policy via adaptive training on the generated model set. Rehearsal enables the policy to make decision plans for various hypothetical dynamics and to naturally generalize to previously unseen environments. Our experimental results demonstrate that {ReDM} is capable of learning a valid policy solely through rehearsal, even with zero interaction data. We further extend {ReDM} to scenarios where limited or mismatched interaction data is available, and our experimental results reveal that {ReDM} produces high-performing policies compared to other offline {RL} baselines.},
  author = {Chengxing Jia and Chenxiao Gao and Hao Yin and Fuxiang Zhang and Xiong-Hui Chen and Tian Xu and Lei Yuan and Zongzhang Zhang and Zhi-Hua Zhou and Yang Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jia2024policy.pdf:pdf},
  keywords = {Reinforcement Learning, Model-based Reinforcement Learning, Offline Reinforcement Learning},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=m3xVPaZp6Z},
  publisher = {OpenReview.net},
  title = {Policy Rehearsing: Training Generalizable Policies for Reinforcement Learning},
  url = {https://openreview.net/forum?id=m3xVPaZp6Z},
  year = {2024}
}

@inproceedings{jia2024aligning,
  abstract = {Relational learning has gained significant attention, led by the expressiveness of Graph Neural Networks ({GNN}s) on graph data. While the inherent biases in common graph data are involved in {GNN} training, it poses a serious challenge to constraining the {GNN} output perturbations induced by input biases, thereby safeguarding fairness during training. The {Lipschitz} bound, a technique from robust statistics, can limit the maximum changes in the output concerning the input, taking into account associated irrelevant biased factors. This work formulates a {Lipschitz} bound to limit the changes in the output regarding biases associated with the input and theoretically analyzes how the {Lipschitz} bound of a {GNN} model could constrain the output perturbations induced by biases learned from data for fairness training. We experimentally validate the {Lipschitz} bound's effectiveness in limiting biases of the model output and demonstrate from a training dynamics perspective why the theoretical {Lipschitz} bound can effectively guide {GNN} training to better trade-off between accuracy and fairness.},
  author = {Yaning Jia and Chunhui Zhang and Soroush Vosoughi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jia2024aligning.pdf:pdf},
  keywords = {Graph Neural Networks, Fairness, Lipschitz Bounds},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=ODSgo2m8aE},
  publisher = {OpenReview.net},
  title = {Aligning Relational Learning with {Lipschitz} Fairness},
  url = {https://openreview.net/forum?id=ODSgo2m8aE},
  year = {2024}
}

@inproceedings{jiang2024negative,
  abstract = {Out-of-distribution ({OOD}) detection aims at identifying samples from unknown classes, playing a crucial role in trustworthy models against errors on unexpected inputs. Vision-language models ({VLM}s) can leverage both textual and visual information for various multi-modal applications, whereas few {OOD} detection methods take into account information from the text modality. This paper proposes a novel post hoc {OOD} detection method called {NegLabel} that takes a vast number of negative labels from extensive corpus databases. We design a novel scheme for the {OOD} score collaborated with negative labels and provide theoretical analysis to understand the mechanism of negative labels. Extensive experiments demonstrate that {NegLabel} achieves state-of-the-art performance on various {OOD} detection benchmarks and demonstrates remarkable robustness against diverse domain shifts, generalizing well on multiple {VLM} architectures.},
  author = {Xue Jiang and Feng Liu and Zhen Fang and Hong Chen and Tongliang Liu and Feng Zheng and Bo Han},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024negative.pdf:pdf},
  keywords = {Out-of-Distribution Detection, Vision-Language Models, Multimodal Learning},
  note = {ICLR 2024 spotlight},
  pdf = {https://openreview.net/pdf?id=xUO1HXz4an},
  publisher = {OpenReview.net},
  title = {Negative Label Guided {OOD} Detection with Pretrained Vision-Language Models},
  url = {https://openreview.net/forum?id=xUO1HXz4an},
  year = {2024}
}

@inproceedings{jiang2024consistent4d,
  abstract = {In this paper, we present Consistent4{D}, a novel approach for generating 4{D} dynamic objects from uncalibrated monocular videos. Uniquely, we cast the 360-degree dynamic object reconstruction as a 4{D} generation problem, eliminating the need for tedious multi-view data collection and camera calibration. This is achieved by leveraging the object-level 3{D}-aware image diffusion model as the primary supervision signal for training Dynamic Neural Radiance Fields ({DyNeRF}). Specifically, we propose a Cascade {DyNeRF} to facilitate stable convergence and temporal continuity under the supervision signal which is discrete along the time axis. To achieve spatial and temporal consistency of the 4{D} generation, an interpolation-driven consistency loss is further introduced, which aligns the rendered frames with the interpolated frames from a pre-trained video interpolation model. Extensive experiments show that our approach significantly outperforms previous 4{D} reconstruction approaches as well as per-frame 3{D} generation approaches.},
  author = {Yanqin Jiang and Li Zhang and Jin Gao and Weiming Hu and Yao Yao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024consistent4d.pdf:pdf},
  keywords = {Dynamic Object Generation, 4D Reconstruction, Generative Models},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=sPUrdFGepF},
  publisher = {OpenReview.net},
  title = {Consistent4{D}: Consistent 360¬∞ Dynamic Object Generation from Monocular Video},
  url = {https://openreview.net/forum?id=sPUrdFGepF},
  year = {2024}
}

@inproceedings{jiang2024leap,
  abstract = {Are camera poses necessary for multi-view 3{D} modeling? Existing approaches predominantly assume access to accurate camera poses. While this assumption might hold for dense views, accurately estimating camera poses for sparse views is often elusive. This paper presents {LEAP}, a novel pose-free approach that discards pose-based operations and learns geometric knowledge from data. {LEAP} is equipped with a neural volume, which is shared across scenes and parameterized to encode geometry and texture priors. For each incoming scene, we update the neural volume by aggregating 2{D} image features in a feature-similarity-driven manner. The updated neural volume is decoded into the radiance field, enabling novel view synthesis from any viewpoint. The method enables {NeRF} from sparse (2--5) views without camera poses, runs in a second, and generalizes to novel instances.},
  author = {Hanwen Jiang and Zhenyu Jiang and Yue Zhao and Qixing Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024leap.pdf:pdf},
  keywords = {3D Reconstruction, Neural Radiance Fields, Computer Vision},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=KPmajBxEaF},
  publisher = {OpenReview.net},
  title = {{LEAP}: Liberate Sparse-View 3{D} Modeling from Camera Poses},
  url = {https://openreview.net/forum?id=KPmajBxEaF},
  year = {2024}
}

@inproceedings{jiang2024joint,
  abstract = {Learning features from data is one of the defining characteristics of deep learning, but our theoretical understanding of the role features play in deep learning is still rudimentary. To address this gap, we introduce a new tool, the interaction tensor, for empirically analyzing the interaction between data and model through features. With the interaction tensor, we make several key observations about how features are distributed in data and how models with different random seeds learn different features. Based on these observations, we propose a conceptual framework for feature learning. Under this framework, the expected accuracy for a single hypothesis and agreement for a pair of hypotheses can both be derived in closed-form.},
  author = {Yiding Jiang and Christina Baek and J. Zico Kolter},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024joint.pdf:pdf},
  keywords = {Feature Learning, Deep Learning Theory, Machine Learning Theory},
  note = {ICLR 2024 oral},
  pdf = {https://openreview.net/pdf?id=ze7DOLi394},
  publisher = {OpenReview.net},
  title = {On the Joint Interaction of Models, Data, and Features},
  url = {https://openreview.net/forum?id=ze7DOLi394},
  year = {2024}
}

@inproceedings{jiang2024dos,
  abstract = {Modern neural networks are known to give overconfident predictions for out-of-distribution inputs when deployed in the open world. It is common practice to leverage a surrogate outlier dataset to regularize the model during training, and recent studies emphasize the role of uncertainty in designing the sampling strategy for outlier datasets. However, the {OOD} samples selected solely based on predictive uncertainty can be biased towards certain types, which may fail to capture the full outlier distribution. In this work, we empirically show that diversity is critical in sampling outliers for {OOD} detection performance. We propose a straightforward and novel sampling strategy named {DOS} (Diverse Outlier Sampling) to select diverse and informative outliers. Our method works by clustering the normalized features at each iteration, and the most informative outlier from each cluster is selected for model training with absent category loss. Extensive experiments demonstrate the superiority of {DOS}, reducing the average {FPR95} by up to 25.79\% on {CIFAR}-100 with {TI}-300{K}.},
  author = {Wenyu Jiang and Hao Cheng and Mingcai Chen and Chongjun Wang and Hongxin Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024dos.pdf:pdf},
  keywords = {Out-of-Distribution Detection, Sampling Strategies, Machine Learning Safety},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=iriEqxFB4y},
  publisher = {OpenReview.net},
  title = {{DOS}: Diverse Outlier Sampling for Out-of-Distribution Detection},
  url = {https://openreview.net/forum?id=iriEqxFB4y},
  year = {2024}
}

@inproceedings{jiang2024blackbox,
  abstract = {We investigate learning the equilibria in non-stationary multi-agent systems and address the challenges that differentiate multi-agent learning from single-agent learning. Specifically, we focus on games with bandit feedback, where testing an equilibrium can result in substantial regret even when the gap to be tested is small, and the existence of multiple optimal solutions (equilibria) in stationary games poses extra challenges. We propose a versatile black-box approach applicable to a broad spectrum of problems, such as general-sum games, potential games, and Markov games, when equipped with appropriate learning and testing oracles for stationary environments. Our algorithms can achieve $\tilde{O}(\Delta^{1/4}T^{3/4})$ regret when the degree of nonstationarity, as measured by total variation $\Delta$, is known, and $\tilde{O}(\Delta^{1/5}T^{4/5})$ regret when $\Delta$ is unknown, where $T$ is the number of rounds.},
  author = {Haozhe Jiang and Qiwen Cui and Zhihan Xiong and Maryam Fazel and Simon Shaolei Du},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024blackbox.pdf:pdf},
  keywords = {Multi-agent Reinforcement Learning, Game Theory, Non-stationary Environments},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=LWuYsSD94h},
  publisher = {OpenReview.net},
  title = {A Black-box Approach for Non-stationary Multi-agent Reinforcement Learning},
  url = {https://openreview.net/forum?id=LWuYsSD94h},
  year = {2024}
}

@inproceedings{jiang2024circuitnet,
  abstract = {Integrated circuits or chips are key to enable computing in modern industry. Designing a chip relies on human experts to produce chip data through professional electronic design automation ({EDA}) software and complicated procedures. In order to approach the realistic chip design space, this work collects more than 10,000 samples with a variety of chip designs (e.g., {CPU}, {GPU}, and {AI} Chip). All the designs are conducted through complete commercial design flows in a widely-used technology node, 14nm {FinFET}. The comprehensive data includes routability, timing, and power information from the design flow to support versatile machine learning tasks in {EDA}.},
  author = {Xun Jiang and Zhuomin Chai and Yuxiang Zhao and Yibo Lin and Runsheng Wang and Ru Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024circuitnet.pdf:pdf},
  keywords = {Electronic Design Automation, Machine Learning Datasets, Chip Design},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=nMFSUjxMIl},
  publisher = {OpenReview.net},
  title = {{CircuitNet} 2.0: An Advanced Dataset for Promoting Machine Learning Innovations in Realistic Chip Design Environment},
  url = {https://openreview.net/forum?id=nMFSUjxMIl},
  year = {2024}
}

@inproceedings{jiang2024spatiotemporal,
  abstract = {Spiking neural networks ({SNN}s) are energy-efficient and hold great potential for large-scale inference. Since training {SNN}s from scratch is costly and has limited performance, converting pretrained artificial neural networks ({ANN}s) to {SNN}s is an attractive approach that retains robust performance without additional training data and resources. However, while existing conversion methods work well on convolution networks, emerging Transformer models introduce unique mechanisms like self-attention and test-time normalization, leading to non-causal non-linear interactions unachievable by current {SNN}s. To address this, we approximate these operations in both temporal and spatial dimensions, thereby providing the first {SNN} conversion pipeline for Transformers. We propose Universal Group Operators to approximate non-linear operations spatially and a Temporal-Corrective Self-Attention Layer that approximates spike multiplications at inference through an estimation-correction approach. Our algorithm is implemented on a pretrained {ViT}-{B}/32 from {CLIP}, inheriting its zero-shot classification capabilities, while improving control over conversion losses. To our knowledge, this is the first direct training-free conversion of a pretrained Transformer to a purely event-driven {SNN}.},
  author = {Yizhou Jiang and Kunlin Hu and Tianren Zhang and Haichuan Gao and Yuqian Liu and Ying Fang and Feng Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024spatiotemporal.pdf:pdf},
  keywords = {Spiking Neural Networks, Transformers, Neural Network Conversion},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=XrunSYwoLr},
  publisher = {OpenReview.net},
  title = {Spatio-Temporal Approximation: A Training-Free {SNN} Conversion for Transformers},
  url = {https://openreview.net/forum?id=XrunSYwoLr},
  year = {2024}
}

@inproceedings{jiang2024heterogeneous,
  abstract = {Personalized federated learning (PFL) has emerged as a promising technique for addressing the challenge of data heterogeneity. While recent studies have made notable progress in mitigating heterogeneity associated with label distributions, the issue of effectively handling feature heterogeneity remains an open question. In this paper, we propose a personalization approach by Local-global updates Mixing (LG-Mix) via Neural Tangent Kernel (NTK)-based convergence. The core idea is to leverage the convergence rate induced by NTK to quantify the importance of local and global updates, and subsequently mix these updates based on their importance. Specifically, we find the trace of the NTK matrix can manifest the convergence rate, and propose an efficient and effective approximation to calculate the trace of a feature matrix instead of the NTK matrix. Such approximation significantly reduces the cost of computing NTK, and the feature matrix explicitly considers the heterogeneous features among samples. We have theoretically analyzed the convergence of our method in the over-parameterize regime, and experimentally evaluated our method on five datasets. These datasets present heterogeneous data features in natural and medical images. With comprehensive comparison to existing state-of-the-art approaches, our LG-Mix has consistently outperformed them across all datasets (largest accuracy improvement of 5.01\%), demonstrating the outstanding efficacy of the proposed method for model personalization.},
  address = {Vienna, Austria},
  author = {Meirui Jiang and Anjie Le and Xiaoxiao Li and Qi Dou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024heterogeneous.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=7pWRLDBAtc},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Heterogeneous Personalized Federated Learning by Local-Global Updates Mixing via Convergence Rate},
  url = {https://openreview.net/forum?id=7pWRLDBAtc},
  year = {2024}
}

@inproceedings{jiang2024graphcare,
  abstract = {Clinical predictive models often rely on patients' electronic health records (EHR), but integrating medical knowledge to enhance predictions and decision-making is challenging. This is because personalized predictions require personalized knowledge graphs (KGs), which are difficult to generate from patient EHR data. To address this, we propose GraphCare, an open-world framework that uses external KGs to improve EHR-based predictions. Our method extracts knowledge from large language models (LLMs) and external biomedical KGs to build patient-specific KGs, which are then used to train our proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare predictions. On two public datasets, MIMIC-III and MIMIC-IV, GraphCare surpasses baselines in four vital healthcare prediction tasks: mortality, readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by 7.9\% and 10.8\% for LOS and drug recommendation, respectively. Notably, GraphCare demonstrates a substantial edge in scenarios with limited data availability. Our findings highlight the potential of using external KGs in healthcare prediction tasks and demonstrate the promise of GraphCare in generating personalized KGs for promoting personalized medicine.},
  address = {Vienna, Austria},
  author = {Pengcheng Jiang and Cao Xiao and Adam Cross and Jimeng Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024graphcare.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=tVTN7Zs0ml},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs},
  url = {https://openreview.net/forum?id=tVTN7Zs0ml},
  year = {2024}
}

@inproceedings{jiang2024adaptive,
  abstract = {The training method of Spiking Neural Networks ({SNNs}) is an essential problem, and how to integrate local and global learning is a worthy research interest. However, the current integration methods do not consider the network conditions suitable for local and global learning, and thus fail to balance their advantages. In this paper, we propose an Excitation-Inhibition Mechanism-assisted Hybrid Learning ({EIHL}) algorithm that adjusts the network connectivity by using the excitation-inhibition mechanism and then switches between local and global learning according to the network connectivity. The experimental results on {CIFAR-10/100} and {DVS-CIFAR-10} demonstrate that the {EIHL} not only has better accuracy performance than other methods but also has excellent sparsity advantage. Especially, the Spiking {VGG-11} is trained by {EIHL}, {STBP}, and {STDP} on {DVS-CIFAR-10}, respectively. The accuracy of the Spiking {VGG-11} model on {EIHL} is 62.45\%, which is 4.35\% higher than {STBP} and 11.40\% higher than {STDP}, and the sparsity is 18.74\%, which is 18.74\% higher than the other two methods. Moreover, the excitation-inhibition mechanism used in our method also offers a new perspective on the field of {SNN} learning.},
  address = {Vienna, Austria},
  author = {Tingting Jiang and Qi Xu and Xuming Ran and Jiangrong Shen and Pan Lv and Qiang Zhang and Gang Pan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024adaptive.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=wpnlc2ONu0},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Adaptive deep spiking neural network with global-local learning via balanced excitatory and inhibitory mechanism},
  url = {https://openreview.net/forum?id=wpnlc2ONu0},
  year = {2024}
}

@inproceedings{jiang2024hgap,
  abstract = {Humanoid control is an important research challenge offering avenues for integration into human-centric infrastructures and enabling physics-driven humanoid animations. The daunting challenges in this field stem from the difficulty of optimizing in high-dimensional action spaces and the instability introduced by the bipedal morphology of humanoids. However, the extensive collection of human motion-captured data and the derived datasets of humanoid trajectories, such as {MoCapAct}, paves the way to tackle these challenges. In this context, we present Humanoid Generalist Autoencoding Planner ({H-GAP}), a state-action trajectory generative model trained on humanoid trajectories derived from human motion-captured data, capable of adeptly handling downstream control tasks with Model Predictive Control ({MPC}). For 56 degrees of freedom humanoid, we empirically demonstrate that {H-GAP} learns to represent and generate a wide range of motor behaviors. Further, without any learning from online interactions, it can also flexibly transfer these behaviours to solve novel downstream control tasks via planning. Notably, {H-GAP} excels established {MPC} baselines with access to the ground truth model, and is superior or comparable to offline {RL} methods trained for individual tasks. Finally, we do a series of empirical studies on the scaling properties of {H-GAP}, showing the potential for performance gains via additional data but not computing.},
  address = {Vienna, Austria},
  author = {Zhengyao Jiang and Yingchen Xu and Nolan Wagener and Yicheng Luo and Michael Janner and Edward Grefenstette and Tim Rockt{\"a}schel and Yuandong Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024hgap.pdf:pdf},
  month = {5},
  note = {Spotlight},
  pdf = {https://openreview.net/pdf?id=LYG6tBlEX0},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {H-GAP: Humanoid Control with a Generalist Planner},
  url = {https://openreview.net/forum?id=LYG6tBlEX0},
  year = {2024}
}

@inproceedings{jiang2024principled,
  abstract = {Federated Domain Adaptation (FDA) describes the federated learning (FL) setting where source clients and a server work collaboratively to improve the performance of a target client where limited data is available. The domain shift between the source and target domains, coupled with limited data of the target client, makes FDA a challenging problem, e.g., common techniques such as federated averaging and fine-tuning fail due to domain shift and data scarcity. To theoretically understand the problem, we introduce new metrics that characterize the FDA setting and a theoretical framework with novel theorems for analyzing the performance of server aggregation rules. Further, we propose a novel lightweight aggregation rule, Federated Gradient Projection (FedGP), which significantly improves the target performance with domain shift and data scarcity. Moreover, our theory suggests an auto-weighting scheme that finds the optimal combinations of the source and target gradients. This scheme improves both FedGP and a simpler heuristic aggregation rule. Extensive experiments verify the theoretical insights and illustrate the effectiveness of the proposed methods in practice.},
  address = {Vienna, Austria},
  author = {Enyi Jiang and Yibo Jacky Zhang and Sanmi Koyejo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024principled.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=6J3ehSUrMU},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Principled Federated Domain Adaptation: Gradient Projection and Auto-Weighting},
  url = {https://openreview.net/forum?id=6J3ehSUrMU},
  year = {2024}
}

@inproceedings{jiang2024large,
  abstract = {The current electroencephalogram ({EEG}) based deep learning models are typically designed for specific datasets and applications in brain-computer interaction ({BCI}), limiting the scale of the models and thus diminishing their perceptual capabilities and generalizability. Recently, Large Language Models ({LLMs}) have achieved unprecedented success in text processing, prompting us to explore the capabilities of Large {EEG} Models ({LEMs}). We hope that {LEMs} can break through the limitations of different task types of {EEG} datasets, and obtain universal perceptual capabilities of {EEG} signals through unsupervised pre-training. Then the models can be fine-tuned for different downstream tasks. However, compared to text data, the volume of {EEG} datasets is generally small and the format varies widely. For example, there can be mismatched numbers of electrodes, unequal length data samples, varied task designs, and low signal-to-noise ratio. To overcome these challenges, we propose a unified foundation model for {EEG} called Large Brain Model ({LaBraM}). {LaBraM} enables cross-dataset learning by segmenting the {EEG} signals into {EEG} channel patches. Vector-quantized neural spectrum prediction is used to train a semantically rich neural tokenizer that encodes continuous raw {EEG} channel patches into compact neural codes. We then pre-train neural Transformers by predicting the original neural codes for the masked {EEG} channel patches. The {LaBraMs} were pre-trained on about 2,500 hours of various types of {EEG} signals from around 20 datasets and validated on multiple different types of downstream tasks.},
  address = {Vienna, Austria},
  author = {Wei-Bang Jiang and Li-Ming Zhao and Bao-Liang Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024large.pdf:pdf},
  month = {5},
  note = {Spotlight},
  pdf = {https://openreview.net/pdf?id=QzTpTRVtrP},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Large Brain Model for Learning Generic Representations with Tremendous EEG Data in BCI},
  url = {https://openreview.net/forum?id=QzTpTRVtrP},
  year = {2024}
}

@inproceedings{jiang2024tab,
  abstract = {Spiking Neural Networks (SNNs) are attracting growing interest for their energy-efficient computing when implemented on neuromorphic hardware. However, directly training SNNs, even adopting batch normalization (BN), is highly challenging due to their non-differentiable activation function and the temporally delayed accumulation of outputs over time. For SNN training, this temporal accumulation gives rise to Temporal Covariate Shifts (TCS) along the temporal dimension, a phenomenon that would become increasingly pronounced with layer-wise computations across multiple layers and multiple time-steps. To address this issue, we propose a novel TAB (Temporal Accumulated Batch normalization) method that effectively encapsulates the historical temporal dependencies that underlie the membrane potential accumulation process, thereby establishing a natural connection between neuron dynamics and TAB batch normalization. Experimental results on CIFAR-10, CIFAR-100, and DVS-CIFAR10 show that our TAB method outperforms other state-of-the-art methods.},
  address = {Vienna, Austria},
  author = {Haiyan Jiang and Vincent Zoonekynd and Giulia {De Masi} and Bin Gu and Huan Xiong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiang2024tab.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=k1wlmtPGLq},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {TAB: Temporal Accumulated Batch Normalization in Spiking Neural Networks},
  url = {https://openreview.net/forum?id=k1wlmtPGLq},
  year = {2024}
}

@inproceedings{jiao2024space,
  abstract = {Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the {Wyckoff} position constraint of the fractional coordinates. Upon the derived constraints, we then propose {DiffCSP++}, a novel diffusion model that has enhanced a previous work {DiffCSP} by further taking space group constraint into account. Experiments on several popular datasets verify the benefit of the involvement of the space group constraint, and show that our {DiffCSP++} achieves promising performance on crystal structure prediction, ab initio crystal generation and controllable generation with customized space groups.},
  address = {Vienna, Austria},
  author = {Rui Jiao and Wenbing Huang and Yu Liu and Deli Zhao and Yang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiao2024space.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=jkvZ7v4OmP},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Space Group Constrained Crystal Generation},
  url = {https://openreview.net/forum?id=jkvZ7v4OmP},
  year = {2024}
}

@inproceedings{jiao2024naisr,
  abstract = {Deep implicit functions (DIFs) have emerged as a powerful paradigm for many computer vision tasks such as 3D shape reconstruction, generation, registration, completion, editing, and understanding. However, given a set of 3D shapes with associated covariates there is at present no shape representation method which allows to precisely represent the shapes while capturing the individual dependencies on each covariate. Such a method would be of high utility to researchers to discover knowledge hidden in a population of shapes. For scientific shape discovery, we propose a 3D Neural Additive Model for Interpretable Shape Representation (NAISR) which describes individual shapes by deforming a shape atlas in accordance to the effect of disentangled covariates. Our approach captures shape population trends and allows for patient-specific predictions through shape transfer. NAISR is the first approach to combine the benefits of deep implicit shape representations with an atlas deforming according to specified covariates. We evaluate NAISR with respect to shape reconstruction, shape disentanglement, shape evolution, and shape transfer on three datasets: 1) Starman, a simulated 2D shape dataset; 2) the ADNI hippocampus 3D shape dataset; and 3) a pediatric airway 3D shape dataset. Our experiments demonstrate that NAISR achieves excellent shape reconstruction performance while retaining interpretability.},
  address = {Vienna, Austria},
  author = {Yining Jiao and Carlton J. Zdanski and Julia S. Kimbell and Andrew Prince and Cameron Worden and Samuel Kirse and Christopher Rutter and Benjamin Shields and William Dunn and Jisan Mahmud and Marc Niethammer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiao2024naisr.pdf:pdf},
  month = {5},
  note = {Spotlight},
  pdf = {https://openreview.net/pdf?id=wg8NPfeMF9},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {NAISR: A 3D Neural Additive Model for Interpretable Shape Representation},
  url = {https://openreview.net/forum?id=wg8NPfeMF9},
  year = {2024}
}

@inproceedings{jimenez2024swebench,
  abstract = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96\% of the issues.},
  address = {Vienna, Austria},
  author = {Carlos E. Jimenez and John Yang and Alexander Wettig and Shunyu Yao and Kexin Pei and Ofir Press and Karthik R. Narasimhan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jimenez2024swebench.pdf:pdf},
  month = {5},
  note = {Oral},
  pdf = {https://openreview.net/pdf?id=VTF8yNQM66},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {SWE-bench: Can Language Models Resolve Real-world Github Issues?},
  url = {https://openreview.net/forum?id=VTF8yNQM66},
  year = {2024}
}

@inproceedings{jin2024stationary,
  abstract = {Proximal policy optimization ({PPO}) has gained popularity in reinforcement learning ({RL}). Its {PPO}-Clip variant is one the most frequently implemented algorithms and is one of the first-to-try algorithms in {RL} tasks. This variant uses a clipped surrogate objective function not typically found in other algorithms. Many works have demonstrated the practical performance of {PPO}-Clip, but the theoretical understanding of it is limited to specific settings. In this work, we provide a comprehensive analysis that shows the stationary point convergence of {PPO}-Clip and the convergence rate thereof. Our analysis is new and overcomes many challenges, including the non-smooth nature of the clip operator, the potentially unbounded score function, and the involvement of the ratio of two stochastic policies. Our results and techniques might share new insights into {PPO}-Clip.},
  address = {Vienna, Austria},
  author = {Ruinan Jin and Shuai Li and Baoxiang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024stationary.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=uznKlCpWjV},
  publisher = {OpenReview.net},
  title = {On Stationary Point Convergence of {PPO}-Clip},
  url = {https://openreview.net/forum?id=uznKlCpWjV},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jin2024structural,
  abstract = {Conventional causal discovery approaches, which seek to uncover causal relationships among measured variables, are typically fragile to the presence of latent variables. While various methods have been developed to address this confounding issue, they often rely on strong assumptions about the underlying causal structure. In this paper, we consider a general scenario where measured and latent variables collectively form a partially observed causally sufficient linear system and latent variables may be anywhere in the causal structure. We theoretically show that with the aid of high-order statistics, the proposed approach can identify the causal structure and estimate parameters in this partially observed setting.},
  address = {Vienna, Austria},
  author = {Songyao Jin and Feng Xie and Guangyi Chen and Biwei Huang and Zhengming Chen and Xinshuai Dong and Kun Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024structural.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=nHkMm0ywWm},
  publisher = {OpenReview.net},
  title = {Structural Estimation of Partially Observed Linear Non-Gaussian Acyclic Model: A Practical Approach with Identifiability},
  url = {https://openreview.net/forum?id=nHkMm0ywWm},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jin2024can,
  abstract = {Causal inference is one of the hallmarks of human intelligence. While the field of {C}ausal{NLP} has attracted much interest in the recent years, existing causal inference datasets in {NLP} primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models ({LLM}s). Specifically, we formulate a novel task {C}orr2{C}ause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing {LLM}s. Through our experiments, we identify a key shortcoming of {LLM}s in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose {LLM}s for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. {C}orr2{C}ause is a challenging task for {LLM}s, and can be helpful in guiding future research on improving {LLM}s' pure reasoning skills and generalizability.},
  address = {Vienna, Austria},
  author = {Zhijing Jin and Jiarui Liu and Zhiheng Lyu and Spencer Poff and Mrinmaya Sachan and Rada Mihalcea and Mona T. Diab and Bernhard Sch{\"o}lkopf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024can.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=vqIH0ObdqL},
  publisher = {OpenReview.net},
  title = {Can Large Language Models Infer Causation from Correlation?},
  url = {https://openreview.net/forum?id=vqIH0ObdqL},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jin2024unified,
  abstract = {Recently, the remarkable advance of the Large Language Model ({LLM}) has inspired researchers to transfer its extraordinary reasoning capability to both vision and language data. However, the prevailing approaches primarily regard the visual input as a prompt and focus exclusively on optimizing the text generation process conditioned upon vision content by a frozen {LLM}. Such an inequitable treatment of vision and language heavily constrains the model's potential. In this paper, we break through this limitation by representing both vision and language in a unified form. Specifically, we introduce a well-designed visual tokenizer to translate the non-linguistic image into a sequence of discrete tokens like a foreign language that {LLM} can read. The resulting visual tokens encompass high-level semantics worthy of a word and also support dynamic sequence length varying from the image. Coped with this tokenizer, the presented foundation model called {L}a{VIT} can handle both image and text indiscriminately under the same generative learning paradigm. This unification empowers {L}a{VIT} to serve as an impressive generalist interface to understand and generate multi-modal content simultaneously. Extensive experiments further showcase that it outperforms the existing models by a large margin on massive vision-language tasks.},
  address = {Vienna, Austria},
  author = {Yang Jin and Kun Xu and Kun Xu and Liwei Chen and Chao Liao and Jianchao Tan and Quzhe Huang and Bin Chen and Chengru Song and Dai Meng and Di Zhang and Wenwu Ou and Kun Gai and Yadong Mu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024unified.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=FlvtjAB0gl},
  publisher = {OpenReview.net},
  title = {Unified Language-Vision Pretraining in {LLM} with Dynamic Discrete Visual Tokenization},
  url = {https://openreview.net/forum?id=FlvtjAB0gl},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jin2024cost,
  abstract = {How does scaling the number of parameters in large language models ({LLM}s) affect their core capabilities? We study two natural scaling techniques -- weight pruning and simply training a smaller or larger model, which we refer to as dense scaling -- and their effects on two core capabilities of {LLM}s: (a) recalling facts presented during pre-training and (b) processing information presented in-context during inference. By curating a suite of tasks that help disentangle these two capabilities, we find a striking difference in how these two abilities evolve due to scaling. Reducing the model size by more than 30\% (via either scaling approach) significantly decreases the ability to recall facts seen in pre-training. Yet, a 60--70\% reduction largely preserves the various ways the model can process in-context information, ranging from retrieving answers from a long context to learning parameterized functions from in-context exemplars. The fact that both dense scaling and weight pruning exhibit this behavior suggests that scaling model size has an inherently disparate effect on fact recall and in-context learning.},
  address = {Vienna, Austria},
  author = {Tian Jin and Nolan Clement and Xin Dong and Vaishnavh Nagarajan and Michael Carbin and Jonathan Ragan-Kelley and Gintare Karolina Dziugaite},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024cost.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=ldJXXxPE0L},
  publisher = {OpenReview.net},
  title = {The Cost of Scaling Down Large Language Models: Reducing Model Size Affects Memory before In-context Learning},
  url = {https://openreview.net/forum?id=ldJXXxPE0L},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jin2024improved,
  abstract = {Given a $K$-vertex simplex in a $d$-dimensional space, suppose we measure $n$ points on the simplex with noise (hence, some of the observed points fall outside the simplex). Vertex hunting is the problem of estimating the $K$ vertices of the simplex. A popular vertex hunting algorithm is successive projection algorithm ({SPA}). However, {SPA} is observed to perform unsatisfactorily under strong noise or outliers. We propose pseudo-point {SPA} (pp-{SPA}). It uses a projection step and a denoise step to generate pseudo-points and feed them into {SPA} for vertex hunting. We derive error bounds for pp-{SPA}, leveraging on extreme value theory of (possibly) high-dimensional random vectors. The results suggest that pp-{SPA} has faster rates and better numerical performances than {SPA}. Our analysis includes an improved non-asymptotic bound for the original {SPA}, which is of independent interest.},
  address = {Vienna, Austria},
  author = {Jiashun Jin and Zheng Tracy Ke and Gabriel Moryoussef and Jiajun Tang and Jingming Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jin2024improved.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=GlpawHh80l},
  publisher = {OpenReview.net},
  title = {Improved Algorithm and Bounds for Successive Projection},
  url = {https://openreview.net/forum?id=GlpawHh80l},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jing2024equivariant,
  abstract = {Molecular docking is critical to structure-based virtual screening, yet the throughput of such workflows is limited by the expensive optimization of scoring functions involved in most docking algorithms. We explore how machine learning can accelerate this process by learning a scoring function with a functional form that allows for more rapid optimization. Specifically, we define the scoring function to be the cross-correlation of multi-channel ligand and protein scalar fields parameterized by equivariant graph neural networks, enabling rapid optimization over rigid-body degrees of freedom with fast Fourier transforms. The runtime of our approach can be amortized at several levels of abstraction, and is particularly favorable for virtual screening settings with a common binding pocket. We benchmark our scoring functions on two simplified docking-related tasks: decoy pose scoring and rigid conformer docking. Our method attains similar but faster performance on crystal structures compared to the widely-used Vina and Gnina scoring functions, and is more robust on computationally predicted structures. Code is available at https://github.com/bjing2016/scalar-fields.},
  address = {Vienna, Austria},
  author = {Bowen Jing and Tommi S. Jaakkola and Bonnie Berger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jing2024equivariant.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=BIveOmD1Nh},
  publisher = {OpenReview.net},
  title = {Equivariant Scalar Fields for Molecular Docking with Fast Fourier Transforms},
  url = {https://openreview.net/forum?id=BIveOmD1Nh},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jing2024towards,
  abstract = {Opponent modeling aims at learning the opponent's behaviors, goals, or beliefs to reduce the uncertainty of the competitive environment and assist decision-making. Existing work has mostly focused on learning opponent models online, which is impractical and inefficient in practical scenarios. To this end, we formalize an Offline Opponent Modeling ({OOM}) problem with the objective of utilizing pre-collected offline datasets to learn opponent models that characterize the opponent from the viewpoint of the controlled agent, which aids in adapting to the unknown fixed policies of the opponent. Drawing on the promises of the Transformers for decision-making, we introduce a general approach, Transformer Against Opponent ({TAO}), for {OOM}. Essentially, {TAO} tackles the problem by harnessing the full potential of the supervised pre-trained Transformers' in-context learning capabilities. The foundation of {TAO} lies in three stages: an innovative offline policy embedding learning stage, an offline opponent-aware response policy training stage, and a deployment stage for opponent adaptation with in-context learning. Theoretical analysis establishes {TAO}'s equivalence to Bayesian posterior sampling in opponent modeling and guarantees {TAO}'s convergence in opponent policy recognition. Extensive experiments and ablation studies on competitive environments with sparse and dense rewards demonstrate the impressive performance of {TAO}. Our approach manifests remarkable prowess for fast adaptation, especially in the face of unseen opponent policies, confirming its in-context learning potency.},
  address = {Vienna, Austria},
  author = {Yuheng Jing and Kai Li and Bingyun Liu and Yifan Zang and Haobo Fu and Qiang Fu and Junliang Xing and Jian Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jing2024towards.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=2SwHngthig},
  publisher = {OpenReview.net},
  title = {Towards Offline Opponent Modeling with In-context Learning},
  url = {https://openreview.net/forum?id=2SwHngthig},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jiralerspong2024expected,
  abstract = {Generative flow networks ({GFlowNets}) are sequential sampling models trained to match a given distribution. {GFlowNets} have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks ({EFlowNets}), which extend {GFlowNets} to stochastic environments. We show that {EFlowNets} outperform other {GFlowNet} formulations in stochastic tasks such as protein design. We then extend the concept of {EFlowNets} to adversarial environments, proposing adversarial flow networks ({AFlowNets}) for two-player zero-sum games. We show that {AFlowNets} learn to find above 80\% of optimal moves in Connect-4 via self-play and outperform {AlphaZero} in tournaments.},
  address = {Vienna, Austria},
  author = {Marco Jiralerspong and Bilun Sun and Danilo Vucetic and Tianyu Zhang and Yoshua Bengio and Gauthier Gidel and Nikolay Malkin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jiralerspong2024expected.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=uH0FGECSEI},
  publisher = {OpenReview.net},
  title = {Expected Flow Networks in Stochastic Environments and Two-Player Zero-Sum Games},
  url = {https://openreview.net/forum?id=uH0FGECSEI},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{johnson2024sampleefficiency,
  abstract = {We theoretically explore the relationship between sample-efficiency and adaptivity in reinforcement learning. An algorithm is sample-efficient if it uses a number of queries $n$ to the environment that is polynomial in the dimension $d$ of the problem. Adaptivity refers to the frequency at which queries are sent and feedback is processed to update the querying strategy. To investigate this interplay, we employ a learning framework that allows sending queries in $K$ batches, with feedback being processed and queries updated after each batch. This model encompasses the whole adaptivity spectrum, ranging from non-adaptive `offline' ($K=1$) to fully adaptive ($K=n$) scenarios, and regimes in between. For the problems of policy evaluation and best-policy identification under $d$-dimensional linear function approximation, we establish $\Omega(\log \log d)$ lower bounds on the number of batches $K$ required for sample-efficient algorithms with $n = O(poly(d))$ queries. Our results show that just having adaptivity ($K>1$) does not necessarily guarantee sample-efficiency. Notably, the adaptivity-boundary for sample-efficiency is not between offline reinforcement learning ($K=1$), where sample-efficiency was known to not be possible, and adaptive settings. Instead, the boundary lies between different regimes of adaptivity and depends on the problem dimension.},
  address = {Vienna, Austria},
  author = {Emmeran Johnson and Ciara Pike-Burke and Patrick Rebeschini},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/johnson2024sampleefficiency.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=ey3GhWXQ97},
  publisher = {OpenReview.net},
  title = {Sample-Efficiency in Multi-Batch Reinforcement Learning: The Need for Dimension-Dependent Adaptivity},
  url = {https://openreview.net/forum?id=ey3GhWXQ97},
  venue = {ICLR 2024 poster},
  year = {2024}
}

@inproceedings{jones2024teaching,
  abstract = {Large language models (LLMs) frequently hallucinate on abstractive summarization tasks. Training LLMs with standard methods makes it difficult to determine the relationship between hallucinations on different tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. We then use PrefixTuning to train the model on this task, and observe whether this training affects hallucinations on downstream tasks.},
  author = {Erik Jones and Hamid Palangi and Clarisse Sim{\~o}es and Varun Chandrasekaran and Subhabrata Mukherjee and Arindam Mitra and Ahmed Hassan Awadallah and Ece Kamar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jones2024teaching.pdf:pdf},
  keywords = {hallucination, large language models, synthetic data, prefix tuning, safety},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xpw7V0P136},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Teaching Language Models to Hallucinate Less with Synthetic Tasks},
  url = {https://openreview.net/forum?id=xpw7V0P136},
  venue = {poster},
  year = {2024}
}

@inproceedings{jordan2024variance,
  abstract = {Neural network trainings are stochastic, causing the performance of trained networks to vary across repeated runs of training. This paper explores variance in neural network training across four key dimensions: performance variance on standard datasets, an "independent errors assumption", test-set variance proofs, and preliminary studies on factors like data augmentation and learning rate.},
  address = {Vienna, Austria},
  author = {Keller Jordan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jordan2024variance.pdf:pdf},
  keywords = {stochasticity, distribution shift, randomness, deep learning, variance, random seed},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pEGSdJu52I},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {On the Variance of Neural Network Training with respect to Test Sets and Distributions},
  url = {https://openreview.net/forum?id=pEGSdJu52I},
  venue = {poster},
  year = {2024}
}

@inproceedings{joren2024classification,
  abstract = {We propose a new approach to promote safety in classification tasks with concept annotations. Our approach -- called a conceptual safeguard -- acts as a verification layer for models that predict a target outcome by first predicting the presence of intermediate concepts.},
  address = {Vienna, Austria},
  author = {Hailey Joren and Charles T. Marx and Berk Ustun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/joren2024classification.pdf:pdf},
  keywords = {classification, selective classification, concept bottleneck models, uncertainty quantification},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=t8cBsT9mcg},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Classification with Conceptual Safeguards},
  url = {https://openreview.net/forum?id=t8cBsT9mcg},
  venue = {poster},
  year = {2024}
}

@inproceedings{joshi2024noisy,
  abstract = {Understanding how overparameterized neural networks generalize despite perfect interpolation of noisy training data is a fundamental question. This paper analyzes overfitting behavior in univariate two-layer ReLU networks, showing that overfitting can be "tempered" with L1 loss, but potentially "catastrophic" with L2 loss.},
  address = {Vienna, Austria},
  author = {Nirmit Joshi and Gal Vardi and Nathan Srebro},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/joshi2024noisy.pdf:pdf},
  keywords = {Interpolation Learning, Benign Overfitting, ReLU Networks},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=GTUoTJXPBf},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Noisy Interpolation Learning with Shallow Univariate {ReLU} Networks},
  url = {https://openreview.net/forum?id=GTUoTJXPBf},
  venue = {spotlight},
  year = {2024}
}

@inproceedings{ju2024achieving,
  abstract = {This paper proposes a Reinforcement Learning (RL) approach to achieve fairness in multi-agent finite-horizon episodic MDPs. Instead of maximizing the sum of individual agents' value functions, the authors introduce a fairness function that ensures equitable rewards across agents. While existing research has focused on studying fairness in known environments, this work addresses fairness in unknown environments where classical Bellman's equation does not hold when the sum of individual value functions is not maximized.},
  author = {Peizhong Ju and Arnob Ghosh and Ness B. Shroff},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ju2024achieving.pdf:pdf},
  keywords = {fairness, multi-agent reinforcement learning, MDP, equitable rewards},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=yoVq2BGQdP},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Achieving Fairness in Multi-Agent {MDP} Using Reinforcement Learning},
  url = {https://openreview.net/forum?id=yoVq2BGQdP},
  venue = {poster},
  year = {2024}
}

@inproceedings{ju2024deep,
  abstract = {The paper addresses the challenge of learning shared representations from multi-modal neuroimaging data. To capitalize on the geometric structure, the authors introduce a measure called geodesic correlation which expands traditional correlation consistency to covariance-based data on the symmetric positive definite (SPD) manifold. They propose DeepGeoCCA, an innovative geometric deep learning framework that enhances the geodesic correlation of unlabeled, paired data, thereby generating novel representations while retaining the geometric structures.},
  address = {Vienna, Austria},
  author = {Ce Ju and Reinmar J. Kobler and Liyao Tang and Cuntai Guan and Motoaki Kawanabe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ju2024deep.pdf:pdf},
  keywords = {geometric deep learning, canonical correlation analysis, neuroimaging, SPD manifold},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=PnR1MNen7u},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Deep Geodesic Canonical Correlation Analysis for Covariance-Based Neuroimaging Data},
  url = {https://openreview.net/forum?id=PnR1MNen7u},
  venue = {spotlight},
  year = {2024}
}

@inproceedings{ju2024pnp,
  abstract = {The paper introduces "PnP Inversion," a novel technique that rectifies inversion deviations directly within the source diffusion branch using just three lines of code, while leaving the target diffusion branch unaltered. The key insight is that disentangling the source and target branches leads to a clear separation of responsibilities -- one for essential content preservation and the other for edit fidelity. The authors also present PIE-Bench, an editing benchmark featuring 700 images with diverse scenes and editing types.},
  address = {Vienna, Austria},
  author = {Xuan Ju and Ailing Zeng and Yuxuan Bian and Shaoteng Liu and Qiang Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ju2024pnp.pdf:pdf},
  keywords = {diffusion models, image inversion, image editing, benchmark},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FoMZ4ljhVw},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{PnP} Inversion: Boosting Diffusion-based Editing with 3 Lines of Code},
  url = {https://openreview.net/forum?id=FoMZ4ljhVw},
  venue = {poster},
  year = {2024}
}

@inproceedings{jung2024learning,
  abstract = {Learning compositional representation is a key aspect of object-centric learning as it enables flexible systematic generalization and supports complex visual reasoning. Most existing approaches rely on auto-encoding objective, while the compositionality is implicitly imposed by the architectural or algorithmic bias in the encoder. This study proposes a novel objective that explicitly encourages compositionality of the representations by incorporating additional constraints that an arbitrary mixture of object representations from two images should be valid by maximizing the likelihood of the composite data.},
  address = {Vienna, Austria},
  author = {Whie Jung and Jaehoon Yoo and Sungjin Ahn and Seunghoon Hong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/jung2024learning.pdf:pdf},
  keywords = {object-centric learning, compositionality, visual reasoning, slot attention},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HT2dAhh4uV},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Learning to Compose: Improving Object Centric Learning by Injecting Compositionality},
  url = {https://openreview.net/forum?id=HT2dAhh4uV},
  venue = {poster},
  year = {2024}
}

@inproceedings{kacprzyk2024ode,
  abstract = {The paper introduces a different type of solution in the longitudinal setting: a closed-form ordinary differential equation (ODE). While they still rely on continuous optimization to learn an ODE, the resulting inference machine is no longer a neural network. This approach yields several advantages such as interpretability, irregular sampling, and a different set of identification assumptions. The authors contribute a framework that can transform any ODE discovery method into a treatment effects method.},
  address = {Vienna, Austria},
  author = {Krzysztof Kacprzyk and Samuel Holt and Jeroen Berrevoets and Zhaozhi Qian and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kacprzyk2024ode.pdf:pdf},
  keywords = {ordinary differential equations, treatment effects, longitudinal data, causal inference},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=pxI5IPeWgW},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{ODE} Discovery for Longitudinal Heterogeneous Treatment Effects Inference},
  url = {https://openreview.net/forum?id=pxI5IPeWgW},
  venue = {spotlight},
  year = {2024}
}

@inproceedings{kacprzyk2024towards,
  abstract = {Transparent machine learning (ML) models are essential for ensuring interpretability and trustworthiness in decision-making systems, particularly in high-stakes domains such as healthcare, finance, and criminal justice. While transparent machine learning models have been proposed for classification and regression, time series forecasting presents some unique challenges for ensuring transparency. This paper addresses these challenges by proposing methods for transparent time series forecasting.},
  address = {Vienna, Austria},
  author = {Krzysztof Kacprzyk and Tennison Liu and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kacprzyk2024towards.pdf:pdf},
  keywords = {time series forecasting, transparency, interpretability, trustworthy machine learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TYXtXLYHpR},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Towards Transparent Time Series Forecasting},
  url = {https://openreview.net/forum?id=TYXtXLYHpR},
  venue = {poster},
  year = {2024}
}

@inproceedings{kadkhodaie2024generalization,
  abstract = {Deep neural networks ({DNN}s) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the `true' continuous density of the data. To investigate this question, we study training of score-based diffusion models and examine the learned score functions. We show that two {DNN}s trained on non-overlapping subsets of a dataset learn nearly identical score functions, and that the diffusion-generated images are distinct from the training set. This suggests that the networks are learning the `true' score of the underlying continuous distribution. We provide evidence that this generalization behavior is connected to an inductive bias that favors geometry-adaptive harmonic representations.},
  address = {Vienna, Austria},
  author = {Zahra Kadkhodaie and Florentin Guth and Eero P. Simoncelli and St{\'e}phane Mallat},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kadkhodaie2024generalization.pdf:pdf},
  month = {5},
  note = {Oral presentation. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ANvmVS2Yr0},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Generalization in diffusion models arises from geometry-adaptive harmonic representations},
  url = {https://openreview.net/forum?id=ANvmVS2Yr0},
  year = {2024}
}

@inproceedings{kahl2024values,
  abstract = {Uncertainty estimation is an essential and heavily-studied component for the reliable application of semantic segmentation methods. While various studies exist claiming methodological advances on the one hand, and successful application on the other hand, the field is currently hampered by a gap between theory and practice leaving fundamental questions unanswered: Can data-related and model-related uncertainty really be separated in practice? Which components of an uncertainty method are essential for real-world performance? Which uncertainty method works well for which application? In this work, we link this research gap to a lack of systematic and comprehensive evaluation of uncertainty methods. Specifically, we identify three key pitfalls in current literature and present an evaluation framework that bridges the research gap by providing: (1) A controlled environment for studying data ambiguities as well as distribution shifts (2) Systematic ablations of relevant method components (3) Test-beds for the five predominant uncertainty applications: {OoD}-detection, active learning, failure detection, calibration, and ambiguity modeling. Empirical results on simulated as well as real-world data demonstrate that: (1) Separation of uncertainty types works on simulated data but does not necessarily translate to real-world data (2) Aggregation of scores is a crucial but currently neglected component of uncertainty methods (3) While ensembles are performing most robustly across the different downstream tasks and settings, test-time augmentation often constitutes a light-weight alternative.},
  address = {Vienna, Austria},
  author = {Kim-Celine Kahl and Carsten T. L{\"u}th and Maximilian Zenk and Klaus Maier-Hein and Paul F. Jaeger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kahl2024values.pdf:pdf},
  month = {5},
  note = {Oral presentation. Code: https://github.com/IML-DKFZ/values. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=yV6fD7LYkF},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{ValUES}: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation},
  url = {https://openreview.net/forum?id=yV6fD7LYkF},
  year = {2024}
}

@inproceedings{kajitsuka2024transformers,
  abstract = {Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the {B}oltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that one-layer and single-head Transformers have a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.},
  address = {Vienna, Austria},
  author = {Tokio Kajitsuka and Issei Sato},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kajitsuka2024transformers.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nJnky5K944},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?},
  url = {https://openreview.net/forum?id=nJnky5K944},
  year = {2024}
}

@inproceedings{kalan2024tight,
  abstract = {A critical barrier to learning an accurate decision rule for outlier detection is the scarcity of outlier data. As such, practitioners often turn to the use of similar but imperfect outlier data from which they might transfer information to the target outlier detection task. Despite the recent empirical success of transfer learning in outlier detection, a fundamental understanding of when and how knowledge can be transferred from a source to a target in outlier detection remains elusive. In this work, we adopt the traditional framework of {N}eyman-{P}earson classification---which formalizes supervised outlier detection, i.e., unbalanced classification---with the added assumption that we have access to both source and (some or no) target outlier data. Our main results are then as follows: We first determine the information-theoretic limits of the problem under a measure of discrepancy that extends some existing notions from traditional balanced classification; interestingly, unlike in balanced classification, seemingly very dissimilar sources can provide much information about a target, thus resulting in fast transfer. We then show that, in principle, these information-theoretic limits are achievable by adaptive procedures, i.e., procedures with no a priori information on the discrepancy between source and target distributions.},
  address = {Vienna, Austria},
  author = {Mohammadreza Mousavi Kalan and Samory Kpotufe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kalan2024tight.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nUBLhhVM1l},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Tight Rates in Supervised Outlier Transfer Learning},
  url = {https://openreview.net/forum?id=nUBLhhVM1l},
  year = {2024}
}

@inproceedings{kalantidis2024weatherproofing,
  abstract = {State-of-the-art visual localization approaches generally rely on a first image retrieval step whose role is crucial. Yet, retrieval often struggles when facing varying conditions, due to e.g. weather or time of day, with dramatic consequences on the visual localization accuracy. In this paper, we improve this retrieval step and tailor it to the final localization task. Among the several changes we advocate for, we propose to synthesize variants of the training set images, obtained from generative text-to-image models, in order to automatically expand the training set towards a number of nameable variations that particularly hurt visual localization. After expanding the training set, we propose a training approach that leverages the specificities and the underlying geometry of this mix of real and synthetic data. We validate our approach on challenging visual localization benchmarks and report substantial improvements over baselines, with {SOTA} results on the challenging {C}yber{L}ocalization dataset.},
  address = {Vienna, Austria},
  author = {Yannis Kalantidis and Mert B{\"u}lent Sar{\i}y{\i}ld{\i}z and Rafael S. Rezende and Philippe Weinzaepfel and Diane Larlus and Gabriela Csurka},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kalantidis2024weatherproofing.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5EniAcsO7f},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Weatherproofing Retrieval for Localization with Generative {AI} and Geometric Consistency},
  url = {https://openreview.net/forum?id=5EniAcsO7f},
  year = {2024}
}

@inproceedings{kallel2024augmented,
  abstract = {Deterministic policies are often preferred over stochastic ones when implemented on physical systems. They can prevent erratic and harmful behaviors while being easier to implement and interpret. However, in practice, exploration is largely performed by stochastic policies. First-order {B}ayesian Optimization ({BO}) methods offer a principled way of performing exploration using deterministic policies. This is done through a learned probabilistic model of the objective function and its gradient. Nonetheless, such approaches treat policy search as a black-box problem, and thus, neglect the reinforcement learning nature of the problem. In this work, we leverage the performance difference lemma to introduce a novel mean function for the probabilistic model. We introduce a principled solution to this problem by building a novel {RL}-aware mean function to enhance local {BO} methods, leveraging the performance difference lemma to inject an action-value function into the {GP} prior of the objective function, thus effectively incorporating knowledge of past trajectories into their belief about the return of untested policies.},
  address = {Vienna, Austria},
  author = {Mahdi Kallel and Debabrota Basu and Riad Akrour and Carlo D'Eramo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kallel2024augmented.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OvlcyABNQT},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Augmented {B}ayesian Policy Search},
  url = {https://openreview.net/forum?id=OvlcyABNQT},
  year = {2024}
}

@inproceedings{kanatsoulis2024counting,
  abstract = {Graph Neural Networks ({GNN}s) are powerful representation learning tools that have achieved remarkable performance in various downstream tasks. However, there are still open questions regarding their ability to count and list substructures, which play a crucial role in biological and social networks. In this work, we fill this gap and characterize the representation and generalization power of {GNN}s in terms of their ability to produce powerful representations that count substructures. In particular, we study the message-passing operations of {GNN}s with random node input in a novel fashion, and show how they can produce equivariant representations that are associated with high-order statistical moments. Using these representations, we prove that {GNN}s can learn how to count cycles, cliques, quasi-cliques, and the number of connected components in a graph.},
  address = {Vienna, Austria},
  author = {Charilaos I. Kanatsoulis and Alejandro Ribeiro},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kanatsoulis2024counting.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qaJxPhkYtD},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Counting Graph Substructures with Graph Neural Networks},
  url = {https://openreview.net/forum?id=qaJxPhkYtD},
  year = {2024}
}

@inproceedings{kang2024labelfocused,
  abstract = {Most neural networks for classification primarily learn features differentiated by input-domain related information such as visual similarity of objects in an image. While this focus is natural behavior, it can inadvertently introduce an inductive bias that conflicts with unseen relations in an implicit output-domain determined by human labeling based on their own world knowledge. Such conflicts can limit generalization of models by potential dominance of the input-domain focused bias in inference. To overcome this limitation without external resources, we introduce Output-Domain focused Biasing (ODB) training strategy that constructs inductive biases on features differentiated by only output labels.},
  address = {Vienna, Austria},
  author = {Ilmin Kang and HyounYoung Bae and Kangil Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kang2024labelfocused.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cH3oufN8Pl},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Label-Focused Inductive Bias over Latent Object Features in Visual Classification},
  url = {https://openreview.net/forum?id=cH3oufN8Pl},
  year = {2024}
}

@inproceedings{kang2024colep,
  abstract = {Conformal prediction has shown spurring performance in constructing statistically rigorous prediction sets for arbitrary black-box machine learning models, assuming the data is exchangeable. However, even small adversarial perturbations during the inference can violate the exchangeability assumption, challenge the coverage guarantees, and result in a subsequent decline in empirical coverage. We propose a certifiably robust learning-reasoning conformal prediction framework (COLEP) via probabilistic circuits, which comprise a data-driven learning component that trains statistical models to learn different semantic concepts, and a reasoning component that encodes knowledge and characterizes the relationships among the trained models for logic reasoning. To achieve exact and efficient reasoning, we employ probabilistic circuits (PCs) within the reasoning component. Theoretically, we provide end-to-end certification of prediction coverage for COLEP in the presence of bounded adversarial perturbations. We also provide certified coverage considering the finite size of the calibration set. Furthermore, we prove that COLEP achieves higher prediction coverage and accuracy over a single model as long as the utilities of knowledge models are non-trivial. Empirically, we show the validity and tightness of our certified coverage, demonstrating the robust conformal prediction of COLEP on various datasets, including GTSRB, CIFAR10, and AwA2.},
  address = {Vienna, Austria},
  author = {Mintong Kang and Nezihe Merve G{\"u}rel and Linyi Li and Bo Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kang2024colep.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=XN6ZPINdSg},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{COLEP}: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits},
  url = {https://openreview.net/forum?id=XN6ZPINdSg},
  year = {2024}
}

@inproceedings{kang2024unr,
  abstract = {The paper introduces {UNR}-Explainer, a method that generates counterfactual explanations for unsupervised node representation learning models. It aims to identify the most important subgraphs that cause a significant change in the $k$-nearest neighbors of a node in the embedding space. This approach provides insights into how unsupervised node representation models make decisions about node similarities and relationships within the graph structure, enabling better understanding and interpretation of these complex models in graph-based machine learning tasks.},
  address = {Vienna, Austria},
  author = {Hyunju Kang and Geonhee Han and Hogun Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kang2024unr.pdf:pdf},
  month = {5},
  note = {Poster presentation. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0j9ZDzMPqr},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{UNR}-Explainer: Counterfactual Explanations for Unsupervised Node Representation Learning Models},
  url = {https://openreview.net/forum?id=0j9ZDzMPqr},
  year = {2024}
}

@inproceedings{kang2024get,
  abstract = {This work focuses on leveraging and selecting from vast, unlabeled, open data to pre-fine-tune a pre-trained language model. The goal is to minimize the need for costly domain-specific data for subsequent fine-tuning while achieving desired performance levels. The key idea is to select data that nudges the pre-training distribution closer to the target distribution. We show the optimality of this approach for fine-tuning tasks under certain conditions. This differs from previous approaches that simply prioritize data that aligns with the target distribution. While this strategy may be effective when training a model from scratch, it can yield limited results when the model has already been pre-trained on a different distribution.},
  address = {Vienna, Austria},
  author = {Feiyang Kang and Hoang Anh Just and Yifan Sun and Himanshu Jahagirdar and Yuanzhi Zhang and Rongxing Du and Anit Kumar Sahu and Ruoxi Jia},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/kang2024get.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-03-12},
  pdf = {https://openreview.net/pdf?id=QmYNBVukex},
  publisher = {OpenReview.net},
  title = {Get more for less: {P}rincipled {D}ata {S}election for {W}arming {U}p {F}ine-{T}uning in {LLM}s},
  url = {https://openreview.net/forum?id=QmYNBVukex},
  year = {2024}
}

@inproceedings{kang2024deep,
  abstract = {Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate empirically and then study theoretically in a simplified setting involving deep homogeneous networks with ReLU activations. Finally, we show how one can leverage our insights in practice to enable risk-sensitive decision-making in the presence of OOD inputs.},
  address = {Vienna, Austria},
  author = {Katie Kang and Amrith Setlur and Claire J. Tomlin and Sergey Levine},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/kang2024deep.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ljwoQ3cvQh},
  publisher = {OpenReview.net},
  title = {Deep Neural Networks Tend To Extrapolate Predictably},
  url = {https://openreview.net/forum?id=ljwoQ3cvQh},
  year = {2024}
}

@inproceedings{kang2024deceptive,
  abstract = {We study deceptive fairness attacks on graphs to answer the following question: How can we achieve poisoning attacks on a graph learning model to exacerbate the bias deceptively? We answer this question via a bi-level optimization problem and propose a meta learning-based framework named FATE. FATE is broadly applicable with respect to various fairness definitions and graph learning models, as well as arbitrary choices of manipulation operations. We further instantiate FATE to attack statistical parity or individual fairness on graph neural networks. We conduct extensive experimental evaluations on real-world datasets in the task of semi-supervised node classification. The experimental results demonstrate that FATE could amplify the bias of graph neural networks with or without fairness consideration while maintaining the utility on the downstream task. We hope this paper provides insights into the adversarial robustness of fair graph learning and can shed light on designing robust and fair graph learning in future studies.},
  address = {Vienna, Austria},
  author = {Jian Kang and Yinglong Xia and Ross Maciejewski and Jiebo Luo and Hanghang Tong},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/kang2024deceptive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=iS5ADHNg2A},
  publisher = {OpenReview.net},
  title = {Deceptive Fairness Attacks on Graphs via Meta Learning},
  url = {https://openreview.net/forum?id=iS5ADHNg2A},
  year = {2024}
}

@inproceedings{kang2024progressive,
  abstract = {Neural Implicit Representation (NIR) has recently gained significant attention due to its remarkable ability to encode complex and high-dimensional data into representation space and easily reconstruct it through a trainable mapping function. However, NIR methods assume a one-to-one mapping between the target data and representation models regardless of data relevancy or similarity. This results in poor generalization over multiple complex data and limits their efficiency and scalability. Motivated by continual learning, this work investigates how to accumulate and transfer neural implicit representations for multiple complex video data over sequential encoding sessions. To overcome the limitation of NIR, we propose a novel method, Progressive Fourier Neural Representation (PFNR), that aims to find an adaptive and compact sub-module in Fourier space to encode videos in each training session. This sparsified neural encoding allows the neural network to hold free weights, enabling an improved adaptation for future videos. In addition, when learning a representation for a new video, PFNR transfers the representation of previous videos with frozen weights. This design allows the model to continuously accumulate high-quality neural representations for multiple videos while ensuring lossless decoding that perfectly preserves the learned representations for previous videos. We validate our PFNR method on the UVG8/17 and DAVIS50 video sequence benchmarks and achieve impressive performance gains over strong continual learning baselines.},
  address = {Vienna, Austria},
  author = {Haeyong Kang and Jaehong Yoon and Dahyun Kim and Sung Ju Hwang and Chang D. Yoo},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/kang2024progressive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-11-15},
  pdf = {https://openreview.net/pdf?id=rGFrRMBbOq},
  publisher = {OpenReview.net},
  title = {Progressive {F}ourier Neural Representation for Sequential Video Compilation},
  url = {https://openreview.net/forum?id=rGFrRMBbOq},
  year = {2024}
}

@inproceedings{kang2024unleashing,
  abstract = {We introduce the FRactional-Order graph Neural Dynamical network (FROND), a new continuous graph neural network (GNN) framework. Unlike traditional continuous GNNs that rely on integer-order differential equations, FROND employs the Caputo fractional derivative to leverage the non-local properties of fractional calculus. This approach enables the capture of long-term dependencies in feature updates, moving beyond the Markovian update mechanisms in conventional integer-order models and offering enhanced capabilities in graph representation learning. We offer an interpretation of the node feature updating process in FROND from a non-Markovian random walk perspective when the feature updating is particularly governed by a diffusion process. We demonstrate analytically that oversmoothing can be mitigated in this setting. Experimentally, we validate the FROND framework by comparing the fractional adaptations of various established integer-order continuous GNNs, demonstrating their consistently improved performance and underscoring the framework's potential as an effective extension to enhance traditional continuous GNNs.},
  address = {Vienna, Austria},
  author = {Qiyu Kang and Kai Zhao and Qinxu Ding and Feng Ji and Xuhao Li and Wenfei Liang and Yang Song and Wee Peng Tay},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/kang2024unleashing.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-12-12},
  pdf = {https://openreview.net/pdf?id=wcka3bd7P4},
  publisher = {OpenReview.net},
  title = {Unleashing the Potential of Fractional Calculus in Graph Neural Networks with {FROND}},
  url = {https://openreview.net/forum?id=wcka3bd7P4},
  year = {2024}
}

@inproceedings{karami2024higen,
  abstract = {Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using separate neural networks. This modular approach enables scalable graph generation for large and complex graphs. We model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution. This enables us to generate community graphs with integer-valued edge weights in an autoregressive manner. Empirical studies demonstrate the effectiveness and scalability of our proposed generative model, achieving state-of-the-art performance in terms of graph quality across various benchmark datasets.},
  address = {Vienna, Austria},
  author = {Mahdi Karami},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/karami2024higen.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KNvubydSB5},
  publisher = {OpenReview.net},
  title = {{HiGen}: Hierarchical Graph Generative Networks},
  url = {https://openreview.net/forum?id=KNvubydSB5},
  year = {2024}
}

@inproceedings{karlas2024data,
  abstract = {When a machine learning (ML) model exhibits poor quality (e.g., poor accuracy or fairness), the problem can often be traced back to errors in the training data. Being able to discover the data examples that are the most likely culprits is a fundamental concern that has received a lot of attention recently. One prominent way to measure 'data importance' with respect to model quality is the Shapley value. Unfortunately, existing methods only focus on the ML model in isolation, without considering the broader ML pipeline for data preparation and feature extraction, which appears in the majority of real-world ML code. This presents a major limitation to applying existing methods in practical settings. In this paper, we propose Datascope, a method for efficiently computing Shapley-based data importance over ML pipelines. We introduce several approximations that lead to dramatic improvements in terms of computational speed. Finally, our experimental evaluation demonstrates that our methods are capable of data error discovery that is as effective as existing Monte Carlo baselines, and in some cases even outperform them. We release our code as an open-source data debugging library available at https://github.com/easeml/datascope.},
  address = {Vienna, Austria},
  author = {Bojan Karlas and David Dao and Matteo Interlandi and Sebastian Schelter and Wentao Wu and Ce Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/karlas2024data.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qxGXjWxabq},
  publisher = {OpenReview.net},
  title = {Data Debugging with {S}hapley Importance over Machine Learning Pipelines},
  url = {https://openreview.net/forum?id=qxGXjWxabq},
  year = {2024}
}

@inproceedings{karnioltambour2024modeling,
  abstract = {Understanding how multiple brain regions interact to produce behavior is a major challenge in systems neuroscience, with many regions causally implicated in common tasks such as sensory processing and decision making. A precise description of interactions between regions remains an open problem. Moreover, neural dynamics are nonlinear and non-stationary. Here, we propose MR-SDS, a multiregion, switching nonlinear state space model that decomposes global dynamics into local and cross-communication components in the latent space. MR-SDS includes directed interactions between brain regions, allowing for estimation of state-dependent communication signals, and accounts for sensory inputs effects. We show that our model accurately recovers latent trajectories, vector fields underlying switching nonlinear dynamics, and cross-region communication profiles in three simulations. We then apply our method to two large-scale, multi-region neural datasets involving mouse decision making. The first includes hundreds of neurons per region, recorded simultaneously at single-cell-resolution across 3 distant cortical regions. The second is a mesoscale widefield dataset of 8 adjacent cortical regions imaged across both hemispheres. On these multi-region datasets, our model outperforms existing piece-wise linear multi-region models and reveals multiple distinct dynamical states and a rich set of cross-region communication profiles.},
  address = {Vienna, Austria},
  author = {Orren Karniol-Tambour and David M. Zoltowski and E. Mika Diamanti and Lucas Pinto and Carlos D. Brody and David W. Tank and Jonathan W. Pillow},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/karnioltambour2024modeling.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WQwV7Y8qwa},
  publisher = {OpenReview.net},
  title = {Modeling state-dependent communication between brain regions with switching nonlinear dynamical systems},
  url = {https://openreview.net/forum?id=WQwV7Y8qwa},
  year = {2024}
}

@inproceedings{karthik2024visionbylanguage,
  abstract = {Given an image and a target modification (e.g an image of the Eiffel tower and the text 'without people and at night-time'), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art approaches in ZS-CIR still require training task-specific, customized models over large amounts of image-text pairs. In this work, we propose to tackle CIR in a training-free manner via our Compositional Image Retrieval through Vision-by-Language (CIReVL), a simple, yet human-understandable and scalable pipeline that effectively recombines large-scale VLMs with large language models (LLMs). By captioning the reference image using a pre-trained generative VLM and asking a LLM to recompose the caption based on the textual target modification for subsequent retrieval via e.g. CLIP, we achieve modular language reasoning. In four ZS-CIR benchmarks, we find competitive, in-part state-of-the-art performance - improving over supervised methods Moreover, the modularity of CIReVL offers simple scalability without re-training, allowing us to both investigate scaling laws and bottlenecks for ZS-CIR while easily scaling up to in parts more than double of previously reported results. Finally, we show that CIReVL makes CIR human-understandable by composing image and text in a modular fashion in the language domain, thereby making it intervenable, allowing to post-hoc re-align failure cases},
  address = {Vienna, Austria},
  author = {Shyamgopal Karthik and Karsten Roth and Massimiliano Mancini and Zeynep Akata},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/karthik2024visionbylanguage.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EDPxCjXzSb},
  publisher = {OpenReview.net},
  title = {Vision-by-Language for Training-Free Compositional Image Retrieval},
  url = {https://openreview.net/forum?id=EDPxCjXzSb},
  year = {2024}
}

@inproceedings{karwowski2024goodharts,
  abstract = {Implementing a reward function that perfectly captures a complex task in the real world is impractical. As a result, it is often appropriate to think of the reward function as a proxy for the true objective rather than as its definition. We study this phenomenon through the lens of Goodhart's law, which predicts that increasing optimisation of an imperfect proxy beyond some critical point decreases performance on the true objective. We propose a way to quantify the magnitude of this effect and show empirically that optimising an imperfect proxy reward often leads to the behaviour predicted by Goodhart's law for a wide range of environments and reward functions. We provide a geometric explanation for why Goodhart's law occurs in Markov decision processes. We then use these theoretical insights to propose an optimal early stopping method that provably avoids the aforementioned pitfall and derive theoretical regret bounds for this method. We also derive a training method that maximises worst-case reward, for the setting where there is uncertainty about the true reward function. Finally, we empirically analyse the degree to which our theoretical results transfer to neural policy gradient algorithms.},
  address = {Vienna, Austria},
  author = {Jacek Karwowski and Oliver Hayman and Xingjian Bai and Klaus Kiendlhofer and Charlie Griffin and Joar Max Viktor Skalse},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/karwowski2024goodharts.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5o9G4XF1LI},
  publisher = {OpenReview.net},
  title = {Goodhart's Law in Reinforcement Learning},
  url = {https://openreview.net/forum?id=5o9G4XF1LI},
  year = {2024}
}

@inproceedings{katzir2024noisefree,
  abstract = {Score Distillation Sampling (SDS) has emerged as the de facto approach for text-to-content generation in non-image domains. In this paper, we reexamine the SDS process and introduce a straightforward interpretation that demystifies the necessity for large Classifier-Free Guidance (CFG) scales, rooted in the distillation of an undesired noise term. Building upon our interpretation, we propose a novel Noise-Free Score Distillation (NFSD) process, which requires minimal modifications to the original SDS framework. Through this streamlined design, we achieve more effective distillation of pre-trained text-to-image diffusion models while using a nominal CFG scale. This strategic choice allows us to prevent the over-smoothing of results, ensuring that the generated data is both realistic and complies with the desired prompt. To demonstrate the efficacy of NFSD, we provide qualitative examples that compare NFSD and SDS, as well as several other methods.},
  address = {Vienna, Austria},
  author = {Oren Katzir and Or Patashnik and Daniel Cohen-Or and Dani Lischinski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/katzir2024noisefree.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dlIMcmlAdk},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Noise-free Score Distillation},
  url = {https://openreview.net/forum?id=dlIMcmlAdk},
  year = {2024}
}

@inproceedings{kaushik2024sourcefree,
  abstract = {We consider the problem of source-free unsupervised category-level pose estimation from only RGB images to a target domain without any access to source domain data or 3D annotations during adaptation. Collecting and annotating real-world 3D data and corresponding images is laborious, expensive, yet unavoidable process, since even 3D pose domain adaptation methods require 3D data in the target domain. We introduce 3DUDA, a method capable of adapting to a nuisance-ridden target domain without 3D or depth data. Our key insight stems from the observation that specific object subparts remain stable across out-of-domain (OOD) scenarios, enabling strategic utilization of these invariant subcomponents for effective model updates. We represent object categories as simple cuboid meshes, and harness a generative model of neural feature activations modeled at each mesh vertex learnt using differential rendering. We focus on individual locally robust mesh vertex features and iteratively update them based on their proximity to corresponding features in the target domain even when the global pose is not correct. Our model is then trained in an EM fashion, alternating between updating the vertex features and the feature extractor. We show that our method simulates fine-tuning on a global pseudo-labeled dataset under mild assumptions, which converges to the target domain asymptotically. Through extensive empirical validation, including a complex extreme UDA setup which combines real nuisances, synthetic noise, and occlusion, we demonstrate the potency of our simple approach in addressing the domain shift challenge and significantly improving pose estimation accuracy.},
  address = {Vienna, Austria},
  author = {Prakhar Kaushik and Aayush Mishra and Adam Kortylewski and Alan L. Yuille},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kaushik2024sourcefree.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UPvufoBAIs},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation},
  url = {https://openreview.net/forum?id=UPvufoBAIs},
  year = {2024}
}

@inproceedings{ke2024learning,
  abstract = {Large vision and language models learned directly through image-text associations often lack detailed visual substantiation, whereas image segmentation tasks are treated separately from recognition, supervisedly learned without interconnections. Our key observation is that, while an image can be recognized in multiple ways, each has a consistent part-and-whole visual organization. Segmentation thus should be treated not as an end task to be mastered through supervised learning, but as an internal process that evolves with and supports the ultimate goal of recognition. We propose to integrate a hierarchical segmenter into the recognition process, train and adapt the entire model solely on image-level recognition objectives. We learn hierarchical segmentation for free alongside recognition, automatically uncovering part-to-whole relationships that not only underpin but also enhance recognition. Enhancing the Vision Transformer (ViT) with adaptive segment tokens and graph pooling, our model surpasses ViT in unsupervised part-whole discovery, semantic segmentation, image classification, and efficiency. Notably, our model (trained on unlabeled 1M ImageNet images) outperforms SAM (trained on 11M images and 1 billion masks) by absolute 8% in mIoU on PartImageNet object segmentation.},
  address = {Vienna, Austria},
  author = {Tsung-Wei Ke and Sangwoo Mo and Stella X. Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ke2024learning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=IRcv4yFX6z},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Learning Hierarchical Image Segmentation For Recognition and By Recognition},
  url = {https://openreview.net/forum?id=IRcv4yFX6z},
  year = {2024}
}

@inproceedings{ke2024ccil,
  abstract = {We present a new technique to enhance the robustness of imitation learning methods by generating corrective data to account for compounding errors and disturbances. While existing methods rely on interactive expert labeling, additional offline datasets, or domain-specific invariances, our approach requires minimal additional assumptions beyond access to expert data. The key insight is to leverage local continuity in the environment dynamics to generate corrective labels. Our method first constructs a dynamics model from the expert demonstration, encouraging local Lipschitz continuity in the learned model. In locally continuous regions, this model allows us to generate corrective labels within the neighborhood of the demonstrations but beyond the actual set of states and actions in the dataset. Training on this augmented data enhances the agent's ability to recover from perturbations and deal with compounding errors. We demonstrate the effectiveness of our generated labels through experiments in a variety of robotics domains in simulation that have distinct forms of continuity and discontinuity, including classic control problems, drone flying, navigation with high-dimensional sensor observations, legged locomotion, and tabletop manipulation.},
  address = {Vienna, Austria},
  author = {Liyiming Ke and Yunchu Zhang and Abhay Deshpande and Siddhartha S. Srinivasa and Abhishek Gupta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ke2024ccil.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LQ6LQ8f4y8},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {CCIL: Continuity-Based Data Augmentation for Corrective Imitation Learning},
  url = {https://openreview.net/forum?id=LQ6LQ8f4y8},
  year = {2024}
}

@inproceedings{keller2024traveling,
  abstract = {Traveling waves of neural activity have been observed throughout the brain at a diversity of regions and scales; however, their precise computational role is still debated. One physically inspired hypothesis suggests that the cortical sheet may act like a wave-propagating system capable of invertibly storing a short-term memory of sequential stimuli through induced waves traveling across the cortical surface, and indeed many experimental results from neuroscience correlate wave activity with memory tasks. To date, however, the computational implications of this idea have remained hypothetical due to the lack of a simple recurrent neural network architecture capable of exhibiting such waves. In this work, we introduce a model to fill this gap, which we denote the Wave-RNN (wRNN), and demonstrate how such an architecture indeed efficiently encodes the recent past through a suite of synthetic memory tasks where wRNNs learn faster and reach significantly lower error than wave-free counterparts. We further explore the implications of this memory storage system on more complex sequence modeling tasks such as sequential image classification and find that wave-based models not only again outperform comparable wave-free RNNs while using significantly fewer parameters, but additionally perform comparably to more complex gated architectures such as LSTMs and GRUs.},
  address = {Vienna, Austria},
  author = {T. Anderson Keller and Lyle Muller and Terrence J. Sejnowski and Max Welling},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/keller2024traveling.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-11-14},
  pdf = {https://openreview.net/pdf?id=p4S5Z6Sah4},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Traveling Waves Encode The Recent Past and Enhance Sequence Learning},
  url = {https://openreview.net/forum?id=p4S5Z6Sah4},
  year = {2024}
}

@inproceedings{keramati2024conr,
  abstract = {Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. ConR discerns the disagreements between the label space and feature space and imposes a penalty on these disagreements. ConR addresses the continuous nature of label space with two main strategies in a contrastive manner: incorrect proximities are penalized proportionate to the label similarities and the correct ones are encouraged to model local similarities. ConR consolidates essential considerations into a generic, easy-to-integrate, and efficient method that effectively addresses deep imbalanced regression. Moreover, ConR is orthogonal to existing approaches and smoothly extends to uni- and multi-dimensional label spaces. Our comprehensive experiments show that ConR significantly boosts the performance of all the state-of-the-art methods on four large-scale deep imbalanced regression benchmarks.},
  address = {Vienna, Austria},
  author = {Mahsa Keramati and Lili Meng and R. David Evans},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/keramati2024conr.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RIuevDSK5V},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {ConR: Contrastive Regularizer for Deep Imbalanced Regression},
  url = {https://openreview.net/forum?id=RIuevDSK5V},
  year = {2024}
}

@inproceedings{kesen2024vilma,
  abstract = {With the ever-increasing popularity of pretrained Video-Language Models (VidLMs), there is a pressing need to develop robust evaluation methodologies that delve deeper into their visio-linguistic capabilities. To address this challenge, we present ViLMA (Video Language Model Assessment), a task-agnostic benchmark that places the assessment of fine-grained capabilities of these models on a firm footing. Task-based evaluations, while valuable, fail to capture the complexities and specific temporal aspects of moving images that VidLMs need to process. Through carefully curated counterfactuals, ViLMA offers a controlled evaluation suite that sheds light on the true potential of these models, as well as their performance gaps compared to human-level understanding. ViLMA also includes proficiency tests, which assess basic capabilities deemed essential to solving the main counterfactual tests. We show that current VidLMs' grounding abilities are no better than those of vision-language models which use static images. This is especially striking once the performance on proficiency tests is factored in. Our benchmark serves as a catalyst for future research on VidLMs, helping to highlight areas that still need to be explored.},
  address = {Vienna, Austria},
  author = {Ilker Kesen and Andrea Pedrotti and Mustafa Dogan and Michele Cafagna and Emre Can Acikgoz and Letitia Parcalabescu and Iacer Calixto and Anette Frank and Albert Gatt and Aykut Erdem and Erkut Erdem},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kesen2024vilma.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=liuqDwmbQJ},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models},
  url = {https://openreview.net/forum?id=liuqDwmbQJ},
  year = {2024}
}

@inproceedings{khajehabdollahi2024emergent,
  abstract = {Recurrent neural networks (RNNs) in the brain and in silico excel at solving tasks with intricate temporal dependencies. Long timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, œÑ, e.g., membrane time constant in biological neurons) or recurrent interactions among them (network-mediated timescale, œÑnet). However, the contribution of each mechanism for optimally solving memory-dependent tasks remains poorly understood.},
  address = {Vienna, Austria},
  author = {Sina Khajehabdollahi and Roxana Zeraati and Emmanouil Giannakakis and Tim Jakob Sch√§fer and Georg Martius and Anna Levina},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/khajehabdollahi2024emergent.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xwKt6bUkXj},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks},
  url = {https://openreview.net/forum?id=xwKt6bUkXj},
  year = {2024}
}

@inproceedings{khaki2024need,
  abstract = {We introduce the One-shot Pruning Technique for Interchangeable Networks (OPTIN) framework as a tool to increase the efficiency of pre-trained transformer architectures without requiring re-training. OPTIN leverages intermediate feature distillation to compress networks while maintaining competitive accuracy across various tasks. We achieve ‚â§2\% accuracy degradation in NLP baselines and 0.5\% improvement over state-of-the-art image classification methods, demonstrating generalizability across natural language, image classification, transfer learning, and semantic segmentation tasks.},
  address = {Vienna, Austria},
  author = {Samir Khaki and Konstantinos N. Plataniotis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/khaki2024need.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=MVmT6uQ3cQ},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {The Need for Speed: Pruning Transformers with One Recipe},
  url = {https://openreview.net/forum?id=MVmT6uQ3cQ},
  year = {2024}
}

@inproceedings{khandelwal2024large,
  abstract = {Shannon and Weaver's seminal information theory divides communication into three levels: technical, semantic, and effectiveness. While the technical level deals with the accurate reconstruction of transmitted symbols, the semantic and effectiveness levels deal with the inferred meaning and its effect on the receiver. Large Language Models (LLMs), with their wide generalizability, make some progress towards the second level. However, LLMs and other communication models are not conventionally designed for predicting and optimizing communication for desired receiver behaviors and intents. As a result, the effectiveness level remains largely untouched by modern communication systems. In this paper, we introduce the receivers' 'behavior tokens,' such as shares, likes, clicks, purchases, and retweets, in the LLM's training corpora to optimize content for the receivers and predict their behaviors. Other than showing similar performance to LLMs on content understanding tasks, our trained models show generalization capabilities on the behavior dimension for behavior simulation, content simulation, behavior understanding, and behavior domain adaptation. We show results on all these capabilities using a wide range of tasks on three corpora. We call these models Large Content and Behavior Models (LCBMs). Further, to spur more research on LCBMs, we release our new Content Behavior Corpus (CBC), a repository containing communicator, message, and corresponding receiver behavior.},
  address = {Vienna, Austria},
  author = {Ashmit Khandelwal and Aditya Agrawal and Aanisha Bhattacharyya and Yaman Kumar and Somesh Singh and Uttaran Bhattacharya and Ishita Dasgupta and Stefano Petrangeli and Rajiv Ratn Shah and Changyou Chen and Balaji Krishnamurthy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/khandelwal2024large.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=TrKq4Wlwcz},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior},
  url = {https://openreview.net/forum?id=TrKq4Wlwcz},
  year = {2024}
}

@inproceedings{khani2024slime,
  abstract = {Significant strides have been made using large vision-language models, like Stable Diffusion ({SD}), for a variety of downstream tasks, including image generation, image editing, and {3D} shape generation. Inspired by these advancements, we explore leveraging these vision-language models for segmenting images at any desired granularity using as few as one annotated sample.},
  author = {Aliasghar Khani and Saeid Asgari Taghanaki and Aditya Sanghi and Ali Mahdavi-Amiri and Ghassan Hamarneh},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/khani2024slime.pdf:pdf},
  keywords = {one-shot segmentation, computer vision, text-to-image models, stable diffusion, cross attention},
  note = {Poster presentation. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7FeIRqCedv},
  publisher = {OpenReview.net},
  title = {{SLiMe}: Segment Like Me},
  url = {https://openreview.net/forum?id=7FeIRqCedv},
  year = {2024}
}

@inproceedings{khanna2024diffusionsat,
  abstract = {Diffusion models have achieved state-of-the-art results across various modalities, but existing models are not tailored for remote sensing data. Satellite images differ significantly from natural images -- being multi-spectral and irregularly sampled. The paper introduces {DiffusionSat}, the largest generative foundation model trained on a collection of publicly available large, high-resolution remote sensing datasets.},
  author = {Samar Khanna and Patrick Liu and Linqi Zhou and Chenlin Meng and Robin Rombach and Marshall Burke and David B. Lobell and Stefano Ermon},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/khanna2024diffusionsat.pdf:pdf},
  keywords = {satellite images, generative models, diffusion models, computer vision},
  note = {Poster presentation. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=I5webNFDgQ},
  publisher = {OpenReview.net},
  title = {{DiffusionSat}: A Generative Foundation Model for Satellite Imagery},
  url = {https://openreview.net/forum?id=I5webNFDgQ},
  year = {2024}
}

@inproceedings{khanov2024args,
  abstract = {Aligning large language models with human objectives is paramount, yet common approaches including {RLHF} suffer from unstable and resource-intensive training. In response to this challenge, we introduce {ARGS}, Alignment as Reward-Guided Search, a novel framework that integrates alignment into the decoding process, eliminating the need for expensive {RL} training. By adjusting the model's probabilistic predictions using a reward signal, {ARGS} generates texts with semantic diversity while being aligned with human preferences, offering a promising and flexible solution for aligning language models.},
  author = {Maxim Khanov and Jirayu Burapacheep and Yixuan Li},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/khanov2024args.pdf:pdf},
  keywords = {alignment, {LLM}},
  note = {Poster presentation. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=shgx0eqdw6},
  publisher = {OpenReview.net},
  title = {{ARGS}: Alignment as Reward-Guided Search},
  url = {https://openreview.net/forum?id=shgx0eqdw6},
  year = {2024}
}

@inproceedings{khattab2024dspy,
  abstract = {The paper discusses a new programming model called {DSPy} that aims to systematically develop and optimize language model pipelines. Addresses limitations of current hard-coded "prompt templates" and introduces {DSPy} as a programming model that abstracts {LM} pipelines as text transformation graphs. Enables parameterized modules that can learn prompting, finetuning, and reasoning techniques.},
  author = {Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan and Saiful Haq and Ashutosh Sharma and Thomas T. Joshi and Hanna Moazam and Heather Miller and Matei Zaharia and Christopher Potts},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/khattab2024dspy.pdf:pdf},
  keywords = {programming models, prompting techniques, in-context learning, few-shot learning, chain of thought, multi-hop reasoning, language agents},
  note = {Spotlight presentation. {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=sY5N0zY5Od},
  publisher = {OpenReview.net},
  title = {{DSPy}: Compiling Declarative Language Model Calls into State-of-the-Art Pipelines},
  url = {https://openreview.net/forum?id=sY5N0zY5Od},
  year = {2024}
}

@inproceedings{khodak2024learning,
  abstract = {The paper discusses solving linear systems and proposes a method for sequentially choosing solver parameters. Focuses on Successive Over-Relaxation ({SOR}) solver and uses bandit online learning to select parameters. Aims to approach the best fixed œâ as the sequence length increases.},
  author = {Mikhail Khodak and Edmond Chow and Maria-Florina Balcan and Ameet Talwalkar},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/khodak2024learning.pdf:pdf},
  keywords = {scientific computing, data-driven algorithm design, online learning, multi-armed bandits, contextual bandits, numerical analysis, learning-augmented algorithms, algorithms with predictions},
  note = {Spotlight presentation. {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=5t57omGVMw},
  publisher = {OpenReview.net},
  title = {Learning to Relax: Setting Solver Parameters Across a Sequence of Linear System Instances},
  url = {https://openreview.net/forum?id=5t57omGVMw},
  year = {2024}
}

@inproceedings{khromov2024some,
  abstract = {Lipschitz continuity is a crucial functional property of any predictive model, that naturally governs its robustness, generalisation, as well as adversarial vulnerability. Contrary to other works that focus on obtaining tighter bounds and developing different practical strategies to enforce certain Lipschitz properties, we aim to thoroughly examine and characterise the Lipschitz behaviour of Neural Networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, datasets, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. As a highlight of this investigation, we showcase a remarkable fidelity of the lower Lipschitz bound, identify a striking Double Descent trend in both upper and lower bounds to the Lipschitz and explain the intriguing effects of label noise on function smoothness and generalisation.},
  author = {Grigory Khromov and Sidak Pal Singh},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/khromov2024some.pdf:pdf},
  keywords = {Lipschitz continuity, Double Descent, Label Noise, Generalization},
  note = {Poster presentation. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5jWsW08zUh},
  publisher = {OpenReview.net},
  title = {Some Fundamental Aspects about Lipschitz Continuity of Neural Networks},
  url = {https://openreview.net/forum?id=5jWsW08zUh},
  year = {2024}
}

@inproceedings{kiani2024hardness,
  abstract = {We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries ("equivariance") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query ({CSQ}) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative.},
  author = {Bobak T. Kiani and Thien Le and Hannah Lawrence and Stefanie Jegelka and Melanie Weber},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/kiani2024hardness.pdf:pdf},
  keywords = {Equivariance, statistical query, lower bound, computational hardness, invariance, symmetry, neural networks},
  note = {Spotlight presentation. {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ARPrtuzAnQ},
  publisher = {OpenReview.net},
  title = {On the hardness of learning under symmetries},
  url = {https://openreview.net/forum?id=ARPrtuzAnQ},
  year = {2024}
}

@inproceedings{kim2024hierarchical,
  abstract = {We propose a novel hierarchical Bayesian model for the few-shot meta learning problem. We consider episode-wise random variables to model episode-specific generative processes, where these local random variables are governed by a higher-level global random variable. The global variable captures information shared across episodes, while controlling how much the model needs to be adapted to new episodes in a principled Bayesian manner. Within our framework, prediction on a novel episode/task can be seen as a Bayesian inference problem.},
  author = {Minyoung Kim and Timothy M. Hospedales},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/kim2024hierarchical.pdf:pdf},
  keywords = {few-shot learning, meta learning, hierarchical Bayesian models, Bayesian inference},
  note = {Poster presentation. {DBLP} last modified: 2024-12-23},
  pdf = {https://openreview.net/pdf?id=mQ72XRfYRZ},
  publisher = {OpenReview.net},
  title = {A Hierarchical Bayesian Model for Few-Shot Meta Learning},
  url = {https://openreview.net/forum?id=mQ72XRfYRZ},
  year = {2024}
}

@inproceedings{kim2024confidenceaware,
  abstract = {Fine-tuning text-to-image models with reward functions trained on human feedback data has proven effective for aligning model behavior with human intent. However, excessive optimization with such reward models, which serve as mere proxy objectives, can compromise the performance of fine-tuned models, a phenomenon known as reward overoptimization. To investigate this issue in depth, we introduce the Text-Image Alignment Assessment ({TIA2}) benchmark, which comprises a diverse collection of text prompts, images, and human annotations. We propose {TextNorm}, a simple method that enhances alignment based on a measure of reward model confidence estimated across a set of semantically contrastive text prompts.},
  author = {Kyuyoung Kim and Jongheon Jeong and Minyong An and Mohammad Ghavamzadeh and Krishnamurthy Dj Dvijotham and Jinwoo Shin and Kimin Lee},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/kim2024confidenceaware.pdf:pdf},
  keywords = {text-to-image models, reward optimization, human feedback, alignment assessment},
  note = {Poster presentation. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Let8OMe20n},
  publisher = {OpenReview.net},
  title = {Confidence-aware Reward Optimization for Fine-tuning Text-to-Image Models},
  url = {https://openreview.net/forum?id=Let8OMe20n},
  year = {2024}
}

@inproceedings{kim2024srgb,
  abstract = {Noise poses a widespread challenge in signal processing, particularly when it comes to denoising images. Although convolutional neural networks ({CNNs}) have exhibited remarkable success in this field, they are predicated upon the belief that noise follows established distributions, which restricts their practicality when dealing with real-world noise. To overcome this limitation, several efforts have been taken to collect noisy image datasets from the real world. Generative methods, employing techniques such as generative adversarial networks ({GANs}) and normalizing flows ({NFs}), have emerged as a solution for generating realistic noisy images. The paper addresses the challenge of modeling real-world noise in {sRGB} images using normalizing flows, with a focus on noise-aware sampling that doesn't require camera metadata during the sampling phase.},
  author = {Dongjin Kim and Donggoo Jung and Sungyong Baik and Tae Hyun Kim},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024), Vienna, Austria},
  file = {:/home/b/documents/inproceedings/kim2024srgb.pdf:pdf},
  keywords = {{sRGB} noise modeling, normalizing flows, noise-aware sampling, image denoising},
  note = {Poster presentation. {DBLP} last modified: 2024-11-14},
  pdf = {https://openreview.net/pdf?id=2XBBumBGeP},
  publisher = {OpenReview.net},
  title = {sRGB Real Noise Modeling via Noise-Aware Sampling with Normalizing Flows},
  url = {https://openreview.net/forum?id=2XBBumBGeP},
  year = {2024}
}

@inproceedings{kim2024fixed,
  abstract = {Fixed classifiers in neural networks for classification problems have demonstrated cost efficiency and even outperformed learnable classifiers in some popular benchmarks when incorporating orthogonality. Despite these advantages, prior research has yet to investigate the training dynamics of fixed orthogonal classifiers on neural collapse, a recently clarified phenomenon that last-layer features converge to a specific form, called simplex ETF, in training classification models involving the post-zero-error phase. Ensuring this phenomenon is critical for obtaining global optimality in a layer-peeled model, potentially leading to enhanced performance in practice. However, fixed orthogonal classifiers cannot invoke neural collapse due to their geometric limitations. To overcome the limits, we analyze a zero-mean neural collapse considering the orthogonality in non-negative Euclidean space. Then, we propose a fixed non-negative orthogonal classifier that induces the optimal solution and maximizes the margin of an orthogonal layer-peeled model by satisfying the properties of zero-mean neural collapse. Building on this foundation, we exploit a feature dimension separation effect inherent in our classifier for further purposes: (1) enhances softmax masking by mitigating feature interference in continual learning and (2) tackles the limitations of mixup on the hypersphere in imbalanced learning. We conducted comprehensive experiments on various datasets and architectures and demonstrated significant performance improvements.},
  author = {Hoyong Kim and Kangil Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024fixed.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=F4bmOrmUwc},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {Fixed Non-negative Orthogonal Classifier: Inducing Zero-mean Neural Collapse with Feature Dimension Separation},
  url = {https://openreview.net/forum?id=F4bmOrmUwc},
  year = {2024}
}

@inproceedings{kim2024hypeboy,
  abstract = {Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple nodes with hyperedges, and better capturing the topology is essential for effective representation learning. Recent advances in generative self-supervised learning (SSL) suggest that hypergraph neural networks (HNNs) learned from generative self-supervision have the potential to effectively encode the complex hypergraph topology. Designing a generative SSL strategy for hypergraphs, however, is not straightforward. Questions remain with regard to its generative SSL task, connection to downstream tasks, and empirical properties of learned representations. In light of the promises and challenges, we propose a novel generative SSL strategy for hypergraphs. We first formulate a generative SSL task on hypergraphs, hyperedge filling, and highlight its theoretical connection to node classification. Based on the generative SSL task, we propose a hypergraph SSL method, HYPEBOY. HYPEBOY learns effective general-purpose hypergraph representations, outperforming 15 baseline methods across 11 benchmark datasets. To our knowledge, this is the first study on generative SSL on hypergraphs, and we demonstrate its theoretical and empirical strengths for hypergraph representation learning.},
  author = {Sunwoo Kim and Shinhwan Kang and Fanchen Bu and Soo Yong Lee and Jaemin Yoo and Kijung Shin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024hypeboy.pdf:pdf},
  note = {DBLP last modified: 2025-05-21},
  pdf = {https://openreview.net/pdf?id=DZUzOKE6og},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {{HypeBoy}: Generative Self-Supervised Representation Learning on Hypergraphs},
  url = {https://openreview.net/forum?id=DZUzOKE6og},
  year = {2024}
}

@inproceedings{kim2024t,
  abstract = {The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of KL divergence between two statistical manifolds and replacing with $\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-density regions when trained on heavy-tailed synthetic data. Furthermore, we show that $t^3$VAE significantly outperforms other models on CelebA and imbalanced CIFAR-100 datasets.},
  author = {Juno Kim and Jaehyuk Kwon and Mincheol Cho and Hyunjong Lee and Joong-Ho Won},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024t.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RzNlECeoOB},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {{\$t\textasciicircum{}3\$}-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence},
  url = {https://openreview.net/forum?id=RzNlECeoOB},
  year = {2024}
}

@inproceedings{kim2024unpaired,
  abstract = {Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. While diffusion models have achieved remarkable progress, they have limitations in unpaired image-to-image (I2I) translation tasks due to the Gaussian prior assumption. Schr{\"o}dinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. Yet, to our best knowledge, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose Unpaired Neural Schr{\"o}dinger Bridge (UNSB), which expresses the SB problem as a sequence of adversarial learning problems. This allows us to incorporate advanced discriminators and regularization to learn a SB between unpaired data. We show that UNSB is scalable and successfully solves various unpaired I2I translation tasks.},
  author = {Beomsu Kim and Gihyun Kwon and Kwanyoung Kim and Jong Chul Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024unpaired.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uQBW7ELXfO},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {Unpaired Image-to-Image Translation via Neural {S}chr{\"o}dinger Bridge},
  url = {https://openreview.net/forum?id=uQBW7ELXfO},
  year = {2024}
}

@inproceedings{kim2024instructive,
  abstract = {While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a noisy instruction. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like 'opposite' that elicit the deviated responses. Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessitating any additional parameter updates. Notably, utilizing 'opposite' as the noisy instruction in ID, which shows the maximum divergence from the original instruction, consistently produces the most significant performance gains across multiple models and tasks.},
  author = {Taehyeon Kim and Joonkee Kim and Gihun Lee and Se-Young Yun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024instructive.pdf:pdf},
  note = {DBLP last modified: 2024-11-04},
  pdf = {https://openreview.net/pdf?id=LebzzClHYw},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {Instructive Decoding: Instruction-Tuned Large Language Models are Self-Refiner from Noisy Instructions},
  url = {https://openreview.net/forum?id=LebzzClHYw},
  year = {2024}
}

@inproceedings{kim2024stream,
  abstract = {Image generative models have made significant progress in generating realistic and diverse images, supported by comprehensive guidance from various evaluation metrics. However, current video generative models struggle to generate even short video clips, with limited tools that provide insights for improvements. Current video evaluation metrics are simple adaptations of image metrics by switching the embeddings with video embedding networks, which may underestimate the unique characteristics of video. Our analysis reveals that the widely used Frechet Video Distance (FVD) has a stronger emphasis on the spatial aspect than the temporal naturalness of video and is inherently constrained by the input size of the embedding networks used, limiting it to 16 frames. Additionally, it demonstrates considerable instability and diverges from human evaluations. To address the limitations, we propose STREAM, a new video evaluation metric uniquely designed to independently evaluate spatial and temporal aspects. This feature allows comprehensive analysis and evaluation of video generative models from various perspectives, unconstrained by video length. We provide analytical and experimental evidence demonstrating that STREAM provides an effective evaluation tool for both visual and temporal quality of videos, offering insights into area of improvement for video generative models. To the best of our knowledge, STREAM is the first evaluation metric that can separately assess the temporal and spatial aspects of videos.},
  author = {Pum Jun Kim and Seojun Kim and Jaejun Yoo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024stream.pdf:pdf},
  note = {DBLP last modified: 2024-11-14},
  pdf = {https://openreview.net/pdf?id=7JfKCZQPxJ},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {{STREAM}: Spatio-{T}empo{R}al Evaluation and Analysis Metric for Video Generative Models},
  url = {https://openreview.net/forum?id=7JfKCZQPxJ},
  year = {2024}
}

@inproceedings{kim2024scalable,
  abstract = {In this research, we focus on the problem of learning monotonic neural networks, as preserving the monotonicity of a model with respect to a subset of inputs is crucial for practical applications across various domains. Although several methods have recently been proposed to address this problem, they have limitations such as not guaranteeing monotonicity in certain cases, requiring additional inference time, lacking scalability with increasing network size and number of monotonic inputs, and manipulating network weights during training. To overcome these limitations, we introduce a simple but novel architecture of the partially connected network which incorporates a 'scalable monotonic hidden layer' comprising three units: the exponentiated unit, ReLU unit, and confluence unit. This allows for the repetitive integration of the scalable monotonic hidden layers without other structural constraints. Consequently, our method offers ease of implementation and rapid training through the conventional error-backpropagation algorithm. We accordingly term this method as Scalable Monotonic Neural Networks (SMNN). Numerical experiments demonstrated that our method achieved comparable prediction accuracy to the state-of-the-art approaches while effectively addressing the aforementioned weaknesses.},
  author = {Hyunho Kim and Jong-Seok Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024scalable.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DjIsNDEOYX},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {Scalable Monotonic Neural Networks},
  url = {https://openreview.net/forum?id=DjIsNDEOYX},
  year = {2024}
}

@inproceedings{kim2024clamtts,
  abstract = {With the emergence of neural audio codecs, which encode multiple streams of discrete tokens from audio, large language models have recently gained attention as a promising approach for zero-shot Text-to-Speech (TTS) synthesis. Despite the ongoing rush towards scaling paradigms, audio tokenization ironically amplifies the scalability challenge, stemming from its long sequence length and the complexity of modelling the multiple sequences. To mitigate these issues, we present CLaM-TTS that employs a probabilistic residual vector quantization to (1) achieve superior compression in the token length, and (2) allow a language model to generate multiple tokens at once, thereby eliminating the need for cascaded modeling to handle the number of token streams. Our experimental results demonstrate that CLaM-TTS is better than or comparable to state-of-the-art neural codec-based TTS models regarding naturalness, intelligibility, speaker similarity, and inference speed. In addition, we examine the impact of the pretraining extent of the language models and their text tokenization strategies on performances.},
  author = {Jaehyeon Kim and Keon Lee and Seungjun Chung and Jaewoong Cho},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024clamtts.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ofzeypWosV},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {CLaM-TTS: Improving Neural Codec Language Model for Zero-Shot Text-to-Speech},
  url = {https://openreview.net/forum?id=ofzeypWosV},
  year = {2024}
}

@inproceedings{kim2024decision,
  abstract = {The recent success of Transformer in natural language processing has sparked its use in various domains. In offline reinforcement learning (RL), Decision Transformer (DT) is emerging as a promising model based on Transformer. However, we discovered that the attention module of DT is not appropriate to capture the inherent local dependence pattern in trajectories of RL modeled as a Markov decision process. To overcome the limitations of DT, we propose a novel action sequence predictor, named Decision ConvFormer (DC), based on the architecture of MetaFormer, which is a general structure to process multiple entities in parallel and understand the interrelationship among the multiple entities. DC employs local convolution filtering as the token mixer and can effectively capture the inherent local associations of the RL dataset. In extensive experiments, DC achieved state-of-the-art performance across various standard RL benchmarks while requiring fewer resources. Furthermore, we show that DC better understands the underlying meaning in data and exhibits enhanced generalization capability.},
  author = {Jeonghye Kim and Suyoung Lee and Woojun Kim and Youngchul Sung},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024decision.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=af2c8EaKl8},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {Decision ConvFormer: Local Filtering in MetaFormer is Sufficient for Decision Making},
  url = {https://openreview.net/forum?id=af2c8EaKl8},
  year = {2024}
}

@inproceedings{kim2024consistency,
  abstract = {Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64x64 resolution (FID 1.92).},
  author = {Dongjun Kim and Chieh-Hsin Lai and Wei-Hsiang Liao 0001 and Naoki Murata and Yuhta Takida and Toshimitsu Uesaka and Yutong He and Yuki Mitsufuji and Stefano Ermon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024consistency.pdf:pdf},
  note = {DBLP last modified: 2025-06-10},
  pdf = {https://openreview.net/pdf?id=ymjI8feDTD},
  publisher = {OpenReview.net},
  series = {ICLR '24},
  title = {Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion},
  url = {https://openreview.net/forum?id=ymjI8feDTD},
  year = {2024}
}

@inproceedings{kim2024minimum,
  abstract = {The paper investigates the minimum network width required for universal approximation, showing that for {R}e{LU}-like activation functions on a compact domain [0,1]^d_x, the minimum width is exactly max\{d_x, d_y, 2\}. This differs from previous results for unbounded domains and reveals a dichotomy between L^p and uniform approximations.},
  address = {Vienna, Austria},
  author = {Namjun Kim and Chanho Min and Sejun Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024minimum.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dpDw5U04SU},
  publisher = {OpenReview.net},
  title = {Minimum width for universal approximation using {R}e{LU} networks on compact domain},
  url = {https://openreview.net/forum?id=dpDw5U04SU},
  year = {2024}
}

@inproceedings{kim2024s,
  abstract = {Large language models ({LLM}s) have made significant advancements in various natural language processing tasks, including question answering. While incorporating new information with the retrieval of relevant passages is promising, existing methods often require additional fine-tuning. The authors propose {S}u{R}e, a framework to enhance open-domain {QA} by generating summaries of retrieved passages and evaluating answer candidates, demonstrating improvements of up to 4.6\% in exact match and 4.0\% in {F}1 score over standard prompting approaches.},
  address = {Vienna, Austria},
  author = {Jaehyung Kim and Jaehyun Nam and Sangwoo Mo and Jongjin Park and Sang-Woo Lee and Minjoon Seo and Jung-Woo Ha and Jinwoo Shin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024s.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=w4DW6qkRmt},
  publisher = {OpenReview.net},
  title = {{S}u{R}e: Summarizing Retrievals using Answer Candidates for Open-domain {QA} of {LLM}s},
  url = {https://openreview.net/forum?id=w4DW6qkRmt},
  year = {2024}
}

@inproceedings{kim2024training,
  abstract = {The paper addresses dataset bias in diffusion models by proposing time-dependent importance reweighting to mitigate bias. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. The authors introduce a time-dependent density ratio that is more precise than previous approaches and develop a method to use density ratio for both reweighting and score correction, creating a tractable form of the objective function to regenerate the unbiased data density. Experimental validation on {CIFAR}-10, {CIFAR}-100, {FFHQ}, and {C}eleb{A} demonstrates performance improvements over time-independent importance reweighting baselines.},
  address = {Vienna, Austria},
  author = {Yeongmin Kim and Byeonghu Na and Minsang Park and JoonHo Jang and Dongjun Kim and Wanmo Kang and Il-Chul Moon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024training.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=39cPKijBed},
  publisher = {OpenReview.net},
  title = {Training Unbiased Diffusion Models From Biased Dataset},
  url = {https://openreview.net/forum?id=39cPKijBed},
  year = {2024}
}

@inproceedings{kim2024prometheus,
  abstract = {Prometheus is a fully open-source {LLM} that is on par with {GPT}-4's evaluation capabilities when appropriate reference materials (reference answer, score rubric) are provided. The researchers trained Prometheus, a 13B evaluator {LLM} that can assess any given long-form text based on customized score rubric provided by the user. They constructed the Feedback Collection, a new dataset consisting of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by {GPT}-4. During human evaluation with hand-crafted score rubrics, Prometheus shows a Pearson correlation of 0.897 with human evaluators, which is on par with {GPT}-4-0613 (0.882), and greatly outperforms {C}hat{GPT} (0.392).},
  address = {Vienna, Austria},
  author = {Seungone Kim and Jamin Shin and Yejin Choi and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024prometheus.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8euJaTveKw},
  publisher = {OpenReview.net},
  title = {Prometheus: Inducing Fine-Grained Evaluation Capability in Language Models},
  url = {https://openreview.net/forum?id=8euJaTveKw},
  year = {2024}
}

@inproceedings{kim2024online,
  abstract = {In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. The authors argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. They propose two continual learning setups for embodied agents: learning new behaviors (Behavior Incremental Learning, Behavior-{IL}) and new environments (Environment Incremental Learning, Environment-{IL}). They introduce {CL}-{ALFRED}, a benchmark that continuously learns new types of behaviors and environments for household tasks, and propose to update stored information based on confidence scores without task boundary information during training (i.e., task-free).},
  address = {Vienna, Austria},
  author = {Byeonghwi Kim and Minhyuk Seo and Jonghyun Choi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024online.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7M0EzjugaN},
  publisher = {OpenReview.net},
  title = {Online Continual Learning for Interactive Instruction Following Agents},
  url = {https://openreview.net/forum?id=7M0EzjugaN},
  year = {2024}
}

@inproceedings{kim2024adaptive,
  abstract = {Federated learning ({FL}) is a distributed machine learning framework where the global model of a central server is trained via multiple collaborative steps by participating clients without sharing their data. While being a flexible framework, where the distribution of local data, participation rate, and computing power of each client can greatly vary, such flexibility gives rise to many new challenges, especially in the hyperparameter tuning on the client side. The authors propose Œî-{SGD}, a simple step size rule for {SGD} that enables each client to use its own step size by adapting to the local smoothness of the function each client is optimizing.},
  address = {Vienna, Austria},
  author = {Junhyung Lyle Kim and Mohammad Taha Toghani and C√©sar A. Uribe and Anastasios Kyrillidis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024adaptive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=g0mlwqs8pi},
  publisher = {OpenReview.net},
  title = {Adaptive Federated Learning with Auto-Tuned Clients},
  url = {https://openreview.net/forum?id=g0mlwqs8pi},
  year = {2024}
}

@inproceedings{kim2024fast,
  abstract = {The Deep Ensemble approach is a straightforward technique used to enhance the performance of deep neural networks by training them from different initial points, converging towards various local optima. However, a limitation of this methodology lies in its high computational overhead for inference, arising from the necessity to store numerous learned parameters and execute individual forward passes for each parameter during the inference stage. To address this challenge, the authors propose a novel approach called Diffusion Bridge Network ({DBN}). Based on the theory of the {S}chr√∂dinger bridge, this method directly learns to simulate a Stochastic Differential Equation ({SDE}) that connects the output distribution of a single ensemble member to the output distribution of the ensembled model, allowing them to obtain ensemble prediction without having to invoke forward passes for multiple models.},
  address = {Vienna, Austria},
  author = {Hyunsu Kim and Jongmin Yoon and Juho Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024fast.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Mgq6kxl115},
  publisher = {OpenReview.net},
  title = {Fast Ensembling with Diffusion {S}chr√∂dinger Bridge},
  url = {https://openreview.net/forum?id=Mgq6kxl115},
  year = {2024}
}

@inproceedings{kim2024local,
  abstract = {Generative Flow Networks ({GF}low{N}ets) are amortized sampling methods that learn a distribution over discrete objects proportional to their rewards. {GF}low{N}ets exhibit a remarkable ability to generate diverse samples, yet occasionally struggle to consistently produce samples with high rewards due to over-exploration on wide sample space. This paper proposes to train {GF}low{N}ets with local search, which focuses on exploiting high-rewarded sample space to resolve this issue. The main idea is to explore the local neighborhood via backtracking and reconstruction guided by backward and forward policies, respectively. Extensive experiments demonstrate a remarkable performance improvement in several biochemical tasks, suggesting that combining the inter-mode exploration capabilities of {GF}low{N}ets and intra-mode exploration through local search methods is a powerful paradigm.},
  address = {Vienna, Austria},
  author = {Minsu Kim and Taeyoung Yun and Emmanuel Bengio and Dinghuai Zhang and Yoshua Bengio and Sungsoo Ahn and Jinkyoo Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024local.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-06-27},
  pdf = {https://openreview.net/pdf?id=6cFcw1Rxww},
  publisher = {OpenReview.net},
  title = {Local Search {GF}low{N}ets},
  url = {https://openreview.net/forum?id=6cFcw1Rxww},
  year = {2024}
}

@inproceedings{kim2024adaptive,
  abstract = {Scene graph generation ({SGG}) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. The authors introduce a Self-Training framework for {SGG} ({ST}-{SGG}) that assigns pseudo-labels for unannotated triplets based on which the {SGG} models are trained. They propose a novel pseudo-labeling technique for {SGG}, called Class-specific Adaptive Thresholding with Momentum ({CATM}), which is a model-agnostic framework that can be applied to any existing {SGG} models. They also devise a graph structure learner ({GSL}) that is beneficial when adopting their proposed self-training framework to the state-of-the-art message-passing neural network ({MPNN})-based {SGG} models. Extensive experiments verify the effectiveness of {ST}-{SGG} on various {SGG} models, particularly in enhancing the performance on fine-grained predicate classes.},
  address = {Vienna, Austria},
  author = {Kibum Kim and Kanghoon Yoon and Yeonjun In and Jinyoung Moon and Donghyun Kim and Chanyoung Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024adaptive.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-03},
  pdf = {https://openreview.net/pdf?id=WipsLtH77t},
  publisher = {OpenReview.net},
  title = {Adaptive Self-training Framework for Fine-grained Scene Graph Generation},
  url = {https://openreview.net/forum?id=WipsLtH77t},
  year = {2024}
}

@inproceedings{kim2024symmetric,
  abstract = {This paper extends mean-field {L}angevin dynamics to minimax optimization over probability distributions for the first time with symmetric and provably convergent updates. The authors propose two main algorithms: Mean-field {L}angevin averaged gradient ({MFL}-{AG}), a single-loop algorithm that implements gradient descent ascent in the distribution spaces with a novel weighted averaging and establishes average-iterate convergence to the mixed {N}ash equilibrium; and Mean-field {L}angevin anchored best response ({MFL}-{ABR}), a symmetric double-loop algorithm based on best response dynamics with linear last-iterate convergence. They study both time and particle discretization regimes and prove a new uniform-in-time propagation of chaos result which accounts for the dependency of the particle interactions on all previous distributions. Applications to zero-sum {M}arkov games demonstrate long-term optimality.},
  address = {Vienna, Austria},
  author = {Juno Kim and Kakei Yamamoto and Kazusato Oko and Zhuoran Yang and Taiji Suzuki},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024symmetric.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=YItWKZci78},
  publisher = {OpenReview.net},
  title = {Symmetric Mean-field {L}angevin Dynamics for Distributional Minimax Problems},
  url = {https://openreview.net/forum?id=YItWKZci78},
  year = {2024}
}

@inproceedings{kim2024compressed,
  abstract = {This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments.},
  address = {Vienna, Austria},
  author = {Jang-Hyun Kim and Junyoung Yeom and Sangdoo Yun and Hyun Oh Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024compressed.pdf:pdf},
  keywords = {context compression, efficient inference, natural language processing, transformer},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=64kSvC4iPg},
  publisher = {OpenReview.net},
  title = {Compressed Context Memory for Online Language Model Interaction},
  url = {https://openreview.net/forum?id=64kSvC4iPg},
  year = {2024}
}

@inproceedings{kim2024retasa,
  abstract = {The presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications. This work focuses on the target shift problem in a regression setting where the target variable $y$, which is continuous, has different marginal distributions in the training source and testing domain, while the conditional distribution of features $\boldsymbol{x}$ given $y$ remains the same. We propose ReTaSA to address the challenge of continuous target shift by estimating an importance weight function through an ill-posed integral equation.},
  address = {Vienna, Austria},
  author = {Hwanwoo Kim and Xin Zhang and Jiwei Zhao and Qinglong Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kim2024retasa.pdf:pdf},
  keywords = {label shift, target shift, distributional shift, domain adaptation, transfer learning, importance weight},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KdVvOA00Or},
  publisher = {OpenReview.net},
  title = {{ReTaSA}: {A} Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift},
  url = {https://openreview.net/forum?id=KdVvOA00Or},
  year = {2024}
}

@inproceedings{kini2024symmetric,
  abstract = {Supervised contrastive loss (SCL) is a competitive and often superior alternative to the cross-entropy loss for classification. While prior studies have demonstrated that both losses yield symmetric training representations under balanced data, this symmetry breaks under class imbalances. This paper presents an intriguing discovery: the introduction of a ReLU activation at the final layer effectively restores the symmetry in SCL-learned representations.},
  address = {Vienna, Austria},
  author = {Ganesh Ramachandra Kini and Vala Vakilian and Tina Behnia and Jaidev Gill and Christos Thrampoulidis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kini2024symmetric.pdf:pdf},
  keywords = {supervised contrastive learning, neural collapse, implicit bias, class imbalance},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AyXIDfvYg8},
  publisher = {OpenReview.net},
  title = {Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of {ReLU} and Batching},
  url = {https://openreview.net/forum?id=AyXIDfvYg8},
  year = {2024}
}

@inproceedings{kirchenbauer2024reliability,
  abstract = {As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. This paper investigates the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after paraphrasing, with high-confidence detection possible after observing around 800 tokens.},
  address = {Vienna, Austria},
  author = {John Kirchenbauer and Jonas Geiping and Yuxin Wen and Manli Shu and Khalid Saifullah and Kezhi Kong and Kasun Fernando and Aniruddha Saha and Micah Goldblum and Tom Goldstein},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kirchenbauer2024reliability.pdf:pdf},
  keywords = {machine learning, {LLM}, watermark, language model, natural language processing, generative {AI}},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DEJIDCmWOz},
  publisher = {OpenReview.net},
  title = {On the Reliability of Watermarks for Large Language Models},
  url = {https://openreview.net/forum?id=DEJIDCmWOz},
  year = {2024}
}

@inproceedings{kirilenko2024objectcentric,
  abstract = {Object-centric architectures usually apply a differentiable module to the entire feature map to decompose it into sets of entity representations called slots. Some of these methods structurally resemble clustering algorithms, where the cluster's center in latent space serves as a slot representation. Slot Attention is an example of such a method, acting as a learnable analog of the soft k-means algorithm. Our work employs a learnable clustering method based on the Gaussian Mixture Model. Unlike other approaches, we represent slots not only as centers of clusters but also incorporate information about the distance between clusters and assigned vectors, leading to more expressive slot representations. Our experiments demonstrate that using this approach instead of Slot Attention improves performance in object-centric scenarios, achieving state-of-the-art results in the set property prediction task.},
  address = {Vienna, Austria},
  author = {Daniil E. Kirilenko and Vitaliy Vorobyov and Alexey K. Kovalev and Aleksandr Panov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kirilenko2024objectcentric.pdf:pdf},
  keywords = {object-centric representations, {G}aussian {M}ixture {M}odel, {S}lot {A}ttention, set prediction task},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=aBUidW4Nkd},
  publisher = {OpenReview.net},
  title = {Object-Centric Learning with Slot Mixture Module},
  url = {https://openreview.net/forum?id=aBUidW4Nkd},
  year = {2024}
}

@inproceedings{kirjner2024improving,
  abstract = {The ability to engineer novel proteins with higher fitness for a desired property would be revolutionary for biotechnology and medicine. Modeling the combinatorially large space of sequences is infeasible; prior methods often constrain optimization to a small mutational radius, but this drastically limits the design space. Instead of heuristics, we propose smoothing the fitness landscape to facilitate protein optimization. First, we formulate protein fitness as a graph signal then use Tikunov regularization to smooth the fitness landscape. We find optimizing in this smoothed landscape leads to improved performance across multiple methods in the GFP and AAV benchmarks. Second, we achieve state-of-the-art results utilizing discrete energy-based models and MCMC in the smoothed landscape. Our method, called Gibbs sampling with Graph-based Smoothing (GGS), demonstrates a unique ability to achieve 2.5 fold fitness improvement over its training set.},
  address = {Vienna, Austria},
  author = {Andrew Kirjner and Jason Yim and Raman Samusevich and Shahar Bracha and Tommi S. Jaakkola and Regina Barzilay and Ila R. Fiete},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kirjner2024improving.pdf:pdf},
  keywords = {protein design, discrete optimization, protein engineering, {M}arkov chain {M}onte {C}arlo, graph signal processing},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rxlF2Zv8x0},
  publisher = {OpenReview.net},
  title = {Improving protein optimization with smoothed fitness landscapes},
  url = {https://openreview.net/forum?id=rxlF2Zv8x0},
  year = {2024}
}

@inproceedings{kirk2024understanding,
  abstract = {Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity.},
  address = {Vienna, Austria},
  author = {Robert Kirk and Ishita Mediratta and Christoforos Nalmpantis and Jelena Luketina and Eric Hambro and Edward Grefenstette and Roberta Raileanu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kirk2024understanding.pdf:pdf},
  keywords = {reinforcement learning, large language models, {RLHF}, {OOD} generalisation, diversity},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PXD3FAVHJT},
  publisher = {OpenReview.net},
  title = {Understanding the Effects of {RLHF} on {LLM} Generalisation and Diversity},
  url = {https://openreview.net/forum?id=PXD3FAVHJT},
  year = {2024}
}

@inproceedings{kiyohara2024towards,
  abstract = {Off-Policy Evaluation (OPE) aims to assess the effectiveness of counterfactual policies using offline logged data and is frequently utilized to identify the top-$k$ promising policies for deployment in online A/B tests. Existing evaluation metrics for OPE estimators primarily focus on the 'accuracy' of OPE or that of downstream policy selection, neglecting risk-return tradeoff and efficiency in subsequent online policy deployment. To address this issue, we draw inspiration from portfolio evaluation in finance and develop a new metric, called SharpeRatio@k, which measures the risk-return tradeoff and efficiency of policy portfolios formed by an OPE estimator under varying online evaluation budgets. We conduct a benchmark experiment of various OPE estimators regarding their risk-return tradeoff, presenting several future directions for OPE research.},
  address = {Vienna, Austria},
  author = {Haruka Kiyohara and Ren Kishimoto and Kosuke Kawakami and Ken Kobayashi and Kazuhide Nakata and Yuta Saito},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kiyohara2024towards.pdf:pdf},
  keywords = {off-policy evaluation, offline reinforcement learning, offline policy selection, risk-return tradeoff},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ycF7mKfVGO},
  publisher = {OpenReview.net},
  title = {Towards Assessing and Benchmarking Risk-Return Tradeoff of Off-Policy Evaluation},
  url = {https://openreview.net/forum?id=ycF7mKfVGO},
  year = {2024}
}

@inproceedings{klein2024beyond,
  abstract = {Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. This paper introduces a combination of dynamic programming and policy gradient called dynamical policy gradient, where the parameters are trained backwards in time. For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation.},
  address = {Vienna, Austria},
  author = {Sara Klein and Simon Weissmann and Leif D√∂ring},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/klein2024beyond.pdf:pdf},
  keywords = {reinforcement learning, policy gradient, stochastic approximation, finite-time {MDP}},
  month = {5},
  note = {{ICLR} 2024 poster. {DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1VeQ6VBbev},
  publisher = {OpenReview.net},
  title = {Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods},
  url = {https://openreview.net/forum?id=1VeQ6VBbev},
  year = {2024}
}

@inproceedings{kleinman2024critical,
  abstract = {Critical learning periods are periods early in development where temporary sensory deficits can have a permanent effect on behavior and learned representations. Despite the radical differences between biological and artificial networks, critical learning periods have been empirically observed in both systems. This suggests that critical periods may be fundamental to learning and not an accident of biology. Yet, why exactly critical periods emerge in deep networks is still an open question. To isolate the key underlying factors, we focus on deep linear network models, and show that, surprisingly, such networks also display much of the behavior seen in biology and artificial networks, while being amenable to analytical treatment. We show that critical periods depend on the depth of the model and structure of the data distribution, and that the learning of features is tied to competition between sources.},
  address = {Vienna, Austria},
  author = {Michael Kleinman and Alessandro Achille and Stefano Soatto},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kleinman2024critical.pdf:pdf},
  keywords = {critical learning periods, deep neural networks, gradient descent, linear networks},
  month = {5},
  note = {{ICLR} 2024 spotlight. {DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Aq35gl2c1k},
  publisher = {OpenReview.net},
  title = {Critical Learning Periods Emerge Even in Deep Linear Networks},
  url = {https://openreview.net/forum?id=Aq35gl2c1k},
  year = {2024}
}

@inproceedings{klissarov2024motif,
  abstract = {Exploring rich environments and evaluating one's actions without prior knowledge is immensely challenging. In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model ({LLM}) with an agent. We distill an {LLM}'s feedback into a reward function to train an agent with reinforcement learning in an open ended environment.},
  author = {Martin Klissarov and Pierluca D'Oro and Shagun Sodhani and Roberta Raileanu and Pierre-Luc Bacon and Pascal Vincent and Amy Zhang and Mikael Henaff},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/klissarov2024motif.pdf:pdf},
  keywords = {intrinsic motivation, rlaif, large language models, exploration, open-endedness, nethack, alignment, diversity},
  pdf = {https://openreview.net/pdf?id=tmBKIecDE9},
  publisher = {OpenReview.net},
  title = {{Motif}: {Intrinsic} {Motivation} from {Artificial} {Intelligence} {Feedback}},
  url = {https://openreview.net/forum?id=tmBKIecDE9},
  year = {2024}
}

@inproceedings{kniesel2024weakly,
  abstract = {Current state-of-the-art methods for object detection rely on annotated bounding boxes of large data sets for training. However, obtaining such annotations is expensive and can require up to hundreds of hours of manual labor. This poses a challenge, especially since such annotations can only be provided by experts, as they require knowledge about the scientific domain. We propose a domain-specific weakly supervised object detection algorithm that only relies on image-level annotations, which are significantly easier to acquire.},
  author = {Hannah Kniesel and Leon Sick and Tristan Payer and Tim Bergner and Kavitha Shaga Devan and Clarissa Read and Paul Walther and Timo Ropinski and Pedro Hermosilla},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kniesel2024weakly.pdf:pdf},
  keywords = {weakly supervised object detection, limited annotation time, bounding box regression, electron microscopy},
  pdf = {https://openreview.net/pdf?id=RJDjSXNuAZ},
  publisher = {OpenReview.net},
  title = {Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images},
  url = {https://openreview.net/forum?id=RJDjSXNuAZ},
  year = {2024}
}

@inproceedings{ko2024geometrically,
  abstract = {Transfer learning is a crucial technique for handling a small amount of data that is potentially related to other abundant data. However, most of the existing methods are focused on classification tasks using images and language datasets. Therefore, in order to expand the transfer learning scheme to regression tasks, we propose a novel transfer technique based on differential geometry, namely the Geometrically Aligned Transfer Encoder ({GATE}). In this method, we interpret the latent vectors from the model to exist on a Riemannian curved manifold. We find a proper diffeomorphism between pairs of tasks to ensure that every arbitrary point maps to a locally flat coordinate in the overlapping region, allowing the transfer of knowledge from the source to the target data. This also serves as an effective regularizer for the model to behave in extrapolation regions. In this article, we demonstrate that {GATE} outperforms conventional methods and exhibits stable behavior in both the latent space and extrapolation regions for various molecular graph datasets.},
  author = {Sung Moon Ko and Sumin Lee and Dae-Woong Jeong and Woohyung Lim and Sehui Han},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/ko2024geometrically.pdf:pdf},
  keywords = {transfer learning, inductive transfer, geometrical deeplearning, regression},
  pdf = {https://openreview.net/pdf?id=3z60EWfh1p},
  publisher = {OpenReview.net},
  title = {Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks},
  url = {https://openreview.net/forum?id=3z60EWfh1p},
  year = {2024}
}

@inproceedings{ko2024learning,
  abstract = {In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that 'hallucinate' robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on {RGB} videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four {GPUs} within a single day.},
  author = {Po-Chen Ko and Jiayuan Mao and Yilun Du and Shao-Hua Sun and Joshua B. Tenenbaum},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/ko2024learning.pdf:pdf},
  keywords = {video-based policy, video dense correspondence, robot learning, actionless videos},
  pdf = {https://openreview.net/pdf?id=Mhb5fpA1T0},
  publisher = {OpenReview.net},
  title = {Learning to Act from Actionless Videos through Dense Correspondences},
  url = {https://openreview.net/forum?id=Mhb5fpA1T0},
  year = {2024}
}

@inproceedings{kobayashi2024analyzing,
  abstract = {Transformers are ubiquitous in wide tasks. Interpreting their internals is a pivotal goal. Nevertheless, their particular components, feed-forward ({FF}) blocks, have typically been less analyzed despite their substantial parameter amounts. We analyze the input contextualization effects of {FF} blocks by rendering them in the attention maps as a human-friendly visualization scheme. Our experiments with both masked- and causal-language models reveal that {FF} networks modify the input contextualization to emphasize specific types of linguistic compositions. In addition, {FF} and its surrounding components tend to cancel out each other's effects, suggesting potential redundancy in the processing of the Transformer layer.},
  author = {Goro Kobayashi and Tatsuki Kuribayashi and Sho Yokoi and Kentaro Inui},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kobayashi2024analyzing.pdf:pdf},
  keywords = {transformer, attention map, feed-forward, contextualization, interpretation, analysis, pre-trained models, masked language models, causal language models},
  pdf = {https://openreview.net/pdf?id=mYWsyTuiRp},
  publisher = {OpenReview.net},
  title = {Analyzing Feed-Forward Blocks in Transformers through the Lens of Attention Maps},
  url = {https://openreview.net/forum?id=mYWsyTuiRp},
  year = {2024}
}

@inproceedings{koehler2024sampling,
  abstract = {There is a long history, as well as a recent explosion of interest, in statistical and generative modeling approaches based on *score functions* --- derivatives of the log-likelihood of a distribution. In seminal works, Hyv√§rinen proposed vanilla score matching as a way to learn distributions from data by computing an estimate of the score function of the underlying ground truth, and established connections between this method and established techniques like Contrastive Divergence and Pseudolikelihood estimation. It is by now well-known that vanilla score matching has significant difficulties learning multimodal distributions. Although there are various ways to overcome this difficulty, the following question has remained unanswered --- is there a natural way to sample multimodal distributions using just the vanilla score? Inspired by a long line of related experimental works, we prove that the Langevin diffusion with early stopping, initialized at the empirical distribution, and run on a score function estimated from data successfully generates natural multimodal distributions (mixtures of log-concave distributions).},
  author = {Frederic Koehler and Thuy-Duong Vuong},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/koehler2024sampling.pdf:pdf},
  keywords = {sampling, score matching, contrastive divergence, langevin dynamics},
  pdf = {https://openreview.net/pdf?id=oAMArMMQxb},
  publisher = {OpenReview.net},
  title = {Sampling Multimodal Distributions with the Vanilla Score: Benefits of Data-Based Initialization},
  url = {https://openreview.net/forum?id=oAMArMMQxb},
  year = {2024}
}

@inproceedings{kofinas2024graph,
  abstract = {Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods.},
  author = {Miltiadis Kofinas and Boris Knyazev and Yan Zhang and Yunlu Chen and Gertjan J. Burghouts and Efstratios Gavves and Cees G. M. Snoek and David W. Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kofinas2024graph.pdf:pdf},
  keywords = {deep weight space, graph neural networks, transformers, permutation equivariance, implicit neural representations, networks for networks, neural graphs},
  pdf = {https://openreview.net/pdf?id=oO6FsMyDBt},
  publisher = {OpenReview.net},
  title = {Graph Neural Networks for Learning Equivariant Representations of Neural Networks},
  url = {https://openreview.net/forum?id=oO6FsMyDBt},
  year = {2024}
}

@inproceedings{koke2024holonets,
  abstract = {Within the graph learning community, conventional wisdom dictates that spectral convolutional networks may only be deployed on undirected graphs: Only there could the existence of a well-defined graph Fourier transform be guaranteed, so that information may be translated between spatial- and spectral domains. Here we show this traditional reliance on the graph Fourier transform to be superfluous and --- making use of certain advanced tools from complex analysis and spectral theory --- extend spectral convolutions to directed graphs. We provide a frequency-response interpretation of newly developed filters, investigate the influence of the basis used to express filters and discuss the interplay with characteristic operators on which networks are based. In order to thoroughly test the developed theory, we conduct experiments in real world settings, showcasing that directed spectral convolutional networks provide new state of the art results for heterophilic node classification on many datasets and --- as opposed to baselines --- may be rendered stable to resolution-scale varying topological perturbations.},
  author = {Christian Koke and Daniel Cremers},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/koke2024holonets.pdf:pdf},
  keywords = {spectral graph convolutions, directed graphs, weighted graphs, complex analysis, spectral graph theory, heterophily, node classification, graph regression},
  pdf = {https://openreview.net/pdf?id=EhmEwfavOW},
  publisher = {OpenReview.net},
  title = {{HoloNets}: Spectral Convolutions do extend to Directed Graphs},
  url = {https://openreview.net/forum?id=EhmEwfavOW},
  year = {2024}
}

@inproceedings{kolossov2024towards,
  abstract = {Given a sample of size {$N$}, it is often useful to select a subsample of smaller size {$n<N$} to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given {$N$} unlabeled samples {$x_i$}, and to be given access to a 'surrogate model' that can predict labels {$y_i$} better than random guessing. Our goal is to select a subset of the samples, to be denoted by {$\{x_i\}_{i\in G}$}, of size {$|G|=n<N$}. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization. By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: {$(i)$} Data selection can be very effective, in particular beating training on the full sample in some cases; {$(ii)$} Certain popular choices in data selection methods (e.g. unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.},
  author = {Germain Kolossov and Andrea Montanari and Pulkit Tandon},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kolossov2024towards.pdf:pdf},
  keywords = {data selection, empirical risk minimization, influence functions, high dimensional asymptotics},
  pdf = {https://openreview.net/pdf?id=HhfcNgQn6p},
  publisher = {OpenReview.net},
  title = {Towards a statistical theory of data selection under weak supervision},
  url = {https://openreview.net/forum?id=HhfcNgQn6p},
  year = {2024}
}

@inproceedings{komorowska2024dynamicsinformed,
  abstract = {Current protein generative models are able to design novel backbones with desired shapes or functional motifs. However, despite the importance of a protein's dynamical properties for its function, conditioning on dynamical properties remains elusive. We present a new approach to protein generative modeling by leveraging Normal Mode Analysis that enables us to capture dynamical properties too. We introduce a method for conditioning the diffusion probabilistic models on protein dynamics, specifically on the lowest non-trivial normal mode of oscillation. Our method, similar to the classifier guidance conditioning, formulates the sampling process as being driven by conditional and unconditional terms. However, unlike previous works, we approximate the conditional term with a simple analytical function rather than an external neural network, thus making the eigenvector calculations approachable. We present the corresponding {SDE} theory as a formal justification of our approach. We extend our framework to conditioning on structure and dynamics at the same time, enabling scaffolding of the dynamical motifs. We demonstrate the empirical effectiveness of our method by turning the open-source unconditional protein diffusion model Genie into the conditional model with no retraining. Generated proteins exhibit the desired dynamical and structural properties while still being biologically plausible.},
  author = {Urszula Julia Komorowska and Simon V. Mathis and Kieran Didi and Francisco Vargas and Pietro Lio and Mateja Jamnik},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/komorowska2024dynamicsinformed.pdf:pdf},
  keywords = {diffusion models, generative modeling, protein design, normal mode analysis},
  pdf = {https://openreview.net/pdf?id=jZPqf2G9Sw},
  publisher = {OpenReview.net},
  title = {Dynamics-Informed Protein Design with Structure Conditioning},
  url = {https://openreview.net/forum?id=jZPqf2G9Sw},
  year = {2024}
}

@inproceedings{kong2024opentab,
  abstract = {Large Language Models (LLMs) trained on large volumes of data excel at various natural language tasks, but they cannot handle tasks requiring knowledge that has not been trained on previously. One solution is to use a retriever that fetches relevant information to expand LLM's knowledge scope. However, existing textual-oriented retrieval-based LLMs are not ideal on structured table data due to diversified data modalities and large table sizes. In this work, we propose OpenTab, an open-domain table reasoning framework powered by LLMs. Overall, OpenTab leverages table retriever to fetch relevant tables and then generates SQL programs to parse the retrieved tables efficiently. Utilizing the intermediate data derived from the SQL executions, it conducts grounded inference to produce accurate response. Extensive experimental evaluation shows that OpenTab significantly outperforms baselines in both open- and closed-domain settings, achieving up to 21.5\% higher accuracy. We further run ablation studies to validate the efficacy of our proposed designs of the system.},
  author = {Kezhi Kong and Jiani Zhang and Zhengyuan Shen and Balasubramaniam Srinivasan and Chuan Lei and Christos Faloutsos and Huzefa Rangwala and George Karypis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kong2024opentab.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=Qa0ULgosc9},
  publisher = {OpenReview.net},
  title = {{OpenTab}: Advancing Large Language Models as Open-domain Table Reasoners},
  url = {https://openreview.net/forum?id=Qa0ULgosc9},
  year = {2024}
}

@inproceedings{kong2024efficient,
  abstract = {Diffusion models have shown remarkable success in generating high-quality images, but their training on large datasets raises privacy concerns regarding membership inference attacks (MIA). We propose an efficient query-based membership inference attack, namely Proximal Initialization Attack (PIA), which utilizes groundtruth trajectory obtained by initialization in t=0 and predicted point to infer memberships. Experimental results indicate that the proposed method can achieve competitive performance with only two queries, achieving at least 6√ó efficiency than the previous SOTA baseline on both discrete-time and continuous-time diffusion models. We explore membership inference attacks beyond vision tasks, specifically investigating the robustness of diffusion models to MIA in the text-to-speech (TTS) task. This work is the first to study the robustness of diffusion models to MIA in the TTS task, finding that mel-spectrogram models are vulnerable while audio output models are more robust.},
  author = {Fei Kong and Jinhao Duan and Ruipeng Ma and Heng Tao Shen and Xiaoshuang Shi and Xiaofeng Zhu 0001 and Kaidi Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kong2024efficient.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rpH9FcCEV6},
  publisher = {OpenReview.net},
  title = {An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization},
  url = {https://openreview.net/forum?id=rpH9FcCEV6},
  year = {2024}
}

@inproceedings{kong2024interpretable,
  abstract = {Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. For diffusion models, we show that a natural non-negative decomposition of mutual information emerges, allowing us to quantify informative relationships between words and pixels in an image. We exploit these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects when selectively editing images through prompt interventions.},
  author = {Xianghao Kong and Ollie Liu and Han Li and Dani Yogatama and Greg Ver Steeg},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kong2024interpretable.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=X6tNkN6ate},
  publisher = {OpenReview.net},
  title = {Interpretable Diffusion via Information Decomposition},
  url = {https://openreview.net/forum?id=X6tNkN6ate},
  year = {2024}
}

@inproceedings{konz2024effect,
  abstract = {This paper investigates discrepancies in how neural networks learn from different imaging domains, which are commonly overlooked when adopting computer vision techniques from the domain of natural images to other specialized domains such as medical images. Recent works have found that the generalization error of a trained network typically increases with the intrinsic dimension (d\_data) of its training set. Yet, the steepness of this relationship varies significantly between medical (radiological) and natural imaging domains, with no existing theoretical explanation. We address this gap in knowledge by establishing and empirically validating a generalization scaling law with respect to d\_data, and propose that the substantial scaling discrepancy between the two considered domains may be at least partially attributed to the higher intrinsic ``label sharpness'' (K\_F) of medical imaging datasets, a metric which we propose. Next, we demonstrate an additional benefit of measuring the label sharpness of a training set: it is negatively correlated with the trained model's adversarial robustness, which notably leads to models for medical images having a substantially higher vulnerability to adversarial attack. Finally, we extend our d\_data formalism to the related metric of learned representation intrinsic dimension (d\_repr), derive a generalization scaling law with respect to d\_repr, and show that d\_data serves as an upper bound for d\_repr. Our theoretical results are supported by thorough experiments with six models and eleven natural and medical imaging datasets over a range of training set sizes.},
  author = {Nicholas Konz and Maciej A. Mazurowski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/konz2024effect.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ixP76Y33y1},
  publisher = {OpenReview.net},
  title = {The Effect of Intrinsic Dataset Properties on Generalization: Unraveling Learning Differences Between Natural and Medical Images},
  url = {https://openreview.net/forum?id=ixP76Y33y1},
  year = {2024}
}

@inproceedings{koohpayegani2024nola,
  abstract = {Fine-tuning Large Language Models (LLMs) and storing them for each downstream task or domain is impractical because of the massive model size (e.g., 350GB in GPT-3). Current literature, such as LoRA, showcases the potential of low-rank modifications to the original weights of an LLM, enabling efficient adaptation and storage for task-specific models. These methods can reduce the number of parameters needed to fine-tune an LLM by several orders of magnitude. Yet, these methods face two primary limitations: (1) the parameter count is lower-bounded by the rank one decomposition, and (2) the extent of reduction is heavily influenced by both the model architecture and the chosen rank. We introduce NOLA, which overcomes the rank one lower bound present in LoRA. It achieves this by re-parameterizing the low-rank matrices in LoRA using linear combinations of randomly generated matrices (basis) and optimizing the linear mixture coefficients only. This approach allows us to decouple the number of trainable parameters from both the choice of rank and the network architecture. We present adaptation results using GPT-2, LLaMA-2, and ViT in natural language and computer vision tasks. NOLA performs as well as LoRA models with much fewer number of parameters compared to LoRA with rank one, the best compression LoRA can archive. Particularly, on LLaMA-2 70B, our method is almost 20 times more compact than the most compressed LoRA without degradation in accuracy.},
  author = {Soroush Abbasi Koohpayegani and Navaneet K. L. and Parsa Nooralinejad and Soheil Kolouri and Hamed Pirsiavash},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/koohpayegani2024nola.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TjfXcDgvzk},
  publisher = {OpenReview.net},
  title = {NOLA: Compressing LoRA using Linear Combination of Random Basis},
  url = {https://openreview.net/forum?id=TjfXcDgvzk},
  year = {2024}
}

@inproceedings{kopiczko2024vera,
  abstract = {Low-rank adaptation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous per-user or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA), which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.},
  author = {Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki M. Asano},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kopiczko2024vera.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NjNfLdxr3A},
  publisher = {OpenReview.net},
  title = {VeRA: Vector-based Random Matrix Adaptation},
  url = {https://openreview.net/forum?id=NjNfLdxr3A},
  year = {2024}
}

@inproceedings{kori2024grounded,
  abstract = {The extraction of object-centric representations for downstream tasks is an emerging area of research. Learning grounded representations of objects that are guaranteed to be stable and invariant promises robust performance across different tasks and environments. Slot Attention (SA) learns object-centric representations by assigning objects to slots, but presupposes a single distribution from which all slots are randomly initialised. This results in an inability to learn specialized slots which bind to specific object types and remain invariant to identity-preserving changes in object appearance. We present Conditional Slot Attention (CoSA) using a novel concept of Grounded Slot Dictionary (GSD) inspired by vector quantization. The proposed GSD comprises (i) canonical object-level property vectors and (ii) parametric Gaussian distributions, which define a prior over the slots. Our approach enables learning grounded representations of objects that remain stable and invariant to changes in object appearance while providing interpretable slot attention mechanisms that can generalize to downstream tasks.},
  author = {Avinash Kori and Francesco Locatello and Fabio De Sousa Ribeiro and Francesca Toni and Ben Glocker},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kori2024grounded.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pBxeZ6pVUD},
  publisher = {OpenReview.net},
  title = {Grounded Object-Centric Learning},
  url = {https://openreview.net/forum?id=pBxeZ6pVUD},
  year = {2024}
}

@inproceedings{korotin2024light,
  abstract = {Despite the recent advances in the field of computational Schr{\"o}dinger Bridges (SB), most existing SB solvers are still heavy-weighted and require complex optimization of several neural networks. It turns out that there is no principal solver which plays the role of simple-yet-effective baseline for SB just like, e.g., k-means method in clustering, logistic regression in classification or Sinkhorn algorithm in discrete optimal transport. We address this issue and propose a novel fast and simple SB solver. Our development is a smart combination of two ideas which recently appeared in the field: (a) parameterization of the Schr{\"o}dinger potentials with sum-exp quadratic functions and (b) viewing the log-Schr{\"o}dinger potentials as the energy functions. We show that combined together these ideas yield a lightweight, simulation-free and theoretically justified SB solver with a simple straightforward optimization objective. As a result, it allows solving SB in moderate dimensions in a matter of minutes on CPU without a painful hyperparameter selection. Our light solver resembles the Gaussian mixture model which is widely used for density estimation. Inspired by this similarity, we also prove an important theoretical result showing that our light solver is a universal approximator of SBs.},
  author = {Alexander Korotin and Nikita Gushchin and Evgeny Burnaev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/korotin2024light.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WhZoCLRWYJ},
  publisher = {OpenReview.net},
  title = {Light {S}chr{\"o}dinger Bridge},
  url = {https://openreview.net/forum?id=WhZoCLRWYJ},
  year = {2024}
}

@inproceedings{kosan2024gnnxbench,
  abstract = {Numerous explainability methods have been proposed to shed light on the inner workings of Graph Neural Networks (GNNs). Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. However, we also identify several troubling trends such as the universal instability of counterfactual explainers when faced with noisy data and the general ineffectiveness of current explainers when faced with graph data which incorporates realistic domain-specific constraints.},
  author = {Mert Kosan and Samidha Verma and Burouj Armgaan and Khushbu Pahwa and Ambuj K. Singh and Sourav Medya and Sayan Ranu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kosan2024gnnxbench.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=VJvbOSXRUq},
  publisher = {OpenReview.net},
  title = {{GNNX-BENCH}: Unravelling the Utility of Perturbation-based {GNN} Explainers through In-depth Benchmarking},
  url = {https://openreview.net/forum?id=VJvbOSXRUq},
  year = {2024}
}

@inproceedings{kossen2024incontext,
  abstract = {The predictions of Large Language Models (LLMs) on downstream tasks often improve significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works. For example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022) argue ICL does not even learn label relationships from in-context examples. In this paper, we provide novel insights into how ICL leverages label information, revealing both capabilities and limitations. To ensure we obtain a comprehensive picture of ICL behavior, we study probabilistic aspects of ICL predictions and thoroughly examine the dynamics of ICL as more examples are provided. Our experiments show that ICL predictions almost always depend on in-context labels and that ICL can learn truly novel tasks in-context. However, we also find that ICL struggles to fully overcome prediction preferences acquired from pre-training data and, further, that ICL does not consider all in-context information equally.},
  author = {Jannik Kossen and Yarin Gal and Tom Rainforth},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kossen2024incontext.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=YPIA7bgd5y},
  publisher = {OpenReview.net},
  title = {In-Context Learning Learns Label Relationships but Is Not Conventional Learning},
  url = {https://openreview.net/forum?id=YPIA7bgd5y},
  year = {2024}
}

@inproceedings{kostic2024learning,
  abstract = {We consider the general class of time-homogeneous stochastic dynamical systems, both discrete and continuous, and study the problem of learning a representation of the state that faithfully captures its dynamics. This is instrumental to learning the transfer operator or the generator of the system, which in turn can be used for numerous tasks, such as forecasting and interpreting the system dynamics. We show that the search for a good representation can be cast as an optimization problem over neural networks. Our approach is supported by recent results in statistical learning theory, highlighting the role of approximation error and metric distortion in the learning problem. The objective function we propose is associated with projection operators from the representation space to the data space, overcomes metric distortion, and can be empirically estimated from data. In the discrete-time setting, we further derive a relaxed objective function that is differentiable and numerically well-conditioned. We compare our method against state-of-the-art approaches on different datasets, showing better performance across the board.},
  author = {Vladimir R. Kostic and Pietro Novelli and Riccardo Grazzi and Karim Lounici and Massimiliano Pontil},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kostic2024learning.pdf:pdf},
  keywords = {Dynamical systems, Statistical learning},
  pdf = {https://openreview.net/pdf?id=twSnZwiOIm},
  publisher = {OpenReview.net},
  title = {Learning invariant representations of time-homogeneous stochastic dynamical systems},
  url = {https://openreview.net/forum?id=twSnZwiOIm},
  year = {2024}
}

@inproceedings{kotha2024understanding,
  abstract = {We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT.},
  author = {Suhas Kotha and Jacob Mitchell Springer and Aditi Raghunathan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kotha2024understanding.pdf:pdf},
  keywords = {implicit inference in language models, fine-tuning, catastrophic forgetting},
  pdf = {https://openreview.net/pdf?id=VrHiF2hsrm},
  publisher = {OpenReview.net},
  title = {Understanding Catastrophic Forgetting in Language Models via Implicit Inference},
  url = {https://openreview.net/forum?id=VrHiF2hsrm},
  year = {2024}
}

@inproceedings{kothari2024prediction,
  abstract = {Machine learning models are often used to decide who receives a loan, a job interview, or a public benefit. Models in such settings use features without considering their actionability. As a result, they can assign predictions that are fixed -- meaning that individuals who are denied loans and interviews are, in fact, precluded from access to credit and employment. In this work, we introduce a procedure called recourse verification to test if a model assigns fixed predictions to its decision subjects. We propose a model-agnostic approach for verification with reachable sets -- i.e., the set of all points that a person can reach through their actions in feature space. We develop methods to construct reachable sets for discrete feature spaces, which can certify the responsiveness of any model by simply querying its predictions. We conduct a comprehensive empirical study on the infeasibility of recourse on datasets from consumer finance. Our results highlight how models can inadvertently preclude access by assigning fixed predictions and underscore the need to account for actionability in model development.},
  author = {Avni Kothari and Bogdan Kulynych and Tsui-Wei Weng and Berk Ustun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kothari2024prediction.pdf:pdf},
  keywords = {algorithmic recourse, fairness, robustness, consumer finance, integer programming, trustworthy AI},
  pdf = {https://openreview.net/pdf?id=SCQfYpdoGE},
  publisher = {OpenReview.net},
  title = {Prediction without Preclusion: Recourse Verification with Reachable Sets},
  url = {https://openreview.net/forum?id=SCQfYpdoGE},
  year = {2024}
}

@inproceedings{kou2024bayesdiff,
  abstract = {Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.},
  author = {Siqi Kou and Lei Gan and Dequan Wang and Chongxuan Li and Zhijie Deng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kou2024bayesdiff.pdf:pdf},
  keywords = {diffusion model, Bayesian uncertainty},
  pdf = {https://openreview.net/pdf?id=YcM6ofShwY},
  publisher = {OpenReview.net},
  title = {{BayesDiff}: Estimating Pixel-wise Uncertainty in Diffusion via {Bayesian} Inference},
  url = {https://openreview.net/forum?id=YcM6ofShwY},
  year = {2024}
}

@inproceedings{koyama2024neural,
  abstract = {Symmetry learning has proven to be an effective approach for extracting the hidden structure of data, with the concept of equivariance relation playing the central role. However, most of the current studies are built on architectural theory and corresponding assumptions on the form of data. We propose Neural Fourier Transform (NFT), a general framework of learning the latent linear action of the group without assuming explicit knowledge of how the group acts on data. We present the theoretical foundations of NFT and show that the existence of a linear equivariant feature, which has been assumed ubiquitously in equivariance learning, is equivalent to the existence of a group invariant kernel on the dataspace. We also provide experimental results to demonstrate the application of NFT in typical scenarios with varying levels of knowledge about the acting group.},
  author = {Masanori Koyama and Kenji Fukumizu and Kohei Hayashi and Takeru Miyato},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/koyama2024neural.pdf:pdf},
  keywords = {Fourier transform, Equivariance, Harmonic analysis, Representation learning},
  pdf = {https://openreview.net/pdf?id=eOCvA8iwXH},
  publisher = {OpenReview.net},
  title = {Neural {Fourier} Transform: A General Approach to Equivariant Representation Learning},
  url = {https://openreview.net/forum?id=eOCvA8iwXH},
  year = {2024}
}

@inproceedings{kreiss2024contextref,
  abstract = {Referenceless metrics (e.g., CLIPScore) use pretrained vision-language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging benchmark though, in large part due to the challenge of context dependence.},
  author = {Elisa Kreiss and Eric Zelikman and Christopher Potts and Nick Haber},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kreiss2024contextref.pdf:pdf},
  keywords = {image description evaluation, referenceless metrics, multimodal evaluation},
  pdf = {https://openreview.net/pdf?id=j0ZvKSNZiP},
  publisher = {OpenReview.net},
  title = {{ContextRef}: Evaluating Referenceless Metrics for Image Description Generation},
  url = {https://openreview.net/forum?id=j0ZvKSNZiP},
  year = {2024}
}

@inproceedings{krishna2024sufficient,
  abstract = {During periods of quiescence, such as sleep, neural activity in many brain circuits resembles that observed during periods of task engagement. However, the precise conditions under which task-optimized networks can autonomously reactivate the same network states responsible for online behavior is poorly understood. We develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. We show that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. Our findings demonstrate that multiple interneuron types organized in a canonical excitatory-inhibitory circuit support offline reactivation. We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation.},
  author = {Nanda H. Krishna and Colin Bredenberg and Daniel Levenstein and Blake Aaron Richards and Guillaume Lajoie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/krishna2024sufficient.pdf:pdf},
  keywords = {computational neuroscience, offline reactivation, replay, recurrent neural networks, path integration, noise},
  pdf = {https://openreview.net/pdf?id=RVrINT6MT7},
  publisher = {OpenReview.net},
  title = {Sufficient conditions for offline reactivation in recurrent neural networks},
  url = {https://openreview.net/forum?id=RVrINT6MT7},
  year = {2024}
}

@inproceedings{ku2024imagenhub,
  abstract = {Recently, a myriad of conditional image generation and editing models have been developed to serve different downstream tasks, including text-to-image generation, text-guided image editing, subject-driven image generation, control-guided image generation, etc. However, we observe huge inconsistencies in experimental conditions: datasets, inference, and evaluation metrics - render fair comparisons difficult. This paper proposes ImagenHub, which is a one-stop library to standardize the inference and evaluation of all the conditional image generation models. Firstly, we define seven prominent tasks and curate high-quality evaluation datasets for them. Secondly, we built a unified inference pipeline to ensure fair comparison. Thirdly, we design two human evaluation scores, i.e. Semantic Consistency and Perceptual Quality, along with comprehensive guidelines to evaluate generated images. We train expert raters to evaluate the model outputs based on the proposed metrics. Our human evaluation achieves a high inter-worker agreement of Krippendorff's alpha on 76\% models with a value higher than 0.4. We comprehensively evaluated a total of around 30 models and observed three key takeaways: (1) the existing models' performance is generally unsatisfying except for Text-guided Image Generation and Subject-driven Image Generation, with 74\% models achieving an overall score lower than 0.5. (2) we examined the claims from published papers and found 83\% of them hold with a few exceptions. (3) None of the existing automatic metrics has a Spearman's correlation higher than 0.2 except subject-driven image generation.},
  author = {Max Ku and Tianle Li and Kai Zhang and Yujie Lu and Xingyu Fu and Wenwen Zhuang and Wenhu Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ku2024imagenhub.pdf:pdf},
  keywords = {Image Generation, Image Editing, Evaluation, Benchmark, Diffusion Model},
  pdf = {https://openreview.net/pdf?id=OuV9ZrkQlc},
  publisher = {OpenReview.net},
  title = {{ImagenHub}: Standardizing the evaluation of conditional image generation models},
  url = {https://openreview.net/forum?id=OuV9ZrkQlc},
  year = {2024}
}

@inproceedings{kuang2024rethinking,
  abstract = {Machine learning (ML) has been shown to successfully accelerate solving NP-hard combinatorial optimization (CO) problems under the branch and bound framework. However, the high training and inference cost and limited interpretability of ML approaches severely limit their wide application to modern exact CO solvers. In contrast, human-designed policies -- though widely integrated in modern CO solvers due to their compactness and reliability -- cannot capture data-driven patterns for higher performance. In this paper, we show the potential existence of small symbolic policies and propose the first symbolic discovery framework called Deep Symbolic Optimization (DSO) for exact combinatorial optimization solver (Symb4CO) to learn high-performance symbolic policies on the branching task. Experiments show that the Symb4CO learned purely CPU-based policies consistently achieve comparable performance to previous GPU-based state-of-the-art approaches. The appealing features of Symb4CO include its high training efficiency (ten training instances), inference efficiency (one CPU core), and good interpretability (one-line expressions), making it simple and reliable for deployment.},
  author = {Yufei Kuang and Jie Wang and Haoyang Liu and Fangzhou Zhu and Xijun Li and Jia Zeng and Jianye Hao and Bin Li and Feng Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kuang2024rethinking.pdf:pdf},
  keywords = {Combinatorial Optimization, Branch and Bound, Deep Symbolic Optimization, Learn to Optimize, Machine Learning for Combinatorial Optimization},
  pdf = {https://openreview.net/pdf?id=jKhNBulNMh},
  publisher = {OpenReview.net},
  title = {Rethinking Branching on Exact Combinatorial Optimization Solver: The First Deep Symbolic Discovery Framework},
  url = {https://openreview.net/forum?id=jKhNBulNMh},
  year = {2024}
}

@inproceedings{kumano2024theoretical,
  abstract = {It is not fully understood why adversarial examples can deceive neural networks and transfer between different networks. To elucidate this, several studies have hypothesized that adversarial perturbations, while appearing as noises, contain class features. This is supported by empirical evidence showing that networks trained on mislabeled adversarial examples can still generalize well to correctly labeled test samples. However, a theoretical understanding of how perturbations include class features and contribute to generalization is limited. In this study, we provide a theoretical framework for understanding learning from perturbations using a one-hidden-layer network trained on mutually orthogonal samples. Our results highlight that various adversarial perturbations, even perturbations of a few pixels, contain sufficient class features for generalization. Moreover, we reveal that the decision boundary when learning from perturbations matches that from standard samples except for specific regions under mild conditions.},
  author = {Soichiro Kumano and Hiroshi Kera and Toshihiko Yamasaki},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/kumano2024theoretical.pdf:pdf},
  keywords = {Adversarial Perturbations, Adversarial Examples, Adversarial Attacks, Non-Robust Features},
  pdf = {https://openreview.net/pdf?id=Ww9rWUAcdo},
  publisher = {OpenReview.net},
  title = {Theoretical Understanding of Learning from Adversarial Perturbations},
  url = {https://openreview.net/forum?id=Ww9rWUAcdo},
  year = {2024}
}

@inproceedings{kumar2024grokking,
  abstract = {We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. We examine this transition in a two-layer neural network trained on sparse parity, where we can understand the transition from lazy to rich training using recent advances in the theory of feature learning. We show that the emergence of grokking is caused by competition between feature learning and memorization, and that feature learning only occurs once the network's initial features become sufficiently aligned with the sparse parity task. We identify key factors that determine whether grokking occurs: the feature learning rate, the strength of initial feature-target alignment, and the degree of sparsity in the data. Our theory predicts novel ways to induce and control grokking behavior, which we validate empirically.},
  author = {Tanishq Kumar and Blake Bordelon and Samuel J. Gershman and Cengiz Pehlevan},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kumar2024grokking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vt5mnLVIVo},
  publisher = {OpenReview.net},
  title = {Grokking as the transition from lazy to rich training dynamics},
  url = {https://openreview.net/forum?id=vt5mnLVIVo},
  year = {2024}
}

@inproceedings{kumar2024how,
  abstract = {{SGD} and {AdamW} are the two most used optimizers for fine-tuning large neural networks in computer vision. When the two methods perform the same, {SGD} is preferable because it uses less memory (12 bytes/parameter with momentum and 8 bytes/parameter without) than {AdamW} (16 bytes/parameter). However, on a suite of downstream tasks, especially those with distribution shifts, we find that fine-tuning with {AdamW} performs substantially better than {SGD} on modern Vision Transformer and {ConvNeXt} models. We find that large gaps in performance between {SGD} and {AdamW} occur when the fine-tuning gradients in the first embedding layer are much larger than in the rest of the model. Our analysis suggests an easy fix that works consistently across datasets and models: freezing the embedding layer (less than 1\% of the parameters) leads to {SGD} with or without momentum performing slightly better than {AdamW} while using less memory (e.g., on {ViT-L}, {SGD} uses 33\% less {GPU} memory). Our insights result in state-of-the-art accuracies on five popular distribution shift benchmarks: {WILDS-FMoW}, {WILDS-Camelyon}, {BREEDS-Living-17}, Waterbirds, and {DomainNet}.},
  author = {Ananya Kumar and Ruoqi Shen and Sebastien Bubeck and Suriya Gunasekar},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kumar2024how.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZTssMmhC2X},
  publisher = {OpenReview.net},
  title = {How to Fine-Tune Vision Models with {SGD}},
  url = {https://openreview.net/forum?id=ZTssMmhC2X},
  year = {2024}
}

@inproceedings{kunze2024entropy,
  abstract = {We present shuffle coding, a general method for optimal compression of sequences of unordered objects using bits-back coding. Data structures that can be compressed using shuffle coding include multisets, graphs, hypergraphs, and others. We release an implementation that can easily be adapted to different data types and statistical models, and demonstrate that our implementation achieves state-of-the-art compression rates on a range of graph datasets including molecular data. The key insight behind shuffle coding is that when compressing a sequence of unordered objects, we can permute the objects arbitrarily without changing the content. By choosing the permutation that minimizes the cost of encoding the sequence, we can achieve significant compression gains over existing methods.},
  author = {Julius Kunze and Daniel Severo and Giulio Zani and Jan-Willem van de Meent and James Townsend},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kunze2024entropy.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=afQuNt3Ruh},
  publisher = {OpenReview.net},
  title = {Entropy Coding of Unordered Data Structures},
  url = {https://openreview.net/forum?id=afQuNt3Ruh},
  year = {2024}
}

@inproceedings{kwon2024penalty,
  abstract = {In this work, we study first-order algorithms for solving bilevel optimization ({BO}) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of {BO} through the lens of penalty methods, in which the upper- and lower-level objectives are combined in a weighted sum with penalty parameter $\sigma > 0$. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\sigma)$-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as $O(\sigma)$-approximation of the original {BO}, we propose first-order algorithms that find an $\epsilon$-stationary solution by optimizing the penalty formulation with appropriate penalty parameters. When the perturbed lower-level problem uniformly satisfies the small-error proximal error-bound ({EB}) condition, we propose a first-order algorithm that converges to an $\epsilon$-stationary point of the penalty function. Under an additional assumption on stochastic oracles, we show that the algorithm can be implemented in a fully single-loop manner and achieves improved oracle-complexity.},
  author = {Jeongyeol Kwon and Dohyun Kwon and Stephen Wright and Robert D. Nowak},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kwon2024penalty.pdf:pdf},
  note = {DBLP last modified: 2024-10-14},
  pdf = {https://openreview.net/pdf?id=CvYBvgEUK9},
  publisher = {OpenReview.net},
  title = {On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation},
  url = {https://openreview.net/forum?id=CvYBvgEUK9},
  year = {2024}
}

@inproceedings{kwon2024image,
  abstract = {Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified criteria in the form of text by leveraging modern Vision-Language Models and Large Language Models. We call our method Image Clustering Conditioned on Text Criteria ({IC|TC}), and it represents a different paradigm of image clustering. {IC|TC} requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that {IC|TC} can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.},
  author = {Sehyun Kwon and Jaeseung Park and Minkyu Kim and Jaewoong Cho and Ernest K. Ryu and Kangwook Lee},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kwon2024image.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=G2cG3mQqop},
  publisher = {OpenReview.net},
  title = {Image Clustering Conditioned on Text Criteria},
  url = {https://openreview.net/forum?id=G2cG3mQqop},
  year = {2024}
}

@inproceedings{kwon2024datainf,
  abstract = {{DataInf} is an efficient influence approximation method that is practical for large-scale generative {AI} models. This method can be easily applied to {LLMs} and diffusion models. {DataInf} leverages an easy-to-compute closed-form expression and outperforms existing influence computation algorithms in terms of computational and memory efficiency. The theoretical analysis shows that {DataInf} is particularly well-suited for parameter-efficient fine-tuning techniques such as {LoRA}. Through systematic empirical evaluations, {DataInf} accurately approximates influence scores and is orders of magnitude faster than existing methods. In applications to {RoBERTa}-large, {Llama-2-13B}-chat, and stable-diffusion-v1.5 models, {DataInf} effectively identifies the most influential fine-tuning examples better than other approximate influence scores.},
  author = {Yongchan Kwon and Eric Wu and Kevin Wu and James Zou},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/kwon2024datainf.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9m02ib92Wz},
  publisher = {OpenReview.net},
  title = {{DataInf}: Efficiently Estimating Data Influence in {LoRA}-tuned {LLMs} and Diffusion Models},
  url = {https://openreview.net/forum?id=9m02ib92Wz},
  year = {2024}
}

@inproceedings{labarbarie2024optimal,
  abstract = {The superior performance of adversarial patch attacks for image classification in a black-box transfer attack setting where an attacker does not know the target model is widely studied. Instead of forcing corrupted image representations to cross the nearest decision boundaries or converge to a particular point, we propose a distribution-oriented approach that relies on optimal transport to push the feature distribution of attacked images towards an already modeled distribution. We show that this new approach leads to better transferable patches. Through digital experiments conducted on {ImageNet-1K}, we provide evidence that our new patches are the only ones that can simultaneously influence multiple target models. Our method represents a novel approach to adversarial patch attacks using optimal transport theory to improve transferability across different neural network models.},
  author = {Pol Labarbarie and Adrien Chan-Hon-Tong and St√©phane Herbin and Milad Leyli-Abadi},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/labarbarie2024optimal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nZP10evtkV},
  publisher = {OpenReview.net},
  title = {Optimal transport based adversarial patch to leverage large scale attack transferability},
  url = {https://openreview.net/forum?id=nZP10evtkV},
  year = {2024}
}

@inproceedings{laborieux2024improving,
  abstract = {Equilibrium propagation ({EP}) is a compelling alternative to the backpropagation of error algorithm ({BP}) for computing gradients of neural networks on biological or analog neuromorphic substrates. However, the algorithm requires weight symmetry and infinitesimal equilibrium perturbations, i.e., nudges, to estimate unbiased gradients efficiently, both of which are challenging to implement in physical systems. We study generalized {EP}, which can be formulated without weight symmetry, and analytically isolate the two sources of bias: the finite nudge amplitude and the {Jacobian} asymmetry. We found how finite nudge and non-symmetric weights induce distinct biases in the gradient estimate. While bias due to finite nudge is avoidable using holomorphic {EP}, weight asymmetry remains a significant obstacle by inducing non-normal {Jacobians}. We illustrate how both issues can be overcome through oscillations by combining holomorphic {EP} with a new form of {Jacobian} homeostasis that encourages functional symmetry, but without imposing strict weight symmetry. {Jacobian} homeostasis enables training of deep networks with untied weights on challenging tasks such as {ImageNet} 32√ó32 with minimal drop in accuracy compared to symmetric networks, thereby further strengthening the role of {EP} family algorithms for bio-plausible learning and neuromorphic engineering.},
  author = {Axel Laborieux and Friedemann Zenke},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/laborieux2024improving.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kUveo5k1GF},
  publisher = {OpenReview.net},
  title = {Improving equilibrium propagation without weight symmetry through {Jacobian} homeostasis},
  url = {https://openreview.net/forum?id=kUveo5k1GF},
  year = {2024}
}

@inproceedings{lagemann2024invariancebased,
  abstract = {We propose a new model class aimed at predicting dynamical trajectories from high-dimensional empirical data by combining variational autoencoders and (spatio-)temporal transformers within a framework designed to enforce certain scientifically-motivated invariances. The models allow inference of system behavior at any continuous time and generalize well beyond the data distributions seen during training. Furthermore, the models do not require an explicit neural {ODE} formulation, making them efficient and highly scalable in practice. The approach rests on a separation into an instance-specific encoding (capturing initial conditions, constants etc.) and a latent dynamics model that is itself universal across all instances/realizations of the system. We include extensive empirical experiments that investigate the ability to predict the trajectories of complicated systems based on finite data and show that the proposed approaches can outperform existing neural-dynamical models. The method focuses on learning dynamical systems from high-dimensional empirical data, specifically in the setting where data are available from multiple different instances of a system whose underlying dynamical model is entirely unknown at the outset.},
  author = {Kai Lagemann and Christian Lagemann and Sach Mukherjee},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/lagemann2024invariancebased.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EWTFMkTdkT},
  publisher = {OpenReview.net},
  title = {Invariance-based Learning of Latent Dynamics},
  url = {https://openreview.net/forum?id=EWTFMkTdkT},
  year = {2024}
}

@inproceedings{lahoti2024role,
  abstract = {The superior performance of convolutional neural networks ({CNNs}) on vision tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in {CNNs} over locally connected convolutional neural networks ({LCNs}) and fully connected neural networks ({FCNs}) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution ({DSD}) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is determined by a $d$-sparse signal vector that can freely appear in any one of the $k$ patches. We establish concrete sample complexity separations: for any orthogonally equivariant algorithm like gradient descent, {CNNs} require $\tilde{O}(k+d)$ samples, whereas {LCNs} require $\Omega(kd)$ samples, establishing the statistical advantages of weight sharing in translation invariant tasks. {LCNs} need $\tilde{O}(k(k+d))$ samples, compared to $\Omega(k^2d)$ samples for {FCNs}, showcasing the benefits of locality in local tasks.},
  author = {Aakash Lahoti and Stefani Karp and Ezra Winston and Aarti Singh and Yuanzhi Li},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/lahoti2024role.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=AfnsTnYphT},
  publisher = {OpenReview.net},
  title = {Role of Locality and Weight Sharing in Image-Based Tasks: {A} Sample Complexity Separation between {CNNs}, {LCNs}, and {FCNs}},
  url = {https://openreview.net/forum?id=AfnsTnYphT},
  year = {2024}
}

@inproceedings{lai2024memoryassisted,
  abstract = {Universal domain adaptation (UniDA) aims to align the class distributions and reduce the feature gap between source and target domains, where the target private category set is set unknown during adaptation. However, most existing methods overlook the intra-class structure within the same category, especially when there is significant concept shift between samples in the same category. When samples with large concept shift are forced to be pushed together, it may negatively affect the adaptation performance. From the perspective of interpretability, it is not reasonable to align the visual features with significant difference (e.g. fighter and civil aircraft) to the same category. Due to the semantic ambiguity and annotation cost, the categories are not always classified very carefully. This makes it difficult to adapt precisely. To address these issues, we propose a novel Memory-Assisted Sub-Prototype Mining (MemSPM) method, which can learn the difference between the samples belonging to the same category and mine sub-classes when there is significant concept shift between them. Extensive experiments on widely used benchmarks show that our method achieves state-of-the-art performance in all three domain adaptation scenarios: UniDA, Open Set Domain Adaptation (OSDA) and Partial Domain Adaptation (PDA).},
  author = {Yuxiang Lai and Yi Zhou and Xinghong Liu and Tao Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lai2024memoryassisted.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=sGVmr7KHfn},
  publisher = {OpenReview.net},
  title = {{Memory-Assisted Sub-Prototype Mining for Universal Domain Adaptation}},
  url = {https://openreview.net/forum?id=sGVmr7KHfn},
  year = {2024}
}

@inproceedings{lai2024faithful,
  abstract = {The demand for transparency in healthcare and finance has led to interpretable machine learning (IML) models, notably the concept bottleneck models (CBMs), valued for their potential in performance and insights into deep neural networks. However, CBM's reliance on manually annotated data poses challenges. Label-free CBMs have emerged to address this, but they remain unstable, affecting their faithfulness as explanatory tools. To address this issue of inherent instability, we introduce a formal definition for an alternative concept called the Faithful Vision-Language Concept (FVLC) model. We present a methodology for constructing an FVLC that satisfies four critical properties. Our extensive experiments on four benchmark datasets using Label-free CBM model architectures demonstrate that our FVLC outperforms other baselines regarding stability against input and concept set perturbations. Our approach incurs minimal accuracy degradation compared to the vanilla CBM, making it a promising solution for reliable and faithful model interpretation.},
  author = {Songning Lai and Lijie Hu and Junxiao Wang and Laure Berti-Equille and Di Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lai2024faithful.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rp0EdI8X4e},
  publisher = {OpenReview.net},
  title = {{Faithful Vision-Language Interpretation via Concept Bottleneck Models}},
  url = {https://openreview.net/forum?id=rp0EdI8X4e},
  year = {2024}
}

@inproceedings{lai2024optimality,
  abstract = {Kernel methods are widely used in machine learning, especially for classification problems. However, the theoretical analysis of kernel classification is still limited. In this paper, we investigate the statistical performances of kernel classifiers. With mild assumptions on the conditional probability $\eta(x) = P(Y=1|X=x)$, we derive an upper bound on the classification excess risk of a kernel classifier using recent advances in kernel regression theory. We obtain a minimax lower bound for Sobolev spaces, which demonstrates the optimality of the proposed classifier. Furthermore, the theoretical results can be extended to the generalization error of overparameterized neural network classifiers. Finally, we propose a practical method to estimate the interpolation smoothness of $2\eta(x)-1$ and apply it to real datasets.},
  author = {Jianfa Lai and Zhifan Li and Dongming Huang and Qian Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lai2024optimality.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JfqN3gu0i7},
  publisher = {OpenReview.net},
  title = {{The Optimality of Kernel Classifiers in Sobolev Space}},
  url = {https://openreview.net/forum?id=JfqN3gu0i7},
  year = {2024}
}

@inproceedings{laidlaw2024effective,
  abstract = {Reinforcement learning (RL) theory has largely focused on proving minimax sample complexity bounds. These require strategic exploration algorithms that use relatively limited function classes for representing the policy or value function. Our goal is to explain why deep RL algorithms often perform well in practice, despite using random exploration and much more expressive function classes like neural networks. Our work arrives at an explanation by showing that many stochastic MDPs can be solved by performing only a few steps of value iteration on the random policy's Q function and then acting greedily. When this is true, we find that it is possible to separate the exploration and learning components of RL, making it much easier to analyze. We introduce a new RL algorithm, SQIRL, that iteratively learns a near-optimal policy by exploring randomly to collect rollouts and then performing a limited number of steps of fitted-Q iteration over those roll- outs. We find that any regression algorithm that satisfies basic in-distribution generalization properties can be used in SQIRL to efficiently solve common MDPs. This can explain why deep RL works with complex function approximators like neural networks, since it is empirically established that neural networks generalize well in-distribution. Furthermore, SQIRL explains why random exploration works well in practice, since we show many environments can be solved by effectively estimating the random policy's Q-function and then applying zero or a few steps of value iteration. We leverage SQIRL to derive instance-dependent sample complexity bounds for RL that are exponential only in an "effective horizon" of lookahead‚Äîwhich is typically much smaller than the full horizon‚Äîand on the complexity of the class used for function approximation. Empirically, we also find that SQIRL performance strongly correlates with PPO and DQN performance in a variety of stochastic environments, supporting that our theoretical analysis is predictive of practical performance. Our code and data are available at https://github.com/cassidylaidlaw/effective-horizon.},
  author = {Cassidy Laidlaw and Banghua Zhu and Stuart Russell 0001 and Anca D. Dragan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/laidlaw2024effective.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=5ES5Hdlbxw},
  publisher = {OpenReview.net},
  title = {{The Effective Horizon Explains Deep RL Performance in Stochastic Environments}},
  url = {https://openreview.net/forum?id=5ES5Hdlbxw},
  year = {2024}
}

@inproceedings{lan2024sept,
  abstract = {Motion prediction is crucial for autonomous vehicles to operate safely in complex traffic environments. Extracting effective spatiotemporal relationships among traffic elements is key to accurate forecasting. Inspired by the successful practice of pretrained large language models, this paper presents SEPT, a modeling framework that leverages self-supervised learning to develop powerful spatiotemporal understanding for complex traffic scenes. Specifically, our approach involves three masking-reconstruction modeling tasks on scene inputs including agents' trajectories and road network, pretraining the scene encoder to capture kinematics within trajectory, spatial structure of road network, and interactions among roads and agents. The pretrained encoder is then finetuned on the downstream forecasting task. Extensive experiments demonstrate that SEPT, without elaborate architectural design or manual feature engineering, achieves state-of-the-art performance on the Argoverse 1 and Argoverse 2 motion forecasting benchmarks, outperforming previous methods on all main metrics by a large margin.},
  author = {Zhiqian Lan and Yuxuan Jiang 0011 and Yao Mu 0001 and Chen Chen 0068 and Shengbo Eben Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lan2024sept.pdf:pdf},
  note = {DBLP last modified: 2024-08-30},
  pdf = {https://openreview.net/pdf?id=efeBC1sQj9},
  publisher = {OpenReview.net},
  title = {{SEPT}: Towards Efficient Scene Representation Learning for Motion Prediction},
  url = {https://openreview.net/forum?id=efeBC1sQj9},
  year = {2024}
}

@inproceedings{lan2024influencer,
  abstract = {When a small number of poisoned samples are injected into the training dataset of a deep neural network, the network can be induced to exhibit malicious behavior during inferences, which poses potential threats to real-world applications. While they have been intensively studied in classification, backdoor attacks on semantic segmentation have been largely overlooked. Unlike classification, semantic segmentation aims to classify every pixel within a given image. In this work, we explore backdoor attacks on segmentation models to misclassify all pixels of a victim class by injecting a specific trigger on non-victim pixels during inferences, which is dubbed Influencer Backdoor Attack (IBA). IBA is expected to maintain the classification accuracy of non-victim pixels and mislead classifications of all victim pixels in every single inference and could be easily applied to real-world scenes. Based on the context aggregation ability of segmentation models, we proposed a simple, yet effective, Nearest-Neighbor trigger injection strategy. We also introduce an innovative Pixel Random Labeling strategy which maintains optimal performance even when the trigger is placed far from the victim pixels. Our extensive experiments reveal that current segmentation models do suffer from backdoor attacks, demonstrate IBA real-world applicability, and show that our proposed techniques can further increase attack performance.},
  author = {Haoheng Lan and Jindong Gu and Philip Torr 0001 and Hengshuang Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lan2024influencer.pdf:pdf},
  note = {DBLP last modified: 2024-07-30},
  pdf = {https://openreview.net/pdf?id=VmGRoNDQgJ},
  publisher = {OpenReview.net},
  title = {{Influencer Backdoor Attack on Semantic Segmentation}},
  url = {https://openreview.net/forum?id=VmGRoNDQgJ},
  year = {2024}
}

@inproceedings{lanzinger2024power,
  abstract = {Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $p$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive problem: "graph motif parameters". In this paper, we provide a precise characterization of the WL-dimension of labeled graph motif parameters. As specific instances of this result, we obtain characterizations of the WL-dimension of the subgraph counting and induced subgraph counting problem for every labeled pattern $p$. Particularly noteworthy is our resolution of a problem left open in previous work concerning induced copies. We additionally demonstrate that in cases where the $k$WL test distinguishes between graphs with varying occurrences of a pattern $p$, the exact number of occurrences of $p$ can be computed uniformly using only local information of the last layer of a corresponding GNN. We finally delve into the challenge of recognizing the WL-dimension of various graph parameters. We give a polynomial time algorithm for determining the WL-dimension of the subgraph counting problem for given pattern $p$, answering an open question from previous work. We additionally show how to utilize deep results from the field of graph motif parameters, together with our characterization, to determine the WL-dimension of induced subgraph counting and counting $k$-graphlets.},
  author = {Matthias Lanzinger and Pablo Barcel√≥},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lanzinger2024power.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=FddFxi08J3},
  publisher = {OpenReview.net},
  title = {{On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters}},
  url = {https://openreview.net/forum?id=FddFxi08J3},
  year = {2024}
}

@inproceedings{lasby2024dynamic,
  abstract = {Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically less computationally expensive, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work, we propose a sparse-to-sparse DST method, Structured RigL (SRigL), to learn a variant of fine-grained structured N:M sparsity by imposing a constant fan-in constraint. Using our empirical analysis of existing DST methods at high sparsity, we additionally employ a neuron ablation method which enables SRigL to achieve state-of-the-art sparse-to-sparse structured DST performance on a variety of Neural Network (NN) architectures. Using a 90\% sparse linear layer, we demonstrate a real-world acceleration of 3.4√ó/2.5√ó on CPU for online inference and 1.7√ó/13.0√ó on GPU for inference with a batch size of 256 when compared to equivalent dense/unstructured (CSR) sparse layers, respectively.},
  author = {Mike Lasby and Anna Golubeva and Utku Evci and Mihai Nica and Yani Ioannou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lasby2024dynamic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kOBkxFRKTA},
  publisher = {OpenReview.net},
  title = {{Dynamic Sparse Training with Structured Sparsity}},
  url = {https://openreview.net/forum?id=kOBkxFRKTA},
  year = {2024}
}

@inproceedings{lau2024pinnacle,
  abstract = {Physics-Informed Neural Networks (PINNs), which incorporate PDEs as soft constraints, train with a composite loss function that contains multiple training point types: different types of collocation points chosen during training to enforce each PDE and initial/boundary conditions, and experimental points which are usually costly to obtain via experiments or simulations. Training PINNs using this loss function is challenging as it typically requires selecting large numbers of points of different types, each with different training dynamics. Unlike past works that focused on the selection of either collocation or experimental points, this work introduces PINN Adaptive ColLocation and Experimental points selection (PINNACLE), the first algorithm that jointly optimizes the selection of all training point types, while automatically adjusting the proportion of collocation point types as training progresses. PINNACLE uses information on the interactions among training point types, which had not been considered before, based on an analysis of PINN training dynamics via the Neural Tangent Kernel (NTK). We theoretically show that the criterion used by PINNACLE is related to the PINN generalization error, and empirically demonstrate that PINNACLE is able to outperform existing point selection methods for forward, inverse, and transfer learning problems.},
  author = {Gregory Kang Ruey Lau and Apivich Hemachandra and See-Kiong Ng and Bryan Kian Hsiang Low},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lau2024pinnacle.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=GzNaCp6Vcg},
  publisher = {OpenReview.net},
  title = {{PINNACLE}: {PINN} Adaptive {ColLocation} and Experimental points selection},
  url = {https://openreview.net/forum?id=GzNaCp6Vcg},
  year = {2024}
}

@inproceedings{lawrence2024learning,
  abstract = {Optimizing and certifying the positivity of polynomials are fundamental primitives across mathematics and engineering applications, from dynamical systems to operations research. However, solving these problems in practice requires large semidefinite programs, with poor scaling in dimension and degree. In this work, we demonstrate for the first time that neural networks can effectively solve such problems in a data-driven fashion, achieving tenfold speedups while retaining high accuracy. Moreover, we observe that these polynomial learning problems are equivariant to the non-compact group $SL(2,\mathbb{R})$, which consists of area-preserving linear transformations. We therefore adapt our learning pipelines to accommodate this structure, including data augmentation, a new $SL(2,\mathbb{R})$-equivariant architecture, and an architecture equivariant with respect to its maximal compact subgroup, $SO(2, \mathbb{R})$. Surprisingly, the most successful approaches in practice do not enforce equivariance to the entire group, which we prove arises from an unusual lack of architecture universality for $SL(2,\mathbb{R})$ in particular. A consequence of this result, which is of independent interest, is that there exists an equivariant function for which there is no sequence of equivariant polynomials multiplied by arbitrary invariants that approximates the original function. This is a rare example of a symmetric problem where data augmentation outperforms a fully equivariant architecture, and provides interesting lessons in both theory and practice for other problems with non-compact symmetries.},
  author = {Hannah Lawrence and Mitchell Tong Harris},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lawrence2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gyfXuRfxW2},
  publisher = {OpenReview.net},
  title = {{Learning Polynomial Problems with SL(2, R)-Equivariance}},
  url = {https://openreview.net/forum?id=gyfXuRfxW2},
  year = {2024}
}

@inproceedings{le2024navigating,
  abstract = {Deep generative diffusion models are a promising avenue for {3D} de novo molecular design in materials science and drug discovery. However, their utility is still limited by suboptimal performance on large molecular structures and limited training data. To address this gap, we explore the design space of {E(3)}-equivariant diffusion models, focusing on previously unexplored areas. Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. From this investigation, we present the {EQGAT}-diff model, which consistently outperforms established models for the {QM9} and {GEOM}-Drugs datasets. Significantly, {EQGAT}-diff takes continuous atom positions, while chemical elements and bond types are categorical and uses time-dependent loss weighting, substantially increasing training convergence, the quality of generated samples, and inference time. We also showcase that including chemically motivated additional features like hybridization states in the diffusion process enhances the validity of generated molecules. To further strengthen the applicability of diffusion models to limited training data, we investigate the transferability of {EQGAT}-diff trained on the large {PubChem3D} dataset with implicit hydrogen atoms to target different data distributions. Fine-tuning {EQGAT}-diff for just a few iterations shows an efficient distribution shift, further improving performance throughout data sets. Finally, we test our model on the Crossdocked data set for structure-based de novo ligand generation, underlining the importance of our findings showing state-of-the-art performance on Vina docking scores.},
  address = {Vienna, Austria},
  author = {Tuan Le and Julian Cremer and Frank Noe and Djork-Arn{\'e} Clevert and Kristof T. Sch{\"u}tt},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/le2024navigating.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kzGuiRXZrQ},
  publisher = {OpenReview.net},
  title = {Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo {3D} Molecule Generation},
  url = {https://openreview.net/forum?id=kzGuiRXZrQ},
  year = {2024}
}

@inproceedings{le2024constrained,
  abstract = {Zero-shot cross-lingual transfer utilizing multilingual {LLMs} has become a popular learning paradigm for low-resource languages with no labeled training data. However, for {NLP} tasks that involve fine-grained predictions on words and phrases, the performance of zero-shot cross-lingual transfer learning lags far behind supervised fine-tuning methods. Therefore, it is common to exploit translation and label projection to further improve the performance by (1) translating training data that is available in a high-resource language (e.g., English) together with the gold labels into low-resource languages, and/or (2) translating test data in low-resource languages to a high-source language to run inference on, then projecting the predicted span-level labels back onto the original test data. However, state-of-the-art marker-based label projection methods suffer from translation quality degradation due to the extra label markers injected in the input to the translation model. In this work, we explore a new direction that leverages constrained decoding for label projection to overcome the aforementioned issues. Our new method not only can preserve the quality of translated texts but also has the versatility of being applicable to both translating training and translating test data strategies. This versatility is crucial as our experiments reveal that translating test data can lead to a considerable boost in performance compared to translating only training data. We evaluate on two cross-lingual transfer tasks, namely Named Entity Recognition and Event Argument Extraction, spanning 20 languages. The results demonstrate that our approach outperforms the state-of-the-art marker-based method by a large margin and also shows better performance than other label projection methods that rely on external word alignment.},
  address = {Vienna, Austria},
  author = {Duong Minh Le and Yang Chen and Alan Ritter and Wei Xu},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/le2024constrained.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DayPQKXaQk},
  publisher = {OpenReview.net},
  title = {Constrained Decoding for Cross-lingual Label Projection},
  url = {https://openreview.net/forum?id=DayPQKXaQk},
  year = {2024}
}

@inproceedings{le2024codechain,
  abstract = {Large Language Models ({LLMs}) have already become quite proficient at solving simpler programming tasks like those in {HumanEval} or {MBPP} benchmarks. However, solving more complex and competitive programming tasks is still quite challenging for these models - possibly due to their tendency to generate solutions as monolithic code blocks instead of decomposing them into logical sub-tasks and sub-modules. On the other hand, experienced programmers instinctively write modularized code with abstraction for solving complex tasks, often reusing previously developed modules. To address this gap, we propose {CodeChain}, a novel framework for inference that elicits modularized code generation through a chain of self-revisions, each being guided by some representative sub-modules generated in previous iterations. Concretely, {CodeChain} first instructs the {LLM} to generate modularized codes through chain-of-thought prompting. Then it applies a chain of self-revisions by iterating the two steps: 1) extracting and clustering the generated sub-modules and selecting the cluster representatives as the more generic and re-usable implementations, and 2) augmenting the original chain-of-thought prompt with these selected module-implementations and instructing the {LLM} to re-generate new modularized solutions. We find that by naturally encouraging the {LLM} to reuse the previously developed and verified sub-modules, {CodeChain} can significantly boost both modularity as well as correctness of the generated solutions, achieving relative pass@1 improvements of 35\% on {APPS} and 76\% on {CodeContests}. It is shown to be effective on both {OpenAI} {LLMs} as well as open-sourced {LLMs} like {WizardCoder}. We also conduct comprehensive ablation studies with different methods of prompting, number of clusters, model sizes, program qualities, etc., to provide useful insights that underpin {CodeChain}'s success.},
  address = {Vienna, Austria},
  author = {Hung Le and Hailin Chen and Amrita Saha and Akash Gokul and Doyen Sahoo and Shafiq Joty},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/le2024codechain.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-26},
  pdf = {https://openreview.net/pdf?id=vYhglxSj8j},
  publisher = {OpenReview.net},
  title = {{CodeChain}: Towards Modular Code Generation Through Chain of Self-revisions with Representative Sub-modules},
  url = {https://openreview.net/forum?id=vYhglxSj8j},
  year = {2024}
}

@inproceedings{le2024diffeomorphic,
  abstract = {Mesh deformation plays a pivotal role in many {3D} vision tasks including dynamic simulations, rendering, and reconstruction. However, defining an efficient discrepancy between predicted and target meshes remains an open problem. A prevalent approach in current deep learning is the set-based approach which measures the discrepancy between two surfaces by comparing two randomly sampled point-clouds from the two meshes with Chamfer pseudo-distance. Nevertheless, the set-based approach still has limitations such as lacking a theoretical guarantee for choosing the number of points in sampled point-clouds, and the pseudo-metricity and the quadratic complexity of the Chamfer divergence. To address these issues, we propose a novel metric for learning mesh deformation. The metric is defined by sliced Wasserstein distance on meshes represented as probability measures that generalize the set-based approach. By leveraging probability measure space, we gain flexibility in encoding meshes using diverse forms of probability measures, such as continuous, empirical, and discrete measures via varifold representation. After having encoded probability measures, we can compare meshes by using the sliced Wasserstein distance which is an effective optimal transport distance with linear computational complexity and can provide a fast statistical rate for approximating the surface of meshes. To the end, we employ a neural ordinary differential equation ({ODE}) to deform the input surface into the target shape by modeling the trajectories of the points on the surface. Our experiments on cortical surface reconstruction demonstrate that our approach surpasses other competing methods in multiple datasets and metrics.},
  address = {Vienna, Austria},
  author = {Thanh-Tung Le and Khai Nguyen and Shanlin Sun and Kun Han and Nhat Ho and Xiaohui Xie},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/le2024diffeomorphic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gxhRR8vUQb},
  publisher = {OpenReview.net},
  title = {Diffeomorphic Mesh Deformation via Efficient Optimal Transport for Cortical Surface Reconstruction},
  url = {https://openreview.net/forum?id=gxhRR8vUQb},
  year = {2024}
}

@inproceedings{le2024poincare,
  abstract = {Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit---the graphon. We prove a {Poincar{\'e}} inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for {Paley-Wiener} spaces of graphon signals. Exploiting connections with spectral clustering and {Gaussian} elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks.},
  address = {Vienna, Austria},
  author = {Thien Le and Luana Ruiz and Stefanie Jegelka},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/le2024poincare.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=l3qtSNsPvC},
  publisher = {OpenReview.net},
  title = {A {Poincar{\'e}} Inequality and Consistency Results for Signal Sampling on Large Graphs},
  url = {https://openreview.net/forum?id=l3qtSNsPvC},
  year = {2024}
}

@inproceedings{lebailly2024cribo,
  abstract = {Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, {CrIBo} emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. {CrIBo} shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream segmentation tasks. Our code and pretrained models are publicly available at https://github.com/tileb1/CrIBo.},
  address = {Vienna, Austria},
  author = {Tim Lebailly and Thomas Stegm{\"u}ller and Behzad Bozorgtabar and Jean-Philippe Thiran and Tinne Tuytelaars},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/lebailly2024cribo.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=3M0GXoUEzP},
  publisher = {OpenReview.net},
  title = {{CrIBo}: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping},
  url = {https://openreview.net/forum?id=3M0GXoUEzP},
  year = {2024}
}

@inproceedings{lebeau2024performance,
  abstract = {We study the estimation of a planted signal hidden in a recently introduced nested matrix-tensor model, which is an extension of the classical spiked rank-one tensor model, motivated by multi-view clustering. Prior work has theoretically examined the performance of a tensor-based approach, which relies on finding a best rank-one approximation, a problem known to be computationally hard. A tractable alternative approach consists in computing instead the best rank-one (matrix) approximation of an unfolding of the observed tensor data, but its performance was hitherto unknown. We quantify here the performance gap between these two approaches, in particular by deriving the precise algorithmic threshold of the unfolding approach and demonstrating that it exhibits a {BBP}-type transition behavior. This work is therefore in line with recent contributions which deepen our understanding of why tensor-based methods surpass matrix-based methods in handling structured tensor data.},
  address = {Vienna, Austria},
  author = {Hugo Lebeau and Mohamed El Amine Seddik and Jos{\'e} Henrique de Morais Goulart},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/lebeau2024performance.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ILqA09Oeq2},
  publisher = {OpenReview.net},
  title = {Performance Gaps in Multi-view Clustering under the Nested Matrix-Tensor Model},
  url = {https://openreview.net/forum?id=ILqA09Oeq2},
  year = {2024}
}

@inproceedings{lechowicz2024time,
  abstract = {The online knapsack problem is a classic problem in the field of online algorithms. Its canonical version asks how to pack items of different values and weights arriving online into a capacity-limited knapsack so as to maximize the total value of the admitted items. Although optimal competitive algorithms are known for this problem, they may be fundamentally unfair, i.e., individual items may be treated inequitably in different ways. We formalize a practically-relevant notion of time fairness which effectively models a trade off between static and dynamic pricing in a motivating application such as cloud resource allocation, and show that existing algorithms perform poorly under this metric. We propose a parameterized deterministic algorithm where the parameter precisely captures the {Pareto}-optimal trade-off between fairness (static pricing) and competitiveness (dynamic pricing). We show that randomization is theoretically powerful enough to be simultaneously competitive and fair; however, it does not work well in experiments. To further improve the trade-off between fairness and competitiveness, we develop a nearly-optimal learning-augmented algorithm which is fair, consistent, and robust (competitive), showing substantial performance improvements in numerical experiments.},
  address = {Vienna, Austria},
  author = {Adam Lechowicz and Rik Sengupta and Bo Sun and Shahin Kamali and Mohammad Hajiesmaili},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/lechowicz2024time.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9kG7TwgLYu},
  publisher = {OpenReview.net},
  title = {Time Fairness in Online Knapsack Problems},
  url = {https://openreview.net/forum?id=9kG7TwgLYu},
  year = {2024}
}

@inproceedings{lee2024dreamsmooth,
  abstract = {Model-based reinforcement learning ({MBRL}) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of {MBRL}, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, {DreamSmooth}, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that {DreamSmooth} achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as {Deepmind Control Suite} and {Atari} benchmarks.},
  address = {Vienna, Austria},
  author = {Vint Lee and Pieter Abbeel and Youngwoon Lee},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/lee2024dreamsmooth.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GruDNzQ4ux},
  publisher = {OpenReview.net},
  title = {{DreamSmooth}: Improving Model-based Reinforcement Learning via Reward Smoothing},
  url = {https://openreview.net/forum?id=GruDNzQ4ux},
  year = {2024}
}

@inproceedings{lee2024continual,
  abstract = {Deep neural networks ({DNNs}) have revolutionized tasks such as image classification and speech recognition but often falter when training and test data diverge in distribution. External factors, from weather effects on images to varied speech environments, can cause this discrepancy, compromising {DNN} performance. Online test-time adaptation ({OTTA}) methods present a promising solution, recalibrating models in real-time during the test stage without requiring historical data. However, the {OTTA} paradigm is imperfect, often falling prey to issues such as catastrophic forgetting due to its reliance on noisy, self-trained predictions. Although some contemporary strategies mitigate this by tying adaptations to the static source model, this restricts model flexibility. This paper introduces a continual momentum filtering ({CMF}) framework, leveraging the {Kalman} filter ({KF}) to strike a balance between model adaptability and information retention. The {CMF} intertwines optimization via stochastic gradient descent with a {KF}-based inference process. This methodology not only aids in averting catastrophic forgetting but also provides high adaptability to shifting data distributions. We validate our framework on various {OTTA} scenarios and real-world situations regarding covariate and label shifts, and the {CMF} consistently shows superior performance compared to state-of-the-art methods.},
  address = {Vienna, Austria},
  author = {Jae-Hong Lee and Joon-Hyuk Chang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/lee2024continual.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BllUWdpIOA},
  publisher = {OpenReview.net},
  title = {Continual Momentum Filtering on Parameter Space for Online Test-time Adaptation},
  url = {https://openreview.net/forum?id=BllUWdpIOA},
  year = {2024}
}

@inproceedings{lee2024compose,
  abstract = {Addressing the limitations of text as a source of accurate layout representation in text-conditional diffusion models, many works incorporate additional signals to condition certain attributes within a generated image. Although successful, previous works do not account for the specific localization of said attributes extended into the three dimensional plane. In this context, we present a conditional diffusion model that integrates control over three-dimensional object placement with disentangled representations of global stylistic semantics from multiple exemplar images. Specifically, we first introduce depth disentanglement training to leverage the relative depth of objects as an estimator, allowing the model to identify the absolute positions of unseen objects through the use of synthetic image triplets. We also introduce soft guidance, a method for imposing global semantics onto targeted regions without the use of any additional localization cues. Our integrated framework, Compose and Conquer ({CnC}), unifies these techniques to localize multiple conditions in a disentangled manner. We demonstrate that our approach allows perception of objects at varying depths while offering a versatile framework for composing localized objects with different global semantics.},
  address = {Vienna, Austria},
  archiveprefix = {arXiv},
  author = {Jonghyun Lee and Hansam Cho and {Young Joon} Yoo and {Seoung Bum} Kim and Yonghyun Jeong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2401.09048},
  file = {:/home/b/documents/inproceedings/lee2024compose.pdf:pdf},
  keywords = {Generative Models, Diffusion Models, Image Editing, Image Composition},
  month = {5},
  pdf = {https://openreview.net/pdf?id=p4eG8rCa0b},
  primaryclass = {cs.CV},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Compose and Conquer: Diffusion-Based {3D} Depth Aware Composable Image Synthesis},
  url = {https://openreview.net/forum?id=p4eG8rCa0b},
  year = {2024}
}

@inproceedings{lee2024kernel,
  abstract = {We consider off-policy evaluation ({OPE}) of deterministic target policies for reinforcement learning ({RL}) in environments with continuous action spaces. While it is common to use importance sampling for {OPE}, it suffers from high variance when the behavior policy deviates significantly from the target policy. In order to address this issue, some recent works on {OPE} proposed in-sample learning with importance resampling. Yet, these approaches are not applicable to deterministic target policies for continuous action spaces. To address this limitation, we propose to relax the deterministic target policy using a kernel and learn the kernel metrics that minimize the overall mean squared error of the estimated temporal difference update vector of an action value function, where the action value function is used for policy evaluation. We derive the bias and variance of the estimation error due to this relaxation and provide analytic solutions for the optimal kernel metric. In empirical studies using various test domains, we show that the {OPE} with in-sample learning using the kernel with optimized metric achieves significantly improved accuracy than other baselines.},
  address = {Vienna, Austria},
  author = {Haanvid Lee and Tri Wahyu Guntara and Jongmin Lee and Yung-Kyun Noh and Kee-Eung Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024kernel.pdf:pdf},
  keywords = {Reinforcement Learning, Off-Policy Evaluation, Kernel Methods},
  month = {5},
  pdf = {https://openreview.net/pdf?id=plebgsdiiV},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Kernel Metric Learning for In-Sample Off-Policy Evaluation of Deterministic {RL} Policies},
  url = {https://openreview.net/forum?id=plebgsdiiV},
  year = {2024}
}

@inproceedings{lee2024attention,
  abstract = {Recent studies have shown transformers to be remarkably good in-context learners ({ICL}), but is attention the key architectural component driving this capability? In this work, we evaluate 13 model architectures across synthetic in-context learning tasks to disentangle the effect of architectural components. Our architectures include recurrent and convolution-based neural networks, transformers, state space model inspired architectures like {Mamba}, and hybrid architectures. We find that all of our evaluated architectures are capable of in-context learning under a much broader set of conditions than previously known. While transformers are sample efficient in-context learners, there are stark differences in statistical efficiency and consistency between architectural families. Surprisingly, we find several attention alternatives are sometimes competitive with or better in-context learners than transformers. We also find large variance in which architecture is best as a function of the number of examples provided in-context, the size of the datasets used for training, and the family of functions being learned. No single architecture consistently dominates across all settings.},
  address = {Vienna, Austria},
  author = {Ivan Lee and Nan Jiang and Taylor Berg-Kirkpatrick},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024attention.pdf:pdf},
  keywords = {In-Context Learning, Neural Architectures, Attention Mechanisms, Transformers},
  month = {5},
  pdf = {https://openreview.net/pdf?id=Qwq4cpLtoX},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Is attention required for {ICL}? Exploring the Relationship Between Model Architecture and In-Context Learning Ability},
  url = {https://openreview.net/forum?id=Qwq4cpLtoX},
  year = {2024}
}

@inproceedings{lee2024featurealigned,
  abstract = {We propose Feature-aligned {N-BEATS} as a domain-generalized time series forecasting model. It is a nontrivial extension of {N-BEATS} with doubly residual stacking principle into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of {N-BEATS} in each stack and aligns them stack-wise via an approximate of an optimal transport distance referred to as the {Sinkhorn} divergence. This alignment process mitigates the internal covariate shift across stacks during training, thereby facilitating the domain-generalized learning. We conduct extensive experiments on multiple time series forecasting datasets, demonstrating that the proposed method significantly outperforms the baseline {N-BEATS} and several other state-of-the-art time series forecasting methods in domain generalization scenarios.},
  address = {Vienna, Austria},
  author = {Joonhun Lee and Myeongho Jeon and Myungjoo Kang and Kyunghyun Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024featurealigned.pdf:pdf},
  keywords = {Time Series Forecasting, Deep Learning, Domain Generalization, Representation Learning},
  month = {5},
  pdf = {https://openreview.net/pdf?id=TS8HoIWAPQ},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Feature-aligned {N-BEATS} with {Sinkhorn} divergence},
  url = {https://openreview.net/forum?id=TS8HoIWAPQ},
  year = {2024}
}

@inproceedings{lee2024continual,
  abstract = {Most continual learning ({CL}) algorithms have focused on tackling the stability-plasticity dilemma, that is, the challenge of preventing the forgetting of past tasks while learning new ones. However, we argue that they have overlooked the impact of knowledge transfer when the training dataset of a certain task is biased -- namely, when the dataset contains some spurious correlations that can overly influence the prediction rule of a model. In this work, we carefully design systematic experiments using three benchmark datasets to answer the question from our empirical findings. Specifically, we first show through two-task {CL} experiments that standard {CL} methods, which are oblivious of the dataset bias, can transfer bias from one task to another, both forward and backward. Moreover, we find out this transfer is exacerbated depending on whether the {CL} methods focus on stability or plasticity. We then present that the bias is also transferred and even accumulates in longer task sequences. Finally, we offer a standardized experimental setup and a simple, yet strong plug-in baseline method, dubbed as group-class Balanced Greedy Sampling ({BGS}), which are utilized for the development of more advanced bias-aware {CL} methods.},
  address = {Vienna, Austria},
  author = {Donggyu Lee and Sangwon Jung and Taesup Moon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024continual.pdf:pdf},
  keywords = {Continual Learning, Bias, Spurious Correlation, Dataset Bias},
  month = {5},
  pdf = {https://openreview.net/pdf?id=3Y7r6xueJJ},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Continual Learning in the Presence of Spurious Correlations: Analyses and a Simple Baseline},
  url = {https://openreview.net/forum?id=3Y7r6xueJJ},
  year = {2024}
}

@inproceedings{lee2024geometryaware,
  abstract = {Estimating neural radiance fields ({NeRFs}) is able to generate novel views of a scene from known imagery. Recent approaches have afforded dramatic progress on small bounded regions of the scene. For an unbounded scene where cameras point in any direction and contents exist at any distance, certain mapping functions are used to represent it within a bounded space, yet they either work in object-centric scenes or focus on objects close to the camera. We present geometric understanding of existing mapping functions and propose a novel mapping function based on p-norm distance that adapts to the scene geometry. Our method introduces new ray parameterization for unbounded regions and demonstrates superior performance on both synthetic and real-world datasets for unbounded neural radiance field reconstruction.},
  address = {Vienna, Austria},
  author = {Junoh Lee and Hyunjun Jung and Jin-Hwi Park and Inhwan Bae and Hae-Gon Jeon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024geometryaware.pdf:pdf},
  keywords = {Neural Radiance Fields, Neural Rendering, 3D Scene Reconstruction, Computer Vision},
  month = {5},
  pdf = {https://openreview.net/pdf?id=w7BwaDHppp},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Geometry-Aware Projective Mapping for Unbounded Neural Radiance Fields},
  url = {https://openreview.net/forum?id=w7BwaDHppp},
  year = {2024}
}

@inproceedings{lee2024testam,
  abstract = {Accurate traffic forecasting is challenging due to the complex dependency on road networks, various types of roads, and the abrupt speed change due to the events. Recent works mainly focus on dynamic spatial modeling with adaptive graph embedding or graph attention having less consideration for temporal characteristics and in-situ modeling. We propose a novel deep learning model ({TESTAM}) that uses mixture-of-experts model with three experts: temporal modeling, spatio-temporal modeling with static graph, and dynamic spatio-temporal dependency modeling. Our model achieves state-of-the-art performance on multiple traffic forecasting benchmarks including {METR-LA}, {PEMS-BAY}, and {EXPY-TKY} datasets.},
  address = {Vienna, Austria},
  author = {Hyunwook Lee and Sungahn Ko},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024testam.pdf:pdf},
  keywords = {Traffic Prediction, Deep Learning, Spatio-Temporal Data Modeling, Mixture of Experts},
  month = {5},
  pdf = {https://openreview.net/pdf?id=N0nTk5BSvO},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{TESTAM}: A Time-Enhanced Spatio-Temporal Attention Model with Mixture of Experts},
  url = {https://openreview.net/forum?id=N0nTk5BSvO},
  year = {2024}
}

@inproceedings{lee2024differentiable,
  abstract = {This paper investigates efficient deep neural networks ({DNNs}) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient {DNNs} were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the {Gaussian-Dirichlet} kernel is adopted to learn the structural parameters by proximal gradient descent. On the image and language tasks, our method learns efficient {DNNs} with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.},
  address = {Vienna, Austria},
  author = {Changwoo Lee and Hun-Seok Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024differentiable.pdf:pdf},
  keywords = {Structured Matrices, Efficient Neural Networks, Deep Learning, Optimization},
  month = {5},
  pdf = {https://openreview.net/pdf?id=pAVJKp3Dvn},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks},
  url = {https://openreview.net/forum?id=pAVJKp3Dvn},
  year = {2024}
}

@inproceedings{lee2024sea,
  abstract = {Attention mechanism in transformers has a quadratic complexity with respect to the input sequence length, making it computationally expensive for long sequences. To address this issue, we propose Sparse linear attention with an Estimated Attention mask ({SEA}). Our method uses kernel-based linear attention to estimate the attention matrix and creates a sparse attention matrix via top-k selection. The sparse attention matrix maintains interpretability while reducing computational cost. We demonstrate that {SEA} achieves better perplexity than {OPT-1.3B} baseline while using approximately half the memory. Our approach enables knowledge distillation for transformer complexity reduction and shows promising results on various {NLP} tasks.},
  address = {Vienna, Austria},
  author = {Heejun Lee and Jina Kim and Jeffrey Willette and Sung Ju Hwang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024sea.pdf:pdf},
  keywords = {Sparse Attention, Attention Estimation, Linear Attention, Transformers, Natural Language Processing},
  month = {5},
  pdf = {https://openreview.net/pdf?id=JbcwfmYrob},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{SEA}: Sparse Linear Attention with Estimated Attention Mask},
  url = {https://openreview.net/forum?id=JbcwfmYrob},
  year = {2024}
}

@inproceedings{lee2024dafa,
  abstract = {The disparity in accuracy between classes in standard training is amplified during adversarial training, a phenomenon termed the robust fairness problem. Existing methodologies aimed to enhance robust fairness by sacrificing the model's performance on easier classes in order to improve its performance on harder ones. However, we observe that under adversarial attacks, the majority of the model's predictions for samples from the worst class are biased towards classes similar to the worst class, rather than towards the easy classes. Through theoretical and empirical analysis, we demonstrate that robust fairness deteriorates as the distance between classes decreases. Motivated by these insights, we introduce the Distance-Aware Fair Adversarial Training ({DAFA}) methodology, which addresses robust fairness by taking into account the similarities between classes. Specifically, our method assigns distinct adversarial margins and loss weights to each class and adjusts them to encourage a trade-off in robustness among similar classes. Experimental results across various datasets demonstrate that our method not only maintains average robust accuracy but also significantly improves the worst robust accuracy, indicating a marked improvement in robust fairness compared to existing methods.},
  address = {Vienna, Austria},
  author = {Hyungyu Lee and Saehyung Lee and Hyemi Jang and Junsung Park and Ho Bae and Sungroh Yoon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024dafa.pdf:pdf},
  keywords = {Adversarial Robustness, Robust Fairness, Adversarial Examples, Adversarial Training},
  month = {5},
  pdf = {https://openreview.net/pdf?id=BRdEBlwUW6},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{DAFA}: Distance-Aware Fair Adversarial Training},
  url = {https://openreview.net/forum?id=BRdEBlwUW6},
  year = {2024}
}

@inproceedings{lee2024selfsupervised,
  abstract = {Dataset distillation methods have achieved remarkable success in distilling a large dataset into a small set of representative samples. However, they are not designed to produce a distilled dataset that can be effectively used for facilitating self-supervised pre-training. To address this, we propose a novel problem of distilling an unlabeled dataset into a set of small synthetic samples for efficient self-supervised learning ({SSL}). We prove that gradient calculation in naive bilevel optimization is biased due to randomness from data augmentations or masking. Our solution involves: (1) minimizing mean squared error between a model's synthetic example representations and learnable target feature representations; (2) introducing {MSE} between representations of the inner model and self-supervised target model; (3) optimizing only a linear head on a fixed feature extractor to reduce computational cost. The method is empirically validated on various transfer learning applications.},
  address = {Vienna, Austria},
  author = {Dong Bok Lee and Seanie Lee and Joonho Ko and Kenji Kawaguchi and Juho Lee and Sung Ju Hwang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024selfsupervised.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=h57gkDO2Yg},
  publisher = {OpenReview.net},
  title = {Self-Supervised Dataset Distillation for Transfer Learning},
  url = {https://openreview.net/forum?id=h57gkDO2Yg},
  year = {2024}
}

@inproceedings{lee2024guess,
  abstract = {Maintaining legacy software requires many software and systems engineering hours. Assembly code programs, which demand low-level control over the computer machine state and have no variable names, are particularly difficult for humans to analyze. Existing conventional program translators guarantee correctness, but are hand-engineered for the source and target programming languages in question. Learned transpilation, i.e. automatic translation of code, offers an alternative to manual re-writing and engineering efforts. In this work, we leverage the strengths of language models and symbolic solvers in a neurosymbolic approach to learned transpilation for assembly code. Guess \& Sketch extracts alignment and confidence information from features of the {LM} then passes it to a symbolic solver to resolve semantic equivalence of the transpilation input and output. We test Guess \& Sketch on three different test sets of assembly transpilation tasks, varying in difficulty, and show that it successfully transpiles 57.6\% more examples than {GPT}-4 and 39.6\% more examples than an engineered transpiler.},
  address = {Vienna, Austria},
  author = {Celine Lee and Abdulrahman Mahmoud and Michal Kurek and Simone Campanoni and David Brooks and Stephen Chong and Gu-Yeon Wei and Alexander M. Rush},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024guess.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qPFsIbF3V6},
  publisher = {OpenReview.net},
  title = {Guess \& Sketch: Language Model Guided Transpilation},
  url = {https://openreview.net/forum?id=qPFsIbF3V6},
  year = {2024}
}

@inproceedings{lee2024enhancing,
  abstract = {Transfer learning has recently shown significant performance across various tasks involving deep neural networks. In these transfer learning scenarios, the prior distribution for downstream data becomes crucial in {Bayesian} model averaging ({BMA}). While previous works proposed the prior over the neural network parameters centered around the pre-trained solution, such strategies have limitations when dealing with distribution shifts between upstream and downstream data. This paper introduces nonparametric transfer learning ({NPTL}), a flexible posterior sampling method to address the distribution shift issue within the context of nonparametric learning. The nonparametric learning ({NPL}) method is a recent approach that employs a nonparametric prior for posterior sampling, efficiently accounting for model misspecification scenarios, which is suitable for transfer learning scenarios that may involve the distribution shift between upstream and downstream tasks. Through extensive empirical validations, we demonstrate that our approach surpasses other baselines in {BMA} performance.},
  address = {Vienna, Austria},
  author = {Hyungi Lee and Giung Nam and Edwin Fong and Juho Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024enhancing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vSwu81S33z},
  publisher = {OpenReview.net},
  title = {Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling},
  url = {https://openreview.net/forum?id=vSwu81S33z},
  year = {2024}
}

@inproceedings{lee2024demystifying,
  abstract = {In this work, we prove that, in linear {MDPs}, the feature dimension $d$ is lower bounded by $S/U$ in order to aptly represent transition probabilities, where $S$ is the size of the state space and $U$ is the maximum size of directly reachable states. Hence, $d$ can still scale with $S$ depending on the direct reachability of the environment. To address this limitation of linear {MDPs}, we propose a novel structural aggregation framework based on dynamics, named as the 'dynamics aggregation'. For this newly proposed framework, we design a provably efficient hierarchical reinforcement learning algorithm in linear function approximation that leverages aggregated sub-structures. Our proposed algorithm exhibits statistical efficiency, achieving a regret of $\tilde{O} ( d_{\psi}^{3/2} H^{3/2}\sqrt{ N T} )$, where $d_{\psi}$ represents the feature dimension of aggregated sub{MDPs} and $N$ signifies the number of aggregated sub{MDPs}. We establish that the condition $d_{\psi}^3 N \ll d^{3}$ is readily met in most real-world environments with hierarchical structures, enabling a substantial improvement in the regret bound compared to {LSVI}-{UCB}, which enjoys a regret of $\tilde{O} (d^{3/2} H^{3/2} \sqrt{ T})$. To the best of our knowledge, this work presents the first {HRL} algorithm with linear function approximation that offers provable guarantees.},
  address = {Vienna, Austria},
  author = {Joongkyu Lee and Min-hwan Oh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024demystifying.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RDSj6S8WJe},
  publisher = {OpenReview.net},
  title = {Demystifying Linear {MDPs} and Novel Dynamics Aggregation Framework},
  url = {https://openreview.net/forum?id=RDSj6S8WJe},
  year = {2024}
}

@inproceedings{lee2024soft,
  abstract = {Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose {SoftCLT}, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. {SoftCLT} is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experiments, we demonstrate that {SoftCLT} consistently improves the performance in various downstream tasks including classification, semi-supervised learning, transfer learning, and anomaly detection, showing state-of-the-art performance.},
  address = {Vienna, Austria},
  author = {Seunghan Lee and Taeyoung Park and Kibok Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024soft.pdf:pdf},
  note = {DBLP last modified: 2024-08-22},
  pdf = {https://openreview.net/pdf?id=pAsQSWlDUf},
  publisher = {OpenReview.net},
  title = {Soft Contrastive Learning for Time Series},
  url = {https://openreview.net/forum?id=pAsQSWlDUf},
  year = {2024}
}

@inproceedings{lee2024learning,
  abstract = {Masked time series modeling has recently gained much attention as a self-supervised representation learning strategy for time series. Inspired by masked image modeling in computer vision, recent works first patchify and partially mask out time series, and then train {Transformers} to capture the dependencies between patches by predicting masked patches from unmasked patches. However, we argue that capturing such patch dependencies might not be an optimal strategy for time series representation learning; rather, learning to embed patches independently results in better time series representations. To this end, we propose a simple patch reconstruction task that autoencode each patch without looking at other patches, and a simple patch-wise {MLP} that embeds each patch independently. In addition, we introduce complementary contrastive learning to hierarchically capture adjacent time series information efficiently. Our proposed method improves time series forecasting and classification performance compared to state-of-the-art {Transformer}-based models, while being more efficient in terms of parameters and training/inference time.},
  address = {Vienna, Austria},
  author = {Seunghan Lee and Taeyoung Park and Kibok Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-22},
  pdf = {https://openreview.net/pdf?id=WS7GuBDFa2},
  publisher = {OpenReview.net},
  title = {Learning to Embed Time Series Patches Independently},
  url = {https://openreview.net/forum?id=WS7GuBDFa2},
  year = {2024}
}

@inproceedings{lee2024coordinateaware,
  abstract = {Neural fields, mapping low-dimensional input coordinates to corresponding signals, have shown promising results in representing various signals. Numerous methodologies have been proposed, and techniques employing {MLPs} and grid representations have achieved substantial success. {MLPs} allow compact and high expressibility, yet often suffer from spectral bias and slow convergence speed. On the other hand, methods using grids are free from spectral bias and achieve fast training speed, however, at the expense of high spatial complexity. We propose Coordinate-Aware Modulation ({CAM}), which modulates the intermediate features using scale and shift parameters extracted from the grid representations. This can maintain the strengths of {MLPs} while mitigating any remaining potential biases, facilitating the rapid learning of high-frequency components. In addition, we empirically found that the feature normalizations, which have not been successful in neural field literature, proved to be effective when applied in conjunction with the proposed {CAM}. Experimental results demonstrate that {CAM} enhances the performance of neural representation and improves learning stability across a range of signals.},
  address = {Vienna, Austria},
  author = {Joo Chan Lee and Daniel Rho and Seungtae Nam and Jong Hwan Ko and Eunbyung Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024coordinateaware.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=4UiLqimGm5},
  publisher = {OpenReview.net},
  title = {Coordinate-Aware Modulation for Neural Fields},
  url = {https://openreview.net/forum?id=4UiLqimGm5},
  year = {2024}
}

@inproceedings{lee2024unsupervised,
  abstract = {A novel clustering algorithm for orderable data, called unsupervised order learning ({UOL}), is proposed in this paper. First, we develop the ordered $k$-means to group objects into ordered clusters by reducing the deviation of an object from consecutive clusters. We then train a network to create an embedding space where objects can be sorted along line segments determined by cluster centroids. The network learns to embed objects in such a way that their order along the line segments corresponds to their natural ordering. Through extensive experiments on various datasets, we demonstrate that {UOL} effectively discovers the natural ordering of objects without supervision, outperforming existing clustering methods on orderable data.},
  address = {Vienna, Austria},
  author = {Seon-Ho Lee and Nyeong-Ho Shin and Chang-Su Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024unsupervised.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1CK45cqkEh},
  publisher = {OpenReview.net},
  title = {Unsupervised Order Learning},
  url = {https://openreview.net/forum?id=1CK45cqkEh},
  year = {2024}
}

@inproceedings{lee2024teaching,
  abstract = {Large language models like {GPT}-4 exhibit emergent capabilities across general-purpose tasks, such as basic arithmetic, when trained on extensive text data, even though these tasks are not explicitly encoded by the unsupervised, next-token prediction objective. This study investigates how even small transformers, trained from random initialization, can efficiently learn arithmetic operations such as addition, multiplication, and elementary functions like square root, using the next-token prediction objective. We first demonstrate that conventional training data is not the most effective for arithmetic learning, and simple formatting changes can significantly improve accuracy. This leads to sharp phase transitions as a function of training data scale, which, in some cases, can be explained through connections to low-rank matrix completion. Building on prior work, we then train on chain-of-thought style data that includes intermediate step results. Even in the complete absence of pretraining, this approach significantly and simultaneously improves accuracy, sample complexity, and convergence speed. We also study the interplay between arithmetic and text data during training and examine the effects of few-shot prompting, pretraining, and parameter scaling. Additionally, we discuss the challenges associated with length generalization. Our work highlights the importance of high-quality, instructive data that considers the particular characteristics of the next-word prediction loss for rapidly eliciting arithmetic capabilities.},
  address = {Vienna, Austria},
  author = {Nayoung Lee and Kartik Sreenivasan and Jason D. Lee and Kangwook Lee and Dimitris Papailiopoulos},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024teaching.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dsUB4bst9S},
  publisher = {OpenReview.net},
  title = {Teaching Arithmetic to Small Transformers},
  url = {https://openreview.net/forum?id=dsUB4bst9S},
  year = {2024}
}

@inproceedings{lee2024dreamflow,
  abstract = {Recent progress in text-to-{3D} generation has been achieved through the utilization of score distillation methods: they make use of the pre-trained text-to-image ({T2I}) diffusion models by distilling via the diffusion model training objective. However, such an approach inevitably results in the use of random timesteps at each update, which increases the variance of the gradient and ultimately prolongs the optimization process. In this paper, we propose to enhance the text-to-{3D} optimization by leveraging the {T2I} diffusion prior in the generative sampling process with a predetermined timestep schedule. To this end, we interpret text-to-{3D} optimization as a multi-view image-to-image translation problem, and propose a solution by approximating the probability flow. By leveraging the proposed novel optimization algorithm, we design {DreamFlow}, a practical three-stage coarse-to-fine text-to-{3D} optimization framework that enables fast generation of high-quality and high-resolution (i.e., $1024 \times 1024$) {3D} contents. For example, we demonstrate that {DreamFlow} is 5 times faster than the existing state-of-the-art text-to-{3D} method, while producing more photorealistic {3D} contents.},
  address = {Vienna, Austria},
  author = {Kyungmin Lee and Kihyuk Sohn and Jinwoo Shin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lee2024dreamflow.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=GURqUuTebY},
  publisher = {OpenReview.net},
  title = {{DreamFlow}: High-quality text-to-{3D} generation by Approximating Probability Flow},
  url = {https://openreview.net/forum?id=GURqUuTebY},
  year = {2024}
}

@inproceedings{lee2024pretraining,
  abstract = {Proteins can be represented in various ways, including their sequences, 3D structures, and surfaces. While recent studies have successfully employed sequence- or structure-based representations to address multiple tasks in protein science, there has been significant oversight in incorporating protein surface information, a critical factor for protein function. In this paper, we present a pre-training strategy that incorporates information from protein sequences, 3D structures, and surfaces to improve protein representation learning. Specifically, we utilize Implicit Neural Representations (INRs) for learning surface characteristics, and name it ProteinINR. We confirm that ProteinINR successfully reconstructs protein surfaces, and integrate this surface learning into the existing pre-training strategy of sequences and structures. Our results demonstrate that our approach can enhance performance in various downstream tasks, thereby underscoring the importance of including surface attributes in protein representation learning. These findings underline the importance of understanding protein surfaces for generating effective protein representations.},
  author = {Youhan Lee and Hasun Yu and Jaemyung Lee and Jaehoon Kim},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/lee2024pretraining.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BEH4mGo7zP},
  publisher = {OpenReview.net},
  title = {Pre-training Sequence, Structure, and Surface Features for Comprehensive Protein Representation Learning},
  url = {https://openreview.net/forum?id=BEH4mGo7zP},
  year = {2024}
}

@inproceedings{lee2024netinfof,
  abstract = {Given a node-attributed graph, and a graph task (link prediction or node classification), can we tell if a graph neural network ({GNN}) will perform well? More specifically, do the graph structure and the node features carry enough usable information for the task? This is the question we answer in this paper. We develop a framework that measures the usable information (UI) contained in the graph structure and the node features and quantifies the performance limits of {GNNs}. Specifically, we propose the {NetInfoF} framework, which measures network usable information by quantifying how much information is provided by the network structure and node features for a given task. Our {NetInfoF} framework leverages the concept of conditional entropy to measure the usable information in the graph and provides insights into when {GNNs} can perform well. Notably, our framework can predict the performance of {GNNs} without training them, which is computationally efficient and practically useful.},
  author = {Meng-Chieh Lee and Haiyang Yu and Jian Zhang and Vassilis N. Ioannidis and Xiang Song and Soji Adeshina and Da Zheng and Christos Faloutsos},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/lee2024netinfof.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=KY8ZNcljVU},
  publisher = {OpenReview.net},
  title = {{NetInfoF} Framework: Measuring and Exploiting Network Usable Information},
  url = {https://openreview.net/forum?id=KY8ZNcljVU},
  year = {2024}
}

@inproceedings{lee2024languageinformed,
  abstract = {Our understanding of the visual world is centered around various concept axes, characterizing different aspects of visual entities. While different concept axes can be easily specified by language, e.g., color, the exact visual nuances along each axis often exceed the limitations of linguistic articulations, e.g., a particular style of painting. In this work, our goal is to learn a language-informed visual concept representation, by simply distilling large pre-trained vision-language models. Specifically, we train a concept bottleneck model to predict natural language concept descriptions from visual features, and then use the learned bottleneck features as concept axes. We show that such language-informed concept axes 1) are interpretable to humans, 2) allow flexible specification of new concept axes without additional training, and 3) lead to effective disentangled concept editing. Through extensive experiments and analysis, we demonstrate the effectiveness of our approach over related baselines on standard benchmarks.},
  author = {Sharon Lee and Yunzhi Zhang and Shangzhe Wu and Jiajun Wu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/lee2024languageinformed.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=juuyW8B8ig},
  publisher = {OpenReview.net},
  title = {Language-Informed Visual Concept Learning},
  url = {https://openreview.net/forum?id=juuyW8B8ig},
  year = {2024}
}

@inproceedings{lei2024neural,
  abstract = {Quantum kernels hold great promise for offering computational advantages over classical learners, with the effectiveness of these kernels closely tied to the design of the feature map. However, the challenge of designing effective quantum feature maps for real-world datasets, particularly in the absence of sufficient prior information, remains a significant obstacle. In this study, we present a data-driven approach that automates the design of problem-specific quantum feature maps. Our approach leverages feature-selection techniques to handle high-dimensional data on near-term quantum machines with limited qubits, and incorporates a deep neural predictor to efficiently evaluate the performance of various candidate quantum kernels. Through extensive numerical simulations on different datasets, we demonstrate the superiority of our proposal over prior methods, especially for the capability of eliminating the kernel concentration issue and identifying the feature map with prediction advantages. Our work not only unlocks the potential of quantum kernels for enhancing real-world tasks, but also highlights the substantial role of deep learning in advancing quantum machine learning.},
  author = {Cong Lei and Yuxuan Du and Peng Mi and Jun Yu and Tongliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/lei2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8htNAnMSyP},
  publisher = {OpenReview.net},
  title = {Neural Auto-designer for Enhanced Quantum Kernels},
  url = {https://openreview.net/forum?id=8htNAnMSyP},
  year = {2024}
}

@inproceedings{lei2024unio4,
  abstract = {Combining offline and online reinforcement learning ({RL}) is crucial for efficient and safe learning. However, previous approaches treat offline and online learning as separate procedures, resulting in redundant designs and limited performance. We ask: {*}Can we achieve straightforward yet effective offline and online learning without introducing extra conservatism or regularization?{*} In this study, we propose Uni-{O4}, which utilizes an on-policy objective for both offline and online learning. Owing to the alignment of objectives in two phases, the {RL} agent can transfer between offline and online learning seamlessly. This property enhances the flexibility of the learning paradigm, allowing for arbitrary combinations of pretraining, fine-tuning, offline, and online learning. In the offline phase, specifically, Uni-{O4} leverages diverse ensemble policies to address the mismatch issues between the estimated behavior policy and the offline dataset. Through a simple offline policy evaluation ({OPE}) approach, Uni-{O4} can achieve multi-step policy improvement safely. We demonstrate that by employing the method above, the fusion of these two paradigms can yield superior offline initialization as well as stable and rapid online fine-tuning capabilities. Through real-world robot tasks, we highlight the benefits of this paradigm for rapid deployment in challenging, previously unseen real-world environments.},
  author = {Kun Lei and Zhengmao He and Chenhao Lu and Kaizhe Hu and Yang Gao and Huazhe Xu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/lei2024unio4.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=tbFBh3LMKi},
  publisher = {OpenReview.net},
  title = {Uni-{O4}: Unifying Online and Offline Deep Reinforcement Learning with Multi-Step On-Policy Optimization},
  url = {https://openreview.net/forum?id=tbFBh3LMKi},
  year = {2024}
}

@inproceedings{leng2024prompttts,
  abstract = {Speech conveys more information than text, as the same word can be uttered in various voices to convey diverse information. Compared to traditional text-to-speech ({TTS}) methods relying on speech prompts (reference speech) for voice variability, using text prompts (descriptions) is more user-friendly since speech prompts can be hard to find or may not exist at all. PromptTTS 2 addresses two main challenges: 1) the one-to-many problem, where not all details about voice variability can be described in the text prompt, and 2) the limited availability of text prompt datasets, where vendors and large cost of data labeling are required to write text prompts for speech. {PromptTTS} 2 addresses these challenges with a variation network to provide variability information of voice not captured by text prompts, and a prompt generation pipeline to utilize the large language models ({LLM}) to compose high quality text prompts. Experimental results demonstrate that {PromptTTS} 2 can generate high-quality, diverse speech that matches the text prompt descriptions. Audio samples are available at https://speechresearch.github.io/prompttts2/.},
  author = {Yichong Leng and Zhifang Guo and Kai Shen and Zeqian Ju and Xu Tan and Eric Liu and Yufei Liu and Dongchao Yang and Leying Zhang and Kaitao Song and Lei He and Xiangyang Li and Sheng Zhao and Tao Qin and Jiang Bian},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/leng2024prompttts.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NsCXDyv2Bn},
  publisher = {OpenReview.net},
  title = {{PromptTTS} 2: Describing and Generating Voices with Text Prompt},
  url = {https://openreview.net/forum?id=NsCXDyv2Bn},
  year = {2024}
}

@inproceedings{levi2024grokking,
  abstract = {Grokking is the intriguing phenomenon where a model learns to generalize long after it has fit the training data. We show both analytically and numerically that grokking can surprisingly occur in linear networks performing linear tasks in a simple teacher-student setup with Gaussian inputs. In this setting, the full training dynamics is derived in terms of the training and generalization data covariance matrix. We present exact predictions on how the grokking time depends on input and output dimensionality, train sample size, regularization, and network initialization. We demonstrate that the sharp increase in generalization accuracy may not imply a transition from "memorization" to "understanding", but can simply be an artifact of the accuracy measure. We provide empirical verification for our calculations, along with preliminary results indicating that some predictions also hold for deeper networks, with non-linear activations.},
  author = {Noam Levi and Alon Beck and Yohai Bar-Sinai},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/levi2024grokking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GH2LYb9XV0},
  publisher = {OpenReview.net},
  title = {Grokking in Linear Estimators -- A Solvable Model that Groks without Understanding},
  url = {https://openreview.net/forum?id=GH2LYb9XV0},
  year = {2024}
}

@inproceedings{li2024iphyre,
  abstract = {Current evaluation protocols predominantly assess physical reasoning in stationary scenes, creating a gap in evaluating agents' abilities to interact with dynamic events. While contemporary methods allow agents to modify initial scene configurations and observe consequences, they lack the capability to interact with events in real time. To address this, we introduce {I-PHYRE}, a framework that challenges agents to simultaneously exhibit intuitive physical reasoning, multi-step planning, and in-situ intervention. In {I-PHYRE}, agents are required to reason about the physics and then interact with a scene to achieve a goal by placing objects and applying forces. Successful task solving demands ``intuitive physical reasoning'' (i.e., a quick, approximate understanding of physics), ``multi-step'' planning (i.e., an extensive sequence planning where each intervention can alter subsequent choices), and ``in-situ'' intervention (i.e., timely object manipulation where the timing deviations can cause task failure). We formulated four game splits to scrutinize agents' learning and generalization of interactive physical reasoning principles. We explored three planning strategies and examined both supervised and reinforcement agents' zero-shot generalization proficiency on {I-PHYRE}. The outcomes highlight a notable gap between existing learning algorithms and human performance, emphasizing the need for more research in enhancing agents with interactive physical reasoning capabilities.},
  author = {Shiqian Li and Kewen Wu and Chi Zhang and Yixin Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024iphyre.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1bbPQShCT2},
  publisher = {OpenReview.net},
  title = {{I-PHYRE}: Interactive Physical Reasoning},
  url = {https://openreview.net/forum?id=1bbPQShCT2},
  year = {2024}
}

@inproceedings{li2024humangenerated,
  abstract = {Despite the promising few-shot ability of large language models ({LLMs}), the standard paradigm of In-context Learning ({ICL}) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for {ICL}. To answer this question, we propose self-contemplation prompting strategy ({SEC}), a paradigm free from human-crafted demonstrations. The key point of {SEC} is that, instead of using hand-crafted examples as demonstrations in {ICL}, {SEC} asks {LLMs} to first create demonstrations on their own, based on which the final output is generated. {SEC} is a flexible framework and can be adapted to both the vanilla {ICL} and the chain-of-thought ({CoT}), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understanding, and code generation benchmarks, show that {SEC}, which does not require hand-crafted demonstrations, significantly outperforms the zero-shot learning strategy, and achieves comparable results to {ICL} with hand-crafted demonstrations. Our finding demonstrates that for many tasks, contemporary {LLMs} possess a sufficient level of competence to exclusively depend on their own capacity for decision making, removing the need for external training data.},
  author = {Rui Li and Guoyin Wang and Jiwei Li},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024humangenerated.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=frRDT6EOhg},
  publisher = {OpenReview.net},
  title = {Are Human-generated Demonstrations Necessary for In-context Learning?},
  url = {https://openreview.net/forum?id=frRDT6EOhg},
  year = {2024}
}

@inproceedings{li2024loftq,
  abstract = {Quantization is an indispensable technique for serving Large Language Models ({LLMs}) and has recently found its way into {LoRA} fine-tuning. In this work we focus on the scenario where quantization and {LoRA} fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus {LoRA} fine-tuning approach. In response, we propose {LoftQ} ({LoRA}-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an {LLM} and finds a proper low-rank initialization for {LoRA} fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes.},
  author = {Yixiao Li and Yifan Yu and Chen Liang and Nikos Karampatziakis and Pengcheng He and Weizhu Chen and Tuo Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024loftq.pdf:pdf},
  note = {DBLP last modified: 2025-05-27},
  pdf = {https://openreview.net/pdf?id=LzPWWPAdY4},
  publisher = {OpenReview.net},
  title = {{LoftQ}: {LoRA}-Fine-Tuning-aware Quantization for Large Language Models},
  url = {https://openreview.net/forum?id=LzPWWPAdY4},
  year = {2024}
}

@inproceedings{li2024instant3d,
  abstract = {Text-to-3D with diffusion models has achieved remarkable progress in recent years. However, existing methods either rely on score distillation-based optimization which suffer from slow inference, low diversity and Janus problems, or are feed-forward methods that generate low-quality results due to the scarcity of 3D training data. In this paper, we propose Instant3D, a novel method that generates high-quality and diverse 3D assets from text prompts in a feed-forward manner. We adopt a two-stage paradigm, which first generates a sparse set of four structured and consistent views from text in one shot with a fine-tuned 2D text-to-image diffusion model, and then directly regresses the NeRF from the generated images with a novel transformer-based sparse-view reconstructor. Through extensive experiments, we demonstrate that our method can generate diverse 3D assets of high visual quality within 20 seconds, which is two orders of magnitude faster than previous optimization-based methods that can take 1 to 10 hours.},
  address = {Vienna, Austria},
  author = {Jiahao Li and Hao Tan and Kai Zhang and Zexiang Xu and Fujun Luan and Yinghao Xu and Yicong Hong and Kalyan Sunkavalli and Greg Shakhnarovich and Sai Bi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024instant3d.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=2lDQLiH1W4},
  publisher = {OpenReview.net},
  title = {{Instant3D}: Fast Text-to-{3D} with Sparse-view Generation and Large Reconstruction Model},
  url = {https://openreview.net/forum?id=2lDQLiH1W4},
  year = {2024}
}

@inproceedings{li2024crossloco,
  abstract = {Human motion driven control ({HMDC}) is an effective approach for generating natural and compelling robot motions while preserving high-level semantics. However, establishing the correspondence between humans and robots with different body structures is not straightforward due to the mismatches in kinematics and dynamics properties, which causes intrinsic ambiguity to the problem. Many previous algorithms approach this motion retargeting problem with unsupervised learning, which requires the prerequisite skill sets. However, it will be extremely costly to learn all the skills without understanding the given human motions, particularly for high-dimensional robots. In this work, we introduce CrossLoco, a guided unsupervised reinforcement learning framework that simultaneously learns robot skills and their correspondence to human motions. Our key innovation is to introduce a cycle-consistency-based reward term designed to maximize the mutual information between human motions and robot states. We demonstrate that the proposed framework can generate compelling robot motions by translating diverse human motions, such as running, hopping, and dancing. We quantitatively compare our CrossLoco against the manually engineered and unsupervised baseline algorithms along with the ablated versions of our framework and demonstrate that our method translates human motions with better accuracy, diversity, and user preference. We also showcase its utility in other applications, such as synthesizing robot movements from language input and enabling interactive robot control.},
  address = {Vienna, Austria},
  author = {Tianyu Li and Hyunyoung Jung and Matthew C. Gombolay and Yong Kwon Cho and Sehoon Ha},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024crossloco.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=UCfz492fM8},
  publisher = {OpenReview.net},
  title = {{CrossLoco}: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning},
  url = {https://openreview.net/forum?id=UCfz492fM8},
  year = {2024}
}

@inproceedings{li2024get,
  abstract = {The success of recent text-to-image diffusion models is largely due to their capacity to be guided by a complex text prompt, which enables users to precisely describe the desired content. However, these models struggle to effectively suppress the generation of undesired content, which is explicitly requested to be omitted from the generated image in the prompt. In this paper, we analyze how to manipulate the text embeddings and remove unwanted content from them. We introduce two contributions, which we refer to as soft-weighted regularization and inference-time text embedding optimization. The first regularizes the text embedding matrix and effectively suppresses the undesired content. The second method aims to further suppress the unwanted content generation of the prompt, and encourages the generation of desired content. We evaluate our method quantitatively and qualitatively on extensive experiments, validating its effectiveness. Furthermore, our method is generalizable to both the pixel-space diffusion models (i.e. {DeepFloyd-IF}) and the latent-space diffusion models (i.e. Stable Diffusion).},
  address = {Vienna, Austria},
  author = {Senmao Li and Joost van de Weijer and Taihang Hu and Fahad Shahbaz Khan and Qibin Hou and Yaxing Wang and Jian Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024get.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=zpVPhvVKXk},
  publisher = {OpenReview.net},
  title = {Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models},
  url = {https://openreview.net/forum?id=zpVPhvVKXk},
  year = {2024}
}

@inproceedings{li2024towards,
  abstract = {Language Models ({LMs}) have greatly influenced diverse domains. However, their inherent limitation in comprehending {3D} molecular structures has considerably constrained their potential in the biomolecular domain. To bridge this gap, we focus on {3D} molecule-text interpretation, and propose {3D-MoLM}: {3D}-Molecular Language Modeling. Specifically, {3D-MoLM} enables an {LM} to interpret and analyze {3D} molecules by equipping the {LM} with a {3D} molecular encoder. This integration is achieved by a {3D} molecule-text projector, bridging the {3D} molecular encoder's representation space and the {LM}'s input space. Moreover, to enhance {3D-MoLM}'s ability of cross-modal molecular understanding and instruction following, we meticulously curated a {3D} molecule-centric instruction tuning dataset -- {3D-MoIT}. Through {3D} molecule-text alignment and {3D} molecule-centric instruction tuning, {3D-MoLM} establishes an integration of {3D} molecular encoder and {LM}. It significantly surpasses existing baselines on downstream tasks, including molecule-text retrieval, molecule captioning, and more challenging open-text molecular {QA} tasks, especially focusing on {3D}-dependent properties.},
  address = {Vienna, Austria},
  author = {Sihang Li and Zhiyuan Liu and Yanchen Luo and Xiang Wang and Xiangnan He and Kenji Kawaguchi and Tat-Seng Chua and Qi Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024towards.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=xI4yNlkaqh},
  publisher = {OpenReview.net},
  title = {Towards {3D} Molecule-Text Interpretation in Language Models},
  url = {https://openreview.net/forum?id=xI4yNlkaqh},
  year = {2024}
}

@inproceedings{li2024fewshot,
  abstract = {Can a pre-trained generator be adapted to the hybrid of multiple target domains and generate images with integrated attributes of them? In this work, we introduce a new task -- Few-shot Hybrid Domain Adaptation ({HDA}). Given a source generator and several target domains, {HDA} aims to acquire an adapted generator that preserves the integrated attributes of all target domains, without overriding the source domain's characteristics. Compared with Domain Adaptation ({DA}), {HDA} offers greater flexibility and versatility to adapt generators to more composite and expansive domains. Simultaneously, {HDA} also presents more challenges than {DA} as we have access only to images from individual target domains and lack authentic images from the hybrid domain. To address this issue, we introduce a discriminator-free framework that directly encodes different domains' images into well-separable subspaces. To achieve {HDA}, we propose a novel directional subspace loss comprised of a distance loss and a direction loss. Concretely, the distance loss blends the attributes of all target domains by reducing the distances from generated images to all target subspaces. The direction loss preserves the characteristics from the source domain by guiding the adaptation along the perpendicular to subspaces. Experiments show that our method can obtain numerous domain-specific attributes in a single adapted generator, which surpasses the baseline methods in semantic similarity, image fidelity, and cross-domain consistency.},
  address = {Vienna, Austria},
  author = {Hengjia Li and Yang Liu and Linxuan Xia and Yuqi Lin and Wenxiao Wang and Tu Zheng and Zheng Yang and Xiaohui Zhong and Xiaobo Ren and Xiaofei He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024fewshot.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=FE2e8664Sl},
  publisher = {OpenReview.net},
  title = {Few-shot Hybrid Domain Adaptation of Image Generator},
  url = {https://openreview.net/forum?id=FE2e8664Sl},
  year = {2024}
}

@inproceedings{li2024learning,
  abstract = {Generalized Linear Models ({GLMs}) encompass a wide array of regression and classification models, where prediction is a function of a linear combination of the input variables. Often in real-world scenarios, a number of observations would be added into or removed from the existing training dataset, necessitating the development of learning systems that can efficiently train optimal models with varying observations in an online (sequential) manner instead of retraining from scratch. Despite the significance of data-varying scenarios, most existing approaches to sparse {GLMs} concentrate on offline batch updates, leaving online solutions largely underexplored. In this work, we present the first algorithm without compromising accuracy for {GLMs} regularized by sparsity-enforcing penalties trained on varying observations. Our methodology is capable of handling the addition and deletion of observations simultaneously, while adaptively updating data-dependent regularization parameters to ensure the best statistical performance. Specifically, we recast sparse {GLMs} as a bilevel optimization objective upon varying observations and characterize it as an explicit gradient flow in the underlying space for the inner and outer subproblems we are optimizing over, respectively. We further derive a set of rules to ensure a proper transition at regions of non-smoothness, and establish the guarantees of theoretical consistency and finite convergence. Encouraging results are exhibited on real-world benchmarks.},
  address = {Vienna, Austria},
  author = {Diyang Li and Charles Ling and Zhiqiang Xu and Huan Xiong and Bin Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024learning.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=wISvONp3Kq},
  publisher = {OpenReview.net},
  title = {Learning No-Regret Sparse Generalized Linear Models with Varying Observation(s)},
  url = {https://openreview.net/forum?id=wISvONp3Kq},
  year = {2024}
}

@inproceedings{li2024merge,
  abstract = {Sparsely activated Mixture-of-Experts ({SMoE}) has shown promise to scale up the learning capacity of neural networks, however, they have issues like: (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla {SMoE} models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact {SMoE} model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for {SMoE}. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is missing to bring all of them in alignment. To address these challenges, we propose a novel merging algorithm for {SMoE}, i.e., M-{SMoE}, which leverages routing statistics to guide expert merging. Specifically, it starts with neuron permutation alignment for experts; then, dominant experts and their ``group members'' are formed based on routing policies; lastly, every expert group is merged into a single expert by utilizing each expert's activation frequency as their weight for merging, thus diminishing the impact of insignificant experts. Moreover, we draw an interesting observation that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression. Hence, our final method, MC-{SMoE} (i.e., Merge, then Compress {SMoE}), further decomposes the merged experts into low-rank and structural sparse alternatives. Extensive experiments across 8 benchmarks validate the effectiveness of our proposals. For instance, our MC-{SMoE} achieves up to 80\% memory and a 20\% {FLOPs} reduction, with virtually no loss in performance.},
  address = {Vienna, Austria},
  author = {Pingzhi Li and Zhenyu Zhang and Prateek Yadav and Yi-Lin Sung and Yu Cheng and Mohit Bansal and Tianlong Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024merge.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=eFWG9Cy3WK},
  publisher = {OpenReview.net},
  title = {Merge, Then Compress: Demystify Efficient {SMoE} with Hints from Its Routing Policy},
  url = {https://openreview.net/forum?id=eFWG9Cy3WK},
  year = {2024}
}

@inproceedings{li2024unveiling,
  abstract = {As the cost associated with fine-tuning Large Language Models ({LLMs}) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within {LLMs}. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for {LLMs}. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in {LLMs}---a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp the innate knowledge structure of {LLMs}. Experimental results vividly demonstrate that knowledge editing might inadvertently cast a shadow of unintended consequences on {LLMs}, which warrant attention and efforts for future works. Code and data are available at https://github.com/zjunlp/PitfallsKnowledgeEditing.},
  address = {Vienna, Austria},
  author = {Zhoubo Li and Ningyu Zhang and Yunzhi Yao and Mengru Wang and Xi Chen and Huajun Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024unveiling.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=fNktD3ib16},
  publisher = {OpenReview.net},
  title = {Unveiling the Pitfalls of Knowledge Editing for Large Language Models},
  url = {https://openreview.net/forum?id=fNktD3ib16},
  year = {2024}
}

@inproceedings{li2024efficient,
  abstract = {Temporal abstraction and efficient planning pose significant challenges in offline reinforcement learning, mainly when dealing with domains that involve temporally extended tasks and delayed sparse rewards. Existing methods typically plan in the raw action space and can be inefficient and inflexible. Latent action spaces offer a more flexible approach, capturing only possible actions within the behavior policy support and decoupling the temporal structure between planning and modeling. However, current latent-action-based methods are limited to discrete spaces and require expensive planning steps. This paper presents a unified framework for continuous latent action space representation learning and planning by leveraging latent, score-based diffusion models. The paper establishes the theoretical equivalence between planning in the latent action space and energy-guided sampling with a pretrained diffusion model and incorporates a novel sequence-level exact sampling method.},
  address = {Vienna, Austria},
  author = {Wenhao Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024efficient.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=btpgDo4u4j},
  publisher = {OpenReview.net},
  title = {Efficient Planning with Latent Diffusion},
  url = {https://openreview.net/forum?id=btpgDo4u4j},
  year = {2024}
}

@inproceedings{li2024likelihood,
  abstract = {Cascaded models are multi-scale generative models with a marked capacity for producing perceptually impressive samples at high resolutions. In this work, we show that they can also be excellent likelihood models, so long as we overcome a fundamental difficulty with probabilistic multi-scale models: the intractability of the likelihood function. Chiefly, in cascaded models each intermediary scale introduces extraneous variables that cannot be tractably marginalized out for likelihood evaluation. This issue vanishes by modeling the diffusion process on latent spaces induced by a class of transformations we call hierarchical volume-preserving maps, which decompose spatially structured data in a hierarchical fashion without introducing local distortions in the latent space. We demonstrate that two such maps are well-known in the literature for multiscale modeling: {Laplacian} pyramids and wavelet transforms.},
  address = {Vienna, Austria},
  author = {Henry Li and Ronen Basri and Yuval Kluger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024likelihood.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=sojpn00o8z},
  publisher = {OpenReview.net},
  title = {Likelihood Training of Cascaded Diffusion Models via Hierarchical Volume-preserving Maps},
  url = {https://openreview.net/forum?id=sojpn00o8z},
  year = {2024}
}

@inproceedings{li2024soft,
  abstract = {Current diffusion models have an expressive bottleneck in backward denoising and limited approximation capabilities. We introduce Soft Mixture Denoising (SMD), a new approach that permits diffusion models to well approximate any Gaussian mixture distributions and improves performance, especially with few backward iterations.},
  address = {Vienna, Austria},
  author = {Yangming Li and Boris van Breugel and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024soft.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=aaBnFAyW9O},
  publisher = {OpenReview.net},
  title = {Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models},
  url = {https://openreview.net/forum?id=aaBnFAyW9O},
  year = {2024}
}

@inproceedings{li2024leveraging,
  abstract = {This paper introduces ITIT, a training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data. The method uses a joint encoder with separate image and text decoders to enable bidirectional generation, leveraging a small set of paired data while training on larger unpaired datasets.},
  address = {Vienna, Austria},
  author = {Tianhong Li and Sangnie Bhardwaj and Yonglong Tian and Han Zhang and Jarred Barber and Dina Katabi and Guillaume Lajoie and Huiwen Chang and Dilip Krishnan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024leveraging.pdf:pdf},
  month = {5},
  note = {ICLR 2024 Spotlight. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=kNjrhD67LP},
  publisher = {OpenReview.net},
  title = {Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency},
  url = {https://openreview.net/forum?id=kNjrhD67LP},
  year = {2024}
}

@inproceedings{li2024boosting,
  abstract = {Current defenses against graph attacks often rely on certain properties to eliminate structural perturbations by identifying adversarial edges from normal edges. However, this dependence makes defenses vulnerable to adaptive (white-box) attacks from adversaries with the same knowledge. This work takes an out-of-distribution (OOD) perspective to improve adversarial robustness of Graph Neural Networks.},
  address = {Vienna, Austria},
  author = {Kuan Li and Yiwen Chen and Yang Liu and Jin Wang and Qing He and Minhao Cheng and Xiang Ao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024boosting.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DCDT918ZkI},
  publisher = {OpenReview.net},
  title = {Boosting the Adversarial Robustness of Graph Neural Networks: An {OOD} Perspective},
  url = {https://openreview.net/forum?id=DCDT918ZkI},
  year = {2024}
}

@inproceedings{li2024sweetdreamer,
  abstract = {This paper addresses the challenge of generating consistent 3D models from 2D diffusion models. The key insight is that 2D diffusion models solely learn view-agnostic priors and thus lack 3D knowledge, leading to multi-view inconsistency. The authors propose aligning 2D geometric priors with 3D shapes to resolve this issue.},
  address = {Vienna, Austria},
  author = {Weiyu Li and Rui Chen and Xuelin Chen and Ping Tan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024sweetdreamer.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=extpNXo6hB},
  publisher = {OpenReview.net},
  title = {{SweetDreamer}: Aligning Geometric Priors in {2D} diffusion for Consistent Text-to-{3D}},
  url = {https://openreview.net/forum?id=extpNXo6hB},
  year = {2024}
}

@inproceedings{li2024transformermodulated,
  abstract = {Transformers have gained widespread usage in multivariate time series (MTS) forecasting. This work introduces a Transformer-Modulated Diffusion Model (TMDM) to enable precise distribution forecasting for MTS by combining the strengths of transformers and diffusion models for probabilistic time series prediction.},
  address = {Vienna, Austria},
  author = {Yuxin Li and Wenchao Chen and Xinyue Hu and Bo Chen and Baolin Sun and Mingyuan Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024transformermodulated.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-01-13},
  pdf = {https://openreview.net/pdf?id=qae04YACHs},
  publisher = {OpenReview.net},
  title = {Transformer-Modulated Diffusion Models for Probabilistic Multivariate Time Series Forecasting},
  url = {https://openreview.net/forum?id=qae04YACHs},
  year = {2024}
}

@inproceedings{li2024covlm,
  abstract = {This paper addresses the compositional reasoning capabilities of large vision-language models. The authors propose CoVLM, which guides the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. The method outperforms previous VLMs by large margins on compositional reasoning benchmarks.},
  address = {Vienna, Austria},
  author = {Junyan Li and Delin Chen and Yining Hong and Zhenfang Chen and Peihao Chen and Yikang Shen and Chuang Gan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024covlm.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PHGxChm1l5},
  publisher = {OpenReview.net},
  title = {{CoVLM}: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding},
  url = {https://openreview.net/forum?id=PHGxChm1l5},
  year = {2024}
}

@inproceedings{li2024fedcompass,
  abstract = {Cross-silo federated learning offers a promising solution to collaboratively train robust models without compromising data privacy. However, device heterogeneity causes synchronous algorithms to suffer from degraded efficiency when waiting for stragglers. This work proposes FedCompass, a semi-asynchronous federated learning algorithm with a computing power-aware scheduler that adaptively assigns varying amounts of training tasks to different clients, reducing staleness while maintaining efficiency.},
  address = {Vienna, Austria},
  author = {Zilinghan Li and Pranshu Chaturvedi and Shilan He and Han Chen and Gagandeep Singh and Volodymyr V. Kindratenko and Eliu A. Huerta and Kibaek Kim and Ravi K. Madduri},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024fedcompass.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=msXxrttLOi},
  publisher = {OpenReview.net},
  title = {{FedCompass}: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler},
  url = {https://openreview.net/forum?id=msXxrttLOi},
  year = {2024}
}

@inproceedings{li2024learning,
  abstract = {This work aims to take a manipulation perspective to understand everyday objects and tools. The authors collect a multi-modal visual-tactile dataset that contains paired full-hand force pressure maps and manipulation videos, and propose a novel method to learn a cross-modal latent manifold that allows for cross-modal prediction and discovery of latent structure in different data modalities.},
  address = {Vienna, Austria},
  author = {Yichen Li and Yilun Du and Chao Liu and Francis Williams and Michael Foshey and Benjamin Eckart and Jan Kautz and Joshua B. Tenenbaum and Antonio Torralba and Wojciech Matusik},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024learning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-07},
  pdf = {https://openreview.net/pdf?id=NtQqIcSbqv},
  publisher = {OpenReview.net},
  title = {Learning to Jointly Understand Visual and Tactile Signals},
  url = {https://openreview.net/forum?id=NtQqIcSbqv},
  year = {2024}
}

@inproceedings{li2024risk,
  abstract = {This paper studies accelerated stochastic gradient descent (ASGD) for overparameterized linear regression. While existing optimization theory can explain the faster convergence of ASGD, it cannot explain its better generalization. The authors establish instance-dependent excess risk bounds for ASGD within each eigen-subspace of the data covariance matrix to understand why ASGD achieves better generalization in overparameterized settings.},
  address = {Vienna, Austria},
  author = {Xuheng Li and Yihe Deng and Jingfeng Wu and Dongruo Zhou and Quanquan Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024risk.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AcoXPIPh4A},
  publisher = {OpenReview.net},
  title = {Risk Bounds of Accelerated {SGD} for Overparameterized Linear Regression},
  url = {https://openreview.net/forum?id=AcoXPIPh4A},
  year = {2024}
}

@inproceedings{li2024future,
  abstract = {This paper proposes the task of future language modeling and develops several models for this task. The authors introduce methods for modeling future textual content using a history of documents, focusing on how document history can inform the generation of future text content. The proposed approach outperforms strong non-temporal baselines on both automatic metrics and human evaluation.},
  address = {Vienna, Austria},
  author = {Changmao Li and Jeffrey Flanigan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024future.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bRLed9prWC},
  publisher = {OpenReview.net},
  title = {Future Language Modeling from Temporal Document History},
  url = {https://openreview.net/forum?id=bRLed9prWC},
  year = {2024}
}

@inproceedings{li2024byzantine,
  abstract = {We explore the robustness of cooperative multi-agent reinforcement learning (c-MARL) against Byzantine failures, where any agent can enact arbitrary, worst-case actions due to malfunction or adversarial attack. To address the uncertainty that any agent can be adversarial, we propose a Bayesian Adversarial Robust Dec-POMDP (BARDec-POMDP) framework, which views Byzantine adversaries as nature-dictated types, represented by a separate transition. This allows agents to learn policies grounded on their posterior beliefs about the type of other agents, fostering collaboration with ideal teammates while defending against adversaries. We define the optimal solution to the BARDec-POMDP as an ex post robust Bayesian Markov perfect equilibrium, which we proof to exist and weakly dominates the equilibrium of previous robust MARL approaches. To realize this equilibrium, we put forward a two-timescale actor-critic algorithm with almost sure convergence under specific conditions. Experimentation on matrix games, level-based foraging and StarCraft II indicate that, even under worst-case perturbations, our method successfully acquires intricate micromanagement skills and adaptively aligns with allies, demonstrating resilience against non-oblivious adversaries, random allies, observation-based attacks, and transfer-based attacks.},
  address = {Vienna, Austria},
  author = {Simin Li and Jun Guo and Jingqiao Xiu and Ruixiao Xu and Xin Yu and Jiakai Wang and Aishan Liu and Yaodong Yang and Xianglong Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024byzantine.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-05-19},
  pdf = {https://openreview.net/pdf?id=z6KS9D1dxt},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Byzantine Robust Cooperative Multi-Agent Reinforcement Learning as a Bayesian Game}},
  url = {https://openreview.net/forum?id=z6KS9D1dxt},
  year = {2024}
}

@inproceedings{li2024machine,
  abstract = {Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models.},
  address = {Vienna, Austria},
  author = {Guihong Li and Hsiang Hsu and Chun-Fu Chen and Radu Marculescu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024machine.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9hjVoPWPnh},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Machine Unlearning for Image-to-Image Generative Models}},
  url = {https://openreview.net/forum?id=9hjVoPWPnh},
  year = {2024}
}

@inproceedings{li2024fedda,
  abstract = {Federated learning (FL) is an emerging learning paradigm where a set of distributed clients learns a task under the coordination of a server. The FedAvg algorithm is one of the most widely used methods in FL. In FedAvg, the learning rate is a constant rather than changing adaptively. Adaptive gradient methods have demonstrated superior performance over the constant learning rate schedules in non-distributed settings, and they have recently been adapted to FL. However, the majority of these methods are designed for unconstrained settings. Meanwhile, many crucial FL applications, like disease diagnosis and biomarker identification, often rely on constrained formulations such as Lasso and group Lasso. It remains an open question as to whether adaptive gradient methods can be effectively applied to FL problems with constrains. In this work, we introduce FedDA, a novel adaptive gradient framework for FL. This framework utilizes a restarted dual averaging technique and is compatible with a range of gradient estimation methods and adaptive learning rate schedules. Specifically, an instantiation of our framework FedDA-MVR achieves sample complexity $\tilde{O}(K^{-1}\epsilon^{-1.5})$ and communication complexity $\tilde{O}(K^{-0.25}\epsilon^{-1.25})$ for finding a stationary point $\epsilon$ in the constrained setting with $K$ be the number of clients. We conduct experiments over both constrained and unconstrained tasks to confirm the effectiveness of our approach.},
  address = {Vienna, Austria},
  author = {Junyi Li and Feihu Huang and Heng Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024fedda.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-21},
  pdf = {https://openreview.net/pdf?id=kjn99xFUF3},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{FedDA}: {Faster} {Adaptive} {Gradient} {Methods} for {Federated} {Constrained} {Optimization}},
  url = {https://openreview.net/forum?id=kjn99xFUF3},
  year = {2024}
}

@inproceedings{li2024musc,
  abstract = {This paper studies zero-shot anomaly classification (AC) and segmentation (AS) in industrial vision. We reveal that the abundant normal and abnormal cues implicit in unlabeled test images can be exploited for anomaly determination, which is ignored by prior methods. Our key observation is that for the industrial product images, the normal image patches could find a relatively large number of similar patches in other unlabeled images, while the abnormal ones only have a few similar patches. We leverage such a discriminative characteristic to design a novel zero-shot AC/AS method by Mutual Scoring (MuSc) of the unlabeled images, which does not need any training or prompts. Specifically, we perform Local Neighborhood Aggregation with Multiple Degrees (LNAMD) to obtain the patch features that are capable of representing anomalies in varying sizes. Then we propose the Mutual Scoring Mechanism (MSM) to leverage the unlabeled test images to assign the anomaly score to each other. Furthermore, we present an optimization approach named Re-scoring with Constrained Image-level Neighborhood (RsCIN) for image-level anomaly classification to suppress the false positives caused by noises in normal images. The superior performance on the challenging MVTec AD and VisA datasets demonstrates the effectiveness of our approach.},
  address = {Vienna, Austria},
  author = {Xurui Li and Ziming Huang and Feng Xue and Yu Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024musc.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-03-03},
  pdf = {https://openreview.net/pdf?id=AHgc5SMdtd},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{MuSc}: {Zero-Shot} {Industrial} {Anomaly} {Classification} and {Segmentation} with {Mutual} {Scoring} of the {Unlabeled} {Images}},
  url = {https://openreview.net/forum?id=AHgc5SMdtd},
  year = {2024}
}

@inproceedings{li2024achieving,
  abstract = {Reinforcement learning often needs to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces (often known as the curse of dimensionality). In this work, we address this issue by learning the inherent structure of action-wise similar MDP to appropriately balance the performance degradation versus sample/computational complexity. In particular, we partition the action spaces into multiple groups based on the similarity in transition distribution and reward function, and build a linear decomposition model to capture the difference between the intra-group transition kernel and the intra-group rewards. Both our theoretical analysis and experiments reveal a surprising and counter-intuitive result: while a more refined grouping strategy can achieve better bias-variance trade-off, it may require more samples and computational overhead.},
  address = {Vienna, Austria},
  author = {Yining Li and Peizhong Ju and Ness B. Shroff},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024achieving.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MOmqfJovQ6},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Achieving} {Sample} and {Computational} {Efficient} {Reinforcement} {Learning} by {Action} {Space} {Reduction} via {Grouping}},
  url = {https://openreview.net/forum?id=MOmqfJovQ6},
  year = {2024}
}

@inproceedings{li2024provable,
  abstract = {The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm Memory-Efficient Nash Q-Learning (ME-Nash-QL) for two-player zero-sum Markov games, which is a specific setting of MARL. We prove that ME-Nash-QL can output an Œµ-approximate Nash policy with remarkable space complexity O(SABH), sample complexity √ï(H‚Å¥SAB/Œµ¬≤), and computational complexity O(Tpoly(AB)), where S is the number of states, {A, B} is the number of actions for two players, and H is the horizon length.},
  address = {Vienna, Austria},
  author = {Na Li and Yuchen Jiao and Hangguan Shan and Shefeng Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024provable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vNiI3aGcE6},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Provable} {Memory} {Efficient} {Self-Play} {Algorithm} for {Model-free} {Reinforcement} {Learning}},
  url = {https://openreview.net/forum?id=vNiI3aGcE6},
  year = {2024}
}

@inproceedings{li2024lanesegnet,
  abstract = {A map, as crucial information for downstream applications of an autonomous driving system, is usually represented in lanelines or centerlines. However, existing literature on map learning primarily focuses on either detecting geometry-based lanelines or perceiving topology relationships of centerlines. Both of these methods ignore the intrinsic relationship of lanelines and centerlines, that lanelines bind centerlines. While simply predicting both types of lane in one model is mutually excluded in learning objective, we advocate lane segment as a new representation that seamlessly incorporates both geometry and topology information. Thus, we introduce LaneSegNet, the first end-to-end mapping network generating lane segments to obtain a complete representation of the road structure. Our algorithm features two key modifications. One is a lane attention module to capture pivotal region details within the long-range feature space. Another is an identical initialization strategy for reference points, which enhances the learning of positional priors for lane attention. On the OpenLane-V2 dataset, LaneSegNet outperforms previous counterparts by a substantial gain across three tasks, i.e., map element detection (+4.8 mAP), centerline perception (+6.9 DET_l), and the newly defined one, lane segment perception (+5.6 mAP). Furthermore, it obtains a real-time inference speed of 14.7 FPS.},
  address = {Vienna, Austria},
  author = {Tianyu Li and Peijin Jia and Bangjun Wang and Li Chen and Kun Jiang and Junchi Yan and Hongyang Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024lanesegnet.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-13},
  pdf = {https://openreview.net/pdf?id=LsURkIPYR5},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{LaneSegNet}: {Map} {Learning} with {Lane} {Segment} {Perception} for {Autonomous} {Driving}},
  url = {https://openreview.net/forum?id=LsURkIPYR5},
  year = {2024}
}

@inproceedings{li2024semireward,
  abstract = {Semi-supervised learning (SSL) has witnessed great progress with various improvements in the self-training framework with pseudo labeling. The main challenge is how to distinguish high-quality pseudo labels against the confirmation bias. However, existing pseudo-label selection strategies are limited to pre-defined schemes or complex hand-crafted policies specially designed for classification, failing to achieve high-quality labels, fast convergence, and task versatility simultaneously. To these ends, we propose a Semi-supervised Reward framework (SemiReward) that predicts reward scores to evaluate and filter out high-quality pseudo labels, which is pluggable to mainstream SSL methods in wide task types and scenarios. To mitigate confirmation bias, SemiReward is trained online in two stages with a generator model and subsampling strategy. With classification and regression tasks on 13 standard SSL benchmarks across three modalities, extensive experiments verify that SemiReward achieves significant performance gains and faster convergence speeds upon Pseudo Label, FlexMatch, and Free/SoftMatch.},
  address = {Vienna, Austria},
  author = {Siyuan Li and Weiyang Jin and Zedong Wang and Fang Wu and Zicheng Liu and Cheng Tan and Stan Z. Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024semireward.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-01-28},
  pdf = {https://openreview.net/pdf?id=dnqPvUjyRI},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{SemiReward}: {A} {General} {Reward} {Model} for {Semi-supervised} {Learning}},
  url = {https://openreview.net/forum?id=dnqPvUjyRI},
  year = {2024}
}

@inproceedings{li2024adversarial,
  abstract = {Despite the recent advances in large-scale diffusion models, little progress has been made on the layout-to-image (L2I) synthesis task. Current L2I models either suffer from poor editability via text or weak alignment between the generated image and the input layout. This limits their usability in practice. To mitigate this, we propose to integrate adversarial supervision into the conventional training pipeline of L2I diffusion models (ALDM). Specifically, we employ a segmentation-based discriminator which provides explicit feedback to the diffusion generator on the pixel-level alignment between the denoised image and the input layout. To encourage consistent adherence to the input layout over the sampling steps, we further introduce the multistep unrolling strategy. Instead of looking at a single timestep, we unroll a few steps recursively to imitate the inference process, and ask the discriminator to assess the alignment of denoised images with the layout over a certain time window.},
  address = {Vienna, Austria},
  author = {Yumeng Li and Margret Keuper and Dan Zhang and Anna Khoreva},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024adversarial.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EJPIzl7mgc},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Adversarial} {Supervision} {Makes} {Layout-to-Image} {Diffusion} {Models} {Thrive}},
  url = {https://openreview.net/forum?id=EJPIzl7mgc},
  year = {2024}
}

@inproceedings{li2024detcgd,
  abstract = {This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective's structure and lead to faster convergence compared to a scalar stepsize. As a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. Moreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.},
  address = {Vienna, Austria},
  author = {Hanmin Li and Avetik G. Karagulyan and Peter Richtrik},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024detcgd.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZEZ0CPmoSI},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{Det-CGD}: {Compressed} {Gradient} {Descent} with {Matrix} {Stepsizes} for {Non-Convex} {Optimization}},
  url = {https://openreview.net/forum?id=ZEZ0CPmoSI},
  year = {2024}
}

@inproceedings{li2024onehot,
  abstract = {Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional connectivity can change over time. To model dynamically changing functional connectivity, prior work employs state-switching generalized linear models with hidden Markov models (i.e., {HMM}-{GLM}s). However, we argue they lack biological plausibility, as functional connectivities are shaped and confined by the underlying anatomical connectome. Here, we propose two novel prior-informed state-switching {GLM}s, called {G}aussian {HMM}-{GLM} ({G}aussian prior) and one-hot {HMM}-{GLM} ({G}umbel-{S}oftmax one-hot prior). We show that the learned prior should capture the state-invariant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions. The state-dependent interaction modeled by each {GLM} offers traceability to capture functional variations across multiple brain states. Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood, and enhance the interpretability of interaction patterns and hidden states when applied to real neural data.},
  address = {Vienna, Austria},
  author = {Chengrui Li and Soon Ho Kim and Chris Rodgers and Hannah Choi and Anqi Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024onehot.pdf:pdf},
  note = {Primary area: Applications to neuroscience \& cognitive science},
  pdf = {https://openreview.net/pdf?id=MREQ0k6qvD},
  publisher = {OpenReview.net},
  title = {One-hot Generalized Linear Model for Switching Brain State Discovery},
  url = {https://openreview.net/forum?id=MREQ0k6qvD},
  year = {2024}
}

@inproceedings{li2024federated,
  abstract = {Building recommendation systems via federated learning ({FL}) is a new emerging challenge for next-generation {I}nternet service. Existing {FL} models share item embedding across clients while keeping the user embedding private and local on the client side. However, identical item embedding cannot capture users' individual differences in perceiving the same item and may lead to poor personalization. Moreover, dense item embedding in {FL} results in expensive communication costs and latency. To address these challenges, we propose {F}ederated {R}ecommendation with {A}dditive {P}ersonalization ({FedRAP}), which learns a global view of items via {FL} and a personalized view locally on each user. {FedRAP} encourages a sparse global view to save {FL}'s communication cost and enforces the two views to be complementary via two regularizers. We propose an effective curriculum to learn the local and global views progressively with increasing regularization weights. To produce recommendations for a user, {FedRAP} adds the two views together to obtain a personalized item embedding. {FedRAP} achieves the best performance in {FL} setting on multiple benchmarks.},
  address = {Vienna, Austria},
  author = {Zhiwei Li and Guodong Long and Tianyi Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024federated.pdf:pdf},
  note = {Code available at: https://github.com/mtics/FedRAP},
  pdf = {https://openreview.net/pdf?id=xkXdE81mOK},
  publisher = {OpenReview.net},
  title = {Federated Recommendation with Additive Personalization},
  url = {https://openreview.net/forum?id=xkXdE81mOK},
  year = {2024}
}

@inproceedings{li2024badedit,
  abstract = {Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models ({LLM}s). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the {B}ad{E}dit attack framework. {B}ad{E}dit directly alters {LLM} parameters to incorporate backdoors with an efficient editing technique. It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: {B}ad{E}dit necessitates only a minimal dataset for injection (15 samples). (2) Efficiency: {B}ad{E}dit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. (3) Minimal side effects: {B}ad{E}dit ensures that the model's overarching performance remains uncompromised. (4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning. Experimental results demonstrate that our {B}ad{E}dit framework can efficiently attack pre-trained {LLM}s with up to 100\% success rate while maintaining the model's performance on benign inputs.},
  address = {Vienna, Austria},
  author = {Yanzhou Li and Tianlin Li and Kangjie Chen and Jian Zhang 0001 and Shangqing Liu and Wenhan Wang and Tianwei Zhang 0004 and Yang Liu 0003},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024badedit.pdf:pdf},
  note = {Primary area: societal considerations including fairness, safety, privacy},
  pdf = {https://openreview.net/pdf?id=duZANm2ABX},
  publisher = {OpenReview.net},
  title = {BadEdit: Backdooring Large Language Models by Model Editing},
  url = {https://openreview.net/forum?id=duZANm2ABX},
  year = {2024}
}

@inproceedings{li2024frequencyaware,
  abstract = {Learned image compression ({LIC}) has gained traction as an effective solution for image storage and transmission in recent years. However, existing {LIC} methods are redundant in latent representation due to limitations in capturing anisotropic frequency components and preserving directional details. To overcome these challenges, we propose a novel frequency-aware transformer ({FAT}) block that for the first time achieves multiscale directional analysis for {LIC}. The {FAT} block comprises frequency-decomposition window attention ({FDWA}) modules to capture multiscale and directional frequency components of natural images. Additionally, we introduce frequency-modulation feed-forward network ({FMFFN}) to adaptively modulate different frequency components, improving rate-distortion performance. Furthermore, we present a transformer-based channel-wise autoregressive ({T-CA}) model that effectively exploits channel dependencies. Experiments show that our method achieves state-of-the-art rate-distortion performance compared to existing {LIC} methods, and evidently outperforms latest standardized codec {VTM}-12.1 by 14.5\%, 15.1\%, 13.0\% in {BD}-rate on the {K}odak, {T}ecnick, and {CLIC} datasets.},
  address = {Vienna, Austria},
  author = {Han Li and Shaohui Li and Wenrui Dai and Chenglin Li and Junni Zou and Hongkai Xiong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024frequencyaware.pdf:pdf},
  note = {Primary area: representation learning for computer vision, audio, language, and other modalities},
  pdf = {https://openreview.net/pdf?id=HKGQDDTuvZ},
  publisher = {OpenReview.net},
  title = {Frequency-Aware Transformer for Learned Image Compression},
  url = {https://openreview.net/forum?id=HKGQDDTuvZ},
  year = {2024}
}

@inproceedings{li2024instructpix2nerf,
  abstract = {With the success of Neural Radiance Field ({NeRF}) in 3D-aware portrait editing, a variety of works have achieved promising results regarding both quality and 3D consistency. However, these methods heavily rely on per-prompt optimization when handling natural language as editing instructions. Due to the lack of labeled human face 3D datasets and effective architectures, the area of human-instructed 3D-aware editing for open-world portraits in an end-to-end manner remains under-explored. To solve this problem, we propose an end-to-end diffusion-based framework termed {I}nstruct{P}ix2{NeRF}, which enables instructed 3D-aware portrait editing from a single open-world image with human instructions. At its core lies a conditional latent 3D diffusion process that lifts 2D editing to 3D space by learning the correlation between the paired images' difference and the instructions via triplet data. With the help of our proposed token position randomization strategy, we could even achieve multi-semantic editing through one single pass with the portrait identity well-preserved. Besides, we further propose an identity consistency module that directly modulates the extracted identity signals into our diffusion process, which increases the multi-view 3D identity consistency. Extensive experiments verify the effectiveness of our method and show its superiority against strong baselines quantitatively and qualitatively.},
  address = {Vienna, Austria},
  author = {Jianhui Li and Shilong Liu and Zidong Liu and Yikai Wang 0001 and Kaiwen Zheng and Jinghui Xu and Jianmin Li 0001 and Jun Zhu 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024instructpix2nerf.pdf:pdf},
  note = {Project page: https://mybabyyh.github.io/InstructPix2NeRF},
  pdf = {https://openreview.net/pdf?id=XIxhINXtQk},
  publisher = {OpenReview.net},
  title = {InstructPix2NeRF: Instructed 3D Portrait Editing from a Single Image},
  url = {https://openreview.net/forum?id=XIxhINXtQk},
  year = {2024}
}

@inproceedings{li2024hyper,
  abstract = {Deep neural networks ({DNN}s) have been shown to perform well on exclusive, multi-class classification tasks. However, when different classes have similar visual features, it becomes challenging for human annotators to differentiate them. When an image is ambiguous, such as a blurry one where an annotator can't distinguish between a husky and a wolf, it may be labeled with both classes: \{husky, wolf\}. This scenario necessitates the use of composite set labels. In this paper, we propose a novel framework called Hyper-Evidential Neural Network ({HENN}) that explicitly models predictive uncertainty due to composite class labels in training data in the context of the belief theory called Subjective Logic ({SL}). By placing a grouped Dirichlet distribution on the class probabilities, we treat predictions of a neural network as parameters of hyper-subjective opinions and learn the network that collects both single and composite evidence leading to these hyper-opinions by a deterministic {DNN} from data. We introduce a new uncertainty type called vagueness originally designed for hyper-opinions in {SL} to quantify composite classification uncertainty for {DNN}s. Our results demonstrate that {HENN} outperforms its state-of-the-art counterparts based on four image datasets.},
  address = {Vienna, Austria},
  author = {Changbin Li and Kangshuo Li and Yuzhe Ou and Lance M. Kaplan and Audun Jsang and Jin-Hee Cho and Dong Hyun Jeong and Feng Chen 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024hyper.pdf:pdf},
  note = {Primary area: Uncertainty quantification. Code available at: https://shorturl.at/dhoqx},
  pdf = {https://openreview.net/pdf?id=A7t7z6g6tM},
  publisher = {OpenReview.net},
  title = {Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty},
  url = {https://openreview.net/forum?id=A7t7z6g6tM},
  year = {2024}
}

@inproceedings{li2024communicationefficient,
  abstract = {Federated optimization studies the problem of collaborative function optimization among multiple clients (e.g. mobile devices or organizations) under the coordination of a central server. Since the data is collected separately by each client and always remains decentralized, federated optimization preserves data privacy and allows for large-scale computing, which makes it a promising decentralized machine learning paradigm. Though it is often deployed for tasks that are online in nature, e.g., next-word prediction on keyboard apps, most works formulate it as an offline problem. The few exceptions that consider federated bandit optimization are limited to very simplistic function classes, e.g., linear, generalized linear, or non-parametric function class with bounded {RKHS} norm, which severely hinders its practical usage. In this paper, we propose a new algorithm, named {F}ed-{GO}-{UCB}, for federated bandit optimization with generic non-linear objective function.},
  address = {Vienna, Austria},
  author = {Chuanhao Li and Chong Liu 0007 and Yu-Xiang Wang 0003},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024communicationefficient.pdf:pdf},
  note = {arXiv preprint: 2311.01695},
  pdf = {https://openreview.net/pdf?id=nFI3wFM9yN},
  publisher = {OpenReview.net},
  title = {Communication-Efficient Federated Non-Linear Bandit Optimization},
  url = {https://openreview.net/forum?id=nFI3wFM9yN},
  year = {2024}
}

@inproceedings{li2024asmr,
  abstract = {Coordinate network or implicit neural representation ({INR}) is a fast-emerging method for encoding natural signals (such as images and videos) with the benefits of a compact neural representation. While numerous methods have been proposed to increase the encoding capabilities of an {INR}, an often overlooked aspect is the inference efficiency, usually measured in multiply-accumulate ({MAC}) count. This is particularly critical in use cases where inference bandwidth is greatly limited by hardware constraints. To this end, we propose the Activation-Sharing Multi-Resolution ({ASMR}) coordinate network that combines multi-resolution coordinate decomposition with hierarchical modulations. Specifically, an {ASMR} model enables the sharing of activations across grids of the data. This largely decouples its inference cost from its depth which is directly correlated to its reconstruction capability, and renders a near $O(1)$ inference complexity irrespective of the number of layers. Experiments show that {ASMR} can reduce the {MAC} of a vanilla {SIREN} model by up to 500$\times$ while achieving an even higher reconstruction quality than its {SIREN} baseline.},
  address = {Vienna, Austria},
  author = {Jason Chun Lok Li and Steven Tin Sui Luo and Le Xu and Ngai Wong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024asmr.pdf:pdf},
  note = {Code available at: https://github.com/stevolopolis/asmr},
  pdf = {https://openreview.net/pdf?id=kMp8zCsXNb},
  publisher = {OpenReview.net},
  title = {ASMR: Activation-Sharing Multi-Resolution Coordinate Networks for Efficient Inference},
  url = {https://openreview.net/forum?id=kMp8zCsXNb},
  year = {2024}
}

@inproceedings{li2024visionlanguage,
  abstract = {Recent progress in vision language foundation models has shown their ability to understand multimodal data and resolve complicated vision language tasks, including robotics manipulation. We seek a straightforward way of making use of existing vision-language models ({VLM}s) with simple fine-tuning on robotics data. To this end, we derive a simple and novel vision-language manipulation framework, dubbed {R}obo{F}lamingo, built upon the open-source {VLM}s, {O}pen{F}lamingo. Unlike prior works, {R}obo{F}lamingo utilizes pre-trained {VLM}s for single-step vision-language comprehension, models sequential history information with an explicit policy head, and is slightly fine-tuned by imitation learning only on language-conditioned manipulation datasets. Such a decomposition provides {R}obo{F}lamingo the flexibility for open-loop control and deployment on low-performance platforms. By exceeding the state-of-the-art performance with a large margin on the tested benchmark, we show {R}obo{F}lamingo can be an effective and competitive alternative to adapt {VLM}s to robot control. Our extensive experimental results also reveal several interesting conclusions regarding the behavior of different pre-trained {VLM}s on manipulation tasks. We believe {R}obo{F}lamingo has the potential to be a cost-effective and easy-to-use solution for robotics manipulation, empowering everyone with the ability to fine-tune their own robotics policy.},
  address = {Vienna, Austria},
  author = {Xinghang Li and Minghuan Liu and Hanbo Zhang and Cunjun Yu and Jie Xu and Hongtao Wu and Chilam Cheang and Ya Jing and Weinan Zhang 0001 and Huaping Liu 0001 and Hang Li and Tao Kong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024visionlanguage.pdf:pdf},
  note = {Project page: https://roboflamingo.github.io/},
  pdf = {https://openreview.net/pdf?id=lFYj0oibGR},
  publisher = {OpenReview.net},
  title = {Vision-Language Foundation Models as Effective Robot Imitators},
  url = {https://openreview.net/forum?id=lFYj0oibGR},
  year = {2024}
}

@inproceedings{li2024multitask,
  abstract = {Deep neural networks have become a standard building block for designing models that can perform multiple dense computer vision tasks such as depth estimation and semantic segmentation thanks to their ability to capture complex correlations in high dimensional feature space across tasks. However, the cross-task correlations that are learned in the unstructured feature space can be extremely noisy and susceptible to overfitting, consequently hurting performance. We propose to address this problem by introducing a structured 3D-aware regularizer which interfaces multiple tasks through the projection of features extracted from an image encoder to a shared 3D feature space and decodes them into their task output space through differentiable rendering. The proposed method is architecture agnostic and can be plugged into various prior multi-task backbones to improve their performance; as evidenced using standard benchmarks {NYU}v2 and {PASCAL}-{C}ontext.},
  address = {Vienna, Austria},
  author = {Wei-Hong Li and Steven McDonagh 0001 and Ales Leonardis and Hakan Bilen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024multitask.pdf:pdf},
  note = {arXiv preprint: 2310.00986},
  pdf = {https://openreview.net/pdf?id=TwBY17Hgiy},
  publisher = {OpenReview.net},
  title = {Multi-task Learning with 3D-Aware Regularization},
  url = {https://openreview.net/forum?id=TwBY17Hgiy},
  year = {2024}
}

@inproceedings{li2024training,
  abstract = {Bayesian neural networks ({BNN}s) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse {BNN}s have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense {BNN}s. To solve this challenge, we introduce Sparse Subspace Variational Inference ({SSVI}), the first fully sparse {BNN} framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the optimal solution with a removal-and-addition strategy, guided by novel criteria based on weight distribution statistics. Our extensive experiments show that {SSVI} sets new benchmarks in crafting sparse {BNN}s, achieving, for instance, a 10-20x compression in model size with under 3\% performance drop, and up to 20x {FLOP}s reduction during training compared with dense {VI} training.},
  author = {Junbo Li and Zichen Miao and Qiang Qiu and Ruqi Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024training.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TskzCtpMEO},
  publisher = {OpenReview.net},
  title = {Training {B}ayesian Neural Networks with Sparse Subspace Variational Inference},
  url = {https://openreview.net/forum?id=TskzCtpMEO},
  year = {2024}
}

@inproceedings{li2024federated,
  abstract = {Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery ({FCD}) approaches. However, existing {FCD} methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel {FCD} method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test ({FCIT}) for causal skeleton discovery and establish a federated independent change principle ({FICP}) to determine causal directions. These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy. Owing to the nonparametric properties, {FCIT} and {FICP} make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models. We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method.},
  author = {Loka Li and Ignavier Ng and Gongxu Luo and Biwei Huang and Guangyi Chen and Tongliang Liu and Bin Gu and Kun Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024federated.pdf:pdf},
  note = {DBLP last modified: 2025-06-13},
  pdf = {https://openreview.net/pdf?id=m7tJxajC3G},
  publisher = {OpenReview.net},
  title = {Federated Causal Discovery from Heterogeneous Data},
  url = {https://openreview.net/forum?id=m7tJxajC3G},
  year = {2024}
}

@inproceedings{li2024alleviating,
  abstract = {Diffusion Probabilistic Models ({DPM}) have shown remarkable efficacy in the synthesis of high-quality images. However, their inference process characteristically requires numerous, potentially hundreds, of iterative steps, which could exaggerate the problem of exposure bias due to the training and inference discrepancy. Previous work has attempted to mitigate this issue by perturbing inputs during training, which consequently mandates the retraining of the {DPM}. In this work, we conduct a systematic study of exposure bias in {DPM} and, intriguingly, we find that the exposure bias could be alleviated with a novel sampling method that we propose, without retraining the model. We theoretically and empirically show that by adjusting the next time step during sampling according to the approximated variance of the current generated samples, one can effectively alleviate the exposure bias. We propose a new sampling method called Time-Shift Sampler. The framework can be seamlessly integrated to existing sampling algorithms, such as {DDPM}, {DDIM} and other high-order solvers, inducing merely minimal additional computations. Experimental results show the method brings significant and consistent improvements in {FID} scores on different datasets and sampling methods. For example, integrating Time-Shift Sampler to {F-PNDM} yields a {FID}=3.88, achieving 44.49\% improvements as compared to {F-PNDM}, on {CIFAR}-10 with 10 sampling steps, which is more performant than the vanilla {DDIM} with 100 sampling steps.},
  author = {Mingxiao Li and Tingyu Qu and Ruicong Yao and Wei Sun and Marie-Francine Moens},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024alleviating.pdf:pdf},
  note = {DBLP last modified: 2025-06-02},
  pdf = {https://openreview.net/pdf?id=ZSD3MloKe6},
  publisher = {OpenReview.net},
  title = {Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps},
  url = {https://openreview.net/forum?id=ZSD3MloKe6},
  year = {2024}
}

@inproceedings{li2024study,
  abstract = {Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process ({GP}) surrogate models which are easy to optimize and support exact inference. While standard {GP} surrogates have been well-established in Bayesian optimization, Bayesian neural networks ({BNN}s) have recently become practical function approximators, with many benefits over standard {GP}s such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study {BNN}s as alternatives to standard {GP} surrogates for optimization. We consider a variety of approximate inference procedures for finite-width {BNN}s, including high-quality Hamiltonian Monte Carlo, low-cost stochastic {MCMC}, and heuristics such as deep ensembles. We also consider infinite-width {BNN}s, linearized Laplace approximations, and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate models on diverse problems with varying dimensionality, number of objectives, non-stationarity, and discrete and continuous inputs. Our key findings include: (i) the ranking of methods is highly problem dependent, suggesting the need for tailored inductive biases; (ii) {HMC} is the most successful approximate inference procedure for fully stochastic {BNN}s; (iii) full stochasticity may be unnecessary as deep kernel learning is relatively competitive; (iv) deep ensembles perform relatively poorly; (v) infinite-width {BNN}s are particularly promising, especially in high dimensions.},
  author = {Yucen Lily Li and Tim G. J. Rudner and Andrew Gordon Wilson},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024study.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=SA19ijj44B},
  publisher = {OpenReview.net},
  title = {A Study of {B}ayesian Neural Network Surrogates for {B}ayesian Optimization},
  url = {https://openreview.net/forum?id=SA19ijj44B},
  year = {2024}
}

@inproceedings{li2024error,
  abstract = {Although diffusion models ({DM}s) have shown promising performances in a number of tasks (e.g., speech synthesis and image generation), they might suffer from error propagation because of their sequential structure. However, this is not certain because some sequential models, such as Conditional Random Field ({CRF}), are free from this problem. To address this issue, we develop a theoretical framework to mathematically formulate error propagation in the architecture of {DM}s. The framework contains three elements, including modular error, cumulative error, and propagation equation. The modular and cumulative errors are related by the equation, which interprets that {DM}s are indeed affected by error propagation. Our theoretical study also suggests that the cumulative error is closely related to the generation quality of {DM}s. To alleviate the problem, we propose using cumulative error as an additional regularization term in the training objective. Experiments on toy examples, image generation, and speech synthesis show that our understanding and solution can improve the performance of diffusion models.},
  author = {Yangming Li and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024error.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RtAct1E2zS},
  publisher = {OpenReview.net},
  title = {On Error Propagation of Diffusion Models},
  url = {https://openreview.net/forum?id=RtAct1E2zS},
  year = {2024}
}

@inproceedings{li2024fld,
  abstract = {Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.},
  author = {Chenhao Li and Elijah Stanger-Jones and Steve Heim and Sangbae Kim},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024fld.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=xsd2llWYSA},
  publisher = {OpenReview.net},
  title = {{FLD}: Fourier Latent Dynamics for Structured Motion Representation and Learning},
  url = {https://openreview.net/forum?id=xsd2llWYSA},
  year = {2024}
}

@inproceedings{li2024bayesprompt,
  abstract = {As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models ({PLM}s), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in {PLM}s and the abridged knowledge for target downstream domains, which jointly result in that {PLM}s mis-locate the knowledge distribution. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous guidance for {PLM}s. Guided by such an intuition, we propose a simple yet effective approach, namely {BayesPrompt}, to learn prompts that contain the domain discriminative information against the interference from domain-irrelevant knowledge. {BayesPrompt} primitively leverages known distributions to approximate the debiased factual distributions of target domains and further uniformly samples certain representative features from the approximated distributions to generate the ultimate prompts for {PLM}s. We provide theoretical insights with the connection to domain adaptation. Empirically, our method achieves state-of-the-art performance on benchmarks.},
  author = {Jiangmeng Li and Fei Song and Yifan Jin and Wenwen Qiang and Changwen Zheng and Fuchun Sun and Hui Xiong},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024bayesprompt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DmD1wboID9},
  publisher = {OpenReview.net},
  title = {{BayesPrompt}: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction},
  url = {https://openreview.net/forum?id=DmD1wboID9},
  year = {2024}
}

@inproceedings{li2024benchmarking,
  abstract = {As of September 2023, {ChatGPT} correctly answers ``what is 7+8'' with 15, but when asked ``7+8=15, True or False'' it responds with ``False''. This inconsistency between generating and validating an answer is prevalent in language models ({LM}s) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or {GV}-consistency), finding that even {GPT}-4 (0613), a state-of-the-art {LM}, is {GV}-consistent only 76\% of the time. To improve the consistency of {LM}s, we propose to finetune on the filtered generator and validator responses that are {GV}-consistent, and call this approach consistency fine-tuning. We find that this approach improves {GV}-consistency of Alpaca-30B from 60\% to 93\%, and the improvement extrapolates to unseen tasks and domains (e.g., {GV}-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data. Evaluated across 6 tasks, including math questions, knowledge-intensive {QA}, and instruction following, our method improves generator quality by an average of 16\% and validator accuracy by an average of 6.3\% across all tasks.},
  author = {Xiang Lisa Li and Vaishnavi Shrivastava and Siyan Li and Tatsunori Hashimoto and Percy Liang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024benchmarking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=phBS6YpTzC},
  publisher = {OpenReview.net},
  title = {Benchmarking and Improving Generator-Validator Consistency of Language Models},
  url = {https://openreview.net/forum?id=phBS6YpTzC},
  year = {2024}
}

@inproceedings{li2024efficient,
  abstract = {Recent deep neural networks ({DNN}s) have came to rely on vast amounts of training data, providing an opportunity for malicious attackers to exploit and contaminate the data to carry out backdoor attacks. However, existing backdoor attack methods make unrealistic assumptions, assuming that all training data comes from a single source and that attackers have full access to the training data. In this paper, we introduce a more realistic attack scenario where victims collect data from multiple sources, and attackers cannot access the complete training data. We refer to this scenario as data-constrained backdoor attacks. In such cases, previous attack methods suffer from severe efficiency degradation due to the entanglement between benign and poisoning features during the backdoor injection process. We introduce three {CLIP}-based technologies from two distinct streams: Clean Feature Suppression, which aims to suppress the influence of clean features to enhance the prominence of poisoning features, and Poisoning Feature Augmentation, which focuses on augmenting the presence and impact of poisoning features to effectively manipulate the model's behavior. To evaluate the effectiveness, harmlessness to benign accuracy, and stealthiness of our method, we conduct extensive experiments on 3 target models, 3 datasets, and over 15 different settings. The results demonstrate remarkable improvements, with some settings achieving over 100\% improvement compared to existing attacks in data-constrained scenarios.},
  author = {Ziqiang Li and Hong Sun and Pengfei Xia and Heng Li and Beihao Xia and Yi Wu and Bin Li},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024efficient.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=vRyp2dhEQp},
  publisher = {OpenReview.net},
  title = {Efficient Backdoor Attacks for Deep Neural Networks in Real-world Scenarios},
  url = {https://openreview.net/forum?id=vRyp2dhEQp},
  year = {2024}
}

@inproceedings{li2024generative,
  abstract = {The rapid development of Large Language Models ({LLM}s) has substantially expanded the range of tasks they can address, leading researchers to shift their focus from conventional {NLP} tasks towards tasks that revolve around aligning with human needs (e.g., brainstorming and email writing). This shift in task distribution imposes new requirements on evaluating these aligned models regarding generality (i.e., assessing performance across diverse scenarios), flexibility (i.e., examining under different protocols), and interpretability (i.e., scrutinizing models with explanations). In this paper, we propose a generative judge with 13B parameters, Auto-J, designed to address these challenges. Our model generates critiques and identifies specific errors, providing natural language rationales that can be easily verified by humans. To train Auto-J, we develop a novel training paradigm and construct a comprehensive dataset. To comprehensively evaluate judges, we introduce a new benchmark covering both pairwise and single-answer grading on 13 different tasks. Our experiments show that Auto-J outperforms a series of strong competitors, including both open-source and closed-source models, by a significant margin. We also provide detailed analysis and case studies to further reveal the superiority of our method.},
  author = {Junlong Li and Shichao Sun and Weizhe Yuan and Run-Ze Fan and Hai Zhao and Pengfei Liu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/li2024generative.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gtkFw6sZGS},
  publisher = {OpenReview.net},
  title = {Generative Judge for Evaluating Alignment},
  url = {https://openreview.net/forum?id=gtkFw6sZGS},
  year = {2024}
}

@inproceedings{li2024moganet,
  abstract = {By contextualizing the kernel as global as possible, Modern {ConvNets} have shown great potential in computer vision tasks. However, recent progress on multi-order game-theoretic interaction within deep neural networks ({DNNs}) reveals the representation bottleneck of modern {ConvNets}, where the expressive interactions have not been effectively encoded with the increased kernel size.},
  author = {Siyuan Li and Zedong Wang and Zicheng Liu and Cheng Tan and Haitao Lin and Di Wu and Zhiyuan Chen and Jiangbin Zheng and Stan Z. Li},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024moganet.pdf:pdf},
  note = {{DBLP} last modified: 2025-03-19},
  pdf = {https://openreview.net/pdf?id=XhYWgjqCrV},
  publisher = {OpenReview.net},
  title = {{MogaNet}: Multi-order Gated Aggregation Network},
  url = {https://openreview.net/forum?id=XhYWgjqCrV},
  year = {2024}
}

@inproceedings{li2024exploring,
  abstract = {Interactions among features are central to understanding the behavior of machine learning models. Recent research has made significant strides in detecting and quantifying feature interactions in single predictive models. However, we argue that the feature interactions extracted from a single pre-specified model may not be trustworthy since: a well-trained predictive model may not preserve the true feature interactions and there exist multiple well-performing predictive models that differ in feature interaction strengths. Thus, we recommend exploring feature interaction strengths in a model class of approximately equally accurate predictive models. In this work, we introduce the feature interaction score ({FIS}) in the context of a {Rashomon} set, representing a collection of models that achieve similar accuracy on a given task.},
  author = {Sichao Li and Rong Wang and Quanling Deng and Amanda S. Barnard},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024exploring.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EPNEazJoAg},
  publisher = {OpenReview.net},
  title = {Exploring the cloud of feature interaction scores in a {Rashomon} set},
  url = {https://openreview.net/forum?id=EPNEazJoAg},
  year = {2024}
}

@inproceedings{li2024longshortrange,
  abstract = {Computational simulation of chemical and biological systems using ab initio molecular dynamics has been a challenge over decades. Researchers have attempted to address the problem with machine learning and fragmentation-based methods. However, the two approaches fail to give a satisfactory description of long-range and many-body interactions, respectively. Inspired by fragmentation-based methods, we propose the Long-Short-Range Message-Passing ({LSR}-{MP}) framework as a generalization of the existing equivariant graph neural networks ({EGNN}s) with the intent to incorporate long-range interactions efficiently and effectively. We apply the {LSR}-{MP} framework to the recently proposed {ViSNet} and demonstrate the state-of-the-art results with up to 40\% {MAE} reduction for molecules in {MD}22 and {C}hignolin datasets. Consistent improvements to various {EGNN}s will also be discussed to illustrate the general applicability and robustness of our {LSR}-{MP} framework.},
  author = {Yunyang Li and Yusong Wang and Lin Huang and Han Yang and Xinran Wei and Jia Zhang 0004 and Tong Wang 0014 and Zun Wang and Bin Shao and Tie-Yan Liu},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024longshortrange.pdf:pdf},
  note = {DBLP last modified: 2024-10-01},
  pdf = {https://openreview.net/pdf?id=rvDQtdMnOl},
  publisher = {OpenReview.net},
  title = {Long-Short-Range Message-Passing: A Physics-Informed Framework to Capture Non-Local Interaction for Scalable Molecular Dynamics Simulation},
  url = {https://openreview.net/forum?id=rvDQtdMnOl},
  year = {2024}
}

@inproceedings{li2024forward,
  abstract = {Maximizing the marginal log-likelihood is a crucial aspect of learning latent variable models, and variational inference ({VI}) stands as the commonly adopted method. However, {VI} can encounter challenges in achieving a high marginal log-likelihood when dealing with complicated posterior distributions. In this work, we introduce a novel variational importance sampling ({VIS}) approach that directly estimates and maximizes the marginal log-likelihood. {VIS} leverages an optimal proposal distribution by minimizing the forward œá2 divergence. We apply {VIS} to various latent variable models, including mixture models, variational auto-encoders, and partially observable generalized linear models. The results demonstrate that our approach consistently outperforms state-of-the-art baselines, in terms of both log-likelihood and model parameter estimation.},
  author = {Chengrui Li and Yule Wang and Weihan Li and Anqi Wu},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024forward.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=HD5Y7M8Xdk},
  publisher = {OpenReview.net},
  title = {Forward œá2 Divergence Based Variational Importance Sampling},
  url = {https://openreview.net/forum?id=HD5Y7M8Xdk},
  year = {2024}
}

@inproceedings{li2024rain,
  abstract = {Large language models ({LLM}s) often demonstrate inconsistencies with human preferences. Previous research typically gathered human preference data and then aligned pre-trained models using reinforcement learning or instruction tuning. In contrast, this work explores aligning frozen {LLM}s without requiring alignment data. We discover that by integrating self-evaluation and rewind mechanisms, unaligned {LLM}s can directly produce responses consistent with human preferences via self-boosting. We introduce a novel inference method called Rewindable Auto-regressive {IN}ference ({RAIN}), which allows pre-trained {LLM}s to evaluate their own generation and use the evaluation results to guide rewind and generation for {AI} safety. {RAIN} operates without extra alignment data and abstains from training, gradient computation, or parameter updates. Experimental results evaluated by {GPT}-4 and humans show {RAIN}'s effectiveness. On {HH} dataset, {RAIN} improved {LLaMA} 30{B} harmlessness rate from 82\% to 97\%. On {T}ruthful{QA} dataset, {RAIN} improved {LLaMA}-2-chat 13{B} truthfulness by 5\%.},
  author = {Yuhui Li and Fangyun Wei and Jinjing Zhao and Chao Zhang 0001 and Hongyang Zhang 0001},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024rain.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pETSfWMUzy},
  publisher = {OpenReview.net},
  title = {{RAIN}: Your Language Models Can Align Themselves without Finetuning},
  url = {https://openreview.net/forum?id=pETSfWMUzy},
  year = {2024}
}

@inproceedings{li2024usb,
  abstract = {Neural Radiance Fields ({NeRF}) has received much attention recently due to its impressive capability to represent 3{D} scene and synthesize novel view images. Existing works usually assume that the input images are captured by a global shutter camera. Thus, rolling shutter ({RS}) images cannot be trivially applied to an off-the-shelf {NeRF} algorithm for novel view synthesis. Rolling shutter effect would also affect the accuracy of the camera pose estimation (e.g. via {COLMAP}), which further prevents the success of {NeRF} algorithm with {RS} images. In this paper, we propose Unrolling Shutter Bundle Adjusted Neural Radiance Fields ({USB}-{NeRF}). {USB}-{NeRF} is able to correct rolling shutter distortions and recover accurate camera motion trajectory simultaneously under the framework of {NeRF}, by modeling the physical image formation process of a {RS} camera. Experimental results demonstrate that {USB}-{NeRF} achieves better performance compared to prior works, in terms of {RS} effect removal, novel view image synthesis as well as camera motion estimation. Furthermore, our algorithm can also be used to recover high-fidelity high frame-rate global shutter video from a sequence of {RS} images.},
  author = {Moyang Li and Peng Wang and Lingzhe Zhao and Bangyan Liao and Peidong Liu},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024usb.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=igfDXfMvm5},
  publisher = {OpenReview.net},
  title = {{USB}-{NeRF}: Unrolling Shutter Bundle Adjusted Neural Radiance Fields},
  url = {https://openreview.net/forum?id=igfDXfMvm5},
  year = {2024}
}

@inproceedings{li2024error,
  abstract = {Text generation models are notoriously vulnerable to errors in the training data. With the wide-spread availability of massive amounts of web-crawled data becoming more commonplace, how can we enhance the robustness of models trained on a massive amount of noisy web-crawled text? We propose Error Norm Truncation ({ENT}), a robust enhancement method that truncates noisy data during the standard training objective. {ENT} provides more accurate data quality estimation by considering non-target token distributions and improves generation quality across language modeling, machine translation, and text summarization. We demonstrate that {ENT} increases model robustness, achieving over 2 {BLEU} points improvement over the baseline when up to 50\% noise is added.},
  author = {Tianjian Li and Haoran Xu and Philipp Koehn and Daniel Khashabi and Kenton Murray},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024error.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=zMvMwNvs4R},
  publisher = {OpenReview.net},
  title = {Error Norm Truncation: Robust Training in the Presence of Data Noise for Text Generation Models},
  url = {https://openreview.net/forum?id=zMvMwNvs4R},
  year = {2024}
}

@inproceedings{li2024effective,
  abstract = {Federated learning has emerged as a promising distributed learning paradigm that facilitates collaborative learning among multiple parties without transferring raw data. However, most existing federated learning studies focus on either horizontal or vertical data settings, where the data of different parties are assumed to be from the same feature or sample space. In practice, a common scenario is the hybrid data setting, where data from different parties may differ both in the features and samples. To address this, we propose {H}ybrid{T}ree, a novel federated learning approach that enables federated tree learning on hybrid data. We observe the existence of consistent split rules in trees. With the help of these split rules, we theoretically show that the knowledge of parties can be incorporated into the lower layers of a tree. Based on our theoretical analysis, we propose a layer-level solution that does not need frequent communication traffic to train a tree. Our experiments demonstrate that {H}ybrid{T}ree can achieve comparable accuracy to the centralized setting with low computational and communication overhead. {H}ybrid{T}ree can achieve up to 8 times speedup compared with the other baselines.},
  author = {Qinbin Li and Chulin Xie and Xiaojun Xu and Xiaoyuan Liu and Ce Zhang 0001 and Bo Li 0026 and Bingsheng He and Dawn Song},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024effective.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=py4ZV2qYQI},
  publisher = {OpenReview.net},
  title = {Effective and Efficient Federated Tree Learning on Hybrid Data},
  url = {https://openreview.net/forum?id=py4ZV2qYQI},
  year = {2024}
}

@inproceedings{li2024faster,
  abstract = {Probabilistic values in cooperative game theory are often computationally expensive to compute exactly. Existing generic estimators require {O}(n¬≤log n) utility evaluations. We propose a Generic Estimator based on Least Squares ({GELS}) that reduces computational complexity to {O}(n log n). We introduce a framework called {T}r{ELS} (Training Estimators based on Least Squares) that can train estimators for distributional values. The method can predict distributional values for unseen data, reducing Monte-Carlo method costs. Code is available at https://github.com/watml/fastpvalue.},
  author = {Weida Li and Yaoliang Yu},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/li2024faster.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=lvSMIsztka},
  publisher = {OpenReview.net},
  title = {Faster Approximation of Probabilistic and Distributional Values via Least Squares},
  url = {https://openreview.net/forum?id=lvSMIsztka},
  year = {2024}
}

@inproceedings{2024escape,
  abstract = {Self-consistency ({SC}) has been a widely used decoding strategy for chain-of-thought reasoning. Despite bringing significant performance improvements across a variety of multi-step reasoning tasks, it is a high-cost method that requires multiple sampling with the preset size. We propose Early-Stopping Self-Consistency ({ESC}), a simple and scalable sampling process designed to reduce {SC}'s computational cost without sacrificing performance. We developed a control scheme to dynamically balance performance and cost across different tasks and models. Experimental results demonstrated {ESC}'s effectiveness by significantly reducing sampling numbers across six benchmarks: {MATH} (-33.8\%), {GSM}8{K} (-80.1\%), {S}trategy{QA} (-76.8\%), {C}ommonsense{QA} (-78.5\%), {C}oin {F}lip (-84.2\%), and {L}ast {L}etters (-67.4\%).},
  author = {Yiwei Li 0001 and Peiwen Yuan and Shaoxiong Feng and Boyuan Pan and Xinglin Wang and Bin Sun 0004 and Heda Wang and Kan Li 0001},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/2024escape.pdf:pdf},
  note = {DBLP last modified: 2024-11-20},
  pdf = {https://openreview.net/pdf?id=ndR8Ytrzhh},
  publisher = {OpenReview.net},
  title = {Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step Reasoning},
  url = {https://openreview.net/forum?id=ndR8Ytrzhh},
  year = {2024}
}

@inproceedings{li2024functional,
  abstract = {Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.},
  address = {Vienna, Austria},
  author = {Shanda Li and Chong You and Guru Guruganesh and Joshua Ainslie and Santiago Ontanon and Manzil Zaheer and Sumit Sanghai and Yiming Yang and Sanjiv Kumar and Srinadh Bhojanapalli},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024functional.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rR03qFesqk},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Functional Interpolation for Relative Positions improves Long Context Transformers},
  url = {https://openreview.net/forum?id=rR03qFesqk},
  year = {2024}
}

@inproceedings{li2024neurrev,
  abstract = {Dynamic Sparse Training (DST) uses a greedy search mechanism to identify an optimal sparse subnetwork by periodically pruning and growing network connections during training. However, existing DST methods face challenges including insufficient training of newly grown connections and suboptimal pruning decisions. We introduce NeurRev (Neuron Revitalization), a novel approach that addresses these issues through targeted neuron revitalization and improved sparse training dynamics. Our method enhances the training quality of sparse networks by better utilizing the network capacity and making more informed pruning decisions throughout the training process.},
  address = {Vienna, Austria},
  author = {Gen Li and Lu Yin and Jie Ji and Wei Niu and Minghai Qin and Bin Ren and Linke Guo and Shiwei Liu and Xiaolong Ma},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024neurrev.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-01-09},
  pdf = {https://openreview.net/pdf?id=60lNoatp7u},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {{NeurRev}: Train Better Sparse Neural Network Practically via Neuron Revitalization},
  url = {https://openreview.net/forum?id=60lNoatp7u},
  year = {2024}
}

@inproceedings{li2024how,
  abstract = {While ImageNet pre-training has shown enormous success, it is formed in 2D, and the learned features are for classification tasks; when transferring to more diverse tasks, like 3D image segmentation, its performance is inevitably compromised due to the deviation from the original ImageNet context. We address this limitation by constructing AbdomenAtlas 1.1 that comprises 9,262 three-dimensional computed tomography (CT) volumes with high-quality, per-voxel annotations of 25 anatomical structures and pseudo annotations of seven tumor types. We develop a suite of models that are pre-trained on our AbdomenAtlas 1.1 for transfer learning. Our preliminary analyses indicate that the model trained only with 21 CT volumes, 672 masks, and 40 GPU hours has a transfer learning ability similar to the model trained with 5,050 (unlabeled) CT volumes and 1,152 GPU hours. More importantly, the transfer learning ability of supervised models can further scale up with larger annotated datasets, achieving significantly better performance than preexisting pre-trained models, irrespective of their pre-training methodologies or data sources.},
  address = {Vienna, Austria},
  author = {Wenxuan Li and Alan L. Yuille and Zongwei Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024how.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=AhizIPytk4},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {How Well Do Supervised {3D} Models Transfer to Medical Imaging Tasks?},
  url = {https://openreview.net/forum?id=AhizIPytk4},
  year = {2024}
}

@inproceedings{li2024mert,
  abstract = {Self-supervised learning (SSL) has shown remarkable success in speech and natural language processing, but its application to music audio has yet to be thoroughly explored. This paper proposes MERT, an acoustic Music undERstanding model with large-scale self-supervised Training that incorporates teacher models to provide pseudo labels in the masked language modeling style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which combines an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). The acoustic teacher effectively guides the student model to better model music audio, while the musical teacher provides structured musical knowledge. We also introduce in-batch noise mixture augmentation to enhance representation robustness and explore various settings to overcome instability in acoustic language model pre-training. Experimental results indicate that MERT can generalize well and perform competitively on 14 music understanding tasks, achieving state-of-the-art overall scores. MERT was evaluated with the MARBLE protocol and reported on the music understanding leaderboard.},
  address = {Vienna, Austria},
  author = {Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghao Xiao and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger B. Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Zili Wang and Yike Guo and Jie Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024mert.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-05-23},
  pdf = {https://openreview.net/pdf?id=w3YZ9MSlBu},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {{MERT}: Acoustic Music Understanding Model with Large-Scale Self-supervised Training},
  url = {https://openreview.net/forum?id=w3YZ9MSlBu},
  year = {2024}
}

@inproceedings{li2024selfalignment,
  abstract = {We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). The resulting training set is used to finetune the base model to follow instructions. Finetuning LLaMA on two iterations of this approach yields a model that outperforms all other LLaMA-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment. Our resulting model, Humpback, outperforms all other existing non-distilled models on the Alpaca leaderboard.},
  address = {Vienna, Austria},
  author = {Xian Li and Ping Yu and Chunting Zhou and Timo Schick and Omer Levy and Luke Zettlemoyer and Jason Weston and Mike Lewis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024selfalignment.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-04-22},
  pdf = {https://openreview.net/pdf?id=1oijHJBRsT},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Self-Alignment with Instruction Backtranslation},
  url = {https://openreview.net/forum?id=1oijHJBRsT},
  year = {2024}
}

@inproceedings{li2024aware,
  abstract = {Selection bias in recommender systems arises from the recommendation process of system filtering and the interactive process of user selection. Many previous studies have focused on addressing selection bias to achieve unbiased learning of the prediction model, but ignore the fact that potential outcomes for a given user-item pair may vary with the treatments assigned to other user-item pairs, named neighborhood effect. To fill the gap, this paper formally formulates the neighborhood effect as an interference problem from the perspective of causal inference and introduces a treatment representation to capture the neighborhood effect. On this basis, we propose a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood effect. We develop two estimators for the proposed ideal loss and theoretically establish the connection between the proposed and previous debiasing methods ignoring the neighborhood effect. We show that the proposed methods can achieve unbiased learning when both selection bias and neighborhood effects are present, while the existing methods are biased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed methods.},
  address = {Vienna, Austria},
  author = {Haoxuan Li and Chunyuan Zheng and Sihao Ding and Peng Wu and Zhi Geng and Fuli Feng and Xiangnan He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024aware.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=52fz5sUAy2},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference},
  url = {https://openreview.net/forum?id=52fz5sUAy2},
  year = {2024}
}

@inproceedings{li2024entropymcmc,
  abstract = {In Bayesian deep learning, the quality of posterior distribution estimation is crucial, but the posterior of deep neural networks is highly multi-modal with local modes exhibiting varying generalization performance. Given practical computational budgets, targeting the original posterior can lead to suboptimal performance as samples may become trapped in bad modes and suffer from overfitting. We leverage the observation that good modes with low generalization error often reside in flat basins of the energy landscape, and propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable whose stationary distribution resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. We show that our method, Entropy-MCMC, can successfully sample from flat basins in various neural network architectures, and demonstrate improved generalization performance and calibration across multiple tasks.},
  address = {Vienna, Austria},
  author = {Bolian Li and Ruqi Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024entropymcmc.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oGNdBvymod},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Entropy-{MCMC}: Sampling from Flat Basins with Ease},
  url = {https://openreview.net/forum?id=oGNdBvymod},
  year = {2024}
}

@inproceedings{li2024causally,
  abstract = {A pervasive challenge in Reinforcement Learning (RL) is the curse of dimensionality -- the exponential growth in the state-action space when optimizing high-dimensional target tasks. The framework of curriculum learning trains agents using a sequence of related and more manageable source tasks, with the expectation that shared optimal decision rules across tasks can help agents learn necessary skills more quickly. However, despite its empirical success, the theoretical foundations of curriculum learning remain largely unexplored. In this work, we develop a causal framework for curriculum learning in RL. We formalize the notion of causally aligned source tasks that can effectively transfer knowledge to target tasks through shared causal mechanisms. Our theoretical analysis reveals conditions under which curriculum learning is guaranteed to improve sample efficiency, and we propose algorithms for automatically constructing effective curricula based on causal structure. The framework provides both theoretical insights and practical guidance for designing curriculum learning systems that can provably benefit from task relationships.},
  address = {Vienna, Austria},
  author = {Mingxuan Li and Junzhe Zhang and Elias Bareinboim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024causally.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-05-28},
  pdf = {https://openreview.net/pdf?id=hp4yOjhwTs},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Causally Aligned Curriculum Learning},
  url = {https://openreview.net/forum?id=hp4yOjhwTs},
  year = {2024}
}

@inproceedings{li2024chainofknowledge,
  abstract = {We present Chain-of-Knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources, resulting in more factual rationales and reduced hallucination in generation. CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.},
  address = {Vienna, Austria},
  author = {Xingxuan Li and Ruochen Zhao and Yew Ken Chia and Bosheng Ding and Shafiq Joty and Soujanya Poria and Lidong Bing},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024chainofknowledge.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cPgh4gWZlz},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources},
  url = {https://openreview.net/forum?id=cPgh4gWZlz},
  year = {2024}
}

@inproceedings{li2024causal,
  abstract = {Traditional causal discovery approaches typically assume the absence of latent variables, a simplification that often does not align with real-world situations. Recently, there has been a surge of causal discovery methods that explicitly consider latent variables. While some works aim to reveal causal relations between observed variables in the presence of latent variables, others seek to identify latent variables and recover the causal structure over them. The latter typically entail strong distributional and graphical assumptions, such as the non-Gaussianity, purity, and two-pure-children assumption. In this paper, we endeavor to recover the whole causal structure involving both latent and observed variables under milder assumptions. We formulate two cases: one allows entirely arbitrary distributions and requires only one pure child per latent variable, while the other focuses on linear relationships under mild distributional assumptions. Our theoretical analysis shows that the proposed methods can successfully recover causal structures that previous methods could not identify due to their restrictive assumptions. Extensive experiments on both synthetic and real-world data demonstrate the effectiveness and robustness of our approach.},
  address = {Vienna, Austria},
  author = {Xiu-Chuan Li and Kun Zhang and Tongliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024causal.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=MukGKGtgnr},
  publisher = {OpenReview.net},
  series = {ICLR},
  title = {Causal Structure Recovery with Latent Variables under Milder Distributional and Graphical Assumptions},
  url = {https://openreview.net/forum?id=MukGKGtgnr},
  year = {2024}
}

@inproceedings{li2024open,
  abstract = {Current advancements in reinforcement learning ({RL}) have predominantly focused on learning step-based policies that generate actions for each perceived state. While these methods efficiently leverage step information from environmental interaction, they often ignore the temporal correlation between actions, resulting in inefficient exploration and unsmooth trajectories that are challenging to implement on real hardware. Episodic {RL} ({ERL}) seeks to overcome these challenges by exploring in parameters space that capture the correlation of actions. However, these approaches typically compromise data efficiency, as they treat trajectories as opaque black boxes. In this work, we introduce a novel {ERL} algorithm, Temporally-Correlated Episodic {RL} ({TCE}), which effectively utilizes step information in episodic policy updates, opening the `black box' in existing {ERL} methods while retaining the smooth and consistent exploration in parameter space. {TCE} synergistically combines the advantages of step-based and episodic {RL}, achieving comparable performance to recent {ERL} methods while maintaining data efficiency akin to state-of-the-art ({SoTA}) step-based {RL}.},
  author = {Ge Li and Hongyi Zhou and Dominik Roth and Serge Thilges and Fabian Otto and Rudolf Lioutikov and Gerhard Neumann},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/li2024open.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mnipav175N},
  publisher = {OpenReview.net},
  title = {Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning},
  url = {https://openreview.net/forum?id=mnipav175N},
  year = {2024}
}

@inproceedings{li2024graph,
  abstract = {While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of higher task accuracy requires a larger hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This work explores a promising direction for graph contrastive learning ({GCL}) with spiking neural networks ({SNNs}), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose {SpikeGCL}, a novel {GCL} framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that {SpikeGCL} has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compression, {SpikeGCL} is either comparable to or outperforms many fancy state-of-the-art supervised and self-supervised methods across several graph benchmarks.},
  author = {Jintang Li and Huizhe Zhang and Ruofan Wu and Zulun Zhu and Baokun Wang and Changhua Meng and Zibin Zheng and Liang Chen},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/li2024graph.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LnLySuf1vp},
  publisher = {OpenReview.net},
  title = {A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks},
  url = {https://openreview.net/forum?id=LnLySuf1vp},
  year = {2024}
}

@inproceedings{li2024debiased,
  abstract = {Collaborative filtering builds personalized models from the collected user feedback. However, the collected data is observational rather than experimental, leading to various biases in the data, which can significantly affect the learned model. To address this issue, many studies have focused on propensity-based methods to combat the selection bias by reweighting the sample loss, and demonstrate that balancing is important for debiasing both theoretically and empirically. However, there are two questions that still need to be addressed: which function class should be balanced and how to effectively balance that function class? In this paper, we first perform theoretical analysis to show the effect of balancing finite-dimensional function classes on the bias of {IPS} and {DR} methods, and based on this, we propose a universal kernel-based balancing method to balance functions on the reproducing kernel {Hilbert} space. In addition, we propose a novel adaptive causal balancing method during the alternating update between unbiased evaluation and training of the prediction model. Specifically, the prediction loss of the model is projected in the kernel-based covariate function space, and the projection coefficients are used to determine which functions should be prioritized for balancing to reduce the estimation bias. We conduct extensive experiments on three real-world datasets to demonstrate the effectiveness of the proposed approach.},
  author = {Haoxuan Li and Chunyuan Zheng and Yanghao Xiao and Peng Wu and Zhi Geng and Xu Chen and Peng Cui},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/li2024debiased.pdf:pdf},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=Ffjc8ApSbt},
  publisher = {OpenReview.net},
  title = {Debiased Collaborative Filtering with Kernel-Based Causal Balancing},
  url = {https://openreview.net/forum?id=Ffjc8ApSbt},
  year = {2024}
}

@inproceedings{li2024neural,
  abstract = {Graph Neural Networks ({GNNs}) have been widely adopted for drug discovery with molecular graphs. Nevertheless, current {GNNs} mainly excel in leveraging short-range interactions ({SRI}) but struggle to capture long-range interactions ({LRI}), both of which are crucial for determining molecular properties. In this work, we propose to abstract collective information of atomic groups into {Neural} {Atoms} by implicitly projecting atoms of a molecule. We explicitly exchange information among neural atoms and project them back to atoms' representations as enhancement. Such a mechanism establishes communication channels among distant nodes, effectively reducing the interaction scope of arbitrary node pairs into a single hop. Theoretical analysis reveals the connection with the traditional {LRI} calculation method, {Ewald} {Summation}, which enhances {GNNs} to capture {LRI} by approximating the potential {LRI} of molecules. Extensive experiments on four long-range graph benchmarks covering graph-level and link-level tasks on molecular graphs demonstrate the effectiveness of the proposed method, achieving up to 27.32\% and 38.27\% improvement in 2{D} and 3{D} scenarios respectively. The {Neural} {Atom} can be equipped with arbitrary {GNNs} to help capture {LRI}.},
  author = {Xuan Li and Zhanke Zhou and Jiangchao Yao and Yu Rong and Lu Zhang and Bo Han},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/li2024neural.pdf:pdf},
  note = {DBLP last modified: 2025-05-16},
  pdf = {https://openreview.net/pdf?id=CUfSCwcgqm},
  publisher = {OpenReview.net},
  title = {Neural Atoms: Propagating Long-range Interaction in Molecular Graphs through Efficient Communication Channel},
  url = {https://openreview.net/forum?id=CUfSCwcgqm},
  year = {2024}
}

@inproceedings{li2024adaptive,
  abstract = {Local motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images and wasting unnecessary computation on sharp pixels, especially for high-resolution images. This paper aims to adaptively and efficiently restore high-resolution locally blurred images. We propose a local motion deblurring vision {Transformer} ({LMD}-{ViT}) built on adaptive window pruning {Transformer} blocks ({AdaWPT}). To focus deblurring on local regions and reduce computation, {AdaWPT} prunes unnecessary windows, only allowing the active windows to be involved in the deblurring processes. The pruning operation relies on the blurriness confidence predicted by a confidence predictor that is trained end-to-end using a reconstruction loss with {Gumbel}-{Softmax} re-parameterization and a pruning loss guided by annotated blur masks. Our method removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements (+0.28{dB}) compared to state-of-the-art methods. In addition, our approach substantially reduces {FLOPs} by 66\% and achieves more than a twofold increase in inference speed compared to {Transformer}-based deblurring methods. We will make our code and annotated blur masks publicly available.},
  author = {Haoying Li and Jixin Zhao and Shangchen Zhou and Huajun Feng and Chongyi Li and Chen Change Loy},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/li2024adaptive.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hI18CDyadM},
  publisher = {OpenReview.net},
  title = {Adaptive Window Pruning for Efficient Local Motion Deblurring},
  url = {https://openreview.net/forum?id=hI18CDyadM},
  year = {2024}
}

@inproceedings{lialin2024relora,
  abstract = {Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparameterized models remains poorly understood, while training costs grow exponentially. In this paper, we explore parameter-efficient training techniques as an approach to training large neural networks. We introduce a novel method called {ReLoRA}, which utilizes low-rank updates to train high-rank networks. We apply {ReLoRA} to training transformer language models with up to 1.3{B} parameters and demonstrate comparable performance to regular neural network training. {ReLoRA} saves up to 5.5{Gb} of {RAM} per {GPU} and improves training speed by 9-40\% depending on the model size and hardware setup. Our findings show the potential of parameter-efficient techniques for large-scale pre-training. Our code is available on {GitHub}.},
  author = {Vladislav Lialin and Sherin Muckatira and Namrata Shivagunde and Anna Rumshisky},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/lialin2024relora.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DLJznSp6X3},
  publisher = {OpenReview.net},
  title = {{ReLoRA}: High-Rank Training Through Low-Rank Updates},
  url = {https://openreview.net/forum?id=DLJznSp6X3},
  year = {2024}
}

@inproceedings{lian2024llm,
  abstract = {Text-conditioned diffusion models have emerged as a promising tool for neural video generation. However, current models still struggle with intricate spatiotemporal prompts and often generate restricted or incorrect motion. To address these limitations, we introduce {LLM}-grounded Video Diffusion ({LVD}). Instead of directly generating videos from the text inputs, {LVD} first leverages a large language model ({LLM}) to generate dynamic scene layouts based on the text inputs and subsequently uses the generated layouts to guide a diffusion model for video generation. We show that {LLMs} are able to understand complex spatiotemporal dynamics from text alone and generate layouts that align closely with both the prompts and the object motion patterns typically observed in the real world. We then propose to guide video diffusion models with these layouts by adjusting the attention maps. Our approach is training-free and can be integrated into any video diffusion model that admits classifier guidance. Our results demonstrate that {LVD} significantly outperforms its base video diffusion model and several strong baseline methods in faithfully generating videos with the desired attributes and motion patterns.},
  author = {Long Lian and Baifeng Shi and Adam Yala and Trevor Darrell and Boyi Li},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/lian2024llm.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=exKHibougU},
  publisher = {OpenReview.net},
  title = {{LLM}-grounded Video Diffusion Models},
  url = {https://openreview.net/forum?id=exKHibougU},
  year = {2024}
}

@inproceedings{liang2024generative,
  abstract = {By employing neural networks ({NN}) to learn input-solution mappings and passing a new input through the learned mapping to obtain a solution instantly, recent studies have shown remarkable speed improvements over iterative algorithms for solving optimization problems. Meanwhile, they also highlight methodological challenges to be addressed. In particular, general non-convex problems often present multiple optimal solutions for identical inputs, signifying a complex, multi-valued input-solution mapping. Conventional learning techniques, primarily tailored to learn single-valued mappings, struggle to train {NNs} to accurately decipher multi-valued ones, leading to inferior solutions. We address this fundamental issue by developing a generative learning approach using a rectified flow ({RectFlow}) model built upon ordinary differential equations. In contrast to learning input-solution mapping, we learn the mapping from input to solution distribution, exploiting the universal approximation capability of the {RectFlow} model. Upon receiving a new input, we employ the trained {RectFlow} model to sample high-quality solutions from the input-dependent distribution it has learned. Our approach outperforms conceivable {GAN} and Diffusion models in terms of training stability and run-time complexity. We provide a detailed characterization of the optimality loss and runtime complexity associated with our generative approach. Simulation results for solving non-convex problems show that our method achieves significantly better solution optimality than recent {NN} schemes, with comparable feasibility and speedup performance.},
  author = {Enming Liang and Minghua Chen},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/liang2024generative.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3tM1l5tSbv},
  publisher = {OpenReview.net},
  title = {Generative Learning for Solving Non-Convex Problem with Multi-Valued Input-Solution Mapping},
  url = {https://openreview.net/forum?id=3tM1l5tSbv},
  year = {2024}
}

@inproceedings{liang2024multimodal,
  abstract = {In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: how modalities combine to provide new task-relevant information that was not present in either alone. We study this question of interaction quantification under a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio). Using a precise information-theoretic definition of interactions, we derive lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on 1) shared information between modalities and 2) disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We show how these theoretical results can be used to estimate multimodal model performance, guide data collection, and select appropriate multimodal models for various tasks. Empirical validation across {ImageNet}, {PASCAL} {VOC}, {ModelNet40}, and five multimodal sentiment analysis datasets demonstrates the effectiveness of our bounds.},
  author = {Paul Pu Liang and Chun Kai Ling and Yun Cheng and Alexander Obolenskiy and Yudong Liu and Rohan Pandey and Alex Wilf and Louis-Philippe Morency and Russ Salakhutdinov},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/liang2024multimodal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BrjLHbqiYs},
  publisher = {OpenReview.net},
  title = {Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications},
  url = {https://openreview.net/forum?id=BrjLHbqiYs},
  year = {2024}
}

@inproceedings{liang2024poisoned,
  abstract = {The proliferation of face forgery techniques has raised significant concerns within society, thereby motivating the development of face forgery detection methods. These methods aim to distinguish forged faces from genuine ones and have proven effective in practical applications. However, this paper introduces a novel and previously unrecognized threat in face forgery detection scenarios caused by backdoor attack. By embedding backdoors into models and incorporating specific trigger patterns into the input, attackers can deceive detectors into producing erroneous predictions for forged faces. To achieve this goal, this paper proposes \emph{Poisoned Forgery Face} framework, which enables clean-label backdoor attacks on face forgery detectors. Our approach involves constructing a scalable trigger generator and utilizing a novel convolving process to generate translation-sensitive trigger patterns. Moreover, we employ a relative embedding method based on landmark-based regions to enhance the stealthiness of the poisoned samples. Consequently, detectors trained on our poisoned samples are embedded with backdoors. Notably, our approach surpasses {SoTA} backdoor baselines with a significant improvement in attack success rate (+16.39\% {BD}-{AUC}) and reduction in visibility (-12.65\% $L_\infty$). Furthermore, our attack exhibits promising performance against backdoor defenses. We anticipate that this paper will draw greater attention to the potential threats posed by backdoor attacks in face forgery detection scenarios. Our codes will be made available at \url{https://github.com/JWLiang007/PFF}.},
  author = {Jiawei Liang and Siyuan Liang and Aishan Liu and Xiaojun Jia and Junhao Kuang and Xiaochun Cao},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7--11, 2024},
  file = {:/home/b/documents/inproceedings/liang2024poisoned.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=8iTpB4RNvP},
  publisher = {OpenReview.net},
  title = {Poisoned Forgery Face: Towards Backdoor Attacks on Face Forgery Detection},
  url = {https://openreview.net/forum?id=8iTpB4RNvP},
  year = {2024}
}

@inproceedings{liang2024gametheoretic,
  abstract = {We introduce a temporally-coupled adversary considering the temporal coupling between perturbations over time and propose a game-theoretic response approach for adversarial defense against these adversaries. The paper discusses robustness in reinforcement learning, focusing on temporally-coupled perturbations where uncertainty is connected across time. The authors propose GRAD, a game-theoretic approach treating robust RL as a partially-observable two-player zero-sum game to optimize robustness.},
  address = {Vienna, Austria},
  author = {Yongyuan Liang and Yanchao Sun and Ruijie Zheng and Xiangyu Liu and Benjamin Eysenbach and Tuomas Sandholm and Furong Huang and Stephen Marcus McAleer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liang2024gametheoretic.pdf:pdf},
  keywords = {Reinforcement Learning, Robustness, Adversarial Learning},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=wZWTHU7AsQ},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations},
  url = {https://openreview.net/forum?id=wZWTHU7AsQ},
  year = {2024}
}

@inproceedings{liao2024transferring,
  abstract = {The paper introduces a label transfer problem in object detection where the goal is to modify annotations from source datasets to match target dataset label protocols. They propose a data-centric approach called Label-Guided Pseudo-Labeling (LGPL) that improves downstream object detection across different datasets and architectures. The method was validated across four object detection scenarios with seven datasets and three architectures, showing average improvement of 1.88 mAP and 2.65 AP^75.},
  address = {Vienna, Austria},
  author = {Yuan-Hong Liao and David Acuna and Rafid Mahmood and James Lucas and Viraj Uday Prabhu and Sanja Fidler},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liao2024transferring.pdf:pdf},
  keywords = {Object detection, Data-centric AI, Label translation, Dataset improvements, Transfer learning},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=ChHx5ORqF0},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Transferring Labels to Solve Annotation Mismatches Across Object Detection Datasets},
  url = {https://openreview.net/forum?id=ChHx5ORqF0},
  year = {2024}
}

@inproceedings{liao2024equiformerv2,
  abstract = {Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to 3D atomistic systems. However, they are limited to small degrees of equivariant representations due to computational complexity. This paper addresses this limitation by replacing SO(3) convolutions with eSCN convolutions and proposing three architectural improvements: attention re-normalization, separable S¬≤ activation, and separable layer normalization. The method outperforms previous state-of-the-art on OC20 dataset with up to 9% improvement on forces, 4% improvement on energies, and 2√ó reduction in DFT calculations for adsorption energies.},
  address = {Vienna, Austria},
  author = {Yi-Lun Liao and Brandon M. Wood and Abhishek Das and Tess E. Smidt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liao2024equiformerv2.pdf:pdf},
  keywords = {Equivariant neural networks, Graph neural networks, Computational physics, Transformer networks},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=mCOBKZmrzD},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations},
  url = {https://openreview.net/forum?id=mCOBKZmrzD},
  year = {2024}
}

@inproceedings{lienen2024zero,
  abstract = {This paper approaches turbulent flow simulation as a generative task directly learning the manifold of all possible turbulent flow states without relying on any initial flow state. The authors introduce a challenging 3D turbulence dataset and demonstrate that their generative model can capture flow distributions for unseen objects. Generative models are presented as a promising alternative to time series methods for neural surrogate turbulence solvers.},
  address = {Vienna, Austria},
  author = {Marten Lienen and David L{\"u}dke and Jan Hansen-Palmus and Stephan G{\"u}nnemann},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lienen2024zero.pdf:pdf},
  keywords = {PDE, Generative modeling, Diffusion models, Navier-Stokes equations, CFD},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=ZhlwoC1XaN},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {From Zero to Turbulence: Generative Modeling for {3D} Flow Simulation},
  url = {https://openreview.net/forum?id=ZhlwoC1XaN},
  year = {2024}
}

@inproceedings{lightman2024lets,
  abstract = {Large language models have improved in multi-step reasoning, but still make logical mistakes. This paper compares outcome vs. process supervision, finding process supervision significantly outperforms outcome supervision. The authors use step-level human feedback to build a robust verifier of LLM reasoning that achieves 78% accuracy on a subset of the MATH test set.},
  address = {Vienna, Austria},
  author = {Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lightman2024lets.pdf:pdf},
  keywords = {Large language models, Chain of thought, Verifiers, Human feedback, Mathematical reasoning},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=v8L0pN6EOi},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Let's Verify Step by Step},
  url = {https://openreview.net/forum?id=v8L0pN6EOi},
  year = {2024}
}

@inproceedings{lim2024graph,
  abstract = {Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. This paper introduces Graph Metanetworks (GMNs) that can process diverse neural network architectures by representing them as graphs. GMNs allow expressive, permutation equivariant processing of diverse neural network architectures.},
  address = {Vienna, Austria},
  author = {Derek Lim and Haggai Maron and Marc T. Law and Jonathan Lorraine and James Lucas},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lim2024graph.pdf:pdf},
  keywords = {Metanetwork, Graph neural networks, Equivariance, Neural architecture processing},
  note = {ICLR 2024 spotlight},
  pdf = {https://openreview.net/pdf?id=ijK5hyxs0n},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Graph Metanetworks for Processing Diverse Neural Architectures},
  url = {https://openreview.net/forum?id=ijK5hyxs0n},
  year = {2024}
}

@inproceedings{lim2024parallelizing,
  abstract = {Sequential models have long suffered from slow training due to their inherent sequential nature. This paper challenges this belief with a parallel algorithm that accelerates GPU evaluation of sequential models by up to 3 orders of magnitude faster without compromising output accuracy. The method provides a parallel algorithm to evaluate and train sequential models (e.g., RNN and NeuralODE) despite their inherent sequential nature.},
  address = {Vienna, Austria},
  author = {Yi Heng Lim and Qi Zhu and Joshua Selfridge and Muhammad Firmansyah Kasim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lim2024parallelizing.pdf:pdf},
  keywords = {Parallel algorithms, Recurrent neural networks, Neural ordinary differential equations, Sequential models},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=E34AlVLN0v},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Parallelizing non-linear sequential models over the sequence length},
  url = {https://openreview.net/forum?id=E34AlVLN0v},
  year = {2024}
}

@inproceedings{limisiewicz2024debiasing,
  abstract = {Large language models are becoming the go-to solution for many tasks. However, models are prone to rely on spurious correlations from biases and stereotypes in training data. This work proposes a novel method called DAMA for detecting and mitigating gender bias in language models by analyzing and intervening in mid-upper feed-forward layers. The method significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks.},
  address = {Vienna, Austria},
  author = {Tomasz Limisiewicz and David Mare{\v{c}}ek and Tom{\'a}{\v{s}} Musil},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/limisiewicz2024debiasing.pdf:pdf},
  keywords = {Model editing, Gender bias, Causal inference, Fairness, Language models},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=XIZEFyVGC9},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Debiasing Algorithm through Model Adaptation},
  url = {https://openreview.net/forum?id=XIZEFyVGC9},
  year = {2024}
}

@inproceedings{lin2024radit,
  abstract = {Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. This paper introduces RA-DIT, a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. The method uses a two-stage fine-tuning process: first updating the pre-trained language model to use retrieved information, then updating the retriever to return more relevant results. Their best model (RA-DIT 65B) achieves state-of-the-art performance, outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average.},
  address = {Vienna, Austria},
  author = {Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Richard James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024radit.pdf:pdf},
  keywords = {Retrieval-augmented language models, Large language models, Knowledge intensive NLP, Instruction tuning},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=22OTbutug9},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{RA-DIT}: Retrieval-Augmented Dual Instruction Tuning},
  url = {https://openreview.net/forum?id=22OTbutug9},
  year = {2024}
}

@inproceedings{lin2024transformers,
  abstract = {Large transformer models pretrained on offline reinforcement learning datasets have demonstrated remarkable in-context reinforcement learning (ICRL) capabilities. This paper provides a theoretical framework that analyzes supervised pretraining for ICRL. The authors prove that transformers can provably implement various reinforcement learning algorithms in context, and learn them through supervised pretraining, providing novel insights into their decision-making capabilities.},
  address = {Vienna, Austria},
  author = {Licong Lin and Yu Bai and Song Mei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024transformers.pdf:pdf},
  keywords = {Transformers, In-context learning, Reinforcement learning, Learning theory},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=yN4Wv17ss3},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining},
  url = {https://openreview.net/forum?id=yN4Wv17ss3},
  year = {2024}
}

@inproceedings{lin2024automatic,
  abstract = {The paper extends JAX to automatically differentiate higher-order functions by representing functions as "infinite dimensional generalization of arrays", implementing primitive operators for functionals, deriving linearization and transposition rules, and enabling functional differentiation with the same syntax as traditional function differentiation.},
  address = {Vienna, Austria},
  author = {Min Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024automatic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gzT61ziSCu},
  publisher = {OpenReview.net},
  title = {Automatic Functional Differentiation in {JAX}},
  url = {https://openreview.net/forum?id=gzT61ziSCu},
  year = {2024}
}

@inproceedings{lin2024batchprompt,
  abstract = {The paper addresses token utilization inefficiency in large language models (LLMs) by introducing "BatchPrompt", a method of batching multiple data points in a single prompt. The approach aims to improve token density and performance through Batch Permutation and Ensembling (BPE) and Self-reflection-guided EArly Stopping (SEAS). At batch size 32, the method achieves improved accuracy with significantly reduced token usage.},
  address = {Vienna, Austria},
  author = {Jianzhe Lin and Maurice Diesendruck and Liang Du and Robin Abraham},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024batchprompt.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Agyicd577r},
  publisher = {OpenReview.net},
  title = {{BatchPrompt}: Accomplish more with less},
  url = {https://openreview.net/forum?id=Agyicd577r},
  year = {2024}
}

@inproceedings{lin2024curse,
  abstract = {We uncover a surprising phenomenon in deep reinforcement learning: training a diverse ensemble of data-sharing agents -- a well-established exploration strategy -- can significantly impair the performance of the individual ensemble members when compared to standard single-agent training. Through careful analysis, we attribute the degradation in performance to the low proportion of self-generated data in the shared training data for each ensemble member, as well as the inefficiency of the individual ensemble members to learn from such highly off-policy data. We thus name this phenomenon the curse of diversity.},
  address = {Vienna, Austria},
  author = {Zhixuan Lin and Pierluca D'Oro and Evgenii Nikishin and Aaron C. Courville},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024curse.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=M3QXCOTTk4},
  publisher = {OpenReview.net},
  title = {The Curse of Diversity in Ensemble-Based Exploration},
  url = {https://openreview.net/forum?id=M3QXCOTTk4},
  year = {2024}
}

@inproceedings{lin2024adversarial,
  abstract = {The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. This work proposes a novel pipeline called Adversarial Training on Purification (AToP) that combines both approaches.},
  address = {Vienna, Austria},
  author = {Guang Lin and Chao Li and Jianhai Zhang and Toshihisa Tanaka and Qibin Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024adversarial.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=u7559ZMvwY},
  publisher = {OpenReview.net},
  title = {Adversarial Training on Purification ({AToP}): Advancing Both Robustness and Generalization},
  url = {https://openreview.net/forum?id=u7559ZMvwY},
  year = {2024}
}

@inproceedings{lin2024instructscene,
  abstract = {Comprehending natural language instructions is a charming property for 3D indoor scene synthesis systems. Existing methods directly model object joint distributions and express object relations implicitly within a scene, thereby hindering the controllability of generation. This work introduces a novel generative framework called InstructScene that integrates a semantic graph prior and layout decoder to improve controllability and fidelity of 3D scene synthesis.},
  address = {Vienna, Austria},
  author = {Chenguo Lin and Yadong Mu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024instructscene.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=LtuRgL03pI},
  publisher = {OpenReview.net},
  title = {{InstructScene}: Instruction-Driven {3D} Indoor Scene Synthesis with Semantic Graph Prior},
  url = {https://openreview.net/forum?id=LtuRgL03pI},
  year = {2024}
}

@inproceedings{lin2024tractable,
  abstract = {Posterior sampling, i.e., exponential mechanism to sample from the posterior distribution, provides Œµ-pure differential privacy (DP) guarantees and does not suffer from potentially unbounded privacy breach introduced by (Œµ,Œ¥)-approximate DP. In practice, however, one needs to apply approximate sampling methods such as Markov chain Monte Carlo (MCMC), thus re-introducing the unappealing Œ¥-approximation error into the privacy guarantees. This work proposes the Approximate Sample Perturbation (ASAP) algorithm to address this challenge.},
  address = {Vienna, Austria},
  author = {Yingyu Lin and Yian Ma and Yu-Xiang Wang and Rachel Emily Redberg and Zhiqi Bu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024tractable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=pmweVpJ229},
  publisher = {OpenReview.net},
  title = {Tractable {MCMC} for Private Learning with Pure and {Gaussian} Differential Privacy},
  url = {https://openreview.net/forum?id=pmweVpJ229},
  year = {2024}
}

@inproceedings{lin2024stochastic,
  abstract = {As is well known, both sampling from the posterior and computing the mean of the posterior in Gaussian process regression reduces to solving a large linear system of equations. We study the use of stochastic gradient descent for solving this linear system, and show that when done right---by which we mean using specific insights from the optimisation and kernel communities---stochastic gradient descent is highly effective.},
  address = {Vienna, Austria},
  author = {Jihao Andreas Lin and Shreyas Padhy and Javier Antoran and Austin Tripp and Alexander Terenin and Csaba Szepesvari and Jos\'e Miguel Hern\'andez-Lobato and David Janz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024stochastic.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fj2E5OcLFn},
  publisher = {OpenReview.net},
  title = {Stochastic Gradient Descent for Gaussian Processes Done Right},
  url = {https://openreview.net/forum?id=fj2E5OcLFn},
  year = {2024}
}

@inproceedings{lin2024unlocking,
  abstract = {Alignment tuning has become the de facto standard practice for enabling base large language models (LLMs) to serve as open-domain AI assistants. The alignment tuning process typically involves instruction learning through supervised fine-tuning (SFT) and preference tuning via reinforcement learning from human feedback (RLHF). We analyze what SFT+RLHF exactly teach LLMs and show that untuned LLMs can be aligned surprisingly well by in-context learning with as few as three well-written examples.},
  address = {Vienna, Austria},
  author = {Bill Yuchen Lin and Abhilasha Ravichander and Ximing Lu and Nouha Dziri and Melanie Sclar and Khyathi Chandu and Chandra Bhagavatula and Yejin Choi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024unlocking.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wxJ0eXwwda},
  publisher = {OpenReview.net},
  title = {The Unlocking Spell on Base {LLMs}: Rethinking Alignment via In-Context Learning},
  url = {https://openreview.net/forum?id=wxJ0eXwwda},
  year = {2024}
}

@inproceedings{lin2024class,
  abstract = {Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time. This paper introduces a new method called TPL (Task-id Prediction based on Likelihood Ratio) for addressing challenges in class incremental learning.},
  address = {Vienna, Austria},
  author = {Haowei Lin and Yijia Shao and Weinan Qian and Ningxin Pan and Yiduo Guo and Bing Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024class.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8QfK9Dq4q0},
  publisher = {OpenReview.net},
  title = {Class Incremental Learning via Likelihood Ratio Based Task Prediction},
  url = {https://openreview.net/forum?id=8QfK9Dq4q0},
  year = {2024}
}

@inproceedings{lin2024spurious,
  abstract = {Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. This paper introduces a novel approach called BAlaNced averaGing (BANG) to improve out-of-distribution generalization by utilizing diverse spurious features, challenging conventional wisdom about learning invariant features.},
  address = {Vienna, Austria},
  author = {Yong Lin and Lu Tan and Yifan Hao and Ho Nam Wong and Hanze Dong and Weizhong Zhang and Yujiu Yang and Tong Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024spurious.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-05-14},
  pdf = {https://openreview.net/pdf?id=d6H4RBi7RH},
  publisher = {OpenReview.net},
  title = {Spurious Feature Diversification Improves Out-of-distribution Generalization},
  url = {https://openreview.net/forum?id=d6H4RBi7RH},
  year = {2024}
}

@inproceedings{lin2024cambranch,
  abstract = {Recent advancements have introduced machine learning frameworks to enhance the Branch and Bound (B\&B) branching policies for solving Mixed Integer Linear Programming (MILP). These methods, primarily relying on imitation learning of Strong Branching, have shown superior performance. However, collecting expert samples for imitation learning, particularly for Strong Branching, is a time-consuming endeavor. The paper proposes {CAMBranch}, a framework that generates Augmented {MILPs} ({AMILPs}) by applying variable shifting to limited expert data. It leverages both {MILPs} and {AMILPs} for imitation learning and uses contrastive learning to improve branching decisions.},
  address = {Vienna, Austria},
  author = {Jiacheng Lin and Meng Xu and Zhihua Xiong and Huangang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024cambranch.pdf:pdf},
  keywords = {Contrastive Learning, Mixed Integer Programming, Machine Learning, Branching Strategies, Data Augmentation},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=K6kt50zAiG},
  publisher = {OpenReview.net},
  title = {{CAMBranch}: Contrastive Learning with Augmented {MILPs} for Branching},
  url = {https://openreview.net/forum?id=K6kt50zAiG},
  year = {2024}
}

@inproceedings{lin2024overmemorization,
  abstract = {Overfitting negatively impacts the generalization ability of deep neural networks ({DNNs}) in both natural and adversarial training. Existing methods struggle to consistently address different types of overfitting, typically designing strategies that focus separately on either natural or adversarial patterns. The authors propose a ``Distraction Over-Memorization'' ({DOM}) framework to mitigate overfitting by preventing neural networks from ``over-memorizing'' training patterns.},
  address = {Vienna, Austria},
  author = {Runqi Lin and Chaojian Yu and Bo Han and Tongliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lin2024overmemorization.pdf:pdf},
  keywords = {overfitting, natural overfitting, robust overfitting, catastrophic overfitting, representation learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=2V1Z0Jdmss},
  publisher = {OpenReview.net},
  title = {On the Over-Memorization During Natural, Robust and Catastrophic Overfitting},
  url = {https://openreview.net/forum?id=2V1Z0Jdmss},
  year = {2024}
}

@inproceedings{lingam2024rethinking,
  abstract = {Node labels for graphs are usually generated using an automated process or crowd-sourced from human users. This opens up avenues for malicious users to compromise the training labels, making it unwise to blindly rely on them. While robustness against noisy labels is an active area of research, there are only a handful of papers in the literature that address this for graph-based data. Even more so, the effects of adversarial label perturbations is sparsely studied. More critically, we reveal that the entire literature on label poisoning for {GNNs} is plagued by serious evaluation pitfalls. Thus making it hard to conclude how robust {GNNs} are against label perturbations. After course correcting the state of label poisoning attacks with our faithful evaluation, we identify a discrepancy in attack efficiency of ~9\% on average. Additionally, we introduce two new simple yet effective attacks that are significantly stronger (up to ~8\%) than the previous strongest attack.},
  address = {Vienna, Austria},
  author = {Vijay Lingam and Mohammad Sadegh Akhondzadeh and Aleksandar Bojchevski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lingam2024rethinking.pdf:pdf},
  keywords = {Graph Neural Networks, Label Poisoning, Adversarial Attacks, Robustness, Graph Security},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=J7ioefqDPw},
  publisher = {OpenReview.net},
  title = {Rethinking Label Poisoning for {GNNs}: Pitfalls and Attacks},
  url = {https://openreview.net/forum?id=J7ioefqDPw},
  year = {2024}
}

@inproceedings{lingle2024transformervq,
  abstract = {We introduce Transformer-{VQ}, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-{VQ}'s efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-{VQ} is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on {PG}-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-{VQ} is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.},
  address = {Vienna, Austria},
  author = {Lucas D. Lingle},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/lingle2024transformervq.pdf:pdf},
  keywords = {Transformers, Linear Attention, Vector Quantization, Efficient Models, Language Modeling},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oDdzXQzP2F},
  publisher = {OpenReview.net},
  title = {Transformer-{VQ}: Linear-Time Transformers via Vector Quantization},
  url = {https://openreview.net/forum?id=oDdzXQzP2F},
  year = {2024}
}

@inproceedings{liu2024scaling,
  abstract = {The extrapolation capability of Large Language Models ({LLMs}) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with {LLMs} involves modifying {RoPE} by replacing 10000, the rotary base of $\theta_n = 10000^{-2n/d}$ in the original {RoPE}, with a larger value and providing longer fine-tuning text. The key observation in this work is that fine-tuning a {RoPE}-based {LLM} with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. The authors propose ``Scaling Laws of {RoPE}-based Extrapolation,'' a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length.},
  address = {Vienna, Austria},
  author = {Xiaoran Liu and Hang Yan and Chenxin An and Xipeng Qiu and Dahua Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024scaling.pdf:pdf},
  keywords = {Language Models, Positional Encoding, Extrapolation, Scaling Laws, Context Length},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JO7k0SJ5V6},
  publisher = {OpenReview.net},
  title = {Scaling Laws of {RoPE}-based Extrapolation},
  url = {https://openreview.net/forum?id=JO7k0SJ5V6},
  year = {2024}
}

@inproceedings{liu2024identifiable,
  abstract = {Causal representation learning aims to unveil latent high-level causal representations from observed low-level data. One of its primary tasks is to provide reliable assurance of identifying these latent causal models, known as identifiability. A recent breakthrough explores identifiability by leveraging the change of causal influences among latent causal variables across multiple environments. However, this progress rests on the assumption that the causal relationships among latent causal variables adhere strictly to linear Gaussian models. In this paper, we extend the scope of latent causal models to involve nonlinear causal relationships, represented by polynomial models, and general noise distributions conforming to the exponential family.},
  address = {Vienna, Austria},
  author = {Yuhang Liu and Zhen Zhang and Dong Gong and Mingming Gong and Biwei Huang and Anton van den Hengel and Kun Zhang and Javen Qinfeng Shi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024identifiable.pdf:pdf},
  keywords = {Causal Representation Learning, Polynomial Models, Identifiability, Latent Variables, Exponential Family},
  note = {DBLP last modified: 2025-06-03},
  pdf = {https://openreview.net/pdf?id=ia9fKO1Vjq},
  publisher = {OpenReview.net},
  title = {Identifiable Latent Polynomial Causal Models through the Lens of Change},
  url = {https://openreview.net/forum?id=ia9fKO1Vjq},
  year = {2024}
}

@inproceedings{liu2024sophia,
  abstract = {Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory.},
  address = {Vienna, Austria},
  author = {Hong Liu and Zhiyuan Li and David Leo Wright Hall and Percy Liang and Tengyu Ma},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024sophia.pdf:pdf},
  keywords = {Optimization, Second-order Methods, Language Model Training, Hessian Estimation, Pre-training},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3xHDeA8Noi},
  publisher = {OpenReview.net},
  title = {Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training},
  url = {https://openreview.net/forum?id=3xHDeA8Noi},
  year = {2024}
}

@inproceedings{liu2024generative,
  abstract = {Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. They pre-trained a generative model called SpeechFlow on 60k hours of untranscribed speech with Flow Matching and masked conditions.},
  address = {Vienna, Austria},
  author = {Alexander H. Liu and Matthew Le and Apoorv Vyas and Bowen Shi and Andros Tjandra and Wei-Ning Hsu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024generative.pdf:pdf},
  keywords = {Generative Models, Speech Processing, Flow Matching, Pre-training, Speech Synthesis},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KpoQSgxbKH},
  publisher = {OpenReview.net},
  title = {Generative Pre-training for Speech with Flow Matching},
  url = {https://openreview.net/forum?id=KpoQSgxbKH},
  year = {2024}
}

@inproceedings{liu2024lumvit,
  abstract = {Bandwidth constraints during signal acquisition frequently impede real-time detection applications. Hyperspectral data is a notable example, whose vast volume compromises real-time hyperspectral detection. To tackle this hurdle, we introduce a novel approach leveraging pre-acquisition modulation to reduce the acquisition volume. This modulation process is governed by a deep learning model, utilizing prior information. Central to our approach is {LUM-ViT}, a Vision Transformer variant. Uniquely, {LUM-ViT} incorporates a learnable under-sampling mask tailored for pre-acquisition modulation. To further optimize for optical calculations, we propose a kernel-level weight binarization technique and a three-stage fine-tuning strategy.},
  address = {Vienna, Austria},
  author = {Lingfeng Liu and Dong Ni and Hangjie Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024lumvit.pdf:pdf},
  keywords = {Vision Transformer, Under-sampling, Optical Signal Processing, Hyperspectral Imaging, Real-time Detection},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wkbeqr5XhC},
  publisher = {OpenReview.net},
  title = {{LUM-ViT}: Learnable Under-sampling Mask Vision Transformer for Bandwidth Limited Optical Signal Acquisition},
  url = {https://openreview.net/forum?id=wkbeqr5XhC},
  year = {2024}
}

@inproceedings{liu2024setcse,
  abstract = {Taking inspiration from Set Theory, we introduce {SetCSE}, an innovative information retrieval framework. {SetCSE} employs sets to represent complex semantics and incorporates well-defined operations for structured information querying under the provided context. Within this framework, we introduce an inter-set contrastive learning objective to enhance comprehension of sentence embedding models concerning the given semantics. Furthermore, we present a suite of operations, including {SetCSE} intersection, difference, and operation series, that leverage sentence embeddings of the enhanced model for complex sentence retrieval tasks. Throughout this paper, we demonstrate that {SetCSE} adheres to the conventions of human language expressions regarding compounded semantics, provides a significant enhancement in the discriminatory capability of underlying sentence embedding models, and enables numerous information retrieval tasks involving convoluted and intricate prompts which cannot be achieved using existing querying methods.},
  address = {Vienna, Austria},
  author = {Kang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024setcse.pdf:pdf},
  keywords = {Information Retrieval, Contrastive Learning, Sentence Embeddings, Set Theory, Natural Language Processing},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=zEHGSN8Hy8},
  publisher = {OpenReview.net},
  title = {{SetCSE}: Set Operations using Contrastive Learning of Sentence Embeddings},
  url = {https://openreview.net/forum?id=zEHGSN8Hy8},
  year = {2024}
}

@inproceedings{liu2024large,
  abstract = {Bayesian optimization (BO) is a powerful approach for optimizing complex and expensive-to-evaluate black-box functions. Its importance is underscored in many applications, notably including hyperparameter tuning, but its efficacy depends on efficiently balancing exploration and exploitation. While there has been substantial progress in BO methods, striking this balance remains a delicate process. In this light, we present LLAMBO, a novel approach that integrates the capabilities of Large Language Models (LLM) within BO. At a high level, we frame the BO problem in natural language, enabling LLMs to iteratively propose and evaluate promising solutions conditioned on historical evaluations. More specifically, we explore how combining contextual understanding, few-shot learning proficiency, and domain knowledge of LLMs can improve model-based BO.},
  address = {Vienna, Austria},
  author = {Tennison Liu and Nicolas Astorga and Nabeel Seedat and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024large.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OOxotBmGol},
  publisher = {OpenReview.net},
  title = {Large Language Models to Enhance {B}ayesian Optimization},
  url = {https://openreview.net/forum?id=OOxotBmGol},
  year = {2024}
}

@inproceedings{liu2024how,
  abstract = {In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics. Notably, initial weight distributions with small (resp. large) variance may yield a rich (resp. lazy) regime, where significant (resp. minor) changes to network states and representation are observed over the course of learning. However, in biology, neural circuit connectivity generally has a low-rank structure and therefore differs markedly from the random initializations generally used for these studies. As such, here we investigate how the structure of the initial weights ‚Äî in particular their effective rank ‚Äî influences the network learning regime. Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning, a finding we also confirm with experimentally-driven initial connectivity in recurrent neural networks. Conversely, low-rank initialization biases learning towards richer learning. Importantly, however, as an exception to this rule, we find lazier learning can still occur with a low-rank initialization that aligns with task and data statistics.},
  address = {Vienna, Austria},
  author = {Yuhan Helena Liu and Aristide Baratin and Jonathan Cornford and Stefan Mihalas and Eric Shea-Brown and Guillaume Lajoie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024how.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=slSmYGc8ee},
  publisher = {OpenReview.net},
  title = {How connectivity structure shapes rich and lazy learning in neural circuits},
  url = {https://openreview.net/forum?id=slSmYGc8ee},
  year = {2024}
}

@inproceedings{liu2024improved,
  abstract = {High accuracy, low latency and high energy efficiency represent a set of contradictory goals when searching for system solutions for image classification and detection. While high-quality images naturally result in more precise detection and classification, they also result in a heavier computational workload for imaging and processing, reduce camera refresh rates, and increase the volume of data communication between the camera and processor. Taking inspiration from the foveal-peripheral sampling mechanism, saccade mechanism observed in the human visual system and the filling-in phenomena, this model stitches together information from foveal and peripheral vision, which are sampled from multiple glances. Assisted by a reinforcement learning-based saccade mechanism, our model reduces the required input pixels by over 90% per frame while maintaining the same level of performance in image recognition as with the original images. We evaluated the effectiveness of our model using the GTSRB dataset and the ImageNet dataset. Using an equal number of input pixels, our study demonstrates a 5% higher image recognition accuracy compared to state-of-the-art foveal-peripheral vision systems. Furthermore, we demonstrate that our foveal sampling/saccadic scene reconstruction model exhibits significantly lower complexity and higher data efficiency during the training phase compared to existing approaches.},
  address = {Vienna, Austria},
  author = {Jiayang Liu and Yiming Bu and Daniel Tso and Qinru Qiu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024improved.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=lOwkOIUJtx},
  publisher = {OpenReview.net},
  title = {Improved Efficiency Based on Learned Saccade and Continuous Scene Reconstruction From Foveated Visual Sampling},
  url = {https://openreview.net/forum?id=lOwkOIUJtx},
  year = {2024}
}

@inproceedings{liu2024learning,
  abstract = {This paper presents a framework for learning state and action abstractions in sequential decision-making domains. Our framework, planning abstraction from language (PARL), utilizes language-annotated demonstrations to automatically discover a symbolic and abstract action space and induce a latent state abstraction based on it. PARL consists of three stages: 1) recovering object-level and action concepts, 2) learning state abstractions, abstract action feasibility, and transition models, and 3) applying low-level policies for abstract actions. During inference, given the task description, PARL first makes abstract action plans using the latent transition and feasibility functions, then refines the high-level plan using low-level policies. PARL generalizes across scenarios involving novel object instances and environments, unseen concept compositions, and tasks that require longer planning horizons than settings it is trained on.},
  address = {Vienna, Austria},
  author = {Weiyu Liu and Geng Chen and Joy Hsu and Jiayuan Mao and Jiajun Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024learning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3UWuFoksGb},
  publisher = {OpenReview.net},
  title = {Learning Planning Abstractions from Language},
  url = {https://openreview.net/forum?id=3UWuFoksGb},
  year = {2024}
}

@inproceedings{liu2024rethinking,
  abstract = {Most existing works focus on direct perturbations to the victim's state/action or the underlying transition dynamics to demonstrate the vulnerability of reinforcement learning agents to adversarial attacks. However, such direct manipulations may not be always realizable. In this paper, we consider a multi-agent setting where a well-trained victim agent ŒΩ is exploited by an attacker controlling another agent Œ± with an adversarial policy. Previous models do not account for the possibility that the attacker may only have partial control over Œ± or that the attack may produce easily detectable "abnormal" behaviors. Furthermore, there is a lack of provably efficient defenses against these adversarial policies. In this work, we introduce a generalized attack framework that has the flexibility to model to what extent the adversary is able to control the agent, and allows the attacker to regulate the state distribution shift and produce stealthier adversarial policies. We offer a provably efficient defense with polynomial convergence to the most robust victim policy through adversarial training with timescale separation. This stands in sharp contrast to supervised learning, where adversarial training typically provides only empirical defenses. Using the Robosumo competition experiments, we show that our generalized attack formulation results in much stealthier adversarial policies when maintaining the same winning rate as baselines. Additionally, our adversarial training approach yields stable learning dynamics and less exploitable victim policies.},
  address = {Vienna, Austria},
  author = {Xiangyu Liu and Souradip Chakraborty and Yanchao Sun and Furong Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024rethinking.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pDCublKPmG},
  publisher = {OpenReview.net},
  title = {Rethinking Adversarial Policies: A Generalized Attack Formulation and Provable Defense in {RL}},
  url = {https://openreview.net/forum?id=pDCublKPmG},
  year = {2024}
}

@inproceedings{liu2024integrating,
  abstract = {Despite recent advancements, deep reinforcement learning (DRL) still struggles at learning sparse-reward goal-directed tasks. Classical planning excels at addressing hierarchical tasks by employing symbolic knowledge, yet most of the methods rely on assumptions about pre-defined subtasks. To bridge the best of both worlds, we propose a framework that integrates DRL with classical planning by automatically inducing task structures and substructures from a few demonstrations. Specifically, we adopt abstraction mapping formulation and define critical actions that lead to the transition at the abstraction level. Then, we propose to induce critical action schemata regarded as subtasks by employing genetic programming where the program model reflects prior domain knowledge of effect rules. Experimental results show that our proposed framework outperforms all the abovementioned algorithms in terms of sample efficiency and task performance.},
  address = {Vienna, Austria},
  author = {Jung-Chun Liu and Chi-Hsien Chang and Shao-Hua Sun and Tian-Li Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024integrating.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=PR6RMsxuW7},
  publisher = {OpenReview.net},
  title = {Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures},
  url = {https://openreview.net/forum?id=PR6RMsxuW7},
  year = {2024}
}

@inproceedings{liu2024interpretable,
  abstract = {The interpretability of neural networks is insufficient and the utilization of deep learning techniques for prediction necessitates significant computational expenditures. In this work, we proposed an interpretable sparse system identification method which does not require a time-consuming training through back-propagation. The proposed method integrates advantages from both knowledge-based and data-driven approaches, and constructs dictionary functions by leveraging Fourier basis. A sparse identification method without using neural network, achieving much higher accuracy with significantly lower computational cost in multi-variable long-term predictions compared with recent SOTA deep learning methods.},
  address = {Vienna, Austria},
  author = {Xiaoyi Liu and Duxin Chen and Wenjia Wei and Xia Zhu and Wenwu Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024interpretable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=aFWUY3E7ws},
  publisher = {OpenReview.net},
  title = {Interpretable Sparse System Identification: Beyond Recent Deep Learning Techniques on Time-Series Prediction},
  url = {https://openreview.net/forum?id=aFWUY3E7ws},
  year = {2024}
}

@inproceedings{liu2024segno,
  abstract = {Graph Neural Networks (GNNs) with equivariant properties have emerged as powerful tools for modeling complex dynamics of multi-object physical systems. However, their generalization ability is limited by the inadequate consideration of physical inductive biases: (1) Existing studies overlook the continuity of transitions among system states, opting to employ several discrete transformation layers to learn the direct mapping between two adjacent states; (2) Most models only account for first-order velocity information, despite the fact that many physical systems are governed by second-order motion laws. Different from previous models that use Equiv-GNNs to fit discrete kinematic states, we introduce the second-order equivariant graph neural ordinary differential equation (SEGNO), which employs Neural ODEs to approximate a continuous trajectory between two observed states and incorporates second-order motion equations to better estimate the underlying dynamics. Theoretically, we prove the uniqueness of the learned latent trajectory of SEGNO and further provide an upper bound on the discrepancy between the learned and the actual latent trajectory. Extensive experiments on complex dynamical systems including molecular dynamics and motion capture demonstrate that SEGNO yields a significant improvement over state-of-the-art baselines.},
  address = {Vienna, Austria},
  author = {Yang Liu and Jiashun Cheng and Haihong Zhao and Tingyang Xu and Peilin Zhao and Fugee Tsung and Jia Li and Yu Rong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024segno.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-01-30},
  pdf = {https://openreview.net/pdf?id=3oTPsORaDH},
  publisher = {OpenReview.net},
  title = {{SEGNO}: Generalizing Equivariant Graph Neural Networks with Physical Inductive Biases},
  url = {https://openreview.net/forum?id=3oTPsORaDH},
  year = {2024}
}

@inproceedings{liu2024imitation,
  abstract = {Humans often acquire new skills through observation and imitation. For robotic agents, learning from the plethora of unlabeled video demonstration data available on the Internet necessitates imitating the expert without access to its action, presenting a challenge known as Imitation Learning from Observations (ILfO). A common approach to tackle ILfO problems is to convert them into inverse reinforcement learning problems, utilizing a proxy reward computed from the agent's and the expert's observations. Nonetheless, we identify that tasks characterized by a progress dependency property pose significant challenges for such approaches; in these tasks, the agent needs to initially learn the expert's preceding behaviors before mastering the subsequent ones. Our investigation reveals that the main cause is that the reward signals assigned to later steps hinder the learning of initial behaviors. To address this challenge, we introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively alters the discount factor in reinforcement learning during the training phase, prioritizing earlier rewards initially and gradually engaging later rewards only when the earlier behaviors have been mastered.},
  address = {Vienna, Austria},
  author = {Yuyang Liu and Weijun Dong and Yingdong Hu and Chuan Wen and Zhao-Heng Yin and Chongjie Zhang and Yang Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024imitation.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pPJTQYOpNI},
  publisher = {OpenReview.net},
  title = {Imitation Learning from Observation with Automatic Discount Scheduling},
  url = {https://openreview.net/forum?id=pPJTQYOpNI},
  year = {2024}
}

@inproceedings{liu2024lmuformer,
  abstract = {Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. Inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LMUFormer which augments the LMU with convolutional patch embedding and convolutional channel mixer. Moreover, we present a spiking version of this architecture, which introduces the benefit of states within the patch embedding and channel mixer modules while simultaneously reducing the computing complexity. In comparison to SOTA transformer-based models within the ANN domain on the SCv2 dataset, our LMUFormer demonstrates comparable performance while necessitating a remarkable 53 times reduction in parameters and a substantial 65 times decrement in FLOPs. Furthermore, when benchmarked against extant low-complexity SNN variants, our model establishes a new SOTA with an accuracy of 96.12%. Additionally, owing to our model's proficiency in real-time data processing, we are able to achieve a 32.03% reduction in sequence length, all while incurring an inconsequential decline in performance.},
  address = {Vienna, Austria},
  author = {Zeyu Liu and Gourav Datta and Anni Li and Peter Anthony Beerel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024lmuformer.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=oEF7qExD9F},
  publisher = {OpenReview.net},
  title = {{LMUFormer}: Low Complexity Yet Powerful Spiking Model With {L}egendre Memory Units},
  url = {https://openreview.net/forum?id=oEF7qExD9F},
  year = {2024}
}

@inproceedings{liu2024locality,
  abstract = {Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even with very high dimensional nonlinear features. We validate the representation power of our encoding and verify that it allows efficient online learning under data covariate shift. We also show, in the Dyna MBRL setting, that our world models learned online using a single pass of trajectory data either surpass or match the performance of deep world models trained with replay and other continual learning methods.},
  address = {Vienna, Austria},
  author = {Zichen Liu and Chao Du and Wee Sun Lee and Min Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024locality.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=i8PjQT3Uig},
  publisher = {OpenReview.net},
  title = {Locality Sensitive Sparse Encoding for Learning World Models Online},
  url = {https://openreview.net/forum?id=i8PjQT3Uig},
  year = {2024}
}

@inproceedings{liu2024l2pmip,
  abstract = {Modern solvers for solving mixed integer programming (MIP) often rely on the branch-and-bound (B\&B) algorithm which could be of high time complexity, and presolving techniques are well designed to simplify the instance as pre-processing before B\&B. However, such presolvers in existing literature or open-source solvers are mostly set by default agnostic to specific input instances, and few studies have been reported on tailoring presolving settings. In this paper, we aim to dive into this open question and show that the MIP solver can be indeed largely improved when switching the default instance-agnostic presolving into instance-specific presolving. Specifically, we propose a combination of supervised learning and classic heuristics to achieve efficient presolving adjusting, avoiding tedious reinforcement learning. Notably, our approach is orthogonal from many recent efforts in incorporating learning modules into the B\&B framework after the presolving stage, and to our best knowledge, this is the first work for introducing learning to presolve in MIP solvers. Experiments on multiple real-world datasets show that well-trained neural networks can infer proper presolving for arbitrary incoming MIP instances in less than 0.5s, which is neglectable compared with the solving time often hours or days.},
  address = {Vienna, Austria},
  author = {Chang Liu and Zhichen Dong and Haobo Ma and Weilin Luo and Xijun Li and Bowen Pang and Jia Zeng and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024l2pmip.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=McfYbKnpT8},
  publisher = {OpenReview.net},
  title = {{L2P-MIP}: Learning to Presolve for Mixed Integer Programming},
  url = {https://openreview.net/forum?id=McfYbKnpT8},
  year = {2024}
}

@inproceedings{liu2024beyond,
  abstract = {In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond merely worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic difficulty in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\Pi$. This finding prompts us to refine the baseline policy class $\Pi$ prior to test time, aiming for efficient adaptation within a compact, finite policy class $\tilde{\Pi}$, which can resort to an adversarial bandit subroutine. In light of the importance of a finite and compact $\tilde{\Pi}$, we propose a novel training-time algorithm to iteratively discover non-dominated policies, forming a near-optimal and minimal $\tilde{\Pi}$, thereby ensuring both robustness and test-time efficiency. Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios.},
  address = {Vienna, Austria},
  author = {Xiangyu Liu and Chenghao Deng and Yanchao Sun and Yongyuan Liang and Furong Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024beyond.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=DFTHW0MyiW},
  publisher = {OpenReview.net},
  title = {Beyond Worst-case Attacks: Robust {RL} with Adaptive Defense via Non-dominated Policies},
  url = {https://openreview.net/forum?id=DFTHW0MyiW},
  year = {2024}
}

@inproceedings{liu2024ghost,
  abstract = {The creation of photorealistic virtual worlds requires the accurate modeling of 3D surface geometry for a wide range of objects. For this, meshes are appealing since they enable 1) fast physics-based rendering with realistic material and lighting, 2) physical simulation, and 3) are memory-efficient for modern graphics pipelines. Recent work on reconstructing and statistically modeling 3D shape, however, has critiqued meshes as being topologically inflexible. To capture a wide range of object shapes, any 3D representation must be able to model solid, watertight, shapes as well as thin, open, surfaces. Recent work has focused on the former, and methods for reconstructing open surfaces do not support fast reconstruction with material and lighting or unconditional generative modelling. Inspired by the observation that open surfaces can be seen as islands floating on watertight surfaces, we parametrize open surfaces by defining a manifold signed distance field on watertight templates. With this parametrization, we further develop a grid-based and differentiable representation that parametrizes both watertight and non-watertight meshes of arbitrary topology. Our new representation, called Ghost-on-the-Shell (G-Shell), enables two important applications: differentiable rasterization-based reconstruction from multiview images and generative modelling of non-watertight meshes. We empirically demonstrate that G-Shell achieves state-of-the-art performance on non-watertight mesh reconstruction and generation tasks, while also performing effectively for watertight meshes.},
  address = {Vienna, Austria},
  author = {Zhen Liu and Yao Feng and Yuliang Xiu and Weiyang Liu and Liam Paull and Michael J. Black and Bernhard Sch{\"o}lkopf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024ghost.pdf:pdf},
  note = {DBLP last modified: 2025-05-16},
  pdf = {https://openreview.net/pdf?id=Ad87VjRqUw},
  publisher = {OpenReview.net},
  title = {Ghost on the Shell: An Expressive Representation of General {3D} Shapes},
  url = {https://openreview.net/forum?id=Ad87VjRqUw},
  year = {2024}
}

@inproceedings{liu2024tangent,
  abstract = {We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. When applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, TAFT enjoys several theoretical advantages in terms of model composition, parallel training, machine unlearning, and differential privacy. We demonstrate these advantages in several multi-domain visual classification tasks.},
  address = {Vienna, Austria},
  author = {Tian Yu Liu and Aditya Golatkar and Stefano Soatto},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024tangent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=VLFhbOCz5D},
  publisher = {OpenReview.net},
  title = {Tangent Transformers for Composition, Privacy and Removal},
  url = {https://openreview.net/forum?id=VLFhbOCz5D},
  year = {2024}
}

@inproceedings{liu2024qllm,
  abstract = {Quantizing large language models (LLMs) is one of the most effective approaches to improve inference efficiency, which has been greatly promoted by the recent advance in post-training quantization (PTQ). However, the existing PTQ methods still suffer from low accuracy at low-bitwidth. We identify the activation outliers in particular channels as the bottleneck to PTQ accuracy. Although existing methods that transform magnitudes from activations to weights offer limited alleviation or suffer from unstable gradients, resulting in severe performance drop at low-bitwidth. To tackle this problem, we propose QLLM, an accurate and efficient low-bitwidth PTQ method for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. Specifically, the channel disassembly breaks down the outlier channels into several sub-channels to ensure a more balanced distribution of activation magnitudes, then similar channels are merged to maintain the original channel number for efficiency. Additionally, an adaptive strategy is designed to autonomously determine the optimal number of sub-channels for channel disassembly. To further compensate for the performance loss caused by quantization, we propose an efficient tuning method that only learns a small number of low-rank weights while freezing the pre-trained quantized model. After training, these low-rank parameters can be fused into the frozen weights without affecting inference. Extensive experiments show that QLLM outperforms the previous state-of-the-art method by 7.89% on the average accuracy across five zero-shot tasks. Moreover, QLLM quantizes the 4-bit LLaMA-2-70B within 10 hours on a single A100-80G GPU.},
  address = {Vienna, Austria},
  author = {Jing Liu and Ruihao Gong and Xiuying Wei and Zhiwei Dong and Jianfei Cai and Bohan Zhuang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024qllm.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FIplmUWdm3},
  publisher = {OpenReview.net},
  title = {{QLLM}: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models},
  url = {https://openreview.net/forum?id=FIplmUWdm3},
  year = {2024}
}

@inproceedings{liu2024scalable,
  abstract = {Graph Neural Networks (GNNs) have become the de facto standard for modeling graph-structured data in various applications. Among them, implicit GNNs have shown superior ability to effectively capture long-range dependencies in underlying graphs. However, implicit GNNs tend to be computationally expensive and have high memory usage, due to 1) their use of full-batch training; and 2) they require a large number of iterations to solve a fixed-point equation. These compromise the scalability and efficiency of implicit GNNs especially on large graphs. In this paper, we aim to answer the question: how can we efficiently train implicit GNNs to provide effective predictions on large graphs? To this end, we propose a scalable training framework for implicit GNNs, which addresses both computational and memory challenges. Our approach enables mini-batch training for implicit GNNs while maintaining their theoretical properties, and incorporates efficient fixed-point solvers that reduce the number of iterations required. We demonstrate through extensive experiments on large-scale graphs that our method achieves comparable or better performance than standard implicit GNNs while being significantly more scalable and memory-efficient.},
  address = {Vienna, Austria},
  author = {Juncheng Liu and Bryan Hooi and Kenji Kawaguchi and Yiwei Wang and Chaosheng Dong and Xiaokui Xiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024scalable.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QcMdPYBwTu},
  publisher = {OpenReview.net},
  title = {Scalable and Effective Implicit Graph Neural Networks on Large Graphs},
  url = {https://openreview.net/forum?id=QcMdPYBwTu},
  year = {2024}
}

@inproceedings{liu2024it,
  abstract = {The recent boom of linear forecasting models questions the ongoing passion for architectural sophistication and the necessity of Transformer for time series forecasting. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. In response, we propose iTransformer, that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets and shows that the Transformer architecture can be simply repurposed for effective time series forecasting beyond the vanilla attention over temporal tokens. We further provide theoretical justification and empirical evidence that iTransformer better utilizes the attention mechanism for time series representation learning.},
  address = {Vienna, Austria},
  author = {Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024it.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JePfAI8fah},
  publisher = {OpenReview.net},
  title = {i{T}ransformer: Inverted Transformers Are Effective for Time Series Forecasting},
  url = {https://openreview.net/forum?id=JePfAI8fah},
  year = {2024}
}

@inproceedings{liu2024litcab,
  abstract = {A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding < 2\% of the original model parameters. For evaluation, we construct CaT (Calibration on Text generation), a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks. Through comprehensive evaluation, we find three key insights: (1) larger models within the same family exhibit better calibration on tasks with short generation, but not necessarily for longer ones, (2) GPT-family models show superior calibration compared to Llama-family models, despite having much fewer parameters, (3) finetuning pretrained models may lead to worse calibration, highlighting the importance of calibration for safety-critical applications.},
  address = {Vienna, Austria},
  author = {Xin Liu and Muhammad Khalifa and Lu Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024litcab.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jH67LHVOIO},
  publisher = {OpenReview.net},
  title = {{LitCab}: Lightweight Language Model Calibration over Short- and Long-form Responses},
  url = {https://openreview.net/forum?id=jH67LHVOIO},
  year = {2024}
}

@inproceedings{liu2024discovering,
  abstract = {Text-guided diffusion models (TDMs) are widely applied but can fail unexpectedly. Common failures include natural-looking text prompts generating images with wrong content, or different random samples of the latent variables that generate vastly different outputs despite being conditioned on the same text prompt. To study and understand these failure modes in more detail, we propose SAGE, the first adversarial search method on TDMs that systematically explores the discrete prompt space and the high-dimensional latent space, to automatically discover undesirable behaviors and failure cases in image generation. We use image classifiers as surrogate loss functions during searching, and employ human inspection to validate the identified failures. For the first time, our method enables efficient exploration of both the discrete and intricate human language space and the challenging latent space. We demonstrate the effectiveness of SAGE on five widely used generative models and reveal four typical failure modes: (1) We find a variety of natural text prompts that are natural and easily understood by humans but not by the SOTA diffusion models. (2) We find latent samples that consistently lead to distorted images under various commonly used prompts. (3) We find latent samples that generate objects commonly associated with the key object, rather than generating the key object class itself. (4) We find universal token embeddings that cause irrelevant images across various input prompts. Our findings provide insights into the underlying mechanisms and potential limitations of TDMs and pave the way for more robust and reliable text-guided generative models.},
  address = {Vienna, Austria},
  author = {Qihao Liu and Adam Kortylewski and Yutong Bai and Song Bai and Alan L. Yuille},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024discovering.pdf:pdf},
  note = {DBLP last modified: 2025-06-02},
  pdf = {https://openreview.net/pdf?id=TOWdQQgMJY},
  publisher = {OpenReview.net},
  title = {Discovering Failure Modes of Text-guided Diffusion Models via Adversarial Search},
  url = {https://openreview.net/forum?id=TOWdQQgMJY},
  year = {2024}
}

@inproceedings{liu2024simple,
  abstract = {This paper addresses computational inefficiencies in Vision Transformers ({V}i{T}s) by proposing Multi-Exit Token Reduction ({METR}). The method introduces early task pressure to improve token reduction techniques, specifically focusing on making the [{CLS}] token more effective at gathering task-relevant information in early model blocks. The approach aims to make {V}i{T}s more computationally efficient by developing a novel approach to token reduction that enhances information gathering in early model stages.},
  address = {Vienna, Austria},
  author = {Dongyang Liu and Meina Kan and Shiguang Shan and Xilin Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024simple.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gJeYtRuguR},
  publisher = {OpenReview.net},
  title = {A Simple Romance Between Multi-Exit Vision Transformer and Token Reduction},
  url = {https://openreview.net/forum?id=gJeYtRuguR},
  year = {2024}
}

@inproceedings{liu2024devil,
  abstract = {Pre-trained language models ({PLM}s) contain social biases that can cause negative social impacts. The authors introduce Social Bias Neurons and propose a method called Integrated Gap Gradients ({IG}¬≤) to identify and mitigate these biases by pinpointing specific neurons responsible for undesirable behaviors. The approach aims to achieve a higher degree of fairness while maintaining language modeling ability with low cost.},
  address = {Vienna, Austria},
  author = {Yan Liu and Yu Liu and Xiaokang Chen and Pin-Yu Chen and Daoguang Zan and Min-Yen Kan and Tsung-Yi Ho},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024devil.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-15},
  pdf = {https://openreview.net/pdf?id=SQGUDc9tC8},
  publisher = {OpenReview.net},
  title = {The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models},
  url = {https://openreview.net/forum?id=SQGUDc9tC8},
  year = {2024}
}

@inproceedings{liu2024towards,
  abstract = {This paper explores model selection in multi-modal {AI} agents, focusing on how Large Language Models ({LLM}s) can dynamically select appropriate models for complex multi-step reasoning tasks. The authors propose the {M}¬≥ framework to improve model selection robustness by considering subtask dependencies during model selection.},
  address = {Vienna, Austria},
  author = {Xiangyan Liu and Rongxue Li and Wei Ji and Tao Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024towards.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KTf4DGAzus},
  publisher = {OpenReview.net},
  title = {Towards Robust Multi-Modal Reasoning via Model Selection},
  url = {https://openreview.net/forum?id=KTf4DGAzus},
  year = {2024}
}

@inproceedings{liu2024mitigating,
  abstract = {This paper addresses hallucination in multi-modal models by introducing the Large-scale Robust Visual ({LRV})-Instruction dataset with 400k visual instructions generated by {GPT}4. The dataset covers 16 vision-and-language tasks and includes both positive and negative instructions at three semantic levels: Nonexistent Object Manipulation, Existent Object Manipulation, and Knowledge Manipulation. The method aims to reduce hallucinations in large multi-modal models through more robust instruction tuning.},
  address = {Vienna, Austria},
  author = {Fuxiao Liu and Kevin Lin and Linjie Li and Jianfeng Wang and Yaser Yacoob and Lijuan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024mitigating.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=J44HfH4JCg},
  publisher = {OpenReview.net},
  title = {Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning},
  url = {https://openreview.net/forum?id=J44HfH4JCg},
  year = {2024}
}

@inproceedings{liu2024generalized,
  abstract = {Modern distribution matching algorithms for training diffusion or flow models directly prescribe the time evolution of the marginal distributions between two boundary distributions. In this work, the authors consider a generalized distribution matching setup, where these marginals are only implicitly described as a solution to some task-specific objective function. They propose Generalized {S}chr√∂dinger Bridge Matching, a new distribution matching algorithm for training diffusion models enhanced with task-specific optimality structures.},
  address = {Vienna, Austria},
  author = {Guan-Horng Liu and Yaron Lipman and Maximilian Nickel and Brian Karrer and Evangelos A. Theodorou and Ricky T. Q. Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024generalized.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=SoismgeX7z},
  publisher = {OpenReview.net},
  title = {Generalized {S}chr√∂dinger Bridge Matching},
  url = {https://openreview.net/forum?id=SoismgeX7z},
  year = {2024}
}

@inproceedings{liu2024infocon,
  abstract = {This paper focuses on self-supervised discovery of manipulation concepts that can be adapted and reassembled to address various robotic tasks. The key innovation is proposing that concept discovery should depend on informativeness in its representation regarding the low-level physical state and state changes rather than semantic naming. The approach aims to discover manipulation concepts through generative and discriminative metrics without human annotation.},
  address = {Vienna, Austria},
  author = {Ruizhe Liu and Qian Luo and Yanchao Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024infocon.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-01-17},
  pdf = {https://openreview.net/pdf?id=g6eCbercEc},
  publisher = {OpenReview.net},
  title = {{InfoCon}: Concept Discovery with Generative and Discriminative Informativeness},
  url = {https://openreview.net/forum?id=g6eCbercEc},
  year = {2024}
}

@inproceedings{liu2024syncdreamer,
  abstract = {This paper presents a novel diffusion model called {SyncDreamer} that generates multiview-consistent images from a single-view image. The key innovation is a synchronized multiview diffusion model that models the joint probability distribution of multiview images, enabling generation of consistent images in a single reverse process while maintaining geometric and color consistency when generating novel views of an object from a single input image.},
  address = {Vienna, Austria},
  author = {Yuan Liu and Cheng Lin and Zijiao Zeng and Xiaoxiao Long and Lingjie Liu and Taku Komura and Wenping Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024syncdreamer.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MN3yH2ovHb},
  publisher = {OpenReview.net},
  title = {{SyncDreamer}: Generating Multiview-consistent Images from a Single-view Image},
  url = {https://openreview.net/forum?id=MN3yH2ovHb},
  year = {2024}
}

@inproceedings{liu2024image,
  abstract = {This paper proposes using Tractable Probabilistic Models ({TPM}s) to guide diffusion models in image inpainting. The approach addresses the challenge of exact conditioning in constrained image generation by using Probabilistic Circuits ({PC}s) to compute constrained posteriors. The method demonstrates improved image quality across {CelebA}-{HQ}, {ImageNet}, and {LSUN} datasets with only approximately 10\% additional computational overhead.},
  address = {Vienna, Austria},
  author = {Anji Liu and Mathias Niepert and Guy Van den Broeck},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024image.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NSIVHTbZBR},
  publisher = {OpenReview.net},
  title = {Image Inpainting via Tractable Steering of Diffusion Models},
  url = {https://openreview.net/forum?id=NSIVHTbZBR},
  year = {2024}
}

@inproceedings{liu2024unified,
  abstract = {This paper focuses on diffusion probabilistic models ({DPM}s) and proposes a unified sampling framework ({USF}) to study solver strategies. The approach introduces {S}¬≥, a predictor-based search method for optimizing solver schedules. The method achieves significant performance improvements: 2.69 {FID} with 9 {NFE} and 6.86 {FID} with 5 {NFE} on {CIFAR}-10 dataset, aiming to accelerate and improve sampling quality in diffusion models by developing more flexible solving strategies across different timesteps.},
  address = {Vienna, Austria},
  author = {Enshu Liu and Xuefei Ning and Huazhong Yang and Yu Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024unified.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=W2d3LZbhhI},
  publisher = {OpenReview.net},
  title = {A Unified Sampling Framework for Solver Searching of Diffusion Probabilistic Models},
  url = {https://openreview.net/forum?id=W2d3LZbhhI},
  year = {2024}
}

@inproceedings{liu2024unforgeable,
  abstract = {This paper proposes a novel watermarking algorithm for large language models ({LLM}s) that addresses current limitations in watermark detection. Current watermark detection requires a secret key, making them vulnerable to security breaches. The approach introduces the {UPV} algorithm using two different neural networks for generation and detection, sharing token embedding parameters between networks for efficient, accurate detection. The method aims to mitigate potential harms like fake news and copyright issues.},
  address = {Vienna, Austria},
  author = {Aiwei Liu and Leyi Pan and Xuming Hu and Shuang Li and Lijie Wen and Irwin King and Philip S. Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024unforgeable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=gMLQwKDY3N},
  publisher = {OpenReview.net},
  title = {An Unforgeable Publicly Verifiable Watermark for Large Language Models},
  url = {https://openreview.net/forum?id=gMLQwKDY3N},
  year = {2024}
}

@inproceedings{liu2024semantic,
  abstract = {We propose a semantic invariant watermarking method for large language models that provides both attack and security robustness. The key innovation is determining watermark logits based on the semantics of preceding tokens by using another embedding LLM to generate semantic embeddings.},
  address = {Vienna, Austria},
  author = {Aiwei Liu and Leyi Pan and Xuming Hu and Shiao Meng and Lijie Wen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024semantic.pdf:pdf},
  keywords = {Watermark algorithms, Large Language Models, Robustness},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6p8lpe4MNf},
  publisher = {OpenReview.net},
  title = {A Semantic Invariant Robust Watermark for Large Language Models},
  url = {https://openreview.net/forum?id=6p8lpe4MNf},
  year = {2024}
}

@inproceedings{liu2024metaevolve,
  abstract = {This paper investigates transferring an expert policy from a source robot to multiple different robots using a method called Meta-Evolve. The approach uses continuous robot evolution to efficiently transfer policies through tree-structured evolutionary robot sequences, improving manipulation policy transfer by up to 3.2√ó and agile locomotion policy transfer by 2.4√ó.},
  address = {Vienna, Austria},
  author = {Xingyu Liu and Deepak Pathak and Ding Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024metaevolve.pdf:pdf},
  keywords = {policy transfer, transfer learning, imitation learning, reinforcement learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RthOl4jHw5},
  publisher = {OpenReview.net},
  title = {Meta-Evolve: Continuous Robot Evolution for One-to-many Policy Transfer},
  url = {https://openreview.net/forum?id=RthOl4jHw5},
  year = {2024}
}

@inproceedings{liu2024parameterefficient,
  abstract = {Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. This paper presents a simple yet effective finetuning method for the adaptation of foundation models using parameter-efficient orthogonal finetuning via butterfly factorization.},
  address = {Vienna, Austria},
  author = {Weiyang Liu and Zeju Qiu and Yao Feng and Yuliang Xiu and Yuxuan Xue and Longhui Yu and Haiwen Feng and Zhen Liu and Juyeon Heo and Songyou Peng and Yandong Wen and Michael J. Black and Adrian Weller and Bernhard Sch{\"o}lkopf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024parameterefficient.pdf:pdf},
  keywords = {Parameter-efficient finetuning, orthogonal, Butterfly matrix},
  note = {DBLP last modified: 2025-05-16},
  pdf = {https://openreview.net/pdf?id=7NzgkEdGyr},
  publisher = {OpenReview.net},
  title = {Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization},
  url = {https://openreview.net/forum?id=7NzgkEdGyr},
  year = {2024}
}

@inproceedings{liu2024clifford,
  abstract = {We introduce Clifford Group Equivariant Simplicial Message Passing Networks, a method for steerable E(n)-equivariant message passing on simplicial complexes. Our method integrates the expressivity of Clifford group-equivariant layers with simplicial message passing, which is topologically more intricate than regular graph message passing.},
  address = {Vienna, Austria},
  author = {Cong Liu and David Ruhe and Floor Eijkelboom and Patrick Forr{\'e}},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024clifford.pdf:pdf},
  keywords = {Clifford Algebra, Geometric Algebra, Graph Neural Networks, Simplicial Message Passing, Topological Deep Learning, Geometric Deep Learning, Equivariance},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Zz594UBNOH},
  publisher = {OpenReview.net},
  title = {Clifford Group Equivariant Simplicial Message Passing Networks},
  url = {https://openreview.net/forum?id=Zz594UBNOH},
  year = {2024}
}

@inproceedings{liu2024democratizing,
  abstract = {Identifying subordinate-level categories from images is a longstanding task in computer vision. A major bottleneck in developing FGVR systems is caused by the need of high-quality paired expert annotations. This paper proposes Fine-grained Semantic Category Reasoning (FineR) system that leverages the world knowledge of large language models to reason fine-grained category names.},
  address = {Vienna, Austria},
  author = {Mingxuan Liu and Subhankar Roy and Wenjing Li and Zhun Zhong and Nicu Sebe and Elisa Ricci},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024democratizing.pdf:pdf},
  keywords = {Vision-Language Models, Large Language Models, Prompting, Multimodal, Fine-grained Visual Recognition},
  note = {DBLP last modified: 2024-08-14},
  pdf = {https://openreview.net/pdf?id=c7DND1iIgb},
  publisher = {OpenReview.net},
  title = {Democratizing Fine-grained Visual Recognition with Large Language Models},
  url = {https://openreview.net/forum?id=c7DND1iIgb},
  year = {2024}
}

@inproceedings{liu2024hyperhuman,
  abstract = {This paper addresses challenges in generating hyper-realistic human images, proposing a framework that captures correlations between the explicit appearance and latent structure to create coherent human images. The work introduces a Latent Structural Diffusion Model and Structure-Guided Refiner, supported by the HumanVerse dataset with 340M annotated images.},
  address = {Vienna, Austria},
  author = {Xian Liu and Jian Ren and Aliaksandr Siarohin and Ivan Skorokhodov and Yanyu Li and Dahua Lin and Xihui Liu and Ziwei Liu and Sergey Tulyakov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024hyperhuman.pdf:pdf},
  keywords = {Human Image Generation, Latent Structural Diffusion},
  note = {DBLP last modified: 2024-10-30},
  pdf = {https://openreview.net/pdf?id=duyA42HlCK},
  publisher = {OpenReview.net},
  title = {{HyperHuman}: Hyper-Realistic Human Generation with Latent Structural Diffusion},
  url = {https://openreview.net/forum?id=duyA42HlCK},
  year = {2024}
}

@inproceedings{liu2024meaning,
  abstract = {We propose to extract meaning representations from autoregressive language models by considering the distribution of all possible trajectories extending an input text. The method is prompt-free, applicable to pre-trained models, and can model asymmetric relations using likelihood function operations, showing alignment with human annotations and performance on semantic similarity tasks.},
  address = {Vienna, Austria},
  author = {Tian Yu Liu and Matthew Trager and Alessandro Achille and Pramuditha Perera and Luca Zancato and Stefano Soatto},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024meaning.pdf:pdf},
  keywords = {Definitions of meaning, Semantic similarity, Sentence embeddings},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UyGWafcopT},
  publisher = {OpenReview.net},
  title = {Meaning Representations from Trajectories in Autoregressive Models},
  url = {https://openreview.net/forum?id=UyGWafcopT},
  year = {2024}
}

@inproceedings{liu2024neuron,
  abstract = {The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution. We study the OOD problem from a neuron activation view, introducing neuron activation coverage (NAC) which shows that InD and OOD inputs can be separated by neuron behavior, beating 21 previous methods across CIFAR-10, CIFAR-100, and ImageNet-1K benchmarks.},
  address = {Vienna, Austria},
  author = {Yibing Liu and Chris Xing Tian and Haoliang Li and Lei Ma and Shiqi Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024neuron.pdf:pdf},
  keywords = {Out-of-distribution, Generalization, Neuron Activation},
  note = {DBLP last modified: 2024-12-11},
  pdf = {https://openreview.net/pdf?id=SNGXbZtK6Q},
  publisher = {OpenReview.net},
  title = {Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization},
  url = {https://openreview.net/forum?id=SNGXbZtK6Q},
  year = {2024}
}

@inproceedings{li2024fast,
  abstract = {Normalization layers are ubiquitous in deep learning, greatly accelerating optimization. However, they also introduce many unexpected phenomena during training, for example, the Fast Equilibrium conjecture proposed by Li et al. (2020), which states that the scale-invariant normalized network, when trained by SGD with Œ∑ learning rate and Œª weight decay, mixes to an equilibrium in √ï(1/Œ∑Œª) steps, as opposed to classical e^O(Œ∑^-1) mixing time.},
  address = {Vienna, Austria},
  author = {Zhiyuan Li and Yi Wang and Zhiren Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/li2024fast.pdf:pdf},
  keywords = {optimization, fast mixing, fast equilibrium, SDE, SGD, large deviation principle},
  note = {DBLP last modified: 2025-05-26},
  pdf = {https://openreview.net/pdf?id=qgWJkDiI5p},
  publisher = {OpenReview.net},
  title = {Fast Equilibrium of {SGD} in Generic Situations},
  url = {https://openreview.net/forum?id=qgWJkDiI5p},
  year = {2024}
}

@inproceedings{liu2024conversational,
  abstract = {Recent advancements in conversational large language models (LLMs), such as ChatGPT, have demonstrated remarkable promise in various domains, including drug discovery. However, existing works mainly focus on investigating the capabilities of conversational LLMs on chemical reactions and retrosynthesis. While drug editing, a critical task in the drug discovery pipeline, remains largely unexplored. We propose ChatDrug, a framework that utilizes Large Language Models for conversational drug editing with a retrieval and domain feedback module.},
  address = {Vienna, Austria},
  author = {Shengchao Liu and Jiongxiao Wang and Yijin Yang and Chengpeng Wang and Ling Liu and Hongyu Guo and Chaowei Xiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024conversational.pdf:pdf},
  keywords = {Large Language Models, prompt, retrieval, domain feedback, conversation, drug editing},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=yRrPfKyJQ2},
  publisher = {OpenReview.net},
  title = {Conversational Drug Editing Using Retrieval and Domain Feedback},
  url = {https://openreview.net/forum?id=yRrPfKyJQ2},
  year = {2024}
}

@inproceedings{liu2024towards,
  abstract = {We study online reinforcement learning in linear {M}arkov decision processes with adversarial losses and bandit feedback. We introduce two algorithms that achieve improved regret performance compared to existing approaches. The first algorithm, while computationally inefficient, ensures a regret of $\widetilde{O}(\sqrt{K})$, where $K$ is the number of episodes. This is the first result with the optimal $K$ dependence in the considered setting. The second algorithm, based on the policy optimization framework, guarantees a regret of $\widetilde{O}(K^{\frac{3}{4}})$ and is computationally efficient.},
  address = {Vienna, Austria},
  author = {Haolin Liu and Chen-Yu Wei and Julian Zimmert},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024towards.pdf:pdf},
  keywords = {adversarial {MDP}s, policy optimization, bandit feedback},
  month = {5},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=6yv8UHVJn4},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{Towards Optimal Regret in Adversarial Linear MDPs with Bandit Feedback}},
  url = {https://openreview.net/forum?id=6yv8UHVJn4},
  venue = {spotlight},
  year = {2024}
}

@inproceedings{liu2024autodan,
  abstract = {The aligned Large Language Models ({LLM}s) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned {LLM}s. Existing jailbreak techniques suffer from either scalability issues, where attacks heavily rely on manual crafting of prompts, or stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. We introduce {AutoDAN}, a novel jailbreak attack against aligned {LLM}s that can automatically generate stealthy jailbreak prompts by a carefully designed hierarchical genetic algorithm.},
  address = {Vienna, Austria},
  author = {Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024autodan.pdf:pdf},
  keywords = {Large Language Models, Jailbreak Attack, Adversarial Attack},
  month = {5},
  note = {{DBLP} last modified: 2025-06-26},
  pdf = {https://openreview.net/pdf?id=7Jwpw4qKkb},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{AutoDAN}: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models},
  url = {https://openreview.net/forum?id=7Jwpw4qKkb},
  venue = {poster},
  year = {2024}
}

@inproceedings{liu2024solving,
  abstract = {Cooperative multi-agent reinforcement learning ({MARL}) is extensively used for solving complex cooperative tasks, and value decomposition methods are a prevalent approach for this domain. However, these methods have not been successful in addressing both homogeneous and heterogeneous tasks simultaneously which is a crucial aspect for the practical application of cooperative agents. On one hand, value decomposition methods demonstrate superior performance in homogeneous tasks. Nevertheless, they tend to produce agents with similar policies, which is unsuitable for heterogeneous tasks. On the other hand, solutions based on personalized observation or assigned roles are well-suited for heterogeneous tasks. However, they often lead to a trade-off situation where the agent's performance in homogeneous scenarios is negatively affected due to the aggregation of distinct policies.},
  address = {Vienna, Austria},
  author = {Shanqi Liu and Dong Xing and Pengjie Gu and Xinrun Wang and Bo An and Yong Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024solving.pdf:pdf},
  keywords = {Multi-Agent Cooperation, Credit Assignment, Homogeneous and Heterogeneous Cooperative Tasks},
  month = {5},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=hB2hXtxIPH},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Solving Homogeneous and Heterogeneous Cooperative Tasks with Greedy Sequential Execution},
  url = {https://openreview.net/forum?id=hB2hXtxIPH},
  venue = {spotlight},
  year = {2024}
}

@inproceedings{liu2024geneoh,
  abstract = {In this work, we tackle the challenging problem of denoising hand-object interactions ({HOI}). Given an erroneous interaction sequence, the objective is to refine the incorrect hand trajectory to remove interaction artifacts for a perceptually realistic sequence. This challenge involves intricate interaction noise, including unnatural hand poses and incorrect hand-object relations, alongside the necessity for robust generalization to new interactions and diverse noise patterns. We tackle those challenges through a novel approach, {GeneOH Diffusion}, incorporating two key designs: an innovative contact-centric {HOI} representation named {GeneOH} and a new domain-generalizable denoising scheme.},
  address = {Vienna, Austria},
  author = {Xueyi Liu and Li Yi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024geneoh.pdf:pdf},
  keywords = {motion refinement, hand-object interaction, inverse problem, generative prior},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FvK2noilxT},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{GeneOH Diffusion}: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion},
  url = {https://openreview.net/forum?id=FvK2noilxT},
  venue = {poster},
  year = {2024}
}

@inproceedings{liu2024vida,
  abstract = {Since real-world machine systems are running in non-stationary environments, Continual Test-Time Adaptation ({CTTA}) task is proposed to adapt the pre-trained model to continually changing target domains. However, pseudo labels can be noisy and the updated model parameters are unreliable under dynamic data distributions, leading to error accumulation and catastrophic forgetting in the continual adaptation process. To address these challenges, we design homeostatic low-rank and high-rank Visual Domain Adapters ({ViDA}) for {CTTA}, which explicitly handle both domain-specific and domain-shared knowledge. We comprehensively explore the different domain representations of the adapters with trainable high-rank or low-rank embedding spaces. A Homeostatic Knowledge Allotment ({HKA}) strategy is proposed, which adaptively combines different knowledge from each {ViDA}.},
  address = {Vienna, Austria},
  author = {Jiaming Liu and Senqiao Yang and Peidong Jia and Renrui Zhang and Ming Lu and Yandong Guo and Wei Xue and Shanghang Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024vida.pdf:pdf},
  keywords = {Continual test-time adaptation, Visual Adapter},
  month = {5},
  note = {{DBLP} last modified: 2024-10-31},
  pdf = {https://openreview.net/pdf?id=sJ88Wg5Bp5},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {{ViDA}: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation},
  url = {https://openreview.net/forum?id=sJ88Wg5Bp5},
  venue = {poster},
  year = {2024}
}

@inproceedings{liu2024training,
  abstract = {The goal of social alignment for {AI} systems is to make sure these models can conduct themselves appropriately following social values. Unlike humans who establish a consensus on value judgments through social interaction, current language models ({LM}s) are trained to rigidly recite the corpus in social isolation, which causes poor generalization in unfamiliar cases and the lack of robustness under adversarial attacks. In this work, we introduce a new training paradigm that enables {LM}s to learn from simulated social interactions. Compared with existing methods, our method is much more scalable and efficient, and shows superior performance in alignment benchmarks and human evaluation.},
  address = {Vienna, Austria},
  author = {Ruibo Liu and Ruixin Yang and Chenyan Jia and Ge Zhang and Diyi Yang and Soroush Vosoughi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024training.pdf:pdf},
  keywords = {{AI} alignment, {AI} safety, Natural Language Processing},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NddKiWtdUm},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Training Socially Aligned Language Models on Simulated Social Interactions},
  url = {https://openreview.net/forum?id=NddKiWtdUm},
  venue = {poster},
  year = {2024}
}

@inproceedings{liu2024efficient,
  abstract = {Multi-agent reinforcement learning ({MARL}) algorithms have accomplished remarkable breakthroughs in solving large-scale decision-making tasks. Nonetheless, most existing {MARL} algorithms are model-free, limiting sample efficiency and hindering their applicability in more challenging scenarios. In contrast, model-based reinforcement learning ({MBRL}), particularly algorithms integrating planning, such as {MuZero}, has demonstrated superhuman performance with limited data in many tasks. Hence, we aim to boost the sample efficiency of {MARL} by adopting model-based approaches. However, incorporating planning and search methods into multi-agent systems poses significant challenges. The expansive action space of multi-agent systems often necessitates leveraging the nearly-independent property of agents to accelerate learning. To tackle this issue, we propose the {MAZero} algorithm, which combines a centralized model with Monte Carlo Tree Search ({MCTS}) for policy search.},
  address = {Vienna, Austria},
  author = {Qihan Liu and Jianing Ye and Xiaoteng Ma and Jun Yang and Bin Liang and Chongjie Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024efficient.pdf:pdf},
  keywords = {Model-based {RL}, {MARL}, Planning, {MCTS}},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CpnKq3UJwp},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Efficient Multi-agent Reinforcement Learning by Planning},
  url = {https://openreview.net/forum?id=CpnKq3UJwp},
  venue = {poster},
  year = {2024}
}

@inproceedings{liu2024blending,
  abstract = {While reinforcement learning ({RL}) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning ({IL}) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. In this work, we propose a novel framework that blends imitation and reinforcement learning to achieve robust policy improvement. Our approach strategically combines demonstration data and environmental rewards to train more robust policies that can handle variations in oracle quality and environmental conditions.},
  address = {Vienna, Austria},
  author = {Xuefeng Liu and Takuma Yoneda and Rick Stevens and Matthew R. Walter and Yuxin Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024blending.pdf:pdf},
  keywords = {imitation learning, reinforcement learning, multiple experts},
  month = {5},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=eJ0dzPJq1F},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Blending Imitation and Reinforcement Learning for Robust Policy Improvement},
  url = {https://openreview.net/forum?id=eJ0dzPJq1F},
  venue = {spotlight},
  year = {2024}
}

@inproceedings{liu2024symbol,
  abstract = {This work studies the problem of panoptic symbol spotting, which is to spot and parse both countable object instances (windows, doors, tables, etc.) and uncountable stuff (wall, railing, etc.) from computer-aided design ({CAD}) drawings. Existing methods typically involve either rasterizing the vector graphics into images and using image-based methods for symbol spotting, or directly building graphs and using graph neural networks for symbol recognition. In this paper, we take a different approach, which treats graphic primitives as a set of {2D} points that are locally connected and use point cloud segmentation methods to tackle it. Specifically, we utilize a point transformer to extract the primitive features and append a mask2former-like spotting head to predict the final output.},
  address = {Vienna, Austria},
  author = {Wenlong Liu and Tianyu Yang and Yuhan Wang and Qizhi Yu and Lei Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024symbol.pdf:pdf},
  keywords = {{CAD}, Panoptic Symbol Spotting},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=aOnUe8ah7j},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Symbol as Points: Panoptic Symbol Spotting via Point-based Representation},
  url = {https://openreview.net/forum?id=aOnUe8ah7j},
  venue = {poster},
  year = {2024}
}

@inproceedings{liu2024general,
  abstract = {Zeroth-order optimization algorithms are widely used for black-box optimization problems, such as those in machine learning and prompt engineering, where the gradients are approximated using function evaluations. Recently, a generalization result was provided for zeroth-order stochastic gradient descent ({SGD}) algorithms through stability analysis. However, this result was limited to the vanilla 2-point zeroth-order estimate of {Gaussian} distribution used in {SGD} algorithms. To address these limitations, we propose a general proof framework for stability analysis that applies to convex, strongly convex, and non-convex conditions, and yields results for popular zeroth-order optimization algorithms, including {SGD}, {GD}, and {SVRG}, as well as various zeroth-order estimates, such as 1-point and 2-point with different distributions and coordinate estimates.},
  address = {Vienna, Austria},
  author = {Xinyue Liu and Hualin Zhang and Bin Gu and Hong Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024general.pdf:pdf},
  keywords = {Stability Analysis, Zeroth-Order Optimization, Black-Box Learning},
  month = {5},
  note = {{DBLP} last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=AfhNyr73Ma},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {General Stability Analysis for Zeroth-Order Optimization Algorithms},
  url = {https://openreview.net/forum?id=AfhNyr73Ma},
  venue = {poster},
  year = {2024}
}

@inproceedings{liu2024pilot,
  abstract = {Learning an accurate value function for a given policy is a critical step in solving reinforcement learning (RL) problems. So far, however, the convergence speed and sample complexity performances of most existing policy evaluation algorithms remain unsatisfactory, particularly with non-linear function approximation. This challenge motivates us to develop a new path-integrated primal-dual stochastic gradient (PILOT) method, that is able to achieve a fast convergence speed for RL policy evaluation with nonlinear function approximation. To further alleviate the periodic full gradient evaluation requirement, we further propose an enhanced method with an adaptive-batch adjustment called PILOT$^+$. The main advantages of our methods include: (i) PILOT allows the use of constant step sizes and achieves the O(1/K) convergence rate to first-order stationary points of non-convex policy evaluation problems; (ii) PILOT is a generic single-timescale algorithm that unifies policy evaluation in both on-policy and off-policy settings.},
  address = {Vienna, Austria},
  author = {Zhuqing Liu and Xin Zhang and Jia Liu and Zhengyuan Zhu and Songtao Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024pilot.pdf:pdf},
  month = {5},
  note = {Spotlight Paper},
  pdf = {https://openreview.net/pdf?id=OkHHJcMroY},
  publisher = {OpenReview.net},
  title = {{PILOT}: An {$\mathcal{O}(1/K)$}-Convergent Approach for Policy Evaluation with Nonlinear Function Approximation},
  url = {https://openreview.net/forum?id=OkHHJcMroY},
  year = {2024}
}

@inproceedings{liu2024revisiting,
  abstract = {In the past several years, the last-iterate convergence of the Stochastic Gradient Descent (SGD) algorithm has triggered people's interest due to its good performance in practice but lack of theoretical understanding. For Lipschitz convex functions, different works have established the optimal $O(\log(1/\delta)\log T/\sqrt{T})$ or $O(\sqrt{\log(1/\delta)/T})$ high-probability convergence rates for the final iterate, where $T$ is the time horizon and $\delta$ is the failure probability. However, to prove these bounds, all the existing works are either limited to compact domains or require almost surely bounded noises. It is natural to ask whether the last iterate of SGD can still guarantee the optimal convergence rate but without these two restrictive assumptions. In this work, we revisit the last-iterate convergence of stochastic gradient methods and provide the first unified way to prove the convergence rates both in expectation and in high probability to accommodate general domains, composite objectives, non-Euclidean norms, Lipschitz conditions, smoothness, and (strong) convexity simultaneously. Additionally, we extend our analysis to obtain the last-iterate convergence under heavy-tailed noises.},
  address = {Vienna, Austria},
  author = {Zijian Liu and Zhengyuan Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024revisiting.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=xxaEhwC1I4},
  publisher = {OpenReview.net},
  title = {Revisiting the Last-Iterate Convergence of Stochastic Gradient Methods},
  url = {https://openreview.net/forum?id=xxaEhwC1I4},
  year = {2024}
}

@inproceedings{liu2024empirical,
  abstract = {Machine learning algorithms are commonly being deployed in decision-making systems that have a direct impact on human lives. However, if these algorithms are trained solely to minimize training/test errors, they may inadvertently discriminate against individuals based on their sensitive attributes, such as gender, race or age. Recently, algorithms that ensure the fairness are developed in the machine learning community. Fairness criteria are applied by these algorithms to measure the fairness, but they often use the point estimate to assess the fairness and fail to consider the uncertainty of the sample fairness criterion once the algorithms are deployed. We suggest that assessing the fairness should take the uncertainty into account. In this work, we propose a novel approach that employs covariance as a fairness proxy and establishes confidence regions via empirical likelihood to assess the fairness uncertainty in classification tasks.},
  address = {Vienna, Austria},
  author = {Pangpang Liu and Yichuan Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024empirical.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=GACjMj1MS1},
  publisher = {OpenReview.net},
  title = {Empirical Likelihood for Fair Classification},
  url = {https://openreview.net/forum?id=GACjMj1MS1},
  year = {2024}
}

@inproceedings{liu2024meta,
  abstract = {This paper considers the problem of learning the reward function and constraints of an expert from few demonstrations. This problem can be considered as a meta-learning problem where we first learn meta-priors over reward functions and constraints from other distinct but related tasks and then adapt the learned meta-priors to new tasks from only few expert demonstrations. We propose a novel algorithm for meta-learning reward functions and constraints, with theoretical guarantees of reaching $\epsilon$-stationary points and quantifying generalization error to new tasks. Our method addresses the challenge of learning both reward functions and safety constraints in reinforcement learning from limited expert demonstrations through meta-learning approaches.},
  address = {Vienna, Austria},
  author = {Shicheng Liu and Minghui Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024meta.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=bJ3gFiwRgi},
  publisher = {OpenReview.net},
  title = {Meta Inverse Constrained Reinforcement Learning: Convergence Guarantee and Generalization Analysis},
  url = {https://openreview.net/forum?id=bJ3gFiwRgi},
  year = {2024}
}

@inproceedings{liu2024tail,
  abstract = {The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly because of the scarcity of data and the computational challenges associated with training or fine-tuning these large models for such applications. We propose TAIL (Task-specific Adapters for Imitation Learning), a method of efficient adaptation for large pretrained decision-making models. Our approach uses Low-Rank Adaptation (LoRA) with task-specific adapters that outperform traditional fine-tuning while using only 1\% of additional parameters and avoiding catastrophic forgetting. TAIL enables efficient transfer learning for robotics and control tasks while maintaining the benefits of large-scale pretraining.},
  address = {Vienna, Austria},
  author = {Zuxin Liu and Jesse Zhang and Kavosh Asadi and Yao Liu and Ding Zhao and Shoham Sabach and Rasool Fakoor},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024tail.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=RRayv1ZPN3},
  publisher = {OpenReview.net},
  title = {{TAIL}: Task-specific Adapters for Imitation Learning with Large Pretrained Models},
  url = {https://openreview.net/forum?id=RRayv1ZPN3},
  year = {2024}
}

@inproceedings{liu2024maximum,
  abstract = {Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample complexity, training instability, and the risk of converging to a suboptimal Nash Equilibrium. In this work, we propose a unified framework for learning stochastic policies in cooperative MARL that addresses these challenges through maximum entropy heterogeneous-agent reinforcement learning. Our approach combines heterogeneous-agent soft actor-critic with maximum entropy heterogeneous-agent mirror learning to achieve more stable training and better exploration in multi-agent environments.},
  address = {Vienna, Austria},
  author = {Jiarong Liu and Yifan Zhong and Siyi Hu and Haobo Fu and Qiang Fu and Xiaojun Chang and Yaodong Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024maximum.pdf:pdf},
  month = {5},
  note = {Spotlight Paper},
  pdf = {https://openreview.net/pdf?id=tmqOhBC4a5},
  publisher = {OpenReview.net},
  title = {Maximum Entropy Heterogeneous-Agent Reinforcement Learning},
  url = {https://openreview.net/forum?id=tmqOhBC4a5},
  year = {2024}
}

@inproceedings{liu2024exploring,
  abstract = {Masked autoencoders have become popular training paradigms for self-supervised visual representation learning. These models randomly mask a portion of the input and reconstruct the masked portion according to assigned target representations. However, the choice of target representations significantly impacts learning quality. In this work, we argue that a careful choice of the target representation is unnecessary for learning good visual representation. We propose a multi-stage masked distillation pipeline using a randomly initialized model as the teacher, demonstrating that effective self-supervised learning can be achieved without sophisticated target representation design.},
  address = {Vienna, Austria},
  author = {Xingbin Liu and Jinghao Zhou and Tao Kong and Xianming Lin and Rongrong Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024exploring.pdf:pdf},
  month = {5},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=xmQMz9OPF5},
  publisher = {OpenReview.net},
  title = {Exploring Target Representations for Masked Autoencoders},
  url = {https://openreview.net/forum?id=xmQMz9OPF5},
  year = {2024}
}

@inproceedings{liu2024matcher,
  abstract = {Powered by large-scale pre-training, vision foundation models exhibit significant potential in open-world image understanding. However, unlike large language models that excel at directly tackling various language tasks, vision foundation models require a task-specific model structure followed by fine-tuning on specific tasks. In this work, we present Matcher, a novel perception paradigm that introduces a training-free generalist model for vision foundation models. Matcher achieves 52.7\% mIoU on COCO-20^i with one example and 33.0\% mIoU on LVIS-92^i for one-shot semantic segmentation, demonstrating the effectiveness of all-purpose feature matching for segment anything tasks.},
  address = {Vienna, Austria},
  author = {Yang Liu and Muzhi Zhu and Hengtao Li and Hao Chen and Xinlong Wang and Chunhua Shen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024matcher.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=yzRXdhk2he},
  publisher = {OpenReview.net},
  title = {Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching},
  url = {https://openreview.net/forum?id=yzRXdhk2he},
  year = {2024}
}

@inproceedings{liu2024instaflow,
  abstract = {Diffusion models have revolutionized text-to-image generation, but their multi-step sampling process is slow. We introduce InstaFlow, a novel approach using Rectified Flow to create a one-step text-to-image generator with high-quality results. Our method achieves FID of 23.3 on MS COCO 2017-5k and improves to 22.4 with 1.7B parameter network. InstaFlow generates images in 0.09 seconds with training cost of only 199 A100 GPU days, making high-quality diffusion-based generation significantly more efficient.},
  address = {Vienna, Austria},
  author = {Xingchao Liu and Xiwen Zhang and Jianzhu Ma and Jian Peng and Qiang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024instaflow.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=1k4yZbbDqX},
  publisher = {OpenReview.net},
  title = {{InstaFlow}: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation},
  url = {https://openreview.net/forum?id=1k4yZbbDqX},
  year = {2024}
}

@inproceedings{liu2024explaining,
  abstract = {Explaining multivariate time series is a compound challenge, as it requires identifying important locations in the time series and matching complex temporal patterns. Although previous saliency-based methods addressed the challenges, their perturbation may not alleviate the distribution shift issue, which is inevitable especially in heterogeneous samples. We present ContraLSP, a locally sparse model that introduces counterfactual samples to build uninformative perturbations but keeps distribution using contrastive learning. Furthermore, we incorporate sample-specific sparse gates to generate more binary-skewed and smooth masks, which easily integrate temporal trends and select the salient features parsimoniously. Empirical studies on both synthetic and real-world datasets show that ContraLSP outperforms state-of-the-art models, demonstrating substantial improvement in explanation quality for time series data.},
  address = {Vienna, Austria},
  author = {Zichuan Liu and Yingying Zhang and Tianchun Wang and Zefan Wang and Dongsheng Luo and Mengnan Du and Min Wu and Yi Wang and Chunlin Chen and Lunting Fan and Qingsong Wen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/liu2024explaining.pdf:pdf},
  month = {5},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=qDdSRaOiyb},
  publisher = {OpenReview.net},
  title = {Explaining Time Series via Contrastive and Locally Sparse Perturbations},
  url = {https://openreview.net/forum?id=qDdSRaOiyb},
  year = {2024}
}

@inproceedings{liu2024vbhgnn,
  author = {Chenyu Liu and Xinliang Zhou and Zhengri Zhu and Liming Zhai and Ziyu Jia and Yang Liu 0003},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {VBH-GNN: Variational Bayesian Heterogeneous Graph Neural Networks for Cross-subject Emotion Recognition},
  url = {https://openreview.net/forum?id=EArTDUmILF},
  year = {2024}
}

@inproceedings{livernoche2024diffusion,
  author = {Victor Livernoche and Vineet Jain and Yashar Hezaveh and Siamak Ravanbakhsh},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {On Diffusion Modeling for Anomaly Detection},
  url = {https://openreview.net/forum?id=lR3rk7ysXz},
  year = {2024}
}

@inproceedings{lo2024learning,
  author = {Yat Long Lo and Biswa Sengupta and Jakob Nicolaus Foerster and Michael Noukhovitch},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Learning Multi-Agent Communication with Contrastive Learning},
  url = {https://openreview.net/forum?id=vZZ4hhniJU},
  year = {2024}
}

@inproceedings{loconte2024subtractive,
  author = {Lorenzo Loconte and Aleksanteri M. Sladek and Stefan Mengel and Martin Trapp 0001 and Arno Solin and Nicolas Gillis and Antonio Vergari},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Subtractive Mixture Models via Squaring: Representation and Learning},
  url = {https://openreview.net/forum?id=xIHi5nxu9P},
  year = {2024}
}

@inproceedings{long2024hybrid,
  author = {Junfeng Long and Zirui Wang and Quanyi Li and Liu Cao and Jiawei Gao 0004 and Jiangmiao Pang},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response},
  url = {https://openreview.net/forum?id=93LoCyww8o},
  year = {2024}
}

@inproceedings{loo2024understanding,
  author = {Noel Loo and Ramin M. Hasani and Mathias Lechner and Alexander Amini and Daniela Rus},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Understanding Reconstruction Attacks with the Neural Tangent Kernel and Dataset Distillation},
  url = {https://openreview.net/forum?id=VoLDkQ6yR3},
  year = {2024}
}

@inproceedings{losch2024adversarial,
  author = {Max Maria Losch and Mohamed Omran and David Stutz and Mario Fritz and Bernt Schiele},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-26},
  publisher = {OpenReview.net},
  title = {On Adversarial Training without Perturbing all Examples},
  url = {https://openreview.net/forum?id=pE6gWrASQm},
  year = {2024}
}

@inproceedings{lou2024muffin,
  author = {Renze Lou and Kai Zhang 0033 and Jian Xie and Yuxuan Sun 0002 and Janice Ahn and Hanzi Xu and Yu Su 0001 and Wenpeng Yin 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-06-02},
  publisher = {OpenReview.net},
  title = {MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction Following},
  url = {https://openreview.net/forum?id=1vrS1zwekw},
  year = {2024}
}

@inproceedings{louiset2024separating,
  author = {Robin Louiset and Edouard Duchesnay and Antoine Grigis and Pietro Gori},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Separating common from salient patterns with Contrastive Representation Learning},
  url = {https://openreview.net/forum?id=30N3bNAiw3},
  year = {2024}
}

@inproceedings{louizos2024mutual,
  author = {Christos Louizos and Matthias Reisser and Denis Korzhenkov},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {A Mutual Information Perspective on Federated Contrastive Learning},
  url = {https://openreview.net/forum?id=JrmPG9ufKg},
  year = {2024}
}

@inproceedings{lu2024adaptive,
  author = {Zhou Lu and Qiuyi Zhang 0001 and Xinyi Chen 0001 and Fred Zhang and David P. Woodruff and Elad Hazan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-10-07},
  publisher = {OpenReview.net},
  title = {Adaptive Regret for Bandits Made Possible: Two Queries Suffice},
  url = {https://openreview.net/forum?id=AY9KyTGcnk},
  year = {2024}
}

@inproceedings{lu2024mathvista,
  author = {Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu 0010 and Chunyuan Li and Hannaneh Hajishirzi and Hao Cheng 0002 and Kai-Wei Chang and Michel Galley and Jianfeng Gao 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  url = {https://openreview.net/forum?id=KUNzEQMWU7},
  year = {2024}
}

@inproceedings{lu2024uniadapter,
  author = {Haoyu Lu and Yuqi Huo and Guoxing Yang and Zhiwu Lu 0001 and Wei Zhan and Masayoshi Tomizuka and Mingyu Ding},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling},
  url = {https://openreview.net/forum?id=f5H8WGLQm5},
  year = {2024}
}

@inproceedings{lu2024arm,
  author = {Jiecheng Lu and Xu Han and Shihao Yang 0002},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-02-06},
  publisher = {OpenReview.net},
  title = {ARM: Refining Multivariate Forecasting with Adaptive Temporal-Contextual Learning},
  url = {https://openreview.net/forum?id=JWpwDdVbaM},
  year = {2024}
}

@inproceedings{lu2024extensible,
  author = {Yifan Lu and Yue Hu and Yiqi Zhong and Dequan Wang and Yanfeng Wang 0001 and Siheng Chen},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-11-14},
  publisher = {OpenReview.net},
  title = {An Extensible Framework for Open Heterogeneous Collaborative Perception},
  url = {https://openreview.net/forum?id=KkrDUGIASk},
  year = {2024}
}

@inproceedings{lu2024m3c,
  author = {Jiaxin Lu and Zetian Jiang and Tianzhe Wang and Junchi Yan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {M3C: A Framework towards Convergent, Flexible, and Unsupervised Learning of Mixture Graph Matching and Clustering},
  url = {https://openreview.net/forum?id=AXC9KydyZq},
  year = {2024}
}

@inproceedings{lu2024goat,
  author = {Shengyao Lu and Keith G. Mills and Jiao He and Bang Liu and Di Niu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {GOAt: Explaining Graph Neural Networks via Graph Output Attribution},
  url = {https://openreview.net/forum?id=2Q8TZWAHv4},
  year = {2024}
}

@inproceedings{lu2024debiasing,
  author = {Shenyu Lu and Yipei Wang and Xiaoqian Wang 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Debiasing Attention Mechanism in Transformer without Demographics},
  url = {https://openreview.net/forum?id=jLIUfrAcMQ},
  year = {2024}
}

@inproceedings{lu2024benign,
  author = {Miao Lu and Beining Wu and Xiaodong Yang and Difan Zou},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Benign Oscillation of Stochastic Gradient Descent with Large Learning Rate},
  url = {https://openreview.net/forum?id=wYmvN3sQpG},
  year = {2024}
}

@inproceedings{lu2024instag,
  author = {Keming Lu and Hongyi Yuan and Zheng Yuan 0002 and Runji Lin and Junyang Lin and Chuanqi Tan and Chang Zhou and Jingren Zhou 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-03-19},
  publisher = {OpenReview.net},
  title = {\#InsTag: Instruction Tagging for Analyzing Supervised Fine-tuning of Large Language Models},
  url = {https://openreview.net/forum?id=pszewhybU9},
  year = {2024}
}

@inproceedings{lu2024vdt,
  author = {Haoyu Lu and Guoxing Yang and Nanyi Fei and Yuqi Huo and Zhiwu Lu 0001 and Ping Luo 0002 and Mingyu Ding},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {VDT: General-purpose Video Diffusion Transformers via Mask Modeling},
  url = {https://openreview.net/forum?id=Un0rgm9f04},
  year = {2024}
}

@inproceedings{lu2024towards,
  author = {Feng Lu and Lijun Zhang and Xiangyuan Lan and Shuting Dong and Yaowei Wang 0001 and Chun Yuan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-18},
  publisher = {OpenReview.net},
  title = {Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition},
  url = {https://openreview.net/forum?id=TVg6hlfsKa},
  year = {2024}
}

@inproceedings{lu2024str2str,
  author = {Jiarui Lu and Bozitao Zhong and Zuobai Zhang and Jian Tang 0005},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Str2Str: A Score-based Framework for Zero-shot Protein Conformation Sampling},
  url = {https://openreview.net/forum?id=C4BikKsgmK},
  year = {2024}
}

@inproceedings{luca2024local,
  author = {Artur Back de Luca and Kimon Fountoulakis and Shenghao Yang 0002},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Local Graph Clustering with Noisy Labels},
  url = {https://openreview.net/forum?id=89A5c6enfc},
  year = {2024}
}

@inproceedings{lukas2024leveraging,
  author = {Nils Lukas and Abdulrahman Diaa and Lucas Fenaux and Florian Kerschbaum},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Leveraging Optimization for Adaptive Attacks on Image Watermarks},
  url = {https://openreview.net/forum?id=O9PArxKLe1},
  year = {2024}
}

@inproceedings{luo2024continuous,
  author = {Xihaier Luo and Wei Xu 0020 and Balu Nadiga and Yihui Ren 0001 and Shinjae Yoo},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Continuous Field Reconstruction from Sparse Observations with Implicit Neural Networks},
  url = {https://openreview.net/forum?id=kuTZMZdCPZ},
  year = {2024}
}

@inproceedings{luo2024dv3dlane,
  author = {Yueru Luo and Shuguang Cui and Zhen Li 0026},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {DV-3DLane: End-to-end Multi-modal 3D Lane Detection with Dual-view Representation},
  url = {https://openreview.net/forum?id=l1U6sEgYkb},
  year = {2024}
}

@inproceedings{luo2024enabling,
  author = {Shengjie Luo and Tianlang Chen and Aditi S. Krishnapriyan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Enabling Efficient Equivariant Operations in the Fourier Basis via Gaunt Tensor Products},
  url = {https://openreview.net/forum?id=mhyQXJ6JsK},
  year = {2024}
}

@inproceedings{luo2024rlif,
  author = {Jianlan Luo and Perry Dong and Yuexiang Zhai and Yi Ma 0001 and Sergey Levine},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {RLIF: Interactive Imitation Learning as Reinforcement Learning},
  url = {https://openreview.net/forum?id=oLLZhbBSOU},
  year = {2024}
}

@inproceedings{luo2024controlling,
  author = {Ziwei Luo and Fredrik K. Gustafsson and Zheng Zhao 0004 and Jens Sjlund and Thomas B. Schn},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Controlling Vision-Language Models for Multi-Task Image Restoration},
  url = {https://openreview.net/forum?id=t3vnnLeajU},
  year = {2024}
}

@inproceedings{luo2024image,
  author = {Haochen Luo and Jindong Gu and Fengyuan Liu and Philip Torr 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {An Image Is Worth 1000 Lies: Transferability of Adversarial Images across Prompts on Vision-Language Models},
  url = {https://openreview.net/forum?id=nc5GgFAvtk},
  year = {2024}
}

@inproceedings{luo2024brainscuba,
  author = {Andrew F. Luo and Margaret M. Henderson and Michael J. Tarr and Leila Wehbe},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity},
  url = {https://openreview.net/forum?id=mQYHXUUTkU},
  year = {2024}
}

@inproceedings{luo2024reasoning,
  author = {Linhao Luo and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-09-24},
  publisher = {OpenReview.net},
  title = {Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning},
  url = {https://openreview.net/forum?id=ZGNWW7xZ6Q},
  year = {2024}
}

@inproceedings{luo2024learning,
  author = {Tianze Luo and Zhanfeng Mo and Sinno Jialin Pan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Learning Adaptive Multiresolution Transforms via Meta-Framelet-based Graph Convolutional Network},
  url = {https://openreview.net/forum?id=5RielfrDkP},
  year = {2024}
}

@inproceedings{2024moderntcn,
  author = {Donghao Luo 0002 and Xue Wang},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-05-22},
  publisher = {OpenReview.net},
  title = {ModernTCN: A Modern Pure Convolution Structure for General Time Series Analysis},
  url = {https://openreview.net/forum?id=vpJMJerXHU},
  year = {2024}
}

@inproceedings{luo2024wizardcoder,
  author = {Ziyang Luo and Can Xu and Pu Zhao 0004 and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma 0004 and Qingwei Lin and Daxin Jiang},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {WizardCoder: Empowering Code Large Language Models with Evol-Instruct},
  url = {https://openreview.net/forum?id=UnUwSIgK5W},
  year = {2024}
}

@inproceedings{luo2024rewardconsistent,
  author = {Fan-Ming Luo and Tian Xu 0003 and Xingchen Cao and Yang Yu 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-12-14},
  publisher = {OpenReview.net},
  title = {Reward-Consistent Dynamics Models are Strongly Generalizable for Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=GSBHKiw19c},
  year = {2024}
}

@inproceedings{luong2024revisiting,
  author = {Manh Luong and Khai Nguyen and Nhat Ho and Gholamreza Haffari and Dinh Phung 0001 and Lizhen Qu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-09-24},
  publisher = {OpenReview.net},
  title = {Revisiting Deep Audio-Text Retrieval Through the Lens of Transportation},
  url = {https://openreview.net/forum?id=l60EM8md3t},
  year = {2024}
}

@inproceedings{lupu2024behaviour,
  author = {Andrei Lupu and Chris Lu 0001 and Jarek Liesen and Robert Tjarko Lange and Jakob Nicolaus Foerster},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Behaviour Distillation},
  url = {https://openreview.net/forum?id=qup9xD8mW4},
  year = {2024}
}

@inproceedings{lutati2024separate,
  author = {Shahar Lutati and Eliya Nachmani and Lior Wolf},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Separate and Diffuse: Using a Pretrained Diffusion Model for Better Source Separation},
  url = {https://openreview.net/forum?id=UXALv0lJZS},
  year = {2024}
}

@inproceedings{lyo2024complex,
  author = {Benjamin Lyo and Cristina Savin},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-03-03},
  publisher = {OpenReview.net},
  title = {Complex priors and flexible inference in recurrent circuits with dendritic nonlinearities},
  url = {https://openreview.net/forum?id=S5aUhpuyap},
  year = {2024}
}

@inproceedings{lyu2024dichotomy,
  author = {Kaifeng Lyu and Jikai Jin and Zhiyuan Li 0005 and Simon Shaolei Du and Jason D. Lee and Wei Hu 0014},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking},
  url = {https://openreview.net/forum?id=XsHqr9dEGH},
  year = {2024}
}

@inproceedings{lyu2024seabo,
  author = {Jiafei Lyu and Xiaoteng Ma and Le Wan and Runze Liu 0002 and Li Xiu 0001 and Zongqing Lu 0002},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-04-09},
  publisher = {OpenReview.net},
  title = {SEABO: A Simple Search-Based Method for Offline Imitation Learning},
  url = {https://openreview.net/forum?id=MNyOI3C7YB},
  year = {2024}
}

@inproceedings{ma2024solving,
  author = {Yiyang Ma and Huan Yang 0005 and Wenhan Yang and Jianlong Fu and Jiaying Liu 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution},
  url = {https://openreview.net/forum?id=BtT6o5tfHu},
  year = {2024}
}

@inproceedings{2024efficient,
  author = {Xu Ma 0005 and Xiyang Dai and Jianwei Yang and Bin Xiao 0004 and Yinpeng Chen and Yun Fu 0001 and Lu Yuan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-08},
  publisher = {OpenReview.net},
  title = {Efficient Modulation for Vision Networks},
  url = {https://openreview.net/forum?id=ip5LHJs6QX},
  year = {2024}
}

@inproceedings{ma2024elucidating,
  author = {Jiajun Ma and Tianyang Hu and Wenjia Wang and Jiacheng Sun},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Elucidating the design space of classifier-guided diffusion generation},
  url = {https://openreview.net/forum?id=9DXXMXnIGm},
  year = {2024}
}

@inproceedings{ma2024revisiting,
  author = {Guozheng Ma and Lu Li and Sen Zhang 0006 and Zixuan Liu 0002 and Zhen Wang 0030 and Yixin Chen 0001 and Li Shen 0008 and Xueqian Wang 0001 and Dacheng Tao},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-05-14},
  publisher = {OpenReview.net},
  title = {Revisiting Plasticity in Visual Reinforcement Learning: Data, Modules and Training Stages},
  url = {https://openreview.net/forum?id=0aR1s9YxoL},
  year = {2024}
}

@inproceedings{ma2024which,
  author = {Yingwei Ma and Yue Liu and Yue Yu 0001 and Yuanliang Zhang and Yu Jiang 0001 and Changjian Wang and Shanshan Li 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {At Which Training Stage Does Code Data Help LLMs Reasoning?},
  url = {https://openreview.net/forum?id=KIPJKST4gw},
  year = {2024}
}

@inproceedings{ma2024eureka,
  author = {Yecheng Jason Ma and William Liang and Guanzhi Wang and De-An Huang and Osbert Bastani and Dinesh Jayaraman and Yuke Zhu and Linxi Fan and Anima Anandkumar},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Eureka: Human-Level Reward Design via Coding Large Language Models},
  url = {https://openreview.net/forum?id=IEduRUO55F},
  year = {2024}
}

@inproceedings{ma2024generating,
  author = {Wufei Ma and Qihao Liu and Jiahao Wang 0001 and Angtian Wang and Xiaoding Yuan and Yi Zhang 0099 and Zihao Xiao 0001 and Guofeng Zhang 0020 and Beijia Lu and Ruxiao Duan and Yongrui Qi and Adam Kortylewski and Yaoyao Liu 0001 and Alan L. Yuille},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-11-18},
  publisher = {OpenReview.net},
  title = {Generating Images with 3D Annotations Using Diffusion Models},
  url = {https://openreview.net/forum?id=XlkN11Xj6J},
  year = {2024}
}

@inproceedings{ma2024affinequant,
  author = {Yuexiao Ma and Huixia Li and Xiawu Zheng and Feng Ling and Xuefeng Xiao 0001 and Rui Wang 0089 and Shilei Wen and Fei Chao 0001 and Rongrong Ji},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-04-17},
  publisher = {OpenReview.net},
  title = {AffineQuant: Affine Transformation Quantization for Large Language Models},
  url = {https://openreview.net/forum?id=of2rhALq8l},
  year = {2024}
}

@inproceedings{ma2024scaling,
  author = {Chenxiang Ma and Jibin Wu and Chenyang Si and Kay Chen Tan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-10-22},
  publisher = {OpenReview.net},
  title = {Scaling Supervised Local Learning with Augmented Auxiliary Networks},
  url = {https://openreview.net/forum?id=Qbf1hy8b7m},
  year = {2024}
}

@inproceedings{maggiora2024conditional,
  author = {Gabriel della Maggiora and Luis Alberto Croquevielle and Nikita Deshpande and Harry Horsley and Thomas Heinis and Artur Yakimovich},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Conditional Variational Diffusion Models},
  url = {https://openreview.net/forum?id=YOKnEkIuoi},
  year = {2024}
}

@inproceedings{maggs2024simplicial,
  author = {Kelly Maggs and Celia Hacker and Bastian Rieck},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Simplicial Representation Learning with Neural k-Forms},
  url = {https://openreview.net/forum?id=Djw0XhjHZb},
  year = {2024}
}

@inproceedings{magistri2024elastic,
  author = {Simone Magistri and Tomaso Trinci and Albin Soutif-Cormerais and Joost van de Weijer 0001 and Andrew D. Bagdanov},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Elastic Feature Consolidation For Cold Start Exemplar-Free Incremental Learning},
  url = {https://openreview.net/forum?id=7D9X2cFnt1},
  year = {2024}
}

@inproceedings{mahajan2024empirical,
  author = {Divyat Mahajan and Ioannis Mitliagkas and Brady Neal and Vasilis Syrgkanis},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation},
  url = {https://openreview.net/forum?id=yuy6cGt3KL},
  year = {2024}
}

@inproceedings{mahankali2024one,
  author = {Arvind V. Mahankali and Tatsunori Hashimoto and Tengyu Ma 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention},
  url = {https://openreview.net/forum?id=8p3fu56lKc},
  year = {2024}
}

@inproceedings{maharana2024pruning,
  author = {Adyasha Maharana and Prateek Yadav and Mohit Bansal},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {D2 Pruning: Message Passing for Balancing Diversity \& Difficulty in Data Pruning},
  url = {https://openreview.net/forum?id=thbtoAkCe9},
  year = {2024}
}

@inproceedings{mahdavi2024memorization,
  author = {Sadegh Mahdavi and Renjie Liao and Christos Thrampoulidis},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Memorization Capacity of Multi-Head Attention in Transformers},
  url = {https://openreview.net/forum?id=MrR3rMxqqv},
  year = {2024}
}

@inproceedings{mahmud2024weaklysupervised,
  author = {Tanvir Mahmud and Saeed Amizadeh and Kazuhito Koishida and Diana Marculescu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Weakly-supervised Audio Separation via Bi-modal Semantic Similarity},
  url = {https://openreview.net/forum?id=4N97bz1sP6},
  year = {2024}
}

@inproceedings{maini2024tmars,
  author = {Pratyush Maini and Sachin Goyal and Zachary Chase Lipton and J. Zico Kolter and Aditi Raghunathan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {T-MARS: Improving Visual Representations by Circumventing Text Feature Learning},
  url = {https://openreview.net/forum?id=ViPtjIVzUw},
  year = {2024}
}

@inproceedings{maity2024investigation,
  author = {Subha Maity and Mayank Agarwal and Mikhail Yurochkin and Yuekai Sun},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {An Investigation of Representation and Allocation Harms in Contrastive Learning},
  url = {https://openreview.net/forum?id=q4SiDyYQbo},
  year = {2024}
}

@inproceedings{makelov2024this,
  author = {Aleksandar Makelov and Georg Lange and Atticus Geiger and Neel Nanda},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching},
  url = {https://openreview.net/forum?id=Ebt7JgMHv1},
  year = {2024}
}

@inproceedings{mall2024remote,
  author = {Utkarsh Mall and Cheng Perng Phoo and Meilin Kelsey Liu and Carl Vondrick and Bharath Hariharan and Kavita Bala},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment},
  url = {https://openreview.net/forum?id=w9tc699w3Z},
  year = {2024}
}

@inproceedings{mangoubi2024faster,
  author = {Oren Mangoubi and Nisheeth K. Vishnoi},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers},
  url = {https://openreview.net/forum?id=v63GWletn8},
  year = {2024}
}

@inproceedings{manor2024posterior,
  author = {Hila Manor and Tomer Michaeli},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {On the Posterior Distribution in Denoising: Application to Uncertainty Quantification},
  url = {https://openreview.net/forum?id=adSGeugiuj},
  year = {2024}
}

@inproceedings{mansouri2024object,
  author = {Amin Mansouri and Jason S. Hartford and Yan Zhang and Yoshua Bengio},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Object centric architectures enable efficient causal representation learning},
  url = {https://openreview.net/forum?id=r9FsiXZxZt},
  year = {2024}
}

@inproceedings{manvi2024geollm,
  author = {Rohin Manvi and Samar Khanna and Gengchen Mai and Marshall Burke and David B. Lobell and Stefano Ermon},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {GeoLLM: Extracting Geospatial Knowledge from Large Language Models},
  url = {https://openreview.net/forum?id=TqL2xBwXP3},
  year = {2024}
}

@inproceedings{mao2024iceformer,
  author = {Yuzhen Mao and Martin Ester and Ke Li 0011},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-11-18},
  publisher = {OpenReview.net},
  title = {IceFormer: Accelerated Inference with Long-Sequence Transformers on CPUs},
  url = {https://openreview.net/forum?id=6RR3wU4mSZ},
  year = {2024}
}

@inproceedings{mao2024revisiting,
  author = {Haitao Mao and Juanhui Li and Harry Shomer and Bingheng Li and Wenqi Fan and Yao Ma 0001 and Tong Zhao 0003 and Neil Shah and Jiliang Tang},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Revisiting Link Prediction: a data perspective},
  url = {https://openreview.net/forum?id=8Ur2xmuw7w},
  year = {2024}
}

@inproceedings{mao2024understanding,
  author = {Yuhao Mao and Mark Niklas Mller and Marc Fischer 0002 and Martin T. Vechev},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Understanding Certified Training with Interval Bound Propagation},
  url = {https://openreview.net/forum?id=h05eQniJsQ},
  year = {2024}
}

@inproceedings{mao2024raidar,
  author = {Chengzhi Mao and Carl Vondrick and Hao Wang 0014 and Junfeng Yang},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Raidar: geneRative AI Detection viA Rewriting},
  url = {https://openreview.net/forum?id=bQWE2UqXmf},
  year = {2024}
}

@inproceedings{mao2024stylized,
  author = {Yihuan Mao and Chengjie Wu and Xi Chen and Hao Hu 0006 and Ji Jiang and Tianze Zhou and Tangjie Lv and Changjie Fan and Zhipeng Hu and Yi Wu 0013 and Yujing Hu and Chongjie Zhang},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Stylized Offline Reinforcement Learning: Extracting Diverse High-Quality Behaviors from Heterogeneous Datasets},
  url = {https://openreview.net/forum?id=rnHNDihrIT},
  year = {2024}
}

@inproceedings{mao2024odice,
  author = {Liyuan Mao and Haoran Xu 0003 and Weinan Zhang 0001 and Xianyuan Zhan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-02-10},
  publisher = {OpenReview.net},
  title = {ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update},
  url = {https://openreview.net/forum?id=L8UNn7Llt4},
  year = {2024}
}

@inproceedings{mao2024role,
  author = {Chenjie Mao and Qiaosheng Zhang and Zhen Wang 0004 and Xuelong Li 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-02},
  publisher = {OpenReview.net},
  title = {On the Role of General Function Approximation in Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=JSS9rKHySk},
  year = {2024}
}

@inproceedings{mao2024novo,
  author = {Weian Mao and Muzhi Zhu and Zheng Sun and Shuaike Shen and Lin Yuanbo Wu and Hao Chen 0041 and Chunhua Shen},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {De novo Protein Design Using Geometric Vector Field Networks},
  url = {https://openreview.net/forum?id=9UIGyJJpay},
  year = {2024}
}

@inproceedings{mardani2024variational,
  author = {Morteza Mardani and Jiaming Song and Jan Kautz and Arash Vahdat},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {A Variational Perspective on Solving Inverse Problems with Diffusion Models},
  url = {https://openreview.net/forum?id=1YO4EE3SPB},
  year = {2024}
}

@inproceedings{mariani2024multisource,
  author = {Giorgio Mariani and Irene Tallini and Emilian Postolache and Michele Mancusi and Luca Cosmo and Emanuele Rodol},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Multi-Source Diffusion Models for Simultaneous Music Generation and Separation},
  url = {https://openreview.net/forum?id=h922Qhkmx1},
  year = {2024}
}

@inproceedings{marin2024bend,
  author = {Frederikke Isa Marin and Felix Teufel and Marc Horlacher and Dennis Madsen and Dennis Pultz and Ole Winther and Wouter Boomsma},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks},
  url = {https://openreview.net/forum?id=uKB4cFNQFg},
  year = {2024}
}

@inproceedings{marion2024implicit,
  author = {Pierre Marion and Yu-Han Wu and Michael Eli Sander and Grard Biau},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Implicit regularization of deep residual networks towards neural ODEs},
  url = {https://openreview.net/forum?id=AbXGwqb5Ht},
  year = {2024}
}

@inproceedings{martin2024lcot,
  author = {Rocio Diaz Martin and Ivan Vladimir Medri and Yikun Bai and Xinran Liu and Kangbai Yan and Gustavo K. Rohde and Soheil Kolouri},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {LCOT: Linear Circular Optimal Transport},
  url = {https://openreview.net/forum?id=49z97Y9lMq},
  year = {2024}
}

@inproceedings{martineztaboada2024counterfactual,
  author = {Diego Martinez-Taboada and Edward Kennedy},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Counterfactual Density Estimation using Kernel Stein Discrepancies},
  url = {https://openreview.net/forum?id=wZXlEFO3tZ},
  year = {2024}
}

@inproceedings{marton2024grande,
  author = {Sascha Marton and Stefan Ldtke and Christian Bartelt and Heiner Stuckenschmidt},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {GRANDE: Gradient-Based Decision Tree Ensembles for Tabular Data},
  url = {https://openreview.net/forum?id=XEFWBxi075},
  year = {2024}
}

@inproceedings{masonwilliams2024what,
  author = {Gabryel Mason-Williams and Fredrik Dahlqvist},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {What Makes a Good Prune? Maximal Unstructured Pruning for Maximal Cosine Similarity},
  url = {https://openreview.net/forum?id=jsvvPVVzwf},
  year = {2024}
}

@inproceedings{massidda2024constraintfree,
  author = {Riccardo Massidda and Francesco Landolfi and Martina Cinquini and Davide Bacciu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Constraint-Free Structure Learning with Smooth Acyclic Orientations},
  url = {https://openreview.net/forum?id=KWO8LSUC5W},
  year = {2024}
}

@inproceedings{mavrothalassitis2024efficient,
  author = {Ioannis Mavrothalassitis and Stratis Skoulakis and Leello Tadesse Dadi and Volkan Cevher},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Efficient Continual Finite-Sum Minimization},
  url = {https://openreview.net/forum?id=RR70yWYenC},
  year = {2024}
}

@inproceedings{mayilvahanan2024does,
  author = {Prasanna Mayilvahanan and Thaddus Wiedemer and Evgenia Rusak and Matthias Bethge and Wieland Brendel},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Does CLIP's generalization performance mainly stem from high train-test similarity?},
  url = {https://openreview.net/forum?id=tnBaiidobu},
  year = {2024}
}

@inproceedings{mcaleer2024toward,
  author = {Stephen Marcus McAleer and JB Lanier and Kevin A. Wang and Pierre Baldi and Tuomas Sandholm and Roy Fox},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Toward Optimal Policy Population Growth in Two-Player Zero-Sum Games},
  url = {https://openreview.net/forum?id=J2TZgj3Tac},
  year = {2024}
}

@inproceedings{mediratta2024generalization,
  author = {Ishita Mediratta and Qingfei You and Minqi Jiang and Roberta Raileanu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {The Generalization Gap in Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=3w6xuXDOdY},
  year = {2024}
}

@inproceedings{mehta2024distributionally,
  author = {Ronak Mehta and Vincent Roulet and Krishna Pillutla and Zad Harchaoui},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Distributionally Robust Optimization with Bias and Variance Reduction},
  url = {https://openreview.net/forum?id=TTrzgEZt9s},
  year = {2024}
}

@inproceedings{mei2024srl,
  author = {Zhiyu Mei and Wei Fu and Jiaxuan Gao and Guangju Wang and Huanchen Zhang and Yi Wu 0013},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores},
  url = {https://openreview.net/forum?id=lajn1iROCu},
  year = {2024}
}

@inproceedings{mei2024bayesian,
  author = {Yongsheng Mei and Mahdi Imani and Tian Lan 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-12-09},
  publisher = {OpenReview.net},
  title = {Bayesian Optimization through Gaussian Cox Process Models for Spatio-temporal Data},
  url = {https://openreview.net/forum?id=9j1RD9LlWH},
  year = {2024}
}

@inproceedings{meidani2024snip,
  author = {Kazem Meidani and Parshin Shojaee and Chandan K. Reddy and Amir Barati Farimani},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified Pre-training},
  url = {https://openreview.net/forum?id=KZSEgJGPxu},
  year = {2024}
}

@inproceedings{melnychuk2024bounds,
  author = {Valentyn Melnychuk and Dennis Frauen and Stefan Feuerriegel},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Bounds on Representation-Induced Confounding Bias for Treatment Effect Estimation},
  url = {https://openreview.net/forum?id=d3xKPQVjSc},
  year = {2024}
}

@inproceedings{memmel2024asid,
  author = {Marius Memmel and Andrew Wagenmaker and Chuning Zhu and Dieter Fox and Abhishek Gupta 0004},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {ASID: Active Exploration for System Identification in Robotic Manipulation},
  url = {https://openreview.net/forum?id=jNR6s6OSBT},
  year = {2024}
}

@inproceedings{2024state,
  author = {Li Meng 0002 and Morten Goodwin and Anis Yazidi and Paal E. Engelstad},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-11-13},
  publisher = {OpenReview.net},
  title = {State Representation Learning Using an Unbalanced Atlas},
  url = {https://openreview.net/forum?id=cWdAYDLmPa},
  year = {2024}
}

@inproceedings{mentzer2024finite,
  author = {Fabian Mentzer and David Minnen and Eirikur Agustsson and Michael Tschannen},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Finite Scalar Quantization: VQ-VAE Made Simple},
  url = {https://openreview.net/forum?id=8ishA3LxN8},
  year = {2024}
}

@inproceedings{meo2024tcvae,
  author = {Cristian Meo and Louis Mahon and Anirudh Goyal and Justin Dauwels},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Œ±TC-VAE: On the relationship between Disentanglement and Diversity},
  url = {https://openreview.net/forum?id=ptXo0epLQo},
  year = {2024}
}

@inproceedings{merrill2024expressive,
  author = {William Merrill and Ashish Sabharwal},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {The Expressive Power of Transformers with Chain of Thought},
  url = {https://openreview.net/forum?id=NjNGlPh8Wh},
  year = {2024}
}

@inproceedings{merullo2024circuit,
  author = {Jack Merullo and Carsten Eickhoff and Ellie Pavlick},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Circuit Component Reuse Across Tasks in Transformer Language Models},
  url = {https://openreview.net/forum?id=fpoAYV6Wsk},
  year = {2024}
}

@inproceedings{messaoud2024s2ac,
  author = {Safa Messaoud and Billel Mokeddem and Zhenghai Xue and Linsey Pang and Bo An 0001 and Haipeng Chen 0001 and Sanjay Chawla},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-08},
  publisher = {OpenReview.net},
  title = {S2AC: Energy-Based Reinforcement Learning with Stein Soft Actor Critic},
  url = {https://openreview.net/forum?id=rAHcTCMaLc},
  year = {2024}
}

@inproceedings{meterez2024towards,
  author = {Alexandru Meterez and Amir Joudaki and Francesco Orabona and Alexander Immer and Gunnar Rtsch and Hadi Daneshmand},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Towards Training Without Depth Limits: Batch Normalization Without Gradient Explosion},
  url = {https://openreview.net/forum?id=xhCZD9hiiA},
  year = {2024}
}

@inproceedings{mialon2024gaia,
  author = {Grgoire Mialon and Clmentine Fourrier and Thomas Wolf 0008 and Yann LeCun and Thomas Scialom},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {GAIA: a benchmark for General AI Assistants},
  url = {https://openreview.net/forum?id=fibxvahvs3},
  year = {2024}
}

@inproceedings{miao2024selfcheck,
  author = {Ning Miao and Yee Whye Teh and Tom Rainforth},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning},
  url = {https://openreview.net/forum?id=pTHfApDakA},
  year = {2024}
}

@inproceedings{mihajlovic2024resfields,
  author = {Marko Mihajlovic and Sergey Prokudin and Marc Pollefeys and Siyu Tang 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-04},
  publisher = {OpenReview.net},
  title = {ResFields: Residual Neural Fields for Spatiotemporal Signals},
  url = {https://openreview.net/forum?id=EHrvRNs2Y0},
  year = {2024}
}

@inproceedings{mikula2024magnushammer,
  author = {Maciej Mikula and Szymon Tworkowski and Szymon Antoniak and Bartosz Piotrowski and Albert Q. Jiang and Jin Peng Zhou and Christian Szegedy and Lukasz Kucinski and Piotr Milos and Yuhuai Wu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Magnushammer: A Transformer-Based Approach to Premise Selection},
  url = {https://openreview.net/forum?id=oYjPk8mqAV},
  year = {2024}
}

@inproceedings{milsom2024convolutional,
  author = {Edward Milsom and Ben Anson and Laurence Aitchison},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Convolutional Deep Kernel Machines},
  url = {https://openreview.net/forum?id=1oqedRt6Z7},
  year = {2024}
}

@inproceedings{min2024beyond,
  author = {Marcus J. Min and Yangruibo Ding and Luca Buratti and Saurabh Pujar and Gail E. Kaiser and Suman Jana and Baishakhi Ray},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain},
  url = {https://openreview.net/forum?id=caW7LdAALh},
  year = {2024}
}

@inproceedings{min2024silo,
  author = {Sewon Min and Suchin Gururangan and Eric Wallace and Weijia Shi and Hannaneh Hajishirzi and Noah A. Smith and Luke Zettlemoyer},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore},
  url = {https://openreview.net/forum?id=ruk0nyQPec},
  year = {2024}
}

@inproceedings{min2024early,
  author = {Hancheng Min and Enrique Mallada and Ren Vidal},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-26},
  publisher = {OpenReview.net},
  title = {Early Neuron Alignment in Two-layer ReLU Networks with Small Initialization},
  url = {https://openreview.net/forum?id=QibPzdVrRu},
  year = {2024}
}

@inproceedings{mirakhor2024task,
  author = {Karan Mirakhor and Sourav Ghosh and Dipanjan Das 0003 and Brojeshwar Bhowmick},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Task Planning for Visual Room Rearrangement under Partial Observability},
  url = {https://openreview.net/forum?id=jJvXNpvOdM},
  year = {2024}
}

@inproceedings{mireshghallah2024can,
  author = {Niloofar Mireshghallah and Hyunwoo Kim 0002 and Xuhui Zhou and Yulia Tsvetkov and Maarten Sap and Reza Shokri and Yejin Choi 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-10-06},
  publisher = {OpenReview.net},
  title = {Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory},
  url = {https://openreview.net/forum?id=gmg7t8b4s0},
  year = {2024}
}

@inproceedings{mironenco2024lie,
  author = {Mircea Mironenco and Patrick Forr},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Lie Group Decompositions for Equivariant Neural Networks},
  url = {https://openreview.net/forum?id=p34fRKp8qA},
  year = {2024}
}

@inproceedings{mirzadeh2024relu,
  author = {Seyed-Iman Mirzadeh and Keivan Alizadeh-Vahid and Sachin Mehta and Carlo C. del Mundo and Oncel Tuzel and Golnoosh Samei and Mohammad Rastegari and Mehrdad Farajtabar},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models},
  url = {https://openreview.net/forum?id=osoWxY8q2E},
  year = {2024}
}

@inproceedings{misra2024towards,
  author = {Dipendra Misra and Akanksha Saran and Tengyang Xie and Alex Lamb and John Langford 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Towards Principled Representation Learning from Videos for Reinforcement Learning},
  url = {https://openreview.net/forum?id=3mnWvUZIXt},
  year = {2024}
}

@inproceedings{mitchell2024emulator,
  author = {Eric Mitchell and Rafael Rafailov and Archit Sharma and Chelsea Finn and Christopher D. Manning},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {An Emulator for Fine-tuning Large Language Models using Small Language Models},
  url = {https://openreview.net/forum?id=Eo7kv0sllr},
  year = {2024}
}

@inproceedings{miyato2024gta,
  author = {Takeru Miyato and Bernhard Jaeger and Max Welling and Andreas Geiger 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers},
  url = {https://openreview.net/forum?id=uJVHygNeSZ},
  year = {2024}
}

@inproceedings{mo2024selfsupervised,
  author = {Yujie Mo and Feiping Nie 0001 and Ping Hu 0001 and Heng Tao Shen and Zheng Zhang 0006 and Xinchao Wang and Xiaofeng Zhu 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-11-12},
  publisher = {OpenReview.net},
  title = {Self-Supervised Heterogeneous Graph Learning: a Homophily and Heterogeneity View},
  url = {https://openreview.net/forum?id=3FJOKjooIj},
  year = {2024}
}

@inproceedings{mohammadpour2024decoupling,
  author = {Sobhan Mohammadpour and Emma Frejinger and Pierre-Luc Bacon},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Decoupling regularization from the action space},
  url = {https://openreview.net/forum?id=UaMgmoKEBj},
  year = {2024}
}

@inproceedings{mohri2024learning,
  author = {Christopher Mohri and Daniel Andor and Eunsol Choi and Michael Collins 0001 and Anqi Mao and Yutao Zhong 0002},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Learning to Reject with a Fixed Predictor: Application to Decontextualization},
  url = {https://openreview.net/forum?id=dCHbFDsCZz},
  year = {2024}
}

@inproceedings{mokrov2024energyguided,
  author = {Petr Mokrov and Alexander Korotin and Alexander Kolesov and Nikita Gushchin and Evgeny Burnaev},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Energy-guided Entropic Neural Optimal Transport},
  url = {https://openreview.net/forum?id=d6tUsZeVs7},
  year = {2024}
}

@inproceedings{mondal2024efficient,
  author = {Arnab Kumar Mondal and Siba Smarak Panigrahi and Sai Rajeswar and Kaleem Siddiqi and Siamak Ravanbakhsh},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Efficient Dynamics Modeling in Interactive Environments with Koopman Theory},
  url = {https://openreview.net/forum?id=fkrYDQaHOJ},
  year = {2024}
}

@inproceedings{moosa2024mtranker,
  author = {Ibraheem Muhammad Moosa and Rui Zhang 0037 and Wenpeng Yin 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {MT-Ranker: Reference-free machine translation evaluation by inter-system ranking},
  url = {https://openreview.net/forum?id=Rry1SeSOQL},
  year = {2024}
}

@inproceedings{moriel2024lets,
  author = {Noa Moriel and Matthew Ricci and Mor Nitzan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Let's do the time-warp-attend: Learning topological invariants of dynamical systems},
  url = {https://openreview.net/forum?id=Fj7Fzm5lWL},
  year = {2024}
}

@inproceedings{morris2024orbitequivariant,
  author = {Matthew Morris and Bernardo Cuenca Grau and Ian Horrocks 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Orbit-Equivariant Graph Neural Networks},
  url = {https://openreview.net/forum?id=GkJOCga62u},
  year = {2024}
}

@inproceedings{morris2024language,
  author = {John X. Morris and Wenting Zhao and Justin T. Chiu and Vitaly Shmatikov and Alexander M. Rush},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Language Model Inversion},
  url = {https://openreview.net/forum?id=t9dWHpGkPj},
  year = {2024}
}

@inproceedings{morwani2024feature,
  author = {Depen Morwani and Benjamin L. Edelman and Costin-Andrei Oncescu and Rosie Zhao and Sham M. Kakade},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Feature emergence via margin maximization: case studies in algebraic tasks},
  url = {https://openreview.net/forum?id=i9wDX850jR},
  year = {2024}
}

@inproceedings{moskovitz2024confronting,
  author = {Ted Moskovitz and Aaditya K. Singh and DJ Strouse and Tuomas Sandholm and Ruslan Salakhutdinov and Anca D. Dragan and Stephen Marcus McAleer},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Confronting Reward Model Overoptimization with Constrained RLHF},
  url = {https://openreview.net/forum?id=gkfUvn0fLU},
  year = {2024}
}

@inproceedings{mou2024dragondiffusion,
  author = {Chong Mou and Xintao Wang 0002 and Jiechong Song and Ying Shan and Jian Zhang 0018},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-05-12},
  publisher = {OpenReview.net},
  title = {DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models},
  url = {https://openreview.net/forum?id=OEL4FJMg1b},
  year = {2024}
}

@inproceedings{mouli2024metaphysica,
  author = {S. Chandra Mouli and Muhammad Ashraful Alam and Bruno Ribeiro 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {MetaPhysiCa: Improving OOD Robustness in Physics-informed Machine Learning},
  url = {https://openreview.net/forum?id=KrWuDiW4Qm},
  year = {2024}
}

@inproceedings{mu2024drs,
  author = {Tongzhou Mu and Minghua Liu and Hao Su 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks},
  url = {https://openreview.net/forum?id=6CZ50WgfCG},
  year = {2024}
}

@inproceedings{muennighoff2024octopack,
  author = {Niklas Muennighoff and Qian Liu 0012 and Armel Randy Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-04-27},
  publisher = {OpenReview.net},
  title = {OctoPack: Instruction Tuning Code Large Language Models},
  url = {https://openreview.net/forum?id=mw1PWNSWZP},
  year = {2024}
}

@inproceedings{mukhoty2024certified,
  author = {Bhaskar Mukhoty and Hilal AlQuabeh and Giulia De Masi and Huan Xiong and Bin Gu 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Certified Adversarial Robustness for Rate Encoded Spiking Neural Networks},
  url = {https://openreview.net/forum?id=5bNYf0CqxY},
  year = {2024}
}

@inproceedings{mullen2024learning,
  author = {Thomas Soares Mullen and Marine Schimel and Guillaume Hennequin and Christian K. Machens and Michael B. Orger and Adrien Jouary},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-10-06},
  publisher = {OpenReview.net},
  title = {Learning interpretable control inputs and dynamics underlying animal locomotion},
  url = {https://openreview.net/forum?id=MFCjgEOLJT},
  year = {2024}
}

@inproceedings{mller2024graphchef,
  author = {Peter Mller and Lukas Faber and Karolis Martinkus and Roger Wattenhofer},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {GraphChef: Decision-Tree Recipes to Explain Graph Neural Networks},
  url = {https://openreview.net/forum?id=IjMUGuUmBI},
  year = {2024}
}

@inproceedings{mndler2024selfcontradictory,
  author = {Niels Mndler and Jingxuan He and Slobodan Jenko and Martin T. Vechev},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation},
  url = {https://openreview.net/forum?id=EmQSOi1X2f},
  year = {2024}
}

@inproceedings{murata2024simple,
  author = {Tomoya Murata and Kenta Niwa and Takumi Fukami and Iifan Tyou},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity},
  url = {https://openreview.net/forum?id=1ii8idH4tH},
  year = {2024}
}

@inproceedings{muttenthaler2024set,
  author = {Lukas Muttenthaler and Robert A. Vandermeulen and Qiuyi Zhang 0001 and Thomas Unterthiner and Klaus-Robert Mller},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Set Learning for Accurate and Calibrated Models},
  url = {https://openreview.net/forum?id=HZ3S17EI0o},
  year = {2024}
}

@inproceedings{mutti2024exploiting,
  author = {Mirco Mutti and Riccardo De Santi and Marcello Restelli and Alexander Marx and Giorgia Ramponi},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning},
  url = {https://openreview.net/forum?id=M0xK8nPGvt},
  year = {2024}
}

@inproceedings{na2024labelnoise,
  author = {Byeonghu Na and Yeongmin Kim and HeeSun Bae and Jung Hyun Lee and Se Jung Kwon and Wanmo Kang and Il-Chul Moon},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Label-Noise Robust Diffusion Models},
  url = {https://openreview.net/forum?id=HXWTXXtHNl},
  year = {2024}
}

@inproceedings{na2024guiding,
  author = {Yeongyeon Na and Minje Park and Yunwon Tae and Sunghoon Joo},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Guiding Masked Representation Learning to Capture Spatio-Temporal Relationship of Electrocardiogram},
  url = {https://openreview.net/forum?id=WcOohbsF4H},
  year = {2024}
}

@inproceedings{na2024efficient,
  author = {Hyungho Na and Yunkyeong Seo and Il-Chul Moon},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning},
  url = {https://openreview.net/forum?id=LjivA1SLZ6},
  year = {2024}
}

@inproceedings{nachmani2024spoken,
  author = {Eliya Nachmani and Alon Levkovitch and Roy Hirsch and Julian Salazar and Chulayuth Asawaroengchai and Soroosh Mariooryad and Ehud Rivlin and R. J. Skerry-Ryan and Michelle Tadmor Ramanovich},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM},
  url = {https://openreview.net/forum?id=izrOLJov5y},
  year = {2024}
}

@inproceedings{nahshan2024linear,
  author = {Yury Nahshan and Joseph Kampeas and Emir Haleva},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Linear Log-Normal Attention with Unbiased Concentration},
  url = {https://openreview.net/forum?id=5nM2AHzqUj},
  year = {2024}
}

@inproceedings{nahum2024decongestion,
  author = {Omer Nahum and Gali Noti and David C. Parkes and Nir Rosenfeld},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Decongestion by Representation: Learning to Improve Economic Welfare in Marketplaces},
  url = {https://openreview.net/forum?id=coIaBY8EVF},
  year = {2024}
}

@inproceedings{naiman2024generative,
  author = {Ilan Naiman and N. Benjamin Erichson and Pu Ren and Michael W. Mahoney and Omri Azencot},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs},
  url = {https://openreview.net/forum?id=eY7sLb0dVF},
  year = {2024}
}

@inproceedings{nam2024lipsumft,
  author = {Giung Nam and Byeongho Heo and Juho Lee 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text Guidance},
  url = {https://openreview.net/forum?id=2JF8mJRJ7M},
  year = {2024}
}

@inproceedings{nam2024diffusion,
  author = {Jisu Nam and Gyuseong Lee and Sunwoo Kim and Hyeonsu Kim and Hyoungwon Cho and Seyeon Kim and Seungryong Kim},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Diffusion Model for Dense Matching},
  url = {https://openreview.net/forum?id=Zsfiqpft6K},
  year = {2024}
}

@inproceedings{namekata2024emerdiff,
  author = {Koichi Namekata and Amirmojtaba Sabour and Sanja Fidler and Seung Wook Kim 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-03-03},
  publisher = {OpenReview.net},
  title = {EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models},
  url = {https://openreview.net/forum?id=YqyTXmF8Y2},
  year = {2024}
}

@inproceedings{narasimhan2024learning,
  author = {Harikrishna Narasimhan and Aditya Krishna Menon and Wittawat Jitkrittum and Neha Gupta and Sanjiv Kumar},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Learning to Reject Meets Long-tail Learning},
  url = {https://openreview.net/forum?id=ta26LtNq2r},
  year = {2024}
}

@inproceedings{narasimhan2024plugin,
  author = {Harikrishna Narasimhan and Aditya Krishna Menon and Wittawat Jitkrittum and Sanjiv Kumar},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Plugin estimators for selective classification with out-of-distribution detection},
  url = {https://openreview.net/forum?id=DASh78rJ7g},
  year = {2024}
}

@inproceedings{ngo2024alignment,
  author = {Richard Ngo and Lawrence Chan and Sren Mindermann},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {The Alignment Problem from a Deep Learning Perspective},
  url = {https://openreview.net/forum?id=fh8EYKFKns},
  year = {2024}
}

@inproceedings{nguyen2024statistical,
  author = {Huy Nguyen and Pedram Akbarian and Fanqi Yan and Nhat Ho},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Statistical Perspective of Top-K Sparse Softmax Gating Mixture of Experts},
  url = {https://openreview.net/forum?id=jvtmdK69KQ},
  year = {2024}
}

@inproceedings{nguyen2024quasimonte,
  author = {Khai Nguyen and Nicola Bariletto and Nhat Ho},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Quasi-Monte Carlo for 3D Sliced Wasserstein},
  url = {https://openreview.net/forum?id=Wd47f7HEXg},
  year = {2024}
}

@inproceedings{nguyen2024optimistic,
  author = {Quoc Phong Nguyen and Wan Theng Ruth Chew and Le Song and Bryan Kian Hsiang Low and Patrick Jaillet},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Optimistic Bayesian Optimization with Unknown Constraints},
  url = {https://openreview.net/forum?id=D4NJFfrqoq},
  year = {2024}
}

@inproceedings{nguyen2024sliced,
  author = {Khai Nguyen and Nhat Ho},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Sliced Wasserstein Estimation with Control Variates},
  url = {https://openreview.net/forum?id=StYc4hQAEi},
  year = {2024}
}

@inproceedings{nguyen2024rmae,
  author = {Duy-Kien Nguyen and Yanghao Li and Vaibhav Aggarwal and Martin R. Oswald and Alexander Kirillov and Cees G. M. Snoek and Xinlei Chen},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {R-MAE: Regions Meet Masked Autoencoders},
  url = {https://openreview.net/forum?id=ba84RDHFnz},
  year = {2024}
}

@inproceedings{nguyen2024metavbo,
  author = {Quoc Phong Nguyen and Bryan Kian Hsiang Low and Patrick Jaillet},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Meta-VBO: Utilizing Prior Tasks in Optimizing Risk Measures with Gaussian Processes},
  url = {https://openreview.net/forum?id=ElykcDu5YK},
  year = {2024}
}

@inproceedings{nguyen2024bellman,
  author = {Bao Nguyen and Binh Nguyen and Viet Anh Nguyen},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Bellman Optimal Stepsize Straightening of Flow-Matching Models},
  url = {https://openreview.net/forum?id=Iyve2ycvGZ},
  year = {2024}
}

@inproceedings{nguyen2024topic,
  author = {Thong Thanh Nguyen and Xiaobao Wu and Xinshuai Dong and Cong-Duy T. Nguyen and See-Kiong Ng and Anh Tuan Luu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Topic Modeling as Multi-Objective Contrastive Optimization},
  url = {https://openreview.net/forum?id=HdAoLSBYXj},
  year = {2024}
}

@inproceedings{nguyen2024matrix,
  author = {Xuan Son Nguyen and Shuo Yang and Aymeric Histace},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Matrix Manifold Neural Networks++},
  url = {https://openreview.net/forum?id=30aSE3FB3L},
  year = {2024}
}

@inproceedings{ni2024bridging,
  author = {Tianwei Ni and Benjamin Eysenbach and Erfan Seyedsalehi and Michel Ma and Clement Gehring and Aditya Mahajan and Pierre-Luc Bacon},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Bridging State and History Representations: Understanding Self-Predictive RL},
  url = {https://openreview.net/forum?id=ms0VgzSGF2},
  year = {2024}
}

@inproceedings{ni2024sliced,
  author = {Yuyan Ni and Shikun Feng and Wei-Ying Ma and Zhi-Ming Ma and Yanyan Lan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Sliced Denoising: A Physics-Informed Molecular Pre-Training Method},
  url = {https://openreview.net/forum?id=liKkG1zcWq},
  year = {2024}
}

@inproceedings{nicks2024language,
  author = {Charlotte Nicks and Eric Mitchell and Rafael Rafailov and Archit Sharma and Christopher D. Manning and Chelsea Finn and Stefano Ermon},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Language Model Detectors Are Easily Optimized Against},
  url = {https://openreview.net/forum?id=4eJDMjYZZG},
  year = {2024}
}

@inproceedings{nie2024blessing,
  author = {Shen Nie and Hanzhong Allan Guo and Cheng Lu 0011 and Yuhao Zhou and Chenyu Zheng and Chongxuan Li},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {The Blessing of Randomness: SDE Beats ODE in General Diffusion-based Image Editing},
  url = {https://openreview.net/forum?id=DesYwmUG00},
  year = {2024}
}

@inproceedings{nie2024outofdistribution,
  author = {Jun Nie and Yonggang Zhang 0003 and Zhen Fang 0001 and Tongliang Liu and Bo Han 0003 and Xinmei Tian 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-01-29},
  publisher = {OpenReview.net},
  title = {Out-of-Distribution Detection with Negative Prompts},
  url = {https://openreview.net/forum?id=nanyAujl6e},
  year = {2024}
}

@inproceedings{nielsen2024diffenc,
  author = {Beatrix Miranda Ginn Nielsen and Anders Christensen and Andrea Dittadi and Ole Winther},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {DiffEnc: Variational Diffusion with a Learned Encoder},
  url = {https://openreview.net/forum?id=8nxy1bQWTG},
  year = {2024}
}

@inproceedings{nikolaus2024emergent,
  author = {Mitja Nikolaus},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Emergent Communication with Conversational Repair},
  url = {https://openreview.net/forum?id=Sy8upuD6Bw},
  year = {2024}
}

@inproceedings{ning2024skeletonofthought,
  author = {Xuefei Ning and Zinan Lin 0001 and Zixuan Zhou and Zifu Wang and Huazhong Yang and Yu Wang 0002},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation},
  url = {https://openreview.net/forum?id=mqVgBbNCm9},
  year = {2024}
}

@inproceedings{ning2024elucidating,
  author = {Mang Ning and Mingxiao Li 0002 and Jianlin Su and Albert Ali Salah and Itir nal Ertugrul},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-05-16},
  publisher = {OpenReview.net},
  title = {Elucidating the Exposure Bias in Diffusion Models},
  url = {https://openreview.net/forum?id=xEJMoj1SpX},
  year = {2024}
}

@inproceedings{nishimura2024minimax,
  author = {Yuto Nishimura and Taiji Suzuki},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Minimax optimality of convolutional neural networks for infinite dimensional input-output problems and separation from kernel methods},
  url = {https://openreview.net/forum?id=EW8ZExRZkJ},
  year = {2024}
}

@inproceedings{nitanda2024improved,
  author = {Atsushi Nitanda and Kazusato Oko and Taiji Suzuki and Denny Wu},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Improved statistical and computational complexity of the mean-field Langevin dynamics under structured data},
  url = {https://openreview.net/forum?id=Of2nEDc4s7},
  year = {2024}
}

@inproceedings{niu2024schema,
  author = {Yulei Niu and Wenliang Guo and Long Chen 0016 and Xudong Lin 0003 and Shih-Fu Chang},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos},
  url = {https://openreview.net/forum?id=abL5LJNZ49},
  year = {2024}
}

@inproceedings{niu2024what,
  author = {Jingcheng Niu and Andrew Liu and Zining Zhu 0001 and Gerald Penn},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {What does the Knowledge Neuron Thesis Have to do with Knowledge?},
  url = {https://openreview.net/forum?id=2HJRwwbV3G},
  year = {2024}
}

@inproceedings{oh2024stable,
  author = {YongKyung Oh and Dongyoung Lim and Sungil Kim},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data},
  url = {https://openreview.net/forum?id=4VIgNuQ1pY},
  year = {2024}
}

@inproceedings{okanovic2024repeated,
  author = {Patrik Okanovic and Roger Waleffe and Vasilis Mageirakos and Konstantinos E. Nikolakakis and Amin Karbasi and Dionysios S. Kalogerias and Nezihe Merve Grel and Theodoros Rekatsinas},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Repeated Random Sampling for Minimizing the Time-to-Accuracy of Learning},
  url = {https://openreview.net/forum?id=JnRStoIuTe},
  year = {2024}
}

@inproceedings{olausson2024selfrepair,
  author = {Theo X. Olausson and Jeevana Priya Inala and Chenglong Wang and Jianfeng Gao 0001 and Armando Solar-Lezama},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Is Self-Repair a Silver Bullet for Code Generation?},
  url = {https://openreview.net/forum?id=y0GJXRungR},
  year = {2024}
}

@inproceedings{oren2024proving,
  author = {Yonatan Oren and Nicole Meister and Niladri S. Chatterji and Faisal Ladhak and Tatsunori Hashimoto},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Proving Test Set Contamination in Black-Box Language Models},
  url = {https://openreview.net/forum?id=KS8mIvetg2},
  year = {2024}
}

@inproceedings{ortiz2024magnitude,
  author = {Jose Javier Gonzalez Ortiz and John V. Guttag and Adrian V. Dalca},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Magnitude Invariant Parametrizations Improve Hypernetwork Learning},
  url = {https://openreview.net/forum?id=fJNnerz6iH},
  year = {2024}
}

@inproceedings{oublal2024disentangling,
  author = {Khalid Oublal and Sad Ladjal and David Benhaiem and Emmanuel Le-borgne and Franois Roueff},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Disentangling Time Series Representations via Contrastive Independence-of-Support on l-Variational Inference},
  url = {https://openreview.net/forum?id=iI7hZSczxE},
  year = {2024}
}

@inproceedings{ouderaa2024llm,
  author = {Tycho F. A. van der Ouderaa and Markus Nagel and Mart van Baalen and Tijmen Blankevoort},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {The LLM Surgeon},
  url = {https://openreview.net/forum?id=DYIIRgwg2i},
  year = {2024}
}

@inproceedings{pacchiardi2024how,
  author = {Lorenzo Pacchiardi and Alex James Chan and Sren Mindermann and Ilan Moscovitz and Alexa Y. Pan and Yarin Gal and Owain Evans and Jan Markus Brauner},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions},
  url = {https://openreview.net/forum?id=567BjxgaTp},
  year = {2024}
}

@inproceedings{pace2024delphic,
  author = {Alize Pace and Hugo Yche and Bernhard Schlkopf and Gunnar Rtsch and Guy Tennenholtz},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding},
  url = {https://openreview.net/forum?id=lUYY2qsRTI},
  year = {2024}
}

@inproceedings{pacini2024characterization,
  author = {Marco Pacini and Xiaowen Dong 0001 and Bruno Lepri and Gabriele Santin},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {A Characterization Theorem for Equivariant Networks with Point-wise Activations},
  url = {https://openreview.net/forum?id=79FVDdfoSR},
  year = {2024}
}

@inproceedings{padmakumar2024does,
  author = {Vishakh Padmakumar and He He 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Does Writing with Language Models Reduce Content Diversity?},
  url = {https://openreview.net/forum?id=Feiz5HtCD0},
  year = {2024}
}

@inproceedings{pai2024masked,
  author = {Druv Pai and Sam Buchanan and Ziyang Wu and Yaodong Yu and Yi Ma 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Masked Completion via Structured Diffusion with White-Box Transformers},
  url = {https://openreview.net/forum?id=PvyOYleymy},
  year = {2024}
}

@inproceedings{pal2024backdoor,
  author = {Soumyadeep Pal and Yuguang Yao and Ren Wang 0008 and Bingquan Shen and Sijia Liu 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Backdoor Secrets Unveiled: Identifying Backdoor Data with Optimized Scaled Prediction Consistency},
  url = {https://openreview.net/forum?id=1OfAO2mes1},
  year = {2024}
}

@inproceedings{palma2024expressive,
  author = {Alessandro De Palma and Rudy Bunel and Krishnamurthy (Dj) Dvijotham and M. Pawan Kumar and Robert Stanforth and Alessio Lomuscio},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Expressive Losses for Verified Robustness via Convex Combinations},
  url = {https://openreview.net/forum?id=mzyZ4wzKlM},
  year = {2024}
}

@inproceedings{palumbo2024deep,
  author = {Emanuele Palumbo and Laura Manduchi and Sonia Laguna and Daphn Chopard and Julia E. Vogt},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Deep Generative Clustering with Multimodal Diffusion Variational Autoencoders},
  url = {https://openreview.net/forum?id=k5THrhXDV3},
  year = {2024}
}

@inproceedings{pan2024kosmosg,
  author = {Xichen Pan and Li Dong 0004 and Shaohan Huang and Zhiliang Peng and Wenhu Chen and Furu Wei},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Kosmos-G: Generating Images in Context with Multimodal Large Language Models},
  url = {https://openreview.net/forum?id=he6mX9LTyE},
  year = {2024}
}

@inproceedings{pan2024pretraining,
  author = {Ling Pan and Moksh Jain and Kanika Madan and Yoshua Bengio},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-07-29},
  publisher = {OpenReview.net},
  title = {Pre-Training and Fine-Tuning Generative Flow Networks},
  url = {https://openreview.net/forum?id=ylhiMfpqkm},
  year = {2024}
}

@inproceedings{pan2024adjointdpm,
  author = {Jiachun Pan and Jun Hao Liew and Vincent Y. F. Tan and Jiashi Feng and Hanshu Yan},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models},
  url = {https://openreview.net/forum?id=y33lDRBgWI},
  year = {2024}
}

@inproceedings{2024accelerated,
  author = {Rui Pan 0002 and Yuxing Liu and Xiaoyu Wang 0008 and Tong Zhang 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2025-04-02},
  publisher = {OpenReview.net},
  title = {Accelerated Convergence of Stochastic Heavy Ball Method under Anisotropic Gradient Noise},
  url = {https://openreview.net/forum?id=CIqjp9yTDq},
  year = {2024}
}

@inproceedings{pan2024enhancing,
  author = {Zijie Pan and Jiachen Lu and Xiatian Zhu and Li Zhang 0040},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping},
  url = {https://openreview.net/forum?id=ukidfml68f},
  year = {2024}
}

@inproceedings{pan2024pretraining,
  author = {Tai-Yu Pan and Chenyang Ma and Tianle Chen and Cheng Perng Phoo and Katie Z. Luo and Yurong You and Mark Campbell 0001 and Kilian Q. Weinberger and Bharath Hariharan and Wei-Lun Chao},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Pre-training LiDAR-based 3D Object Detectors through Colorization},
  url = {https://openreview.net/forum?id=fB1iiH9xo7},
  year = {2024}
}

@inproceedings{pan2024skill,
  author = {Hsiao-Ru Pan and Bernhard Schlkopf},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Skill or Luck? Return Decomposition via Advantage Functions},
  url = {https://openreview.net/forum?id=ZFMiHfZwIf},
  year = {2024}
}

@inproceedings{panaitesculiess2024like,
  author = {Michael-Andrei Panaitescu-Liess and Yigitcan Kaya and Sicheng Zhu and Furong Huang and Tudor Dumitras},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Like Oil and Water: Group Robustness Methods and Poisoning Defenses May Be at Odds},
  url = {https://openreview.net/forum?id=rM9VJPB20F},
  year = {2024}
}

@inproceedings{panda2024teach,
  author = {Ashwinee Panda and Christopher A. Choquette-Choo and Zhengming Zhang and Yaoqing Yang and Prateek Mittal},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Teach LLMs to Phish: Stealing Private Information from Language Models},
  url = {https://openreview.net/forum?id=qo21ZlfNu6},
  year = {2024}
}

@inproceedings{pandey2024efficient,
  author = {Kushagra Pandey and Maja Rudolph and Stephan Mandt},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Efficient Integrators for Diffusion Generative Models},
  url = {https://openreview.net/forum?id=qA4foxO5Gf},
  year = {2024}
}

@inproceedings{pang2024language,
  author = {Jing-Cheng Pang and Pengyuan Wang and Kaiyuan Li and Xiong-Hui Chen and Jiacheng Xu 0003 and Zongzhang Zhang and Yang Yu 0001},
  booktitle = {ICLR},
  note = {DBLP last modified: 2024-08-07},
  publisher = {OpenReview.net},
  title = {Language Model Self-improvement by Reinforcement Learning Contemplation},
  url = {https://openreview.net/forum?id=38E4yUbrgr},
  year = {2024}
}

@inproceedings{pang2024frozen,
  abstract = {Large language models (LLMs), despite being trained solely on text data, can serve as surprisingly strong encoders for purely visual tasks in the absence of language. The key innovation is employing a frozen transformer block from pre-trained LLMs to directly process visual tokens. This approach demonstrates performance improvements across various visual tasks and is applicable to different LLMs such as LLaMA and OPT. We propose an information filtering hypothesis explaining LLM effectiveness in visual encoding.},
  author = {Ziqi Pang and Ziyang Xie and Yunze Man and Yu-Xiong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pang2024frozen.pdf:pdf},
  note = {ICLR 2024 Spotlight. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=t0FI3Q66K5},
  publisher = {OpenReview.net},
  title = {Frozen Transformers in Language Models Are Effective Visual Encoder Layers},
  url = {https://openreview.net/forum?id=t0FI3Q66K5},
  year = {2024}
}

@inproceedings{panwar2024incontext,
  abstract = {In-context learning (ICL) is one of the surprising and useful features of large language models and subject of intense research. Recently, stylized meta-learning-like ICL setups have been devised that train transformers on sequences of input-output pairs $(x, f(x))$. The function $f$ comes from a function class and generalization is checked by evaluating on sequences generated from unseen functions from the same class. One of the main discoveries in this line of research has been that for several function classes, such as linear regression, transformers successfully generalize to new functions in the class. However, the inductive biases of these models resulting in this behavior are not clearly understood. A model with unlimited training data and compute is a Bayesian predictor: it learns the pretraining distribution. The Bayesian perspective provides insights into the inductive bias of ICL and how transformers perform a particular task when they are trained on multiple tasks and it is found that transformers can learn to generalize to new function classes that were not seen during pretraining.},
  author = {Madhur Panwar and Kabir Ahuja and Navin Goyal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/panwar2024incontext.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HX5ujdsSon},
  publisher = {OpenReview.net},
  title = {In-Context Learning through the Bayesian Prism},
  url = {https://openreview.net/forum?id=HX5ujdsSon},
  year = {2024}
}

@inproceedings{papez2024sumproductset,
  abstract = {Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks.},
  author = {Milan Papez and Martin Rektoris and V\'{a}clav Sm\'{i}dl and Tom\'{a}\v{s} Pevn\'{y}},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/papez2024sumproductset.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mF3cTns4pe},
  publisher = {OpenReview.net},
  title = {Sum-Product-Set Networks: Deep Tractable Models for Tree-Structured Graphs},
  url = {https://openreview.net/forum?id=mF3cTns4pe},
  year = {2024}
}

@inproceedings{park2024what,
  abstract = {Forward- or reverse-mode automatic differentiation (AD) is a popular algorithm for computing the derivative of a function expressed by a program. AD always outputs the correct derivative if a program does not use any non-differentiable functions and control flows; however, it may return an arbitrary value otherwise. In this work, we investigate what AD computes for neural networks that may contain non-differentiable functions such as ReLU and maxpools. We first prove that AD always returns a generalized derivative called a Clarke subderivative for networks with pointwise activation functions, if the minibatch size is one and all non-differentiable neurons have distinct bias parameters. We show that the same conclusion does not hold otherwise, but does hold under some mild sufficient conditions. We also prove similar results for more general networks that can use maxpools and bias parameters shared across different neurons. We empirically check our sufficient conditions over popular network architectures and observe that AD almost always computes a Clarke subderivative in practical learning setups.},
  author = {Sejun Park and Sanghyuk Chun and Wonyeol Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024what.pdf:pdf},
  note = {ICLR 2024 Spotlight. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=8vKknbgXxf},
  publisher = {OpenReview.net},
  title = {What does automatic differentiation compute for neural networks?},
  url = {https://openreview.net/forum?id=8vKknbgXxf},
  year = {2024}
}

@inproceedings{park2024attentionbased,
  abstract = {In recent research, Tensor Product Representation (TPR) is applied for the systematic generalization task of deep neural networks by learning the compositional structure of data. However, such prior works show limited performance in discovering and representing the symbolic structure from unseen test data because their decomposition to the structural representations was incomplete. In this work, we propose an Attention-based Iterative Decomposition (AID) module designed to enhance the decomposition operations for the structured representations encoded from the sequential input data with TPR. Our AID can be easily adapted to any TPR-based model and provides enhanced systematic decomposition through a competitive attention mechanism between input features and structured representations. In our experiments, AID shows effectiveness by significantly improving the performance of TPR-based prior works on the series of systematic generalization tasks. Moreover, in the quantitative and qualitative evaluations, AID produces more compositional and well-bound structural representations than other works.},
  author = {Taewon Park and Inchul Choi and Minho Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024attentionbased.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FDb2JQZsFH},
  publisher = {OpenReview.net},
  title = {Attention-based Iterative Decomposition for Tensor Product Representation},
  url = {https://openreview.net/forum?id=FDb2JQZsFH},
  year = {2024}
}

@inproceedings{park2024accurate,
  abstract = {Given a pretrained encoder-based language model, how can we accurately compress it without retraining? Retraining-free structured pruning algorithms are crucial in pretrained language model compression due to their significantly reduced pruning cost and capability to prune large language models. However, existing retraining-free algorithms encounter severe accuracy degradation, as they fail to handle pruning errors, especially at high compression rates. In this paper, we propose KPrune (Knowledge-preserving pruning), an accurate retraining-free structured pruning algorithm for pretrained encoder-based language models. KPrune focuses on preserving the useful knowledge of the pretrained model to minimize pruning errors through a carefully designed iterative pruning process composed of knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. As a result, KPrune shows significant accuracy improvements up to 58.02\%p higher F1 score compared to existing retraining-free pruning algorithms under a high compression rate of 80\% on the SQuAD benchmark without any retraining process.},
  author = {Seungcheol Park and Hojun Choi and U Kang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024accurate.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=s2NjWfaYdZ},
  publisher = {OpenReview.net},
  title = {Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models},
  url = {https://openreview.net/forum?id=s2NjWfaYdZ},
  year = {2024}
}

@inproceedings{park2024h2osdf,
  abstract = {Advanced techniques using Neural Radiance Fields (NeRF), Signed Distance Fields (SDF), and Occupancy Fields have recently emerged as solutions for 3D indoor scene reconstruction. We introduce a novel two-phase learning approach, H2O-SDF, that discriminates between object and non-object regions within indoor environments. This method achieves a nuanced balance, carefully preserving the geometric integrity of room layouts while also capturing intricate surface details of specific objects. A cornerstone of our two-phase learning framework is the introduction of the Object Surface Field (OSF), a novel concept designed to mitigate the persistent vanishing gradient problem that has previously hindered the capture of high-frequency details in other methods. Our proposed approach is validated through several experiments that include ablation studies.},
  author = {Minyoung Park and Mirae Do and YeonJae Shin and Jaeseok Yoo and Jongkwang Hong and Joongrock Kim and Chul Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024h2osdf.pdf:pdf},
  note = {ICLR 2024 Spotlight. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=P1ANzoGg3W},
  publisher = {OpenReview.net},
  title = {{H2O-SDF}: Two-phase Learning for {3D} Indoor Reconstruction using Object Surface Fields},
  url = {https://openreview.net/forum?id=P1ANzoGg3W},
  year = {2024}
}

@inproceedings{park2024selfsupervised,
  abstract = {Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, especially in challenging scenarios that require a significantly long output for forecasting.},
  author = {Junwoo Park and Daehoon Gwak and Jaegul Choo and Edward Choi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024selfsupervised.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nBCuRzjqK7},
  publisher = {OpenReview.net},
  title = {Self-Supervised Contrastive Learning for Long-term Forecasting},
  url = {https://openreview.net/forum?id=nBCuRzjqK7},
  year = {2024}
}

@inproceedings{park2024ddmi,
  abstract = {Recent studies have introduced a new class of generative models for synthesizing implicit neural representations (INRs) that capture arbitrary continuous signals in various domains. These models opened the door for domain-agnostic generative models, but they often fail to achieve high-quality generation. We observed that the existing methods generate the weights of neural networks to parameterize INRs and evaluate the network with fixed positional embeddings (PEs). Arguably, this architecture limits the expressive power of generative models and results in low-quality INR generation. To address this limitation, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates adaptive positional embeddings instead of neural networks' weights. Specifically, we develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE) that seamlessly connects discrete data and continuous signal functions in the shared latent space. Additionally, we introduce a novel conditioning mechanism for evaluating INRs with the hierarchically decomposed PEs to further enhance expressive power. Extensive experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets, demonstrate the versatility of DDMI and its superior performance compared to the existing INR generative models.},
  author = {Dogyun Park and Sihyeon Kim and Sojin Lee and Hyunwoo J. Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024ddmi.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=327tbF3S65},
  publisher = {OpenReview.net},
  title = {{DDMI}: Domain-agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations},
  url = {https://openreview.net/forum?id=327tbF3S65},
  year = {2024}
}

@inproceedings{park2024ednerf,
  abstract = {Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain. This novel loss function surpasses the well-known score distillation sampling (SDS) loss in terms of suitability for editing purposes. Our experimental results demonstrate that ED-NeRF achieves faster editing speed while producing improved output quality compared to state-of-the-art 3D editing models.},
  author = {Jangho Park and Gihyun Kwon and Jong Chul Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024ednerf.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9DvDRTTdlu},
  publisher = {OpenReview.net},
  title = {{ED-NeRF}: Efficient Text-Guided Editing of {3D} Scene With Latent Space {NeRF}},
  url = {https://openreview.net/forum?id=9DvDRTTdlu},
  year = {2024}
}

@inproceedings{park2024bridging,
  abstract = {This paper introduces VLAP, a novel approach that bridges pretrained vision models and large language models (LLMs) to make frozen LLMs understand the visual world. VLAP transforms the embedding space of pretrained vision models into the LLMs' word embedding space using a single linear layer for efficient and general-purpose visual and language understanding. Specifically, we harness well-established word embeddings to bridge two modality embedding spaces. The visual and text representations are simultaneously assigned to a set of word embeddings within pretrained LLMs by formulating the assigning procedure as an optimal transport problem. We predict the assignment of one modality from the representation of another modality data, enforcing consistent assignments for paired multimodal data. This allows vision and language representations to contain the same information, grounding the frozen LLMs' word embedding space in visual data. Moreover, a robust semantic taxonomy of LLMs can be preserved with visual data since the LLMs interpret and reason linguistic information from correlations between word embeddings. Experimental results show that VLAP achieves substantial improvements over the previous linear transformation-based approaches across a range of vision-language tasks, including image captioning, visual question answering, and cross-modal retrieval. We also demonstrate the learned visual representations hold a semantic taxonomy of LLMs, making visual semantic arithmetic possible.},
  author = {Jungin Park and Jiyoung Lee and Kwanghoon Sohn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024bridging.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lK2V2E2MNv},
  publisher = {OpenReview.net},
  title = {Bridging Vision and Language Spaces with Assignment Prediction},
  url = {https://openreview.net/forum?id=lK2V2E2MNv},
  year = {2024}
}

@inproceedings{park2024lutgemm,
  abstract = {Recent advances in self-supervised learning and the Transformer architecture have significantly improved natural language processing (NLP), achieving remarkably low perplexity. However, the growing size of NLP models introduces a memory wall problem during the generation phase. To mitigate this issue, recent efforts have focused on quantizing model weights to sub-4-bit precision while preserving full precision for activations, resulting in practical speed-ups during inference on a single GPU. However, these improvements primarily stem from reduced memory movement, which necessitates a resource-intensive dequantization process rather than actual computational reduction. In this paper, we introduce LUT-GEMM, an efficient kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization. Furthermore, we proposed group-wise quantization to offer a flexible trade-off between compression ratio and accuracy. The impact of LUT-GEMM is facilitated by implementing high compression ratios through low-bit quantization and efficient LUT-based operations. We show experimentally that when applied to the OPT-175B model with 3-bit quantization, LUT-GEMM substantially accelerates token generation latency, achieving a remarkable 2.1x improvement on a single GPU when compared to OPTQ, which relies on the costly dequantization process.},
  author = {Gunho Park and Baeseong Park and Minsub Kim and Sungjae Lee and Jeonghoon Kim and Beomseok Kwon and Se Jung Kwon and Byeongwook Kim and Youngjoo Lee and Dongsoo Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024lutgemm.pdf:pdf},
  note = {DBLP last modified: 2025-03-04},
  pdf = {https://openreview.net/pdf?id=gLARhFLE0F},
  publisher = {OpenReview.net},
  title = {{LUT-GEMM}: Quantized Matrix Multiplication based on {LUT}s for Efficient Inference in Large-Scale Generative Language Models},
  url = {https://openreview.net/forum?id=gLARhFLE0F},
  year = {2024}
}

@inproceedings{park2024metra,
  abstract = {Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Our main idea is, instead of directly covering the entire state space, to only cover a compact latent space $\mathcal{Z}$ that is metrically connected to the state space $\mathcal{S}$ by temporal distances. By learning to move in every direction in the latent space, METRA obtains a tractable set of diverse behaviors that approximately cover the state space, being scalable to high-dimensional environments. Through our experiments in five locomotion and manipulation environments, we demonstrate that METRA can discover a variety of useful behaviors even in complex, pixel-based environments, being the first unsupervised RL method that discovers diverse locomotion behaviors in pixel-based Quadruped and Humanoid. Our code and videos are available at https://seohong.me/projects/metra/},
  author = {Seohong Park and Oleh Rybkin and Sergey Levine},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024metra.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=c5pwL0Soay},
  publisher = {OpenReview.net},
  title = {{METRA}: Scalable Unsupervised {RL} with Metric-Aware Abstraction},
  url = {https://openreview.net/forum?id=c5pwL0Soay},
  year = {2024}
}

@inproceedings{park2024longterm,
  abstract = {In the face of escalating climate changes, typhoon intensities and their ensuing damage have surged. Accurate trajectory prediction is crucial for effective damage control. Traditional physics-based models, while comprehensive, are computationally intensive and rely heavily on the expertise of forecasters. Contemporary data-driven methods often rely on reanalysis data, which can be considered to be the closest to the true representation of weather conditions. However, reanalysis data is not produced in real-time and requires time for adjustment since prediction models are calibrated with observational data. This reanalysis data, such as ERA5, falls short in challenging real-world situations. Optimal preparedness necessitates predictions at least 72 hours in advance, beyond the capabilities of standard physics models. In response to these constraints, we present an approach that harnesses real-time Unified Model (UM) data, sidestepping the limitations of reanalysis data. Our model provides predictions at 6-hour intervals for up to 72 hours in advance and outperforms both state-of-the-art data-driven methods and numerical weather prediction models. In line with our efforts to mitigate adversities inflicted by typhoons, we release our preprocessed PHYSICS TRACK dataset, which includes ERA5 reanalysis data, typhoon best-track, and UM forecast data.},
  author = {Young-Jae Park and Minseok Seo and Doyi Kim and Hyeri Kim and Sanghoon Choi and Beomkyu Choi and Jeongwon Ryu and Sohee Son and Hae-Gon Jeon and Yeji Choi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024longterm.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ziDFH8TPPK},
  publisher = {OpenReview.net},
  title = {Long-Term Typhoon Trajectory Prediction: A Physics-Conditioned Approach Without Reanalysis Data},
  url = {https://openreview.net/forum?id=ziDFH8TPPK},
  year = {2024}
}

@inproceedings{park2024denoising,
  abstract = {Diffusion models generate highly realistic images by learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timesteps. (2) Task Weights: During the early stages (higher timesteps) of the denoising process, DTR assigns a greater number of task-specific channels, leveraging the insight that diffusion models prioritize reconstructing global structure and perceptually rich contents in earlier stages, and focus on simple noise removal in later stages. Our experiments reveal that DTR not only consistently boosts diffusion models' performance across different evaluation protocols without adding extra parameters but also accelerates training convergence. Finally, we show the complementarity between our architectural approach and existing MTL optimization techniques, providing a more complete view of MTL in the context of diffusion training. Significantly, by leveraging this complementarity, we attain matched performance of DiT-XL using the smaller DiT-L with a reduction in training iterations from 7M to 2M. Our project page is available at https://byeongjun-park.github.io/DTR/},
  author = {Byeongjun Park and Sangmin Woo and Hyojun Go and Jin-Young Kim and Changick Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024denoising.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MY0qlcFcUg},
  publisher = {OpenReview.net},
  title = {Denoising Task Routing for Diffusion Models},
  url = {https://openreview.net/forum?id=MY0qlcFcUg},
  year = {2024}
}

@inproceedings{park2024forward,
  abstract = {Graph neural networks (GNNs) have achieved remarkable success across a wide range of applications, such as recommendation, drug discovery, and question answering. Behind the success of GNNs lies the backpropagation (BP) algorithm, which is the de facto standard for training deep neural networks (NNs). However, despite its effectiveness, BP imposes several constraints, which are not only biologically implausible, but also limit the scalability, parallelism, and flexibility in learning NNs. Examples of such constraints include storage of neural activities computed in the forward pass for use in the subsequent backward pass, and the dependence of parameter updates on non-local signals. To address these limitations, the forward-forward algorithm (FF) was recently proposed as an alternative to BP in the image classification domain, which trains NNs by performing two forward passes over positive and negative data. Inspired by this advance, we propose ForwardGNN in this work, a new forward learning procedure for GNNs, which avoids the constraints imposed by BP via an effective layer-wise local forward training. ForwardGNN extends the original FF to deal with graph data and GNNs, and makes it possible to operate without generating negative inputs (hence no longer forward-forward). Further, ForwardGNN enables each layer to learn from both the bottom-up and top-down signals without relying on the backpropagation of errors. Extensive experiments on real-world datasets show the effectiveness and generality of the proposed forward graph learning framework. We release our code at https://github.com/facebookresearch/forwardgnn.},
  author = {Namyong Park and Xing Wang and Antoine Simoulin and Shuai Yang and Grey Yang and Ryan A. Rossi and Puja Trivedi and Nesreen K. Ahmed},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/park2024forward.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Abr7dU98ME},
  publisher = {OpenReview.net},
  title = {Forward Learning of Graph Neural Networks},
  url = {https://openreview.net/forum?id=Abr7dU98ME},
  year = {2024}
}

@inproceedings{paster2024openwebmath,
  abstract = {There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, openly released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.},
  author = {Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/paster2024openwebmath.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jKHmjlpViu},
  publisher = {OpenReview.net},
  title = {{OpenWebMath}: An Open Dataset of High-Quality Mathematical Web Text},
  url = {https://openreview.net/forum?id=jKHmjlpViu},
  year = {2024}
}

@inproceedings{patel2024curriculum,
  abstract = {The key challenge in the noisy intermediate-scale quantum era is finding useful circuits compatible with current device limitations. Variational quantum algorithms (VQAs) offer a potential solution by fixing the circuit architecture and optimizing individual gate parameters in an external loop. However, parameter optimization can become intractable, and the overall performance of the algorithm depends heavily on the initially chosen circuit architecture. Several quantum architecture search (QAS) algorithms have been developed to design useful circuit architectures automatically. In the case of parameter optimization alone, noise effects have been observed to dramatically influence the performance of the optimizer and final outcomes, which is a key line of study. However, the effects of noise on the architecture search, which could be just as critical, are poorly understood. This work addresses this gap by introducing a curriculum-based reinforcement learning QAS (CRLQAS) algorithm designed to tackle challenges in realistic VQA deployment. The algorithm incorporates (i) a 3D architecture encoding and restrictions on environment dynamics to explore the search space of possible circuits efficiently, (ii) an episode halting scheme to steer the agent to find shorter circuits, and (iii) a novel variant of simultaneous perturbation stochastic approximation as an optimizer for faster convergence. To facilitate studies, we developed an optimized simulator for our algorithm, significantly improving computational efficiency in simulating noisy quantum circuits by employing the Pauli-transfer matrix formalism in the Pauli-Liouville basis. Numerical experiments focusing on quantum chemistry tasks demonstrate that CRLQAS outperforms existing QAS algorithms across several metrics in both noiseless and noisy environments.},
  author = {Yash J. Patel and Akash Kundu and Mateusz Ostaszewski and Xavier Bonet-Monroig and Vedran Dunjko and Onur Danaci},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/patel2024curriculum.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rINBD8jPoP},
  publisher = {OpenReview.net},
  title = {Curriculum reinforcement learning for quantum architecture search under hardware errors},
  url = {https://openreview.net/forum?id=rINBD8jPoP},
  year = {2024}
}

@inproceedings{pathak2024transformers,
  abstract = {Mixture models arise in many regression problems, but most methods have seen limited adoption partly due to these algorithms' highly-tailored and model-specific nature. On the other hand, transformers are flexible, neural sequence models that present the intriguing possibility of providing general-purpose prediction methods, even in this mixture setting. In this work, we investigate the hypothesis that transformers can learn an optimal predictor for mixtures of regressions. We construct a generative process for a mixture of linear regressions for which the decision-theoretic optimal procedure is given by data-driven exponential weights on a finite set of parameters. We observe that transformers achieve low mean-squared error on data generated via this process. By probing the transformer's output at inference time, we also show that transformers typically make predictions that are close to the optimal predictor. Our experiments also demonstrate that transformers can learn mixtures of regressions in a sample-efficient fashion and are somewhat robust to distribution shifts. We complement our experimental observations by proving constructively that the decision-theoretic optimal procedure is indeed implementable by a transformer.},
  author = {Reese Pathak and Rajat Sen and Weihao Kong and Abhimanyu Das},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pathak2024transformers.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=sLkj91HIZU},
  publisher = {OpenReview.net},
  title = {Transformers can optimally learn regression mixture models},
  url = {https://openreview.net/forum?id=sLkj91HIZU},
  year = {2024}
}

@inproceedings{patil2024can,
  abstract = {Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover 'deleted' information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe societal implications for real-world deployment of language models.},
  author = {Vaidehi Patil and Peter Hase and Mohit Bansal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/patil2024can.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=7erlRDoaV8},
  publisher = {OpenReview.net},
  title = {Can Sensitive Information Be Deleted From {LLM}s? Objectives for Defending Against Extraction Attacks},
  url = {https://openreview.net/forum?id=7erlRDoaV8},
  year = {2024}
}

@inproceedings{patil2024asymptotically,
  abstract = {We employ random matrix theory to establish consistency of generalized cross validation (GCV) for estimating prediction risks of sketched ridge regression ensembles, enabling efficient and consistent tuning of regularization and sketching parameters. For squared prediction risk, we provide a decomposition into an unsketched equivalent implicit ridge bias and a sketching-based variance, and prove that the risk can be globally optimized by only tuning sketch size in infinite ensembles. For general subquadratic prediction risk functionals, we extend GCV to construct consistent risk estimators, and thereby obtain distributional convergence of the GCV-corrected predictions in Wasserstein-2 metric. We also propose an ensemble trick whereby the risk for unsketched ridge regression can be efficiently estimated via GCV using small sketched ridge ensembles. We empirically validate our theoretical results using both synthetic and real large-scale datasets with practical sketches including CountSketch and subsampled randomized discrete cosine transforms.},
  author = {Pratik Patil and Daniel LeJeune},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/patil2024asymptotically.pdf:pdf},
  keywords = {asymptotic freeness, sketching, ensembles, ridge regression, generalized cross-validation, tuning},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=i9Vs5NGDpk},
  publisher = {OpenReview.net},
  title = {Asymptotically Free Sketched Ridge Ensembles: Risks, Cross-Validation, and Tuning},
  url = {https://openreview.net/forum?id=i9Vs5NGDpk},
  year = {2024}
}

@inproceedings{patil2024intelligent,
  abstract = {In the real world, the strong episode resetting mechanisms that are needed to train agents in simulation are unavailable. The resetting assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions. Recent work aims to train agents (forward) with learned resets by constructing a second (backward) agent that returns the forward agent to the initial state. However, the termination and timing of the transitions between these two agents are crucial for algorithm success. We propose Reset Free RL with Intelligently Switching Controller (RISC), a reset-free RL algorithm that intelligently switches between the two agents based on the agent's confidence in achieving its current goal. Our new method achieves state-of-the-art performance on several challenging environments for reset-free RL.},
  author = {Darshan Patil and Janarthanan Rajendran and Glen Berseth and Sarath Chandar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/patil2024intelligent.pdf:pdf},
  keywords = {reinforcement learning, reset-free RL, intelligent switching, dual agents},
  pdf = {https://openreview.net/pdf?id=Nq45xeghcL},
  publisher = {OpenReview.net},
  title = {Intelligent Switching for Reset-Free {RL}},
  url = {https://openreview.net/forum?id=Nq45xeghcL},
  year = {2024}
}

@inproceedings{patnaik2024cabinet,
  abstract = {Table understanding capability of Large Language Models (LLMs) has been extensively studied through the task of question-answering (QA) over tables. Typically, only a small part of the whole table is relevant to derive the answer for a given question. The irrelevant parts act as noise and are distracting information, resulting in sub-optimal performance due to the vulnerability of LLMs to noise. To mitigate this, we propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) - a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer (URS), trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM).},
  author = {Sohan Patnaik and Heril Changwal and Milan Aggarwal and Sumit Bhatia and Yaman Kumar and Balaji Krishnamurthy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/patnaik2024cabinet.pdf:pdf},
  keywords = {table question answering, noise reduction, relevance scoring, large language models},
  pdf = {https://openreview.net/pdf?id=SQrHpTllXa},
  publisher = {OpenReview.net},
  title = {{CABINET}: Content Relevance-based Noise Reduction for Table Question Answering},
  url = {https://openreview.net/forum?id=SQrHpTllXa},
  year = {2024}
}

@inproceedings{patris2024learning,
  abstract = {We focus on bimatrix games (A,B) called rank-1, which are games in which the sum of the payoff matrices A+B is a rank 1 matrix (note that standard zero-sum games are rank 0). Computing Nash equilibria is a computationally intractable task, and the current state-of-the-art in efficiently learning games focuses on landscapes that meet the (weak) Minty property or games characterized by a unique function, often referred to as potential games. We study the problem of learning Nash equilibria in rank-1 games using optimistic gradient methods.},
  author = {Nikolas Patris and Ioannis Panageas},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/patris2024learning.pdf:pdf},
  keywords = {Nash equilibria, rank-1 games, bimatrix games, optimistic gradient, game theory},
  pdf = {https://openreview.net/pdf?id=8utTlmhw8v},
  publisher = {OpenReview.net},
  title = {Learning Nash Equilibria in Rank-1 Games},
  url = {https://openreview.net/forum?id=8utTlmhw8v},
  year = {2024}
}

@inproceedings{paul2024simple,
  abstract = {We present INTR, a simple and interpretable Transformer for fine-grained image classification and analysis. Unlike mainstream classifiers that wait until the last fully connected layer to incorporate class information to make predictions, we investigate a proactive approach, asking each class to search for itself in an image. This is realized via a Transformer encoder-decoder inspired by DETR, where we learn class-specific queries (one for each class) as input to the decoder, enabling each class to localize its patterns in an image via cross-attention. INTR intrinsically encourages each class to attend distinctively; the cross-attention weights thus provide a faithful interpretation of the prediction. Via multi-head cross-attention, INTR can identify different attributes of a class, making it particularly suitable for fine-grained classification and analysis.},
  author = {Dipanjyoti Paul and Arpita Chowdhury and Xinqi Xiong and Feng-Ju Chang and David Edward Carlyn and Samuel Stevens and Kaiya Provost and Anuj Karpatne and Bryan Carstens and Daniel I. Rubenstein and Charles V. Stewart and Tanya Y. Berger-Wolf and Yu Su and Wei-Lun Chao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/paul2024simple.pdf:pdf},
  keywords = {interpretable transformers, fine-grained classification, cross-attention, class-specific queries},
  pdf = {https://openreview.net/pdf?id=bkdWThqE6q},
  publisher = {OpenReview.net},
  title = {A Simple Interpretable Transformer for Fine-Grained Image Classification and Analysis},
  url = {https://openreview.net/forum?id=bkdWThqE6q},
  year = {2024}
}

@inproceedings{pauli2024novel,
  abstract = {Recently, semidefinite programming (SDP) techniques have shown great promise in providing accurate Lipschitz bounds for neural networks. Specifically, the LipSDP approach has received much attention and provides the least conservative Lipschitz upper bounds that can be computed with polynomial time guarantees. However, one main restriction of LipSDP is that its formulation requires the activation functions to be slope-restricted on [0,1], preventing its further use for more general activation functions such as GroupSort, MaxMin, and Householder. Our paper bridges this gap and extends LipSDP beyond slope-restricted activation functions. To this end, we provide novel quadratic constraints for GroupSort, MaxMin, and Householder activations via leveraging their underlying properties such as sum preservation. Our proposed analysis is general and provides a unified approach for estimating ‚Ñì‚ÇÇ and ‚Ñì‚àû Lipschitz bounds for a rich class of neural network architectures.},
  author = {Patricia Pauli and Aaron J. Havens and Alexandre Araujo and Siddharth Garg and Farshad Khorrami and Frank Allg{\"o}wer and Bin Hu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pauli2024novel.pdf:pdf},
  keywords = {Lipschitz bounds, semidefinite programming, neural networks, activation functions, quadratic constraints},
  pdf = {https://openreview.net/pdf?id=HfXDrAzFvG},
  publisher = {OpenReview.net},
  title = {Novel Quadratic Constraints for Extending {LipSDP} beyond Slope-Restricted Activations},
  url = {https://openreview.net/forum?id=HfXDrAzFvG},
  year = {2024}
}

@inproceedings{peach2024implicit,
  abstract = {We introduce RVGP, a generalisation of Gaussian processes for learning vector signals over latent Riemannian manifolds. The method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. Previous approaches assume that the manifold underlying the data is known, limiting their practical utility. This work generalises GPs to vector fields on arbitrary latent manifolds, which can only be approximated based on local similarities between data points, making them applicable to real-world datasets. RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities.},
  author = {Robert L. Peach and Matteo Vinao-Carl and Nir Grossman and Michael David and Emma Mallas and David J. Sharp and Paresh A. Malhotra and Pierre Vandergheynst and Adam Gosztolai},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/peach2024implicit.pdf:pdf},
  keywords = {Gaussian processes, vector fields, Riemannian manifolds, connection Laplacian, positional encoding},
  pdf = {https://openreview.net/pdf?id=YEPlTU5mZC},
  publisher = {OpenReview.net},
  title = {Implicit Gaussian process representation of vector fields over arbitrary latent manifolds},
  url = {https://openreview.net/forum?id=YEPlTU5mZC},
  year = {2024}
}

@inproceedings{pedramfar2024unified,
  abstract = {This paper introduces unified projection-free Frank-Wolfe type algorithms for adversarial continuous DR-submodular optimization, spanning scenarios such as full information and (semi-)bandit feedback, monotone and non-monotone functions, different constraints, and types of stochastic queries. For every problem considered in the non-monotone setting, the proposed algorithms are either the first with proven sub-linear Œ±-regret bounds or have better Œ±-regret bounds than the state of the art, where Œ± is a corresponding approximation bound in the offline setting. In the monotone setting, the proposed approach gives state-of-the-art sub-linear Œ±-regret bounds among projection-free algorithms in 7 of the 8 considered. Additionally, this paper addresses semi-bandit and bandit feedback for adversarial DR-submodular optimization, advancing the understanding of this optimization area.},
  author = {Mohammad Pedramfar and Yididiya Y. Nadew and Christopher John Quinn and Vaneet Aggarwal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pedramfar2024unified.pdf:pdf},
  keywords = {stochastic optimization, submodular maximization, Frank-Wolfe algorithm, adversarial optimization},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=H4A9e8HvIn},
  publisher = {OpenReview.net},
  title = {Unified Projection-Free Algorithms for Adversarial {DR}-Submodular Optimization},
  url = {https://openreview.net/forum?id=H4A9e8HvIn},
  year = {2024}
}

@inproceedings{pegg2024rtfsnet,
  abstract = {Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the prior SOTA method in both inference speed and separation quality while reducing the number of parameters by 90% and MACs by 83%. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.},
  author = {Samuel Pegg and Kai Li and Xiaolin Hu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pegg2024rtfsnet.pdf:pdf},
  keywords = {audio-visual speech separation, time-frequency domain, recurrent neural networks, attention mechanism, computational efficiency},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=PEuDO2EiDr},
  publisher = {OpenReview.net},
  title = {{RTFS-Net}: Recurrent Time-Frequency Modelling for Efficient Audio-Visual Speech Separation},
  url = {https://openreview.net/forum?id=PEuDO2EiDr},
  year = {2024}
}

@inproceedings{pei2024neural,
  abstract = {With the increasing number of new neural architecture designs and substantial existing neural architectures, it becomes difficult for the researchers to situate their contributions compared with existing neural architectures or establish the connections between their designs and other relevant ones. To discover similar neural architectures in an efficient and automatic manner, we define a new problem Neural Architecture Retrieval which retrieves a set of existing neural architectures which have similar designs to the query neural architecture. We divide computational graphs into motifs and use multi-level contrastive learning to improve graph representations for this retrieval task. Experimental results demonstrate the effectiveness of our approach in discovering architecturally similar networks and its potential applications in neural architecture design and understanding.},
  author = {Xiaohuan Pei and Yanxi Li and Minjing Dong and Chang Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pei2024neural.pdf:pdf},
  keywords = {neural architecture search, information retrieval, graph representation learning, contrastive learning},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=1JtTPYBKqt},
  publisher = {OpenReview.net},
  title = {Neural Architecture Retrieval},
  url = {https://openreview.net/forum?id=1JtTPYBKqt},
  year = {2024}
}

@inproceedings{pei2024image,
  abstract = {Out-of-distribution (OOD) detection empowers the model trained on the closed image set to identify unknown data in the open world. Though many prior techniques have yielded considerable improvements in this research direction, two crucial obstacles still remain. Firstly, a unified perspective has yet to be presented to view the developed arts with individual designs, which is vital for providing insights into future work. Secondly, we expect sufficient natural OOD supervision to promote the generation of compact boundaries between the in-distribution (ID) and OOD data without collecting explicit OOD samples. To tackle these issues, we propose a general probabilistic framework to interpret many existing methods and an OOD-data-free model, namely Self-supervised Sampling for OOD Detection (SSOD). SSOD efficiently exploits natural OOD signals from the ID data based on the local property of convolution. With these supervisions, it jointly optimizes the OOD detection and conventional ID classification in an end-to-end manner. Extensive experiments reveal that SSOD establishes competitive state-of-the-art performance on many large-scale benchmarks, outperforming the best previous method by a large margin, e.g., reporting -6.28\% FPR95 and +0.77\% AUROC on ImageNet, -19.01\% FPR95 and +3.04\% AUROC on CIFAR-10, and top-ranked performance on hard OOD datasets, i.e., ImageNet-O and OpenImage-O.},
  author = {Sen Pei},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/pei2024image.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2307.00519.pdf},
  publisher = {OpenReview.net},
  title = {Image Background Serves as Good Proxy for Out-of-distribution Data},
  url = {https://openreview.net/forum?id=ym0ubZrsmm},
  year = {2024}
}

@inproceedings{peng2024kosmos2,
  abstract = {We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent text spans (i.e., referring expressions and noun phrases) as links in Markdown, i.e., [text span](bounding boxes), where object descriptions are sequences of location tokens. To train the model, we construct a large-scale dataset about grounded image-text pairs (GrIT) together with multimodal corpora. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability to downstream applications. Kosmos-2 is evaluated on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This study sheds a light on the big convergence of language, multimodal perception, and world modeling, which is a key step toward artificial general intelligence.},
  author = {Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Qixiang Ye and Furu Wei},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/peng2024kosmos2.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lLmqxkfSIw},
  publisher = {OpenReview.net},
  title = {Kosmos-2: Grounding Multimodal Large Language Models to the World},
  url = {https://openreview.net/forum?id=lLmqxkfSIw},
  year = {2024}
}

@inproceedings{peng2024conjnorm,
  abstract = {Post-hoc out-of-distribution (OOD) detection has garnered intensive attention in reliable machine learning. Many efforts have been dedicated to deriving score functions based on logits, distances, or rigorous data distribution assumptions to identify low-scoring OOD samples. Nevertheless, these estimate scores may fail to accurately reflect the true data density or impose impractical constraints. To provide a unified perspective on density-based score design, we propose a novel theoretical framework grounded in Bregman divergence, which extends distribution considerations to encompass an exponential family of distributions. Leveraging the conjugation constraint revealed in our theorem, we introduce a ConjNorm method, reframing density function design as a search for the optimal norm coefficient $p$ against the given dataset. In light of the computational challenges of normalization, we devise an unbiased and analytically tractable estimator of the partition function using the Monte Carlo-based importance sampling technique. Extensive experiments across OOD detection benchmarks empirically demonstrate that our proposed ConjNorm has established a new state-of-the-art in a variety of OOD detection setups, outperforming the current best method by up to 13.25\% and 28.19\% (FPR95) on CIFAR-100 and ImageNet-1K, respectively.},
  author = {Bo Peng and Yadan Luo and Yonggang Zhang and Yixuan Li and Zhen Fang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/peng2024conjnorm.pdf:pdf},
  note = {DBLP last modified: 2025-01-31},
  pdf = {https://openreview.net/pdf?id=1pSL2cXWoz},
  publisher = {OpenReview.net},
  title = {{ConjNorm}: Tractable Density Estimation for Out-of-Distribution Detection},
  url = {https://openreview.net/forum?id=1pSL2cXWoz},
  year = {2024}
}

@inproceedings{peng2024yarn,
  abstract = {Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset.},
  author = {Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/peng2024yarn.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wHBfxhZu1u},
  publisher = {OpenReview.net},
  title = {{YaRN}: Efficient Context Window Extension of Large Language Models},
  url = {https://openreview.net/forum?id=wHBfxhZu1u},
  year = {2024}
}

@inproceedings{peng2024swapnas,
  abstract = {Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid resource-intensive neural network training, especially in Neural Architecture Search (NAS). Recent studies show that existing training-free metrics have several limitations, such as limited correlation and poor generalisation across different search spaces and tasks. Hence, we propose Sample-Wise Activation Patterns and its derivative, SWAP-Score, a novel high-performance training-free metric. It measures the expressivity of networks over a batch of input samples. The SWAP-Score is strongly correlated with ground-truth performance across various search spaces and tasks, outperforming 15 existing training-free metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101.},
  author = {Yameng Peng and Andy Song and Haytham M. Fayek and Vic Ciesielski and Xiaojun Chang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/peng2024swapnas.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=tveiUXU2aa},
  publisher = {OpenReview.net},
  title = {{SWAP-NAS}: Sample-Wise Activation Patterns for Ultra-fast {NAS}},
  url = {https://openreview.net/forum?id=tveiUXU2aa},
  year = {2024}
}

@inproceedings{peng2024learning,
  abstract = {We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications. We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.},
  author = {Andi Peng and Ilia Sucholutsky and Belinda Z. Li and Theodore R. Sumers and Thomas L. Griffiths and Jacob Andreas and Julie Shah},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/peng2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qi5Xa2cOZg},
  publisher = {OpenReview.net},
  title = {Learning with Language-Guided State Abstractions},
  url = {https://openreview.net/forum?id=qi5Xa2cOZg},
  year = {2024}
}

@inproceedings{peng2024scalable,
  abstract = {Continual learning has gained increasing importance as it facilitates the acquisition and refinement of scalable knowledge and skills in language models. However, existing methods typically encounter strict limitations and challenges in real-world scenarios, such as reliance on experience replay, optimization constraints, and inference task-ID. In this study, we introduce the Scalable Language Model (SLM) to overcome these limitations within a more challenging and generalized setting, representing a significant advancement toward practical applications for continual learning. Specifically, we propose the Joint Adaptive Re-Parameterization (JARe), integrated with Dynamic Task-related Knowledge Retrieval (DTKR), to enable adaptive adjustment of language models based on specific downstream tasks.},
  author = {Bohao Peng and Zhuotao Tian and Shu Liu and Ming-Chang Yang and Jiaya Jia},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/peng2024scalable.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mz8owj4DXu},
  publisher = {OpenReview.net},
  title = {Scalable Language Model with Generalized Continual Learning},
  url = {https://openreview.net/forum?id=mz8owj4DXu},
  year = {2024}
}

@inproceedings{peng2024energybased,
  abstract = {The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real-world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive experiments across modalities, datasets and different architectural backbones to validate MDE's validity, together with its superiority compared with prior approaches. We also prove MDE's versatility by showing its seamless integration with large-scale models, and easy adaption to learning scenarios with noisy- or imbalanced- labels.},
  author = {Ru Peng and Heming Zou and Haobo Wang and Yawen Zeng and Zenan Huang and Junbo Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/peng2024energybased.pdf:pdf},
  note = {DBLP last modified: 2025-05-22},
  pdf = {https://openreview.net/pdf?id=CHGcP6lVWd},
  publisher = {OpenReview.net},
  title = {Energy-based Automated Model Evaluation},
  url = {https://openreview.net/forum?id=CHGcP6lVWd},
  year = {2024}
}

@inproceedings{pernias2024wu,
  abstract = {We introduce W\"{u}rstchen, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models. A key contribution of our work is to develop a latent diffusion technique in which we learn a detailed but extremely compact semantic image representation used to guide the diffusion process. This highly compressed representation of an image provides much more detailed guidance compared to latent representations of language and this significantly reduces the computational requirements to achieve state-of-the-art results. Our approach also improves the quality of text-conditioned image generation based on our user preference study. The training requirements of our approach consists of 24,602 A100-GPU hours - compared to Stable Diffusion 2.1's 200,000 GPU hours.},
  author = {Pablo Pernias and Dominic Rampas and Mats L. Richter and Christopher Pal and Marc Aubreville},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/pernias2024wu.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=gU58d5QeGv},
  publisher = {OpenReview.net},
  title = {W\"{u}rstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models},
  url = {https://openreview.net/forum?id=gU58d5QeGv},
  year = {2024}
}

@inproceedings{petersen2024uncertainty,
  abstract = {We propose a new approach for propagating stable probability distributions through neural networks. The method is based on local linearization, which we show to be an optimal approximation in terms of total variation distance for the ReLU non-linearity. This allows propagating Gaussian and Cauchy input uncertainties through neural networks to quantify their output uncertainties. To demonstrate the utility of propagating distributions, we apply the proposed method to predicting calibrated confidence intervals and selective prediction on out-of-distribution data.},
  author = {Felix Petersen and Aashwin Ananda Mishra and Hilde Kuehne and Christian Borgelt and Oliver Deussen and Mikhail Yurochkin},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/petersen2024uncertainty.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cZttUMTiPL},
  publisher = {OpenReview.net},
  title = {Uncertainty Quantification via Stable Distribution Propagation},
  url = {https://openreview.net/forum?id=cZttUMTiPL},
  year = {2024}
}

@inproceedings{petrov2024when,
  abstract = {Context-based fine-tuning methods, including prompting, in-context learning, soft prompting (also known as prompt tuning), and prefix-tuning, have gained popularity due to their ability to often match the performance of full fine-tuning with a fraction of the parameters. Despite their empirical successes, there is little theoretical understanding of how these techniques influence the internal computation of the model and their expressiveness limitations. We show that despite the continuous embedding space being more expressive than the discrete token space, soft-prompting and prefix-tuning are potentially less expressive than full fine-tuning, even with the same number of learnable parameters. Concretely, context-based fine-tuning cannot change the relative attention pattern over the content and can only bias the outputs of an attention layer in a fixed direction. This suggests that while techniques like prompting, in-context learning, soft prompting, and prefix-tuning can effectively elicit skills present in the pretrained model, they may not be able to learn novel tasks that require new attention patterns.},
  author = {Aleksandar Petrov and Philip Torr and Adel Bibi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/petrov2024when.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JewzobRhay},
  publisher = {OpenReview.net},
  title = {When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and Limitations},
  url = {https://openreview.net/forum?id=JewzobRhay},
  year = {2024}
}

@inproceedings{philion2024trajeglish,
  abstract = {A longstanding challenge for self-driving development is simulating dynamic driving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles, pedestrians and cyclists interact in driving scenarios. Using a simple data-driven tokenization scheme, we discretize trajectories to centimeter-level resolution using a small vocabulary. We then model the multi-agent sequence of discrete motion tokens with a GPT-like encoder-decoder that is autoregressive in time and takes into account intra-timestep interaction between agents. Scenarios sampled from our model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents Benchmark, surpassing prior work along the realism meta metric by 3.3% and along the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned by our model can quickly be adapted to improve performance on nuScenes. We additionally evaluate the scalability of our model with respect to parameter count and dataset size, and use density estimates from our model to quantify the saliency of context length and intra-timestep interaction for the traffic modeling task.},
  author = {Jonah Philion and Xue Bin Peng and Sanja Fidler},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/philion2024trajeglish.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf/6a1754e5443464338e361380403b4fec2f5d94b2.pdf},
  publisher = {OpenReview.net},
  title = {Trajeglish: Traffic Modeling as Next-Token Prediction},
  url = {https://openreview.net/forum?id=Z59Rb5bPPP},
  year = {2024}
}

@inproceedings{pi2024insdetclip,
  abstract = {This paper introduces Instruction-oriented Object Detection (IOD), a new task that enhances human-computer interaction by enabling object detectors to understand user instructions and locate relevant objects. Unlike traditional open-vocabulary object detection tasks that rely on users providing a list of required category names, IOD requires models to comprehend natural-language instructions, contextual reasoning, and output the name and location of the desired categories. This poses fresh challenges for modern object detection systems. To develop an IOD system, we create a dataset called IOD-Bench, which consists of instruction-guided detections, along with specialized evaluation metrics. We leverage large-scale language models (LLMs) to generate a diverse set of instructions (8k+) based on existing public object detection datasets, covering a wide range of real-world scenarios. As an initial approach to the IOD task, we propose a model called Ins-DetCLIP. It harnesses the extensive knowledge within LLMs to empower the detector with instruction-following capabilities. Specifically, our Ins-DetCLIP employs a visual encoder (i.e., DetCLIP, an open-vocabulary detector) to extract object-level features. These features are then aligned with the input instructions using a cross-modal fusion module integrated into a pre-trained LLM.},
  author = {Renjie Pi and Lewei Yao and Jianhua Han and Xiaodan Liang and Wei Zhang and Hang Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pi2024insdetclip.pdf:pdf},
  note = {DBLP last modified: 2024-12-10},
  pdf = {https://openreview.net/pdf?id=M0MF4t3hE9},
  publisher = {OpenReview.net},
  title = {Ins-DetCLIP: Aligning Detection Model to Follow Human-Language Instruction},
  url = {https://openreview.net/forum?id=M0MF4t3hE9},
  year = {2024}
}

@inproceedings{pinedaarango2024quicktune,
  abstract = {With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a multi-fidelity performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.},
  author = {Sebastian Pineda-Arango and Fabio Ferreira and Arlind Kadra and Frank Hutter and Josif Grabocka},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pinedaarango2024quicktune.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=tqh1zdXIra},
  publisher = {OpenReview.net},
  title = {Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How},
  url = {https://openreview.net/forum?id=tqh1zdXIra},
  year = {2024}
}

@inproceedings{pinilla2024global,
  abstract = {Signal restoration is an important constrained optimization problem with significant applications in various domains. Although non-convex constrained optimization problems have been shown to perform better than convex counterparts in terms of reconstruction quality, convex constrained optimization problems have been preferably for its global optima guarantees. Despite the success of non-convex methods in a large number of applications, it is not an overstatement to say that there is little or no hope for non-convex problems to ensure global optima. In this paper, for the first time, we develop invex constrained optimization theory to mitigate the loss of guarantees for global optima in non-convex constrained inverse problems. We also develop relevant theories to extend the global optima guarantee to a set of quasi-invex functions - the largest optimizable mappings. More specifically, we propose a family of invex/quasi-invex of functions for handling constrained inverse problems using the non-convex setting along with guarantees for their global optima. Our experimental evaluation shows that the proposed approach is very promising and can aid in extending existing convex optimization algorithms, such as the alternating direction method of multipliers, and accelerated proximal gradient methods.},
  author = {Samuel Pinilla and Jeyan Thiyagalingam},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pinilla2024global.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fyTPWfXtcc},
  publisher = {OpenReview.net},
  title = {Global Optimality for Non-linear Constrained Restoration Problems via Invexity},
  url = {https://openreview.net/forum?id=fyTPWfXtcc},
  year = {2024}
}

@inproceedings{piquenot2024g2n2,
  abstract = {This paper introduces a framework for formally establishing a connection between a portion of an algebraic language and a Graph Neural Network (GNN). The framework leverages Context-Free Grammars (CFG) to organize algebraic operations into generative rules that can be translated into a GNN layer model. As CFGs derived directly from a language tend to contain redundancies in their rules and variables, we present a grammar reduction scheme. By applying this strategy, we define a CFG that conforms to the third-order Weisfeiler-Lehman (3-WL) test using MATLANG. From this 3-WL CFG, we derive a GNN model, named G2N2, which is provably 3-WL compliant. Through various experiments, we demonstrate the superior efficiency of G2N2 compared to other 3-WL GNNs across numerous downstream tasks. Specifically, one experiment highlights the benefits of grammar reduction within our framework.},
  author = {Jason Piquenot and Aldo Moscatelli and Maxime B√©rar and Pierre H√©roux and Romain Raveaux and Jean-Yves Ramel and S√©bastien Adam},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/piquenot2024g2n2.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eZneJ55mRO},
  publisher = {OpenReview.net},
  title = {{G2N2}: {W}eisfeiler and {L}ehman go grammatical},
  url = {https://openreview.net/forum?id=eZneJ55mRO},
  year = {2024}
}

@inproceedings{pirotta2024fast,
  abstract = {Imitation learning (IL) aims at producing agents that can imitate any behavior given a few expert demonstrations. Yet existing approaches require many demonstrations and/or running (online or offline) reinforcement learning (RL) algorithms for each new imitation task. Here we show that recent RL foundation models based on successor measures can imitate any expert behavior almost instantly with just a few demonstrations and no need for RL or fine-tuning, while accommodating several IL principles (behavioral cloning, feature matching, reward-based, and goal-based reductions). In our experiments, imitation via RL foundation models matches, and often surpasses, the performance of SOTA offline IL algorithms, and produces imitation policies from new demonstrations within seconds instead of hours.},
  author = {Matteo Pirotta and Andrea Tirinzoni and Ahmed Touati and Alessandro Lazaric and Yann Ollivier},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pirotta2024fast.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=qnWtw3l0jb},
  publisher = {OpenReview.net},
  title = {Fast Imitation via Behavior Foundation Models},
  url = {https://openreview.net/forum?id=qnWtw3l0jb},
  year = {2024}
}

@inproceedings{podell2024sdxl,
  abstract = {We present Stable Diffusion XL (SDXL), a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone, achieved by significantly increasing the number of attention blocks and including a second text encoder. Further, we design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. To ensure highest quality results, we also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators.},
  author = {Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas M√ºller and Joe Penna and Robin Rombach},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/podell2024sdxl.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=di52zR8xgf},
  publisher = {OpenReview.net},
  title = {{SDXL}: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
  url = {https://openreview.net/forum?id=di52zR8xgf},
  year = {2024}
}

@inproceedings{pogodin2024synaptic,
  abstract = {A growing literature in computational neuroscience leverages gradient descent and learning algorithms that approximate it to study synaptic plasticity in the brain. However, the vast majority of this work ignores a critical underlying assumption: the choice of distance for synaptic changes - i.e. the geometry of synaptic plasticity. Gradient descent assumes that the distance is Euclidean, but many other distances are possible, and there is no reason that biology necessarily uses Euclidean geometry. Here, using the theoretical tools provided by mirror descent, we show that the distribution of synaptic weights will depend on the geometry of synaptic plasticity. We use these results to show that experimentally-observed log-normal weight distributions found in several brain areas are not consistent with standard gradient descent (i.e. a Euclidean geometry), but rather with non-Euclidean distances. Finally, we show that it should be possible to experimentally test for different synaptic geometries by comparing synaptic weight distributions before and after learning. Overall, our work shows that the current paradigm in theoretical work on synaptic plasticity that assumes Euclidean synaptic geometry may be misguided and that it should be possible to experimentally determine the true geometry of synaptic plasticity in the brain.},
  author = {Roman Pogodin and Jonathan Cornford and Arna Ghosh and Gauthier Gidel and Guillaume Lajoie and Blake Aaron Richards},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pogodin2024synaptic.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=x5txICnnjC},
  publisher = {OpenReview.net},
  title = {Synaptic Weight Distributions Depend on the Geometry of Plasticity},
  url = {https://openreview.net/forum?id=x5txICnnjC},
  year = {2024}
}

@inproceedings{pospisil2024estimating,
  abstract = {Measuring geometric similarity between high-dimensional network representations is a topic of longstanding interest to neuroscience and deep learning. Although many methods have been proposed, only a few works have rigorously analyzed their statistical efficiency or quantified estimator uncertainty in data-limited regimes. Here, we derive upper and lower bounds on the worst-case convergence of standard estimators of shape distance‚Äîa measure of representational dissimilarity proposed by Williams et al. (2021). These bounds reveal the challenging nature of the problem in high-dimensional feature spaces. To overcome these challenges, we introduce a new method-of-moments estimator with a tunable bias-variance tradeoff.},
  author = {Dean A. Pospisil and Brett W. Larsen and Sarah E. Harvey and Alex H. Williams},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/pospisil2024estimating.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kvByNnMERu},
  publisher = {OpenReview.net},
  title = {Estimating Shape Distances on Neural Representations with Limited Samples},
  url = {https://openreview.net/forum?id=kvByNnMERu},
  year = {2024}
}

@inproceedings{poulain2024graph,
  abstract = {This paper explores the integration of transformer-based methods and graph-based approaches for healthcare predictive tasks using electronic health records ({EHR}s). While both transformers and graph methods have shown effectiveness in their respective domains, integrating these two approaches is challenging due to {EHR}-specific problems like high sparsity that make extracting meaningful temporal representations of medical visits difficult. The paper proposes Graph Transformers that address these challenges to improve downstream performance in healthcare prediction tasks.},
  author = {Raphael Poulain and Rahmatollah Beheshti},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/poulain2024graph.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pe0Vdv7rsL},
  publisher = {OpenReview.net},
  title = {Graph Transformers on {EHR}s: Better Representation Improves Downstream Performance},
  url = {https://openreview.net/forum?id=pe0Vdv7rsL},
  year = {2024}
}

@inproceedings{poulakakisdaktylidis2024beclr,
  abstract = {This paper addresses unsupervised few-shot learning ({U-FSL}), which aims to enable quick learning from very few labeled samples without relying on annotations at training time. The paper introduces {BECLR} with two key contributions: a Dynamic Clustered m{E}mory ({DyCE}) module that promotes highly separable latent representation space for enhancing positive sampling at the pretraining phase, and an Optimal Transport-based distribution {A}lignment ({OpTA}) strategy that efficiently addresses sample bias problems in low-shot scenarios. {BECLR} achieves state-of-the-art performance across all existing {U-FSL} benchmarks.},
  author = {Stylianos Poulakakis-Daktylidis and Hadi Jamali-Rad},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/poulakakisdaktylidis2024beclr.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=k9SVcrmXL8},
  publisher = {OpenReview.net},
  title = {{BECLR}: Batch Enhanced Contrastive Few-Shot Learning},
  url = {https://openreview.net/forum?id=k9SVcrmXL8},
  year = {2024}
}

@inproceedings{prajapat2024submodular,
  abstract = {Traditional reinforcement learning assumes that rewards of states are additive and independent of previously visited states, following the {M}arkov assumption. However, in many important applications such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, where their value decreases in light of similar states visited previously. This paper introduces {SubRL}, a paradigm for policy optimization under submodular reward functions that seeks to optimize more general, non-additive and history-dependent rewards modelled via submodular set functions. The authors propose {SubPO}, a policy gradient-based algorithm inspired by submodular optimization's greedy strategy to handle the computational challenges.},
  author = {Manish Prajapat and Mojmir Mutny and Melanie N. Zeilinger and Andreas Krause},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/prajapat2024submodular.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=loYSzjSaAK},
  publisher = {OpenReview.net},
  title = {Submodular Reinforcement Learning},
  url = {https://openreview.net/forum?id=loYSzjSaAK},
  year = {2024}
}

@inproceedings{prakash2024finetuning,
  abstract = {This paper investigates how fine-tuning on tasks like instruction following, code generation, and mathematics enhances language models' performance, but explanations of how such fine-tuning influences internal computations remain elusive. As a case study, the authors explore entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics show substantial performance gains. They identify a mechanism that enables entity tracking and show that both the original model and its fine-tuned version implement entity tracking with the same circuit. This mechanistic interpretability study examines how fine-tuning preserves and enhances existing computational mechanisms rather than creating entirely new ones.},
  author = {Nikhil Prakash and Tamar Rott Shaham and Tal Haklay and Yonatan Belinkov and David Bau},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/prakash2024finetuning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8sKcAWOf2D},
  publisher = {OpenReview.net},
  title = {Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking},
  url = {https://openreview.net/forum?id=8sKcAWOf2D},
  year = {2024}
}

@inproceedings{prasad2024rephrase,
  abstract = {This paper addresses how large vision-language models ({LVLM}s) can handle vision-language tasks in zero and few-shot settings by combining large language models with vision encoders. The authors note that how input is presented to an {LVLM} significantly impacts zero-shot performance. They identify that underspecified inputs can lead to incorrect answers due to missing visual information, complex implicit reasoning, or linguistic ambiguity. Their approach involves adding visually-grounded information to inputs as preemptive clarification to improve model performance by reducing underspecification through object localization and reference disambiguation.},
  author = {Archiki Prasad and Elias Stengel-Eskin and Mohit Bansal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/prasad2024rephrase.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=L4nOxziGf9},
  publisher = {OpenReview.net},
  title = {Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models},
  url = {https://openreview.net/forum?id=L4nOxziGf9},
  year = {2024}
}

@inproceedings{price2024deep,
  abstract = {This paper uses the large width {G}aussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for two of the most natural candidates for hidden layer sparsification: shifted {ReLU} and soft thresholding. The authors show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated {G}aussian process variance map. Numerical experiments verify the theory and show that the proposed magnitude clipped sparsifying activations can be trained with training and test fractional sparsity as high as 85\% while retaining close to full accuracy.},
  author = {Ilan Price and Nicholas Daultry Ball and Adam C. Jones and Samuel C. H. Lam and Jared Tanner},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/price2024deep.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uvXK8Xk9Jk},
  publisher = {OpenReview.net},
  title = {Deep Neural Network Initialization with Sparsity Inducing Activations},
  url = {https://openreview.net/forum?id=uvXK8Xk9Jk},
  year = {2024}
}

@inproceedings{prince2024manipulating,
  abstract = {This paper develops a family of object recognition models with parametrically varying dropout proportion p, which induces systematically varying dimensionality of internal responses to study the tradeoff between efficiency and robustness in neural coding. Increasing dropout produces an increasingly smooth, low-dimensional representational space. Optimal robustness to lesioning is observed at around 70\% dropout, which is associated with maximal emergent neural predictivity when compared to 7T {fMRI} data from human occipitotemporal cortex. The research investigates how dropout manipulation in neural networks can reveal optimal balance points between computational efficiency and robustness, with validation against human brain imaging data from the Natural Scenes Dataset.},
  author = {Jacob S. Prince and Gabriel Fajardo and George A. Alvarez and Talia Konkle},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/prince2024manipulating.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ADDCErFzev},
  publisher = {OpenReview.net},
  title = {Manipulating dropout reveals an optimal balance of efficiency and robustness in biological and machine visual systems},
  url = {https://openreview.net/forum?id=ADDCErFzev},
  year = {2024}
}

@inproceedings{puig2024habitat,
  abstract = {Habitat 3.0 is a simulation platform for studying collaborative human-robot tasks in home environments. The platform offers three main contributions: (1) accurate humanoid simulation that addresses challenges in modeling complex deformable bodies and diversity in appearance and motion while ensuring high simulation speed, (2) human-in-the-loop infrastructure that enables real human interaction with simulated robots via mouse/keyboard or a {VR} interface, facilitating evaluation of robot policies with human input, and (3) collaborative tasks for studying human-robot interaction in realistic home settings.},
  author = {Xavier Puig and Eric Undersander and Andrew Szot and Mikael Dallaire-Cote and Tsung-Yen Yang and Ruslan Partsey and Ruta Desai and Alexander Clegg and Michal Hlavac and So Yeon Min and Vladimir Vondrus and Theophile Gervet and Vincent-Pierre Berges and John M. Turner and Oleksandr Maksymets and Zsolt Kira and Mrinal Kalakrishnan and Jitendra Malik and Devendra Singh Chaplot and Unnat Jain and Dhruv Batra and Akshara Rai and Roozbeh Mottaghi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/puig2024habitat.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4znwzG92CE},
  publisher = {OpenReview.net},
  title = {Habitat 3.0: A Co-Habitat for Humans, Avatars, and Robots},
  url = {https://openreview.net/forum?id=4znwzG92CE},
  year = {2024}
}

@inproceedings{puigcerver2024sparse,
  abstract = {Sparse mixture of expert architectures ({MoE}s) scale model capacity without significant increases in training or inference costs. Despite their success, {MoE}s suffer from a number of issues: training instability, token dropping, inability to scale the number of experts, or ineffective finetuning. In this work, the authors propose Soft {MoE}, a fully-differentiable sparse Transformer that addresses these challenges, while maintaining the benefits of {MoE}s. Soft {MoE} performs an implicit soft assignment by passing different weighted combinations of all input tokens to each expert.},
  author = {Joan Puigcerver and Carlos Riquelme-Ruiz and Basil Mustafa and Neil Houlsby},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/puigcerver2024sparse.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=jxpsAj7ltE},
  publisher = {OpenReview.net},
  title = {From Sparse to Soft Mixtures of Experts},
  url = {https://openreview.net/forum?id=jxpsAj7ltE},
  year = {2024}
}

@inproceedings{qi2024finetuning,
  abstract = {This paper explores how existing safety alignment techniques restrict harmful behaviors of Large Language Models ({LLM}s) at inference time, but do not cover safety risks when fine-tuning privileges are extended to end-users. The authors demonstrate that the safety alignment of {LLM}s can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, they jailbreak {GPT}-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than \$0.20 via {OpenAI}'s {API}s. The research reveals that fine-tuning aligned Large Language Models introduces new safety risks that current alignment infrastructures fall short of addressing.},
  author = {Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qi2024finetuning.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=hTEGyKf0dZ},
  publisher = {OpenReview.net},
  title = {Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!},
  url = {https://openreview.net/forum?id=hTEGyKf0dZ},
  year = {2024}
}

@inproceedings{qi2024civrealm,
  abstract = {The generalization of decision-making agents encompasses two fundamental elements: learning from past experiences and reasoning in novel contexts. However, the predominant emphasis in most interactive environments is on learning, often at the expense of complexity in reasoning. In this paper, we introduce {CivRealm}, an environment inspired by the {Civilization} game. {Civilization's} profound alignment with human society requires sophisticated learning and prior knowledge, while its ever-changing space and action space demand robust reasoning for generalization. Particularly, {CivRealm} sets up an imperfect-information general-sum game with a changing number of players; it presents a plethora of complex features, challenging the agent to deal with open-ended stochastic environments that require diplomacy and negotiation skills. Within {CivRealm}, we provide interfaces for two typical agent types: tensor-based agents that focus on learning, and language-based agents that emphasize reasoning. To catalyze further research, we present initial results for both paradigms. The canonical {RL}-based agents exhibit reasonable performance in mini-games, whereas both {RL}- and {LLM}-based agents struggle to make substantial progress in the full game. Overall, {CivRealm} stands as a unique learning and reasoning challenge for decision-making agents. The code is available at https://github.com/bigai-ai/civrealm.},
  author = {Siyuan Qi and Shuo Chen and Yexin Li and Xiangyu Kong and Junqi Wang and Bangcheng Yang and Pring Wong and Yifan Zhong and Xiaoyuan Zhang and Zhaowei Zhang and Nian Liu and Yaodong Yang and Song-Chun Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qi2024civrealm.pdf:pdf},
  note = {{ICLR} 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=UBVNwD3hPN},
  publisher = {OpenReview.net},
  title = {{CivRealm}: A Learning and Reasoning Odyssey in {Civilization} for Decision-Making Agents},
  url = {https://openreview.net/forum?id=UBVNwD3hPN},
  year = {2024}
}

@inproceedings{qi2024zero,
  abstract = {Pipeline parallelism is one of the key components for large-scale distributed training, yet its efficiency suffers from pipeline bubbles which were deemed inevitable. In this work, we introduce a scheduling strategy that, to our knowledge, is the first to successfully achieve zero pipeline bubbles under synchronous training semantics. The key idea behind this improvement is to split the backward computation into two parts, one that computes gradient for the input and another that computes for the parameters. Based on this idea, we handcraft novel pipeline schedules that significantly outperform the baseline methods. We further develop an algorithm that automatically finds an optimal schedule based on specific model configuration and memory limit. Additionally, to truly achieve zero bubble, we introduce a novel technique to bypass synchronizations during the optimizer step. Experimental evaluations show that our method outperforms the {1F1B} schedule up to 15\% in throughput under a similar memory limit. This number can be further pushed to 30\% when the memory constraint is relaxed. We believe our results mark a major step forward in harnessing the true potential of pipeline parallelism. The source code based on {Megatron-LM} is publicly available at https://github.com/sail-sg/zero-bubble-pipeline-parallelism.},
  author = {Penghui Qi and Xinyi Wan and Guangxing Huang and Min Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qi2024zero.pdf:pdf},
  note = {{ICLR} 2024 Poster},
  pdf = {https://openreview.net/pdf?id=tuzTN0eIO5},
  publisher = {OpenReview.net},
  title = {Zero Bubble (Almost) Pipeline Parallelism},
  url = {https://openreview.net/forum?id=tuzTN0eIO5},
  year = {2024}
}

@inproceedings{qian2024probabilistically,
  abstract = {Message-passing graph neural networks ({MPNNs}) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable k-subset sampling, we devise probabilistically rewired {MPNNs} ({PR-MPNNs}), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how {PR-MPNNs} enhance expressive power, and we identify precise conditions under which they outperform purely randomized approaches. Empirically, we demonstrate that our approach effectively mitigates issues like over-squashing and under-reaching. In addition, on established real-world datasets, our method exhibits competitive or superior predictive performance compared to traditional {MPNN} models and recent graph transformer architectures.},
  author = {Chendi Qian and Andrei Manolache and Kareem Ahmed and Zhe Zeng and Guy Van den Broeck and Mathias Niepert and Christopher Morris},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qian2024probabilistically.pdf:pdf},
  note = {{ICLR} 2024 Poster},
  pdf = {https://openreview.net/pdf?id=Tj6Wcx7gVk},
  publisher = {OpenReview.net},
  title = {Probabilistically Rewired Message-Passing Neural Networks},
  url = {https://openreview.net/forum?id=Tj6Wcx7gVk},
  year = {2024}
}

@inproceedings{qian2024magic123,
  abstract = {We present ``{Magic123}'', a two-stage coarse-to-fine approach for high-quality, textured {3D} mesh generation from a single image in the wild using both {2D} and {3D} priors. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a memory-efficient differentiable mesh representation to yield a high-resolution mesh with a visually appealing texture. In both stages, the {3D} content is learned through reference-view supervision and novel-view guidance by a joint {2D} and {3D} diffusion prior. We introduce a trade-off parameter between the {2D} and {3D} priors to control the details and {3D} consistencies of the generation. {Magic123} demonstrates a significant improvement over previous image-to-{3D} techniques, as validated through extensive experiments on diverse synthetic and real-world images.},
  author = {Guocheng Qian and Jinjie Mai and Abdullah Hamdi and Jian Ren and Aliaksandr Siarohin and Bing Li and Hsin-Ying Lee and Ivan Skorokhodov and Peter Wonka and Sergey Tulyakov and Bernard Ghanem},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qian2024magic123.pdf:pdf},
  note = {{ICLR} 2024 Poster},
  pdf = {https://openreview.net/pdf?id=0jHkUDyEO9},
  publisher = {OpenReview.net},
  title = {{Magic123}: One Image to High-Quality {3D} Object Generation Using Both {2D} and {3D} Diffusion Priors},
  url = {https://openreview.net/forum?id=0jHkUDyEO9},
  year = {2024}
}

@inproceedings{qiao2024prompt,
  abstract = {Prompt-tuning has demonstrated impressive performance in continual learning by querying relevant prompts for each input instance, which can avoid the introduction of task identifier. Its forgetting is therefore reduced as this instance-wise query mechanism enables us to select and update only relevant prompts. In this paper, we further integrate prompt-tuning with gradient projection approach. Our observation is: prompt-tuning releases the necessity of task identifier for gradient projection method; and gradient projection provides theoretical guarantees against forgetting for prompt-tuning. This inspires a new prompt gradient projection approach ({PGP}) for continual learning. In {PGP}, we deduce that reaching the orthogonal condition for prompt gradient can effectively prevent forgetting via the self-attention mechanism in vision-transformer. The condition equations are then realized by conducting Singular Value Decomposition ({SVD}) on an element-wise sum space between input space and prompt space. We validate our method on diverse datasets and experiments demonstrate the efficiency of reducing forgetting both in class incremental, online class incremental, and task incremental settings. The code is available at https://github.com/JingyangQiao/prompt-gradient-projection.},
  author = {Jingyang Qiao and Zhizhong Zhang and Xin Tan and Chengwei Chen and Yanyun Qu and Yong Peng and Yuan Xie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiao2024prompt.pdf:pdf},
  note = {{ICLR} 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=EH2O3h7sBI},
  publisher = {OpenReview.net},
  title = {Prompt Gradient Projection for Continual Learning},
  url = {https://openreview.net/forum?id=EH2O3h7sBI},
  year = {2024}
}

@inproceedings{qin2024infobatch,
  abstract = {Data pruning aims to obtain lossless performances with less overall cost. A common approach is to filter out samples that make less contribution to the training. This could lead to gradient expectation bias compared to the original data. To solve this problem, we propose {InfoBatch}, a novel framework aiming to achieve lossless training acceleration by unbiased dynamic data pruning. Specifically, {InfoBatch} randomly prunes a portion of less informative samples based on the loss distribution and rescales the gradients of the remaining samples to approximate the original gradient. As a plug-and-play and architecture-agnostic framework, {InfoBatch} consistently obtains lossless training results on classification, semantic segmentation, vision pretraining, and instruction fine-tuning tasks. On {CIFAR10/100}, {ImageNet-1K}, and {ADE20K}, {InfoBatch} losslessly saves 40\% overall cost. For pretraining {MAE} and diffusion model, {InfoBatch} can respectively save 24.8\% and 27\% cost. For {LLaMA} instruction fine-tuning, {InfoBatch} is also able to save 20\% cost and is compatible with coreset selection methods.},
  author = {Ziheng Qin and Kai Wang and Zangwei Zheng and Jianyang Gu and Xiangyu Peng and Zhaopan Xu and Daquan Zhou and Lei Shang and Baigui Sun and Xuansong Xie and Yang You},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qin2024infobatch.pdf:pdf},
  note = {{ICLR} 2024 Oral},
  pdf = {https://openreview.net/pdf?id=C61sk5LsK6},
  publisher = {OpenReview.net},
  title = {{InfoBatch}: Lossless Training Speed Up by Unbiased Dynamic Data Pruning},
  url = {https://openreview.net/forum?id=C61sk5LsK6},
  year = {2024}
}

@inproceedings{qin2024adversarial,
  abstract = {Data mixing augmentation has been widely applied to improve the generalization ability of deep neural networks. Recently, offline data mixing augmentation, e.g. handcrafted and saliency information-based mixup, has been gradually replaced by automatic mixing approaches. Through minimizing two sub-tasks, namely, mixed sample generation and mixup classification in an end-to-end way, {AutoMix} significantly improves accuracy on image classification tasks. However, as the optimization objective is consistent for the two sub-tasks, this approach is prone to generating consistent instead of diverse mixed samples, which results in overfitting for target task training. In this paper, we propose {AdAutomixup}, an adversarial automatic mixup augmentation approach that generates challenging samples to train a robust classifier for image classification, by alternatively optimizing the classifier and the mixup sample generator. {AdAutomixup} comprises two modules, a mixed example generator, and a target classifier. The mixed sample generator aims to produce hard mixed examples to challenge the target classifier, while the target classifier's aim is to learn robust features from hard mixed examples to improve generalization. To prevent the collapse of the inherent meanings of images, we further introduce an exponential moving average ({EMA}) teacher and cosine similarity to train {AdAutomixup} in an end-to-end way. Extensive experiments on seven image benchmarks consistently prove that our approach outperforms the state of the art in various classification scenarios.},
  author = {Huafeng Qin and Xin Jin and Yun Jiang and Mounim A. El-Yacoubi and Xinbo Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qin2024adversarial.pdf:pdf},
  note = {{ICLR} 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=o8tjamaJ80},
  publisher = {OpenReview.net},
  title = {Adversarial {AutoMixup}},
  url = {https://openreview.net/forum?id=o8tjamaJ80},
  year = {2024}
}

@inproceedings{qin2024toolllm,
  abstract = {Despite the advancements of open-source large language models ({LLMs}), e.g., {LLaMA}, they remain significantly limited in tool-use capabilities, i.e., using external tools ({APIs}) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art ({SOTA}) closed-source {LLMs}, e.g., {ChatGPT}. To bridge this gap, we introduce {ToolLLM}, a general tool-use framework encompassing data construction, model training, and evaluation. We first present {ToolBench}, an instruction-tuning dataset for tool use, which is constructed automatically using {ChatGPT}. Specifically, the construction can be divided into three stages: (i) {API} collection: we collect 16,464 real-world {RESTful} {APIs} spanning 49 categories from {RapidAPI} Hub; (ii) instruction generation: we prompt {ChatGPT} to generate diverse instructions involving these {APIs}, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use {ChatGPT} to search for a valid solution path (chain of {API} calls) for each instruction. To enhance the reasoning capabilities of {LLMs}, we develop a novel depth-first search-based decision tree algorithm. It enables {LLMs} to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of {LLMs}, we develop an automatic evaluator: {ToolEval}. Based on {ToolBench}, we fine-tune {LLaMA} to obtain an {LLM} {ToolLLaMA}, and equip it with a neural {API} retriever to recommend appropriate {APIs} for each instruction. Experiments show that {ToolLLaMA} demonstrates a remarkable ability to execute complex instructions and generalize to unseen {APIs}, and exhibits comparable performance to {ChatGPT}. Our {ToolLLaMA} also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: {APIBench}.},
  author = {Yujia Qin and Shihao Liang and Yining Ye and Kunlun Zhu and Lan Yan and Yaxi Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Lauren Hong and Runchu Tian and Ruobing Xie and Jie Zhou and Mark Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qin2024toolllm.pdf:pdf},
  note = {{ICLR} 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=dHng2O0Jjr},
  publisher = {OpenReview.net},
  title = {{ToolLLM}: Facilitating Large Language Models to Master 16000+ Real-world {APIs}},
  url = {https://openreview.net/forum?id=dHng2O0Jjr},
  year = {2024}
}

@inproceedings{qiu2024mind,
  abstract = {Dense Self-Supervised Learning ({SSL}) creates positive pairs by building positive paired regions or points, thereby aiming to preserve local features, for example of individual objects. However, existing approaches tend to couple objects by leaking information from the neighboring contextual regions when the pairs have a limited overlap. In this paper, we first quantitatively identify and confirm the existence of such a coupling phenomenon. We then address it by developing a remarkably simple yet highly effective solution comprising a novel augmentation method, Region Collaborative Cutout ({RCC}), and a corresponding decoupling branch. Importantly, our design is versatile and can be seamlessly integrated into existing {SSL} frameworks, whether based on Convolutional Neural Networks ({CNNs}) or Vision Transformers ({ViTs}). We conduct extensive experiments, incorporating our solution into two {CNN}-based and two {ViT}-based methods, with results confirming the effectiveness of our approach.},
  author = {Congpei Qiu and Tong Zhang 0023 and Yanhao Wu and Wei Ke 0003 and Mathieu Salzmann and Sabine S√ºsstrunk},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiu2024mind.pdf:pdf},
  note = {{ICLR} 2024 Poster},
  pdf = {https://openreview.net/pdf?id=WQYHbr36Fo},
  publisher = {OpenReview.net},
  title = {Mind Your Augmentation: The Key to Decoupling Dense Self-Supervised Learning},
  url = {https://openreview.net/forum?id=WQYHbr36Fo},
  year = {2024}
}

@inproceedings{qiu2024phenomenal,
  abstract = {The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models ({LMs}) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of {LMs} through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that {LMs} are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that {LMs} are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of {LMs} and humans, shedding light on both the potentials and limitations of using {LMs} in inductive reasoning tasks.},
  author = {Linlu Qiu and Liwei Jiang and Ximing Lu and Melanie Sclar and Valentina Pyatkin and Chandra Bhagavatula and Bailin Wang and Yoon Kim and Yejin Choi 0001 and Nouha Dziri and Xiang Ren 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiu2024phenomenal.pdf:pdf},
  note = {{ICLR} 2024 Poster},
  pdf = {https://openreview.net/pdf?id=bNt7oajl2a},
  publisher = {OpenReview.net},
  title = {Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement},
  url = {https://openreview.net/forum?id=bNt7oajl2a},
  year = {2024}
}

@inproceedings{qiu2024federated,
  abstract = {Prompt learning for vision-language models, e.g., CoOp, has shown great success in adapting CLIP to different downstream tasks, making it a promising solution for federated learning due to computational reasons. Existing prompt learning techniques replace hand-crafted text prompts with learned vectors that offer improvements on seen classes, but struggle to generalize to unseen classes.},
  author = {Chen Qiu and Xingyu Li and Chaithanya Kumar Mummadi and Madan Ravi Ganesh and Zhenzhen Li and Lu Peng and Wan-Yi Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiu2024federated.pdf:pdf},
  note = {DBLP last modified: 2025-06-16},
  pdf = {https://openreview.net/pdf?id=NW31gAylIm},
  publisher = {OpenReview.net},
  title = {Federated Text-driven Prompt Generation for Vision-Language Models},
  url = {https://openreview.net/forum?id=NW31gAylIm},
  year = {2024}
}

@inproceedings{qiu2024freenoise,
  abstract = {The paper addresses challenges in video generation by proposing a method to generate longer videos with multiple text conditions. Key points include: analyzing noise impact in video diffusion models, introducing FreeNoise, a tuning-free paradigm to enhance video generation, rescheduling noise sequences for long-range correlation, and designing a motion injection method for multi-text prompts.},
  author = {Haonan Qiu and Menghan Xia and Yong Zhang and Yingqing He and Xintao Wang and Ying Shan and Ziwei Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiu2024freenoise.pdf:pdf},
  note = {DBLP last modified: 2025-05-12},
  pdf = {https://openreview.net/pdf?id=ijoqFqSC7p},
  publisher = {OpenReview.net},
  title = {{FreeNoise}: Tuning-Free Longer Video Diffusion via Noise Rescheduling},
  url = {https://openreview.net/forum?id=ijoqFqSC7p},
  year = {2024}
}

@inproceedings{qiu2024dynamic,
  abstract = {The paper explores how Artificial Neural Networks (ANNs) can be improved by mimicking biological neural networks' dynamic response conditions. The authors propose a Dynamic Neural Response Tuning (DNRT) mechanism that includes Response-Adaptive Activation (RAA) and Aggregated Response Regularization (ARR). Key goals: align ANN response patterns with biological neurons, dynamically adjust neural responses based on input characteristics, and enhance network's ability to learn category-specific representations.},
  author = {Tian Qiu and Wenxiang Xu and Lin Chen and Linyun Zhou and Zunlei Feng and Mingli Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiu2024dynamic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HiTg16qhxp},
  publisher = {OpenReview.net},
  title = {Dynamic Neural Response Tuning},
  url = {https://openreview.net/forum?id=HiTg16qhxp},
  year = {2024}
}

@inproceedings{qiu2024understanding,
  abstract = {Rule learning is critical to improving knowledge graph (KG) reasoning due to their ability to provide logical and interpretable explanations. Recently, Graph Neural Networks (GNNs) with tail entity scoring achieve the state-of-the-art performance on KG reasoning. However, the theoretical understandings for these GNNs are either lacking or focusing on single-relational graphs, leaving what the kind of rules these GNNs can learn an open problem. This paper unifies GNNs with tail entity scoring into a common framework and analyzes their expressivity by formally describing the rule structures they can learn.},
  author = {Haiquan Qiu and Yongqi Zhang and Yong Li and Quanming Yao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qiu2024understanding.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=43cYe4oogi},
  publisher = {OpenReview.net},
  title = {Understanding Expressivity of {GNN} in Rule Learning},
  url = {https://openreview.net/forum?id=43cYe4oogi},
  year = {2024}
}

@inproceedings{qu2024cnn,
  abstract = {Shapelets and CNN are two typical approaches to model time series. Shapelets aim at finding a set of sub-sequences that extract feature-based interpretable shapes, but may suffer from accuracy and efficiency issues. CNN performs well by encoding sequences with a series of hidden representations, but lacks interpretability. This paper demonstrates that shapelets are essentially equivalent to a specific type of CNN kernel with a squared norm and pooling. Based on this finding, the authors propose ShapeConv, an interpretable CNN layer with its kernel serving as shapelets to conduct time-series modeling tasks in both supervised and unsupervised settings.},
  author = {Eric Qu and Yansen Wang and Xufang Luo and Wenqiang He and Kan Ren and Dongsheng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/qu2024cnn.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=O8ouVV8PjF},
  publisher = {OpenReview.net},
  title = {{CNN} Kernels Can Be the Best Shapelets},
  url = {https://openreview.net/forum?id=O8ouVV8PjF},
  year = {2024}
}

@inproceedings{quach2024conformal,
  abstract = {The paper proposes a novel approach to conformal prediction for generative language models (LMs) that produces prediction sets with rigorous, statistical performance guarantees. The method translates conformal prediction to language models by calibrating a stopping rule for sampling different outputs from the LM that get added to a growing set of candidates until confident that the output set is sufficient. Since some samples may be low-quality, they also simultaneously calibrate and apply a rejection rule for removing candidates from the output set to reduce noise. They prove that the sampled set returned by their procedure contains at least one acceptable answer with high probability, while still being empirically precise on average.},
  author = {Victor Quach and Adam Fisch and Tal Schuster and Adam Yala and Jae Ho Sohn and Tommi S. Jaakkola and Regina Barzilay},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/quach2024conformal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pzUhfQ74c5},
  publisher = {OpenReview.net},
  title = {Conformal Language Modeling},
  url = {https://openreview.net/forum?id=pzUhfQ74c5},
  year = {2024}
}

@inproceedings{quirke2024understanding,
  abstract = {Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper provides a comprehensive analysis of a one-layer Transformer model trained to perform n-digit integer addition. Our findings suggest that the model dissects the task into parallel streams dedicated to individual digits, employing varied algorithms tailored to different positions within the digits. Furthermore, we identify a rare scenario characterized by high loss, which we explain. By thoroughly elucidating the model's algorithm, we provide new insights into its functioning. These findings are validated through rigorous testing and mathematical modeling, thereby contributing to the broader fields of model understanding and interpretability.},
  author = {Philip Quirke and Fazl Barez},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/quirke2024understanding.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rIx1YXVWZb},
  publisher = {OpenReview.net},
  title = {Understanding Addition in Transformers},
  url = {https://openreview.net/forum?id=rIx1YXVWZb},
  year = {2024}
}

@inproceedings{raab2024single,
  abstract = {Synthesizing realistic animations of humans, animals, and even imaginary creatures has long been a goal for artists and computer graphics professionals. Compared to the imaging domain, which is rich with large available datasets, the number of data instances for the motion domain is limited, particularly for the animation of animals and exotic creatures which have unique skeletons and motion patterns. We introduce SinMDM, a Single Motion Diffusion Model designed to learn the internal motifs of a single motion sequence with arbitrary topology and synthesize a variety of motions of arbitrary length that remain faithful to the learned motifs. The approach harnesses the power of diffusion models and presents a denoising network explicitly designed for the task of learning from a single input motion. SinMDM is crafted as a lightweight architecture that avoids overfitting by using a shallow network with local attention layers that narrow the receptive field and encourage motion diversity.},
  author = {Sigal Raab and Inbal Leibovitch and Guy Tevet and Moab Arar and Amit Haim Bermano and Daniel Cohen-Or},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/raab2024single.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=DrhZneqz4n},
  publisher = {OpenReview.net},
  title = {Single Motion Diffusion},
  url = {https://openreview.net/forum?id=DrhZneqz4n},
  year = {2024}
}

@inproceedings{rakhsha2024maximum,
  abstract = {We propose and theoretically analyze an approach for planning with an approximate model in reinforcement learning that can reduce the adverse impact of model error. If the model is accurate enough, it accelerates the convergence to the true value function too. The procedure corrects the model's next-state distributions based on a Maximum Entropy density estimation formulation. Based on this, we introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its sampled-based variant MoCoDyna. MoCoVI and MoCoDyna's convergence can be much faster than conventional model-free algorithms. Unlike traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an approximate model and still converge to the correct value function.},
  author = {Amin Rakhsha and Mete Kemertas and Mohammad Ghavamzadeh and Amir-massoud Farahmand},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rakhsha2024maximum.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kNpSUN0uCc},
  publisher = {OpenReview.net},
  title = {Maximum Entropy Model Correction in Reinforcement Learning},
  url = {https://openreview.net/forum?id=kNpSUN0uCc},
  year = {2024}
}

@inproceedings{rakotomamonjy2024federated,
  abstract = {We introduce a principled way of computing the Wasserstein distance between two distributions in a federated manner. Specifically, we show how to estimate the Wasserstein distance between two samples stored and kept on different devices/clients whilst a central entity/server orchestrates the computations without having access to the samples. The algorithm, called FedWaD (for Federated Wasserstein Distance), iteratively approximates the Wasserstein distance by manipulating and exchanging distributions from the space of geodesics in lieu of the input samples. To achieve this, we take advantage of the geometric properties of the Wasserstein distance, particularly the triangle inequality, and that of the associated geodesics. In addition to establishing the convergence properties of FedWaD, we provide empirical results on federated coresets and federate optimal transport dataset distance.},
  author = {Alain Rakotomamonjy and Kimia Nadjahi and Liva Ralaivola},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rakotomamonjy2024federated.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rsg1mvUahT},
  publisher = {OpenReview.net},
  title = {Federated Wasserstein Distance},
  url = {https://openreview.net/forum?id=rsg1mvUahT},
  year = {2024}
}

@inproceedings{ramasinghe2024improving,
  abstract = {Synthesizing novel views for dynamic scenes from a collection of {RGB} inputs poses significant challenges due to the inherent under-constrained nature of the problem. To mitigate this ill-posedness, practitioners in the field of neural radiance fields ({NeRF}) often resort to the adoption of intricate geometric regularization techniques, including scene flow, depth estimation, or learned perceptual similarity. While these geometric cues have demonstrated their effectiveness, their incorporation leads to evaluation of computationally expensive off-the-shelf models, introducing substantial computational overhead into the pipeline. Moreover, seamlessly integrating such modules into diverse dynamic {NeRF} models can be a non-trivial task, hindering their utilization in an architecture-agnostic manner. In this paper, we propose a theoretically grounded, lightweight regularizer by treating the dynamics of a time-varying scene as a low-frequency change of a probability distribution of the light intensity. We constrain the dynamics of this distribution using optimal transport ({OT}) and provide error bounds under reasonable assumptions. Our regularization is learning-free, architecture agnostic, and can be implemented with just a few lines of code. Finally, we demonstrate the practical efficacy of our regularizer across state-of-the-art architectures.},
  author = {Sameera Ramasinghe and Violetta Shevchenko and Gil Avraham and Hisham Husain and Anton van den Hengel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ramasinghe2024improving.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=KiespDPaRH},
  publisher = {OpenReview.net},
  title = {Improving the Convergence of Dynamic {NeRFs} via Optimal Transport},
  url = {https://openreview.net/forum?id=KiespDPaRH},
  year = {2024}
}

@inproceedings{ramasubramanian2024selective,
  abstract = {The rise in internet usage has led to the generation of massive amounts of data, resulting in the adoption of various supervised and semi-supervised machine learning algorithms, which can effectively utilize the colossal amount of data to train models. However, before deploying these models in the real world, these must be strictly evaluated on performance measures like worst-case recall and satisfy constraints such as fairness. We find that current state-of-the-art empirical techniques offer sub-optimal performance on these practical, non-decomposable performance objectives. On the other hand, the theoretical techniques necessitate training a new model from scratch for each performance objective. To bridge the gap, we propose {SelMix}, a selective mixup-based inexpensive fine-tuning technique for pre-trained models, to optimize for the desired objective. The core idea of our framework is to determine a sampling distribution to perform a mixup of features between samples from particular classes such that it optimizes the given objective. We comprehensively evaluate our technique against the existing empirical and theoretically principled methods on standard benchmark datasets for imbalanced classification. We find that proposed {SelMix} fine-tuning significantly improves the performance for various practical non-decomposable objectives across benchmarks.},
  author = {Shrinivas Ramasubramanian and Harsh Rangwani and Sho Takemori and Kunal Samanta and Yuhei Umeda and Venkatesh Babu Radhakrishnan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ramasubramanian2024selective.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=rxVBKhyfSo},
  publisher = {OpenReview.net},
  title = {Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives},
  url = {https://openreview.net/forum?id=rxVBKhyfSo},
  year = {2024}
}

@inproceedings{ramkumar2024effectiveness,
  abstract = {Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training ({AT}) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of {AT} is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting ({FOMO})". {FOMO} alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that {FOMO} alleviates robust overfitting by significantly reducing the gap between the best and last robust test accuracy while improving the state-of-the-art robustness. Furthermore, {FOMO} provides a better trade-off between standard and robust accuracy, outperforming baseline adversarial methods. Finally, our framework is robust to {AutoAttacks} and increases generalization in many real-world scenarios.},
  author = {Vijaya Raghavan T. Ramkumar and Bahram Zonooz and Elahe Arani},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ramkumar2024effectiveness.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MEGQGNUfPx},
  publisher = {OpenReview.net},
  title = {The Effectiveness of Random Forgetting for Robust Generalization},
  url = {https://openreview.net/forum?id=MEGQGNUfPx},
  year = {2024}
}

@inproceedings{ramnath2024tailoring,
  abstract = {Large language models ({LMs}) are capable of generating free-text rationales to aid question answering. However, prior work 1) suggests that useful self-rationalization is emergent only at significant scales (e.g., 175B parameter {GPT-3}); and 2) focuses largely on downstream performance, ignoring the semantics of the rationales themselves, e.g., are they faithful, true, and helpful for humans? In this work, we enable small-scale {LMs} (approx. 200x smaller than {GPT-3}) to generate rationales that not only improve downstream task performance, but are also more plausible, consistent, and diverse, assessed both by automatic and human evaluation. Our method, {MaRio} (Multi-rewArd {RatIOnalization}), is a multi-reward conditioned self-rationalization algorithm that optimizes multiple distinct properties like plausibility, diversity and consistency. Results on five difficult question-answering datasets {StrategyQA}, {QuaRel}, {OpenBookQA}, {NumerSense} and {QASC} show that not only does {MaRio} improve task accuracy, but it also improves the self-rationalization quality of small {LMs} across the aforementioned axes better than a supervised fine-tuning ({SFT}) baseline. Extensive human evaluations confirm that {MaRio} rationales are preferred vs. {SFT} rationales, as well as qualitative improvements in plausibility and consistency.},
  author = {Sahana Ramnath and Brihi Joshi and Skyler Hallinan and Ximing Lu and Liunian Harold Li and Aaron Chan and Jack Hessel and Yejin Choi and Xiang Ren},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ramnath2024tailoring.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=t8eO0CiZJV},
  publisher = {OpenReview.net},
  title = {Tailoring Self-Rationalizers with Multi-Reward Distillation},
  url = {https://openreview.net/forum?id=t8eO0CiZJV},
  year = {2024}
}

@inproceedings{ran2024harnessing,
  abstract = {Recent advances in image deraining have focused on training powerful models on mixed multiple datasets comprising diverse rain types and backgrounds. However, this approach tends to overlook the inherent differences among rainy images, leading to suboptimal results. To overcome this limitation, we focus on addressing various rainy images by delving into meaningful representations that encapsulate both the rain and background components. Leveraging these representations as instructive guidance, we put forth a Context-based Instance-level Modulation ({CoI-M}) mechanism adept at efficiently modulating {CNN}- or Transformer-based models. Furthermore, we devise a rain-/detail-aware contrastive learning strategy to help extract joint rain-/detail-aware representations. By integrating {CoI-M} with the rain-/detail-aware Contrastive learning, we develop {CoIC}, an innovative and potent algorithm tailored for training models on mixed datasets. Moreover, {CoIC} offers insight into modeling relationships of datasets, quantitatively assessing the impact of rain and details on restoration, and unveiling distinct behaviors of models given diverse inputs. Extensive experiments validate the efficacy of {CoIC} in boosting the deraining ability of {CNN} and Transformer models. {CoIC} also enhances the deraining prowess remarkably when real-world dataset is included.},
  author = {Wu Ran and Peirong Ma and Zhiquan He and Hao Ren and Hong Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ran2024harnessing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pdJXYfJjz9},
  publisher = {OpenReview.net},
  title = {Harnessing Joint Rain-/Detail-aware Representations to Eliminate Intricate Rains},
  url = {https://openreview.net/forum?id=pdJXYfJjz9},
  year = {2024}
}

@inproceedings{rando2024universal,
  abstract = {Reinforcement Learning from Human Feedback ({RLHF}) is used to align large language models to produce helpful and harmless responses. Yet, these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the {RLHF} data to embed a jailbreak trigger into the model as a backdoor. The trigger then acts like a universal sudo command, enabling arbitrary harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in {RLHF} that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak backdoors.},
  author = {Javier Rando and Florian Tram{\`e}r},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rando2024universal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GxCGsxiAaK},
  publisher = {OpenReview.net},
  title = {Universal Jailbreak Backdoors from Poisoned Human Feedback},
  url = {https://openreview.net/forum?id=GxCGsxiAaK},
  year = {2024}
}

@inproceedings{rao2024rethinking,
  abstract = {Convolutional neural network ({CNN}) is easily affected by backdoor injections, whose models perform normally on clean samples but produce specific outputs on poisoned ones. Most of the existing studies have focused on the effect of trigger feature changes of poisoned samples on model generalization in spatial domain. We focus on the mechanism of {CNN} memorize poisoned samples in frequency domain, and find that {CNN} generate generalization to poisoned samples by memorizing the frequency domain distribution of trigger changes. We also investigate the assistance provided by perturbations generated at different frequencies to the generalization, and explore the influence of trigger perturbations in different frequency domain components on the generalization of poisoned models from visible and invisible backdoor attacks. We also prove that high-frequency components are more susceptible to perturbations than low-frequency components. Furthermore, we design a novel frequency domain backdoor attack method based on low-frequency semantic information, which can achieve high attack success rates on multiple models and datasets.},
  author = {Quanrui Rao and Lin Wang and Wuying Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rao2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mYhH0CDFFa},
  publisher = {OpenReview.net},
  title = {Rethinking {CNN}'s Generalization to Backdoor Attack from Frequency Domain},
  url = {https://openreview.net/forum?id=mYhH0CDFFa},
  year = {2024}
}

@inproceedings{rasul2024vqtr,
  abstract = {Probabilistic time series forecasting is a challenging problem due to the long sequences involved, the large number of samples needed for accurate probabilistic inference, and the need for real-time inference in many applications. These challenges necessitate methods that are not only accurate but computationally efficient. Unfortunately, most current state-of-the-art methods for time series forecasting are based on Transformers, which scale poorly due to quadratic complexity in sequence length, and are therefore needlessly computationally inefficient. Moreover, with a few exceptions, these methods have only been evaluated for non-probabilistic point estimation. In this work, we address these two shortcomings. For the first, we introduce {VQ-TR}, which maps large sequences to a discrete set of latent representations as part of the Attention module. This not only allows us to attend over larger context windows with linear complexity in sequence length but also allows for effective regularization to avoid overfitting. For the second, we provide what is to the best of our knowledge the first systematic comparison of modern Transformer-based time series forecasting methods for probabilistic forecasting. In this comparison, we find that {VQ-TR} performs better or comparably to all other methods while being computationally efficient.},
  author = {Kashif Rasul and Andrew Bennett and Pablo Vicente and Umang Gupta and Hena Ghonia and Anderson Schneider and Yuriy Nevmyvaka},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rasul2024vqtr.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IxpTsFS7mh},
  publisher = {OpenReview.net},
  title = {{VQ-TR}: Vector Quantized Attention for Time Series Forecasting},
  url = {https://openreview.net/forum?id=IxpTsFS7mh},
  year = {2024}
}

@inproceedings{razin2024vanishing,
  abstract = {Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning ({RFT}), which refers to maximizing a (possibly learned) reward function using policy gradient algorithms. This work identifies a fundamental optimization obstacle in {RFT}: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an {RFT} benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in {RFT}. We find the common practice of an initial supervised finetuning ({SFT}) phase to be the most promising candidate, which sheds light on its importance in an {RFT} pipeline. Moreover, we show that a relatively small number of {SFT} optimization steps on as few as 1\% of the input samples can suffice, indicating that the initial {SFT} phase need not be expensive in terms of compute and data labeling efforts.},
  author = {Noam Razin and Hattie Zhou and Omid Saremi and Vimal Thilak and Arwen Bradley and Preetum Nakkiran and Joshua M. Susskind and Etai Littwin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/razin2024vanishing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IcVNBR7qZi},
  publisher = {OpenReview.net},
  title = {Vanishing Gradients in Reinforcement Finetuning of Language Models},
  url = {https://openreview.net/forum?id=IcVNBR7qZi},
  year = {2024}
}

@inproceedings{reddi2024robust,
  abstract = {Robustness against adversarial attacks and distribution shifts is a long-standing goal of Reinforcement Learning ({RL}). To this end, Robust Adversarial Reinforcement Learning ({RARL}) trains a protagonist against destabilizing forces exercised by an adversary in a competitive zero-sum Markov game, whose optimal solution, i.e., rational strategy, corresponds to a Nash equilibrium. However, finding Nash equilibria requires facing complex saddle point optimization problems, which can be prohibitive to solve, especially for high-dimensional control. In this paper, we propose a novel approach for adversarial {RL} based on entropy regularization to ease the complexity of the saddle point optimization problem. We show that the solution of this entropy-regularized problem corresponds to a Quantal Response Equilibrium ({QRE}), a generalization of Nash equilibria that accounts for bounded rationality, i.e., agents sometimes play random actions instead of optimal ones. Crucially, the connection between the entropy-regularized objective and {QRE} enables free modulation of the rationality of the agents by simply tuning the temperature coefficient. We leverage this insight to propose our novel algorithm, Quantal Adversarial {RL} ({QARL}), which gradually increases the rationality of the adversary in a curriculum fashion until it is fully rational, easing the complexity of the optimization problem while retaining robustness.},
  author = {Aryaman Reddi and Maximilian T{\"o}lle and Jan Peters and Georgia Chalvatzaki and Carlo D'Eramo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/reddi2024robust.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=pFOoOdaiue},
  publisher = {OpenReview.net},
  title = {Robust Adversarial Reinforcement Learning via Bounded Rationality Curricula},
  url = {https://openreview.net/forum?id=pFOoOdaiue},
  year = {2024}
}

@inproceedings{reddy2024mechanistic,
  abstract = {Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.},
  author = {Gautam Reddy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/reddy2024mechanistic.pdf:pdf},
  note = {DBLP last modified: 2024-07-29. Oral presentation},
  pdf = {https://openreview.net/pdf?id=aN4Jf6Cx69},
  publisher = {OpenReview.net},
  title = {The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
  url = {https://openreview.net/forum?id=aN4Jf6Cx69},
  year = {2024}
}

@inproceedings{regol2024jointlylearned,
  abstract = {Large pretrained models, coupled with fine-tuning, are slowly becoming established as the dominant architecture in machine learning. Even though these models offer impressive performance, their practical application is often limited by the prohibitive amount of resources required for every inference. Early-exiting dynamic neural networks (EDNN) circumvent this issue by allowing a model to make some of its predictions from intermediate layers (i.e., early-exit). Training an EDNN architecture is challenging as it consists of two intertwined components: the gating mechanism (GM) that controls early-exiting decisions and the intermediate inference modules (IMs) that perform inference from intermediate representations. We identify two fundamental shortcomings in existing approaches: the GMs and the IMs are decoupled during training, leading to a train-test mismatch; and the thresholding gating mechanism introduces a positive bias into the predictive probabilities. We propose a novel architecture that connects these two modules, leading to significant performance improvements on classification datasets and enabling better uncertainty characterization capabilities.},
  author = {Florence Regol and Joud Chataoui and Mark Coates},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/regol2024jointlylearned.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jX2DT7qDam},
  publisher = {OpenReview.net},
  title = {Jointly-Learned Exit and Inference for a Dynamic Neural Network},
  url = {https://openreview.net/forum?id=jX2DT7qDam},
  year = {2024}
}

@inproceedings{reid2024repelling,
  abstract = {We present a novel quasi-Monte Carlo mechanism to improve graph-based sampling, coined repelling random walks. By inducing correlations between the trajectories of an interacting ensemble such that their marginal transition probabilities are unmodified, we are able to explore the graph more efficiently, improving the concentration of statistical estimators whilst leaving them unbiased. The mechanism has a trivial drop-in implementation. We showcase the effectiveness of repelling random walks in a range of settings including estimation of graph kernels, the PageRank vector and graphlet concentrations. We provide detailed experimental evaluation and robust theoretical guarantees. To our knowledge, repelling random walks constitute the first rigorously studied quasi-Monte Carlo scheme correlating the directions of walkers on a graph, inviting new research in this exciting nascent domain.},
  author = {Isaac Reid and Eli Berger and Krzysztof Marcin Choromanski and Adrian Weller},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/reid2024repelling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=31IOmrnoP4},
  publisher = {OpenReview.net},
  title = {Repelling Random Walks},
  url = {https://openreview.net/forum?id=31IOmrnoP4},
  year = {2024}
}

@inproceedings{reid2024general,
  abstract = {We propose a novel random walk-based algorithm for unbiased estimation of arbitrary functions of a weighted adjacency matrix, coined general graph random features (g-GRFs). This includes many of the most popular examples of kernels defined on the nodes of a graph. Our algorithm enjoys subquadratic time complexity with respect to the number of nodes, overcoming the notoriously prohibitive cubic scaling of exact graph kernel evaluation. It can also be trivially distributed across machines, permitting learning on much larger networks. At the heart of the algorithm is a modulation function which upweights or downweights the contribution from different random walks depending on their lengths. We show that by parameterising it with a neural network we can obtain g-GRFs that give higher-quality kernel estimates or perform efficient, scalable kernel learning. We provide robust theoretical analysis and support our findings with experiments including pointwise estimation of fixed graph kernels, solving non-homogeneous graph ordinary differential equations, node clustering and kernel regression on triangular meshes.},
  author = {Isaac Reid and Krzysztof Marcin Choromanski and Eli Berger and Adrian Weller},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/reid2024general.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=viftsX50Rt},
  publisher = {OpenReview.net},
  title = {General Graph Random Features},
  url = {https://openreview.net/forum?id=viftsX50Rt},
  year = {2024}
}

@inproceedings{ren2024learning,
  abstract = {Deep Metric Learning (DML) has long attracted the attention of the machine learning community as a key objective. Existing solutions concentrate on fine-tuning the pre-trained models on conventional image datasets. As a result of the success of recent pre-trained models derived from larger-scale datasets, it is challenging to adapt the model to the DML tasks in the local data domain while retaining the previously gained knowledge. In this paper, we investigate parameter-efficient methods for fine-tuning the pre-trained model for DML tasks. In particular, we propose a novel and effective framework based on learning Visual Prompts (VPT) in the pre-trained Vision Transformers (ViT). Based on the conventional proxy-based DML paradigm, we augment the proxy by incorporating the semantic information from the input image and the ViT, in which we optimize the visual prompts for each class. We demonstrate that our new approximations with semantic information are superior to representative capabilities, thereby improving metric learning performance. We conduct extensive experiments to demonstrate that our proposed framework is superior and efficient by evaluating popular DML benchmarks. In particular, we demonstrate that our fine-tuning method achieves comparable or even better performance than recent state-of-the-art full fine-tuning works of DML while tuning only a small percentage of total parameters.},
  author = {Li Ren and Chen Chen and Liqiang Wang and Kien A. Hua},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ren2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TWVMVPx2wO},
  publisher = {OpenReview.net},
  title = {Learning Semantic Proxies from Visual Prompts for Parameter-Efficient Fine-Tuning in Deep Metric Learning},
  url = {https://openreview.net/forum?id=TWVMVPx2wO},
  year = {2024}
}

@inproceedings{ren2024where,
  abstract = {This study aims to prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained deep neural networks (DNNs). Specifically, we prove the following three conditions for the emergence: (i) The high-order derivatives of the network output with respect to the input variables are all zero. (ii) The DNN can be used on occluded samples, and when the input sample is less occluded, the DNN will yield higher confidence. (iii) The confidence of the DNN does not significantly degrade on occluded samples. These conditions are quite common, and we prove that under these conditions, the DNN will only encode a relatively small number of sparse interactions between input variables. Moreover, we can consider such interactions as symbolic primitive inference patterns encoded by a DNN, because we show that inference scores of the DNN on an exponentially large number of randomly masked samples can always be well mimicked by numerical effects of just a few interactions.},
  author = {Qihan Ren and Jiayang Gao and Wen Shen and Quanshi Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ren2024where.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3pWSL8My6B},
  publisher = {OpenReview.net},
  title = {Where We Have Arrived in Proving the Emergence of Sparse Interaction Primitives in {DNN}s},
  url = {https://openreview.net/forum?id=3pWSL8My6B},
  year = {2024}
}

@inproceedings{ren2024emo,
  abstract = {Neural language models are probabilistic models of human text. They are predominantly trained using maximum likelihood estimation (MLE), which is equivalent to minimizing the forward cross-entropy between the empirical data distribution and the model distribution. However, various degeneration phenomena are still widely observed when decoding from the distributions learned by such models. We establish that the forward cross-entropy is suboptimal as a distance metric for aligning human and model distribution due to its (1) recall-prioritization (2) negative diversity ignorance and (3) train-test mismatch. In this paper, we propose Earth Mover Distance Optimization (EMO) for auto-regressive language modeling. EMO capitalizes on the inherent properties of earth mover distance to address the aforementioned challenges. Due to the high complexity of direct computation, we further introduce a feasible upper bound for EMO to ease end-to-end training. Upon extensive evaluation of language models trained using EMO and MLE, we find that EMO demonstrates a consistently better language modeling performance than MLE across domains. Moreover, EMO demonstrates noteworthy enhancements in downstream performance with minimal fine-tuning on merely 25,000 sentences. This highlights the tremendous potential of EMO as a lightweight calibration method for enhancing large-scale pre-trained language models.},
  author = {Siyu Ren and Zhiyong Wu and Kenny Q. Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ren2024emo.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4bLXfRd0CX},
  publisher = {OpenReview.net},
  title = {{EMO}: {E}arth {M}over {D}istance {O}ptimization for {A}uto-{R}egressive {L}anguage {M}odeling},
  url = {https://openreview.net/forum?id=4bLXfRd0CX},
  year = {2024}
}

@inproceedings{ren2024spikepoint,
  abstract = {Event cameras are bio-inspired sensors that respond to local changes in light intensity and feature low latency, high energy efficiency, and high dynamic range. Meanwhile, Spiking Neural Networks (SNNs) have gained significant attention due to their remarkable efficiency and fault tolerance. By synergistically harnessing the energy efficiency inherent in event cameras and the spike-based processing capabilities of SNNs, their integration could enable ultra-low-power application scenarios, such as action recognition tasks. However, existing approaches often entail converting asynchronous events into conventional frames, leading to additional data mapping efforts and a loss of sparsity, contradicting the design concept of SNNs and event cameras. To address this challenge, we propose SpikePoint, a novel end-to-end point-based SNN architecture that can process sparse event cloud data directly. SpikePoint excels at processing sparse event cloud data, effectively extracting both global and local features through a singular-stage structure. Experimental results on four event-based action recognition datasets demonstrate that SpikePoint achieves state-of-the-art performance using only 16 timesteps. Moreover, SpikePoint also exhibits remarkable efficiency, utilizing approximately 0.3\% of parameters and 0.5\% of power consumption compared to artificial neural networks.},
  author = {Hongwei Ren and Yue Zhou and Xiaopeng Lin and Yulong Huang and Haotian Fu and Jie Song and Bojun Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ren2024spikepoint.pdf:pdf},
  note = {DBLP last modified: 2024-07-29. Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=7etoNfU9uF},
  publisher = {OpenReview.net},
  title = {{SpikePoint}: {A}n {E}fficient {P}oint-based {S}piking {N}eural {N}etwork for {E}vent {C}ameras {A}ction {R}ecognition},
  url = {https://openreview.net/forum?id=7etoNfU9uF},
  year = {2024}
}

@inproceedings{renaud2024plugandplay,
  abstract = {Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining physical measurement models with deep-learning priors specified using image denoisers. However, the intricate relationship between the sampling distribution of PnP-ULA and the mismatched data-fidelity and denoiser has not been theoretically analyzed. We address this gap by proposing a posterior-$L_2$ pseudometric and using it to quantify an explicit error bound for PnP-ULA under mismatched posterior distribution. We numerically validate our theory on several inverse problems such as sampling from Gaussian mixture models and image deblurring. Our results suggest that the sensitivity of the sampling distribution of PnP-ULA to a mismatch in the measurement model and the denoiser can be precisely characterized.},
  author = {Marien Renaud and Jiaming Liu and Valentin De Bortoli and Andrs Almansa and Ulugbek Kamilov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/renaud2024plugandplay.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=66arKkGiFy},
  publisher = {OpenReview.net},
  title = {Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models},
  url = {https://openreview.net/forum?id=66arKkGiFy},
  year = {2024}
}

@inproceedings{rezaei2024prime,
  abstract = {In this work, we study the challenge of providing human-understandable descriptions for failure modes in trained image classification models. Existing works address this problem by first identifying clusters (or directions) of incorrectly classified samples in a latent space and then aiming to provide human-understandable text descriptions for them. We observe that in some cases, describing text does not match well with identified failure modes, partially owing to the fact that shared interpretable attributes of failure modes may not be captured using clustering in the feature space. To improve on these shortcomings, we propose a novel approach that prioritizes interpretability in this problem: we start by obtaining human-understandable concepts (tags) of images in the dataset and then analyze the model's behavior based on the presence or absence of combinations of these tags. Our method also ensures that the tags describing a failure mode form a minimal set, avoiding redundant and noisy descriptions. Through several experiments on different datasets, we show that our method successfully identifies failure modes and generates high-quality text descriptions associated with them. These results highlight the importance of prioritizing interpretability in understanding model failures.},
  author = {Keivan Rezaei and Mehrdad Saberi and Mazda Moayeri and Soheil Feizi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rezaei2024prime.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QrEHs9w5UF},
  publisher = {OpenReview.net},
  title = {{PRIME}: {P}rioritizing {I}nterpretability in {F}ailure {M}ode {E}xtraction},
  url = {https://openreview.net/forum?id=QrEHs9w5UF},
  year = {2024}
}

@inproceedings{richards2024does,
  abstract = {The paper investigates performance on crowdsourced, global datasets to understand model reliability. Key findings include: Progress on global data lags behind standard benchmarks, advances on ImageNet occur 2.5x faster than on global datasets, and geographic performance disparities have more than tripled.},
  author = {Megan Richards and Polina Kirichenko and Diane Bouchacourt and Mark Ibrahim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/richards2024does.pdf:pdf},
  keywords = {Benchmarks, Fairness, Generalization},
  note = {Primary Area: Societal considerations including fairness, safety, privacy},
  pdf = {https://openreview.net/pdf?id=rhaQbS3K3R},
  publisher = {OpenReview.net},
  title = {Does Progress On Object Recognition Benchmarks Improve Generalization on Crowdsourced, Global Data?},
  url = {https://openreview.net/forum?id=rhaQbS3K3R},
  year = {2024}
}

@inproceedings{richardson2024fiber,
  abstract = {Integrals with discontinuous integrands are ubiquitous, arising from discrete structure in applications like topology optimization, graphics, and computational geometry. These integrals are often part of a forward model in an inverse problem where it is necessary to reason backwards about the parameters, ideally using gradient-based optimization. Monte Carlo methods are widely used to estimate the value of integrals, but this results in a non-differentiable approximation that is amenable to neither conventional automatic differentiation nor reparameterization-based gradient methods. This significantly disrupts efforts to integrate machine learning methods in areas that exhibit these discontinuities: physical simulation and robotics, design, graphics, and computational geometry. Although bespoke domain-specific techniques can handle special cases, a general methodology to wield automatic differentiation in these discrete contexts is wanting.},
  author = {Nick Richardson and Deniz Oktay and Yaniv Ovadia and James C. Bowden and Ryan P. Adams},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/richardson2024fiber.pdf:pdf},
  keywords = {Monte Carlo methods, discontinuous integrands, differentiable programming},
  pdf = {https://openreview.net/pdf?id=sP1tCl2QBk},
  publisher = {OpenReview.net},
  title = {Fiber Monte Carlo},
  url = {https://openreview.net/forum?id=sP1tCl2QBk},
  year = {2024}
}

@inproceedings{richens2024robust,
  abstract = {This paper addresses a fundamental question in AI: while it has long been hypothesized that causal reasoning plays a fundamental role in robust and general intelligence, it was not known if agents must learn causal models to generalize under distributional shifts, or if other inductive biases are sufficient. The authors answer this question by showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents.},
  archiveprefix = {arXiv},
  author = {Jonathan Richens and Tom Everitt},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2402.10877},
  file = {:/home/b/documents/inproceedings/richens2024robust.pdf:pdf},
  keywords = {causal reasoning, distributional shifts, transfer learning, robustness},
  note = {ICLR 2024 Oral presentation, Honorable mention outstanding paper award},
  pdf = {https://openreview.net/pdf?id=pOoKI3ouv1},
  publisher = {OpenReview.net},
  title = {Robust agents learn causal world models},
  url = {https://openreview.net/forum?id=pOoKI3ouv1},
  year = {2024}
}

@inproceedings{richtrik2024error,
  abstract = {Error Feedback is a highly popular and immensely effective mechanism for fixing convergence issues which arise in distributed training methods when these are enhanced with greedy communication compression techniques such as TopK. This work studies a modern form of error feedback called EF21 which offers the currently best-known theoretical guarantees, under the weakest assumptions, and also works well in practice. The research involves applying EF21 to an equivalent reformulation of the underlying problem, discovering a new weighted version of EF21 which can be executed without any cloning, and finally improving the analysis of the original EF21 method.},
  archiveprefix = {arXiv},
  author = {Peter Richt√°rik and Elnur Gasanov and Konstantin Burlachenko},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2402.10774},
  file = {:/home/b/documents/inproceedings/richtrik2024error.pdf:pdf},
  keywords = {error feedback, distributed training, communication compression, optimization},
  pdf = {https://openreview.net/pdf?id=Ch7WqGcGmb},
  publisher = {OpenReview.net},
  title = {Error Feedback Reloaded: From Quadratic to Arithmetic Mean of Smoothness Constants},
  url = {https://openreview.net/forum?id=Ch7WqGcGmb},
  year = {2024}
}

@inproceedings{richter2024improved,
  abstract = {This paper proposes deep learning-based approaches to sample from target distributions using controlled diffusion processes, trained only on unnormalized target densities without access to samples. The authors identify these approaches as special cases of a generalized Schr√∂dinger bridge problem, seeking a stochastic evolution between a given prior distribution and the specified target. They further generalize this framework by introducing a variational formulation based on divergences between path space measures of time-reversed diffusion processes.},
  archiveprefix = {arXiv},
  author = {Lorenz Richter and Julius Berner},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2307.01198},
  file = {:/home/b/documents/inproceedings/richter2024improved.pdf:pdf},
  keywords = {diffusion models, sampling methods, Schr√∂dinger bridge, stochastic differential equations},
  pdf = {https://openreview.net/pdf?id=h4pNROsO06},
  publisher = {OpenReview.net},
  title = {Improved sampling via learned diffusions},
  url = {https://openreview.net/forum?id=h4pNROsO06},
  year = {2024}
}

@inproceedings{rietz2024prioritized,
  abstract = {This paper proposes prioritized soft Q-decomposition (PSQD), a novel algorithm for learning and adapting subtask solutions under lexicographic priorities in continuous state-action spaces. The work addresses continuous space lexicographic multi-objective RL problems, consisting of prioritized subtasks, which are notoriously difficult to solve. The authors show that these can be scalarized with a subtask transformation and then solved incrementally using value decomposition. PSQD offers the ability to reuse previously learned subtask solutions in a zero-shot composition, followed by an adaptation step.},
  archiveprefix = {arXiv},
  author = {Finn Rietz and Erik Schaffernicht and Stefan Heinrich and Johannes A. Stork},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.02360},
  file = {:/home/b/documents/inproceedings/rietz2024prioritized.pdf:pdf},
  keywords = {reinforcement learning, multi-objective optimization, lexicographic priorities, value decomposition},
  pdf = {https://openreview.net/pdf?id=c0MyyXyGfn},
  publisher = {OpenReview.net},
  title = {Prioritized Soft Q-Decomposition for Lexicographic Reinforcement Learning},
  url = {https://openreview.net/forum?id=c0MyyXyGfn},
  year = {2024}
}

@inproceedings{rigter2024rewardfree,
  abstract = {There has been a recent surge of interest in developing generally-capable agents that can adapt to new tasks without additional training in the environment. Learning world models from reward-free exploration is a promising approach, and enables policies to be trained using imagined experience for new tasks. However, achieving a general agent requires robustness across different environments. In this work, we address the novel problem of generating curricula in the reward-free setting to train robust world models. We consider robustness in terms of minimax regret over all environment instantiations and show that the minimax regret can be connected to minimising the maximum error in the world model across environment instances. This result informs our algorithm, WAKER: Weighted Acquisition of Knowledge across Environments for Robustness. WAKER selects environments for data collection based on the estimated error of the world model for each environment.},
  archiveprefix = {arXiv},
  author = {Marc Rigter and Minqi Jiang and Ingmar Posner},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2306.09205},
  file = {:/home/b/documents/inproceedings/rigter2024rewardfree.pdf:pdf},
  keywords = {reinforcement learning, world models, curriculum learning, robustness, minimax regret},
  pdf = {https://openreview.net/pdf?id=eCGpNGDeNu},
  publisher = {OpenReview.net},
  title = {Reward-Free Curricula for Training Robust World Models},
  url = {https://openreview.net/forum?id=eCGpNGDeNu},
  year = {2024}
}

@inproceedings{rimon2024mamba,
  abstract = {Meta-reinforcement learning (meta-RL) is a promising framework for tackling challenging domains requiring efficient exploration. Existing meta-RL algorithms are characterized by low sample efficiency, and mostly focus on low-dimensional task distributions. In parallel, model-based RL methods have been successful in solving partially observable MDPs, of which meta-RL is a special case. In this work, we leverage this success and propose a new model-based approach to meta-RL, based on elements from existing state-of-the-art model-based and meta-RL methods. We demonstrate the effectiveness of our approach on common meta-RL benchmark domains, attaining greater return with better sample efficiency (up to 15x improvement) while requiring very little hyperparameter tuning.},
  archiveprefix = {arXiv},
  author = {Zohar Rimon and Tom Jurgenson and Orr Krupnik and Gilad Adler and Aviv Tamar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2403.09859},
  file = {:/home/b/documents/inproceedings/rimon2024mamba.pdf:pdf},
  keywords = {meta-reinforcement learning, world models, model-based RL, sample efficiency, partially observable MDPs},
  pdf = {https://openreview.net/pdf?id=1RE0H6mU7M},
  publisher = {OpenReview.net},
  title = {MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning},
  url = {https://openreview.net/forum?id=1RE0H6mU7M},
  year = {2024}
}

@inproceedings{roberts2024cutoff,
  abstract = {Recent claims about the impressive abilities of large language models (LLMs) are often supported by evaluating publicly available benchmarks. Since LLMs train on wide swaths of the internet, this practice raises concerns of data contamination, i.e., evaluating on examples that are explicitly or implicitly included in the training data. We conduct the first thorough longitudinal analysis of data contamination in LLMs by using the natural experiment of training cutoffs in GPT models to look at benchmarks released over time. Specifically, we consider two code/mathematical problem-solving datasets, Codeforces and Project Euler, and find statistically significant trends among LLM pass rate vs. GitHub popularity and release date that provide strong evidence of contamination.},
  archiveprefix = {arXiv},
  author = {Manley Roberts and Himanshu Thakur and Christine Herlihy and Colin White and Samuel Dooley},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.10628},
  file = {:/home/b/documents/inproceedings/roberts2024cutoff.pdf:pdf},
  keywords = {data contamination, large language models, benchmark evaluation, longitudinal analysis, code generation},
  pdf = {https://openreview.net/pdf?id=m2NVG4Htxs},
  publisher = {OpenReview.net},
  title = {To the Cutoff... and Beyond? A Longitudinal Perspective on LLM Data Contamination},
  url = {https://openreview.net/forum?id=m2NVG4Htxs},
  year = {2024}
}

@inproceedings{robey2024adversarial,
  abstract = {One prominent approach toward resolving the adversarial vulnerability of deep neural networks is the two-player zero-sum paradigm of adversarial training, in which predictors are trained against adversarially chosen perturbations of data. Despite the promise of this approach, algorithms based on this paradigm have not engendered sufficient levels of robustness and suffer from pathological behavior like robust overfitting. To understand this shortcoming, we first show that the commonly used surrogate-based relaxation used in adversarial training algorithms voids all guarantees on the robustness of trained classifiers. The identification of this pitfall informs a novel non-zero-sum bilevel formulation of adversarial training, wherein each player optimizes a different objective function.},
  archiveprefix = {arXiv},
  author = {Alexander Robey and Fabian Latorre and George J. Pappas and Hamed Hassani and Volkan Cevher},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2306.11035},
  file = {:/home/b/documents/inproceedings/robey2024adversarial.pdf:pdf},
  keywords = {adversarial training, robustness, bilevel optimization, non-zero-sum games, deep neural networks},
  pdf = {https://openreview.net/pdf?id=XJ9vjEAqbx},
  publisher = {OpenReview.net},
  title = {Adversarial Training Should Be Cast as a Non-Zero-Sum Game},
  url = {https://openreview.net/forum?id=XJ9vjEAqbx},
  year = {2024}
}

@inproceedings{robin2024random,
  abstract = {We present a framework to define a large class of neural networks for which, by construction, training by gradient flow provably reaches arbitrarily low loss when the number of parameters grows. Distinct from the fixed-space global optimality of non-convex optimization, this new form of convergence, and the techniques introduced to prove such convergence, pave the way for a usable deep learning convergence theory in the near future, without overparameterization assumptions relating the number of parameters and training samples. We define these architectures from a simple computation graph and a mechanism to lift it, thus increasing the number of parameters, generalizing the idea of increasing the widths of multi-layer perceptrons. We show that architectures similar to most common deep learning models are present in this class, obtained by sparsifying the weight tensors of usual architectures at initialization. Leveraging tools of algebraic topology and random graph theory, we use the computation graph's geometry to propagate properties guaranteeing convergence to any precision for these large sparse models.},
  author = {David A. R. Robin and Kevin Scaman and Marc Lelarge},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/robin2024random.pdf:pdf},
  keywords = {neural networks, deep learning theory, convergence guarantees, random graphs, algebraic topology},
  note = {Primary Area: Learning theory, Published: 16 Jan 2024, DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rBH7x87VfJ},
  publisher = {OpenReview.net},
  title = {Random Sparse Lifts: Construction, Analysis and Convergence of finite sparse networks},
  url = {https://openreview.net/forum?id=rBH7x87VfJ},
  year = {2024}
}

@inproceedings{rocamonde2024visionlanguage,
  abstract = {Reinforcement learning ({RL}) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models ({VLMs}) as zero-shot reward models ({RMs}) to specify tasks via natural language. We propose a natural and general approach to using {VLMs} as reward models, which we call {VLM}-{RMs}. We use {VLM}-{RMs} based on {CLIP} to train a {MuJoCo} humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We can improve performance by providing a second "baseline" prompt and projecting out parts of the {CLIP} embedding space irrelevant to distinguish between goal and baseline. Further, we find a strong scaling effect for {VLM}-{RMs}: larger {VLMs} trained with more compute and data are better reward models. The failure modes of {VLM}-{RMs} we encountered are all related to known capability limitations of current {VLMs}, such as limited spatial reasoning ability or visually unrealistic environments that are far off-distribution for the {VLM}. We find that {VLM}-{RMs} are remarkably robust as long as the {VLM} is large enough. This suggests that future {VLMs} will become more and more useful reward models for a wide range of {RL} applications.},
  author = {Juan Rocamonde and Victoriano Montesinos and Elvis Nava and Ethan Perez and David Lindner},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rocamonde2024visionlanguage.pdf:pdf},
  keywords = {reinforcement learning, vision-language models, reward models, CLIP, zero-shot learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=N0I2RtD8je},
  publisher = {OpenReview.net},
  title = {Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning},
  url = {https://openreview.net/forum?id=N0I2RtD8je},
  year = {2024}
}

@inproceedings{roddenberry2024implicit,
  abstract = {Implicit neural representations ({INRs}) have arisen as useful methods for representing signals on Euclidean domains. By parameterizing an image as a multilayer perceptron ({MLP}) on Euclidean space, {INRs} effectively couple spatial and spectral features of the represented signal in a way that is not obvious in the usual discrete representation. Although {INRs} using sinusoidal activation functions have been studied in terms of Fourier theory, recent works have shown the advantage of using wavelets instead of sinusoids as activation functions, due to their ability to simultaneously localize in both frequency and space. In this work, we approach such {INRs} and demonstrate how they resolve high-frequency features of signals from coarse approximations performed in the first layer of the {MLP}. This leads to multiple prescriptions for the design of {INR} architectures, including the use of progressive wavelets, decoupling of low and high-pass approximations, and initialization schemes based on the singularities of the target signal.},
  author = {T. Mitchell Roddenberry and Vishwanath Saragadam and Maarten V. de Hoop and Richard G. Baraniuk},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/roddenberry2024implicit.pdf:pdf},
  keywords = {implicit neural representations, wavelets, harmonic analysis, signal processing, neural networks},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uZfjFyPAvn},
  publisher = {OpenReview.net},
  title = {Implicit Neural Representations and the Algebra of Complex Wavelets},
  url = {https://openreview.net/forum?id=uZfjFyPAvn},
  year = {2024}
}

@inproceedings{ro2024differentiable,
  abstract = {The Euler Characteristic Transform ({ECT}) has proven to be a powerful representation, combining geometrical and topological characteristics of shapes and graphs. However, the {ECT} was hitherto unable to learn task-specific representations. We overcome this issue and develop a novel computational layer that enables learning the {ECT} in an end-to-end fashion. Our method, the Differentiable Euler Characteristic Transform ({DECT}), is fast and computationally efficient, while exhibiting performance on a par with more complex models in both graph and point cloud classification tasks.},
  author = {Ernst R{\"{o}}ell and Bastian Rieck},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ro2024differentiable.pdf:pdf},
  keywords = {Euler characteristic transform, topological data analysis, shape classification, computational topology, differentiable programming},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MO632iPq3I},
  publisher = {OpenReview.net},
  title = {Differentiable Euler Characteristic Transforms for Shape Classification},
  url = {https://openreview.net/forum?id=MO632iPq3I},
  year = {2024}
}

@inproceedings{rosenbluth2024distinguished,
  abstract = {Graph Transformers ({GTs}) such as {SAN} and {GPS} are graph processing models that combine Message-Passing {GNNs} ({MPGNNs}) with global Self-Attention. They were shown to be universal function approximators, with two reservations: 1. The initial node features must be augmented with certain positional encodings. 2. The approximation is non-uniform: Graphs of different sizes may require a different approximating network. We first clarify that this form of universality is not unique to {GTs}: Using the same positional encodings, also pure {MPGNNs} and even 2-layer {MLPs} are non-uniform universal approximators. We then analyze the uniform case, where a single network must approximate the target function on graphs of all sizes. We show that {GTs} and {MPGNNs} with virtual nodes do not subsume each other in terms of uniform function approximation while neither is "universal" in this setting. The paper shows that a virtual node is not sufficient to emulate the intricate computation of self-attention uniformly, and provides a function that {GTs} can express but {MPGNNs} with virtual nodes cannot.},
  author = {Eran Rosenbluth and Jan T{\"{o}}nshoff and Martin Ritzert and Berke Kisin and Martin Grohe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rosenbluth2024distinguished.pdf:pdf},
  keywords = {graph transformers, message passing neural networks, self-attention, virtual nodes, universal approximation, graph theory},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=AcSChDWL6V},
  publisher = {OpenReview.net},
  title = {Distinguished In Uniform: Self-Attention Vs. Virtual Nodes},
  url = {https://openreview.net/forum?id=AcSChDWL6V},
  year = {2024}
}

@inproceedings{rosenfeld2024outliers,
  abstract = {We identify a new phenomenon in neural network optimization which arises from the interaction of depth and a particular heavy-tailed structure in natural data. Our result offers intuitive explanations for several previously reported observations about network training dynamics. In particular, it implies a conceptually new cause for progressive sharpening and the edge of stability; we also highlight connections to other concepts in optimization and generalization including grokking, simplicity bias, and Sharpness-Aware Minimization. Experimentally, we demonstrate the significant influence of paired groups of outliers in the training data with strong opposing signals: consistent, large magnitude features which dominate the network output throughout training and provide gradients which point in opposite directions.},
  author = {Elan Rosenfeld and Andrej Risteski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rosenfeld2024outliers.pdf:pdf},
  keywords = {neural network optimization, outliers, opposing signals, edge of stability, training dynamics, sharpness},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=kIZ3S3tel6},
  publisher = {OpenReview.net},
  title = {Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization},
  url = {https://openreview.net/forum?id=kIZ3S3tel6},
  year = {2024}
}

@inproceedings{roth2024fantastic,
  abstract = {Training deep networks requires various design decisions regarding for instance their architecture, data augmentation, or optimization. In this work, we find these training variations to result in networks learning unique feature sets from the data. Using public model libraries comprising thousands of models trained on canonical datasets like {ImageNet}, we observe that for arbitrary pairings of pretrained models, one model extracts significant data context unavailable in the other -- independent of overall performance. Given any arbitrary pairing of pretrained models and no external rankings (such as separate test sets, e.g. due to data privacy), we investigate if it is possible to transfer such "complementary" knowledge from one model to another without performance degradation -- a task made particularly difficult as additional knowledge can be contained in stronger, equiperformant or weaker models. Yet facilitating robust transfer in scenarios agnostic to pretrained model pairings would unlock auxiliary gains and knowledge fusion from any model repository without restrictions on model and problem specifics - including from weaker, lower-performance models. This work therefore provides an initial, in-depth exploration on the viability of such general-purpose knowledge transfer.},
  author = {Karsten Roth and Lukas Thede and A. Sophia Koepke and Oriol Vinyals and Olivier J. H{\'{e}}naff and Zeynep Akata},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/roth2024fantastic.pdf:pdf},
  keywords = {knowledge transfer, pretrained models, model fusion, knowledge distillation, feature extraction, deep learning},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=m50eKHCttz},
  publisher = {OpenReview.net},
  title = {Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model},
  url = {https://openreview.net/forum?id=m50eKHCttz},
  year = {2024}
}

@inproceedings{roy2024consistencyguided,
  abstract = {We propose Consistency-guided Prompt learning ({CoPrompt}), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of {CoPrompt} is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and output spaces. This facilitates more effective adaptation to downstream tasks in a few-shot learning setting. Experiments show that {CoPrompt} outperforms existing methods on a range of evaluation suites, including base-to-novel generalization, domain generalization, and cross-dataset evaluation.},
  author = {Shuvendu Roy and Ali Etemad},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/roy2024consistencyguided.pdf:pdf},
  keywords = {vision-language models, prompt learning, consistency regularization, few-shot learning, domain adaptation, generalization},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wsRXwlwx4w},
  publisher = {OpenReview.net},
  title = {Consistency-guided Prompt Learning for Vision-Language Models},
  url = {https://openreview.net/forum?id=wsRXwlwx4w},
  year = {2024}
}

@inproceedings{ruan2024identifying,
  abstract = {Recent advances in Language Model ({LM}) agents and tool use, exemplified by applications like {ChatGPT} Plugins, enable a rich set of capabilities but also amplify potential risks---such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, setting up the environment for each test scenario manually, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tail risks. To address these challenges, we introduce {ToolEmu}: a framework that uses an {LM} to emulate tool execution and enables the testing of {LM} agents against a diverse range of tools and scenarios, without manual instantiation. We develop an {LM}-based automatic safety evaluator that examines agent failures and quantifies associated risks.},
  author = {Yangjun Ruan and Honghua Dong and Andrew Wang and Silviu Pitis and Yongchao Zhou and Jimmy Ba and Yann Dubois and Chris J. Maddison and Tatsunori Hashimoto},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ruan2024identifying.pdf:pdf},
  keywords = {language model agents, AI safety, tool use, risk assessment, sandbox testing, emulation framework},
  note = {ICLR 2024 Spotlight, DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=GEcwtMk1uA},
  publisher = {OpenReview.net},
  title = {Identifying the Risks of {LM} Agents with an {LM}-Emulated Sandbox},
  url = {https://openreview.net/forum?id=GEcwtMk1uA},
  year = {2024}
}

@inproceedings{rubin2024grokking,
  abstract = {We investigate a key property of deep neural networks ({DNNs}) -- their ability to learn new features during training. This intriguing aspect of deep learning stands out most clearly in recently reported Grokking phenomena. While mainly reflected as a sudden increase in test accuracy, Grokking is also believed to be a beyond lazy-learning/Gaussian Process ({GP}) phenomenon involving feature learning. Here we apply a recent development in the theory of feature learning, the adaptive kernel approach, to two teacher-student models with cubic-polynomial and modular addition teachers. We provide analytical predictions on feature learning and Grokking properties of these models and demonstrate a mapping between Grokking and the theory of phase transitions. We show that after Grokking, the state of the {DNN} is analogous to the mixed phase following a first-order phase transition. In this mixed phase, the {DNN} generates useful internal representations of the teacher that are sharply distinct from those before the transition.},
  author = {Noa Rubin and Inbar Seroussi and Zohar Ringel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rubin2024grokking.pdf:pdf},
  keywords = {grokking, phase transitions, feature learning, deep learning theory, neural network dynamics, teacher-student models},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3ROGsTX3IR},
  publisher = {OpenReview.net},
  title = {Grokking as a First Order Phase Transition in Two Layer Networks},
  url = {https://openreview.net/forum?id=3ROGsTX3IR},
  year = {2024}
}

@inproceedings{rudman2024stable,
  abstract = {Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few "outlier dimensions" with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore*-based STable Anisotropic Regularization, a novel regularization method that can be used to increase or decrease levels of isotropy in embedding space during training. I-STAR uses IsoScore*, the first accurate measure of isotropy that is both differentiable and stable on mini-batch computations. In contrast to several previous works, we find that decreasing isotropy in contextualized embeddings improves performance on the majority of tasks and models considered in this paper.},
  author = {William Rudman and Carsten Eickhoff},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rudman2024stable.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=dbQH9AOVd5},
  publisher = {OpenReview.net},
  title = {Stable Anisotropic Regularization},
  url = {https://openreview.net/forum?id=dbQH9AOVd5},
  year = {2024}
}

@inproceedings{ruwurm2024geographic,
  abstract = {Learning representations of geographical space is vital for any machine learning model that integrates geolocated data, spanning application domains such as remote sensing, ecology, or epidemiology. Recent work embeds coordinates using sine and cosine projections based on Double Fourier Sphere (DFS) features. These embeddings assume a rectangular data domain even on global data, which can lead to artifacts, especially at the poles.},
  author = {Marc Ru√üwurm and Konstantin Klemmer and Esther Rolf and Robin Zbinden and Devis Tuia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ruwurm2024geographic.pdf:pdf},
  note = {ICLR 2024 spotlight},
  pdf = {https://openreview.net/pdf?id=PudduufFLa},
  publisher = {OpenReview.net},
  title = {Geographic Location Encoding with Spherical Harmonics and Sinusoidal Representation Networks},
  url = {https://openreview.net/forum?id=PudduufFLa},
  year = {2024}
}

@inproceedings{ryck2024operator,
  abstract = {In this paper, we investigate the behavior of gradient descent algorithms in physics-informed machine learning methods like PINNs, which minimize residuals connected to partial differential equations (PDEs). Our key result is that the difficulty in training these models is closely related to the conditioning of a specific differential operator.},
  author = {Tim De Ryck and Florent Bonnet and Siddhartha Mishra and Emmanuel de Bezenac},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ryck2024operator.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=WWlxFtR5sV},
  publisher = {OpenReview.net},
  title = {An operator preconditioning perspective on training in physics-informed machine learning},
  url = {https://openreview.net/forum?id=WWlxFtR5sV},
  year = {2024}
}

@inproceedings{rype2024divide,
  abstract = {Class-incremental learning is becoming more popular as it helps models widen their applicability while not forgetting what they already know. A trend in this area is to use a mixture-of-expert technique, where different models work together to solve the task. However, the experts are usually trained all at once using whole task data, which makes them all prone to forgetting and increasing computational burden. To address this limitation, we introduce a novel approach named SEED. SEED selects only one, the most optimal expert for a considered task, and uses data from this task to fine-tune only this expert. For this purpose, each expert represents each class with a Gaussian distribution, and the optimal expert is selected based on the similarity of those distributions. Consequently, SEED increases diversity and heterogeneity within the experts while maintaining the high stability of this ensemble method. The extensive experiments demonstrate that SEED achieves state-of-the-art performance in exemplar-free settings across various scenarios, showing the potential of expert diversification through data in continual learning.},
  author = {Grzegorz Rype≈õƒá and Sebastian Cygert and Valeriya Khan and Tomasz Trzcinski and Bartosz Micha≈Ç Zieli≈Ñski and Bart≈Çomiej Twardowski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/rype2024divide.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=sSyytcewxe},
  publisher = {OpenReview.net},
  title = {Divide and not forget: Ensemble of selectively trained experts in Continual Learning},
  url = {https://openreview.net/forum?id=sSyytcewxe},
  year = {2024}
}

@inproceedings{saada2024beyond,
  abstract = {The infinitely wide neural network has been proven a useful and manageable mathematical model that enables the understanding of many phenomena appearing in deep learning. One example is the convergence of random deep networks to Gaussian processes that enables a rigorous analysis of the way the choice of activation function and network weights impacts the training dynamics.},
  author = {Thiziri Nait Saada and Alireza Naderi and Jared Tanner},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/saada2024beyond.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=1Wi0Ys33Nm},
  publisher = {OpenReview.net},
  title = {Beyond {IID} weights: sparse and low-rank deep Neural Networks are also {G}aussian Processes},
  url = {https://openreview.net/forum?id=1Wi0Ys33Nm},
  year = {2024}
}

@inproceedings{saadatnejad2024socialtransmotion,
  abstract = {Accurate human trajectory prediction is crucial for applications such as autonomous vehicles, robotics, and surveillance systems. Yet, existing models often fail to fully leverage the non-verbal social cues human subconsciously communicate when navigating the space.},
  author = {Saeed Saadatnejad and Yang Gao and Kaouther Messaoud and Alexandre Alahi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/saadatnejad2024socialtransmotion.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=SQpnEfv9WH},
  publisher = {OpenReview.net},
  title = {Social-Transmotion: Promptable Human Trajectory Prediction},
  url = {https://openreview.net/forum?id=SQpnEfv9WH},
  year = {2024}
}

@inproceedings{saade2024unlocking,
  abstract = {We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes.},
  author = {Alaa Saade and Steven Kapturowski and Daniele Calandriello and Charles Blundell and Pablo Sprechmann and Leopoldo Sarra and Oliver Groth and Michal Valko and Bilal Piot},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/saade2024unlocking.pdf:pdf},
  note = {ICLR 2024 spotlight},
  pdf = {https://openreview.net/pdf?id=OwtMhMSybu},
  publisher = {OpenReview.net},
  title = {Unlocking the Power of Representations in Long-term Novelty-based Exploration},
  url = {https://openreview.net/forum?id=OwtMhMSybu},
  year = {2024}
}

@inproceedings{saberi2024outofdomain,
  abstract = {The paper proposes a novel framework for incorporating unlabeled data into semi-supervised classification problems that allows out-of-domain samples to slightly deviate from the in-domain distribution. The authors combine Distributionally Robust Optimization (DRO) with self-supervised training, focusing on a Gaussian mixture classification problem. Their key insights are that out-of-domain unlabeled samples can help narrow the generalization gap and that semi-supervised learning is a special case of their framework.},
  author = {Seyed Amir Hossein Saberi and Amir Najafi and Alireza Heidari and Mohammad Hosein Movasaghinia and Abolfazl S. Motahari and Babak H. Khalaj},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/saberi2024outofdomain.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=Bo6GpQ3B9a},
  publisher = {OpenReview.net},
  title = {Out-Of-Domain Unlabeled Data Improves Generalization},
  url = {https://openreview.net/forum?id=Bo6GpQ3B9a},
  year = {2024}
}

@inproceedings{saberi2024robustness,
  abstract = {In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa.},
  author = {Mehrdad Saberi and Vinu Sankar Sadasivan and Keivan Rezaei and Aounon Kumar and Atoosa Malemir Chegini and Wenxiao Wang and Soheil Feizi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/saberi2024robustness.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=dLoAdIKENc},
  publisher = {OpenReview.net},
  title = {Robustness of {AI}-Image Detectors: Fundamental Limits and Practical Attacks},
  url = {https://openreview.net/forum?id=dLoAdIKENc},
  year = {2024}
}

@inproceedings{saboksayr2024colide,
  abstract = {The paper addresses learning directed acyclic graph (DAG) structure from observational data using a linear structural equation model. The authors propose a novel convex score function for DAG learning that incorporates concomitant estimation of scale and decouples sparsity parameters from noise levels.},
  author = {Seyed Saman Saboksayr and Gonzalo Mateos and Mariano Tepper},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/saboksayr2024colide.pdf:pdf},
  note = {ICLR 2024 poster},
  pdf = {https://openreview.net/pdf?id=fGAIgO75dG},
  publisher = {OpenReview.net},
  title = {{CoLiDE}: Concomitant Linear {DAG} Estimation},
  url = {https://openreview.net/forum?id=fGAIgO75dG},
  year = {2024}
}

@inproceedings{sadat2024cads,
  abstract = {While conditional diffusion models are known to have good coverage of the data distribution, they still face limitations in output diversity, particularly when sampled with a high classifier-free guidance scale for optimal image quality or when trained on small datasets. We propose a Condition-Annealed Diffusion Sampler ({CADS}) that increases generation diversity by adding scheduled Gaussian noise to conditioning vectors during inference. Achieved new state-of-the-art {FID} of 1.70 and 2.31 for class-conditional {ImageNet} generation at 256√ó256 and 512√ó512 resolutions.},
  author = {Seyedmorteza Sadat and Jakob Buhmann and Derek Bradley and Otmar Hilliges and Romann M. Weber},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sadat2024cads.pdf:pdf},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=zMoNrajk2X},
  publisher = {OpenReview.net},
  title = {{CADS}: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling},
  url = {https://openreview.net/forum?id=zMoNrajk2X},
  year = {2024}
}

@inproceedings{sadek2024algorithms,
  abstract = {{ML}-augmented algorithms utilize predictions to achieve performance beyond their worst-case bounds. Producing these predictions might be a costly operation. We design parsimonious algorithms for caching and {MTS} with action predictions, presenting 1-consistent, smooth, and robust algorithm for caching and consistent, smooth and robust algorithm for {MTS} with limited access to predictor.},
  author = {Karim Ahmed Abdel Sadek and Marek Elias},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sadek2024algorithms.pdf:pdf},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=QuIiLSktO4},
  publisher = {OpenReview.net},
  title = {Algorithms for Caching and {MTS} with reduced number of predictions},
  url = {https://openreview.net/forum?id=QuIiLSktO4},
  year = {2024}
}

@inproceedings{saengkyongam2024identifying,
  abstract = {The premise of identifiable and causal representation learning is to improve the current representation learning paradigm in terms of generalizability or robustness. This paper addresses intervention extrapolation - predicting how interventions affect an outcome, even when those interventions are not observed during training. We show that identifiable representations can provide an effective solution even when interventions affect outcomes non-linearly.},
  archiveprefix = {arXiv},
  author = {Sorawit Saengkyongam and Elan Rosenfeld and Pradeep Kumar Ravikumar and Niklas Pfister and Jonas Peters},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.04295},
  file = {:/home/b/documents/inproceedings/saengkyongam2024identifying.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=3cuJwmPxXj},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Identifying Representations for Intervention Extrapolation},
  url = {https://openreview.net/forum?id=3cuJwmPxXj},
  year = {2024}
}

@inproceedings{saha2024drsm,
  abstract = {We propose {DRSM} (De-Randomized Smoothed {MalConv}), a certified defense that redesigns the de-randomized smoothing technique for malware detection. We develop a window ablation scheme to provably limit the impact of adversarial bytes while maximally preserving local structures of executables. We are the first to offer certified robustness in the realm of static detection of malware executables. When evaluated against 9 empirical attacks of different types, {DRSM} demonstrates empirical robustness against a diverse set of attacks.},
  archiveprefix = {arXiv},
  author = {Shoumik Saha and Wenxiao Wang and Yigitcan Kaya and Soheil Feizi and Tudor Dumitras},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2303.13372},
  file = {:/home/b/documents/inproceedings/saha2024drsm.pdf:pdf},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=m7aPLHwsLr},
  primaryclass = {cs.CR},
  publisher = {OpenReview.net},
  title = {{DRSM}: De-Randomized Smoothing on Malware Classifier Providing Certified Robustness},
  url = {https://openreview.net/forum?id=m7aPLHwsLr},
  year = {2024}
}

@inproceedings{saha2024only,
  abstract = {We address Gaussian bandits with unknown heterogeneous reward variances and develop a Thompson sampling algorithm with prior-dependent Bayes regret bounds. Most bandit algorithms assume that the reward variances or their upper bounds are known, and that they are the same for all arms. This naturally leads to suboptimal performance. We achieve lower regret with lower reward variances and more informative priors, which is precisely why we pay only for what is uncertain. This is the first such result in the bandit literature.},
  archiveprefix = {arXiv},
  author = {Aadirupa Saha and Branislav Kveton},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2303.09033},
  file = {:/home/b/documents/inproceedings/saha2024only.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=p8ujRTjEf3},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Only Pay for What Is Uncertain: Variance-Adaptive Thompson Sampling},
  url = {https://openreview.net/forum?id=p8ujRTjEf3},
  year = {2024}
}

@inproceedings{sahiner2024scaling,
  abstract = {It has been demonstrated that the training problem for a variety of (non) linear two-layer neural networks (such as two-layer perceptrons, convolutional networks, and self-attention) can be posed as equivalent convex optimization problems, with an induced regularizer which encourages low rank. However, this regularizer becomes prohibitively expensive to compute at moderate scales, impeding training convex neural networks. To this end, we propose applying the Burer-Monteiro factorization to convex neural networks, which for the first time enables a Burer-Monteiro perspective on neural networks with non-linearities. This factorization leads to an equivalent yet computationally tractable non-convex alternative with no spurious local minima.},
  author = {Arda Sahiner and Tolga Ergen and Batu Ozturkler and John M. Pauly and Morteza Mardani and Mert Pilanci},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sahiner2024scaling.pdf:pdf},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=ikmuHqugN7},
  publisher = {OpenReview.net},
  title = {Scaling Convex Neural Networks with Burer-Monteiro Factorization},
  url = {https://openreview.net/forum?id=ikmuHqugN7},
  year = {2024}
}

@inproceedings{sainz2024gollie,
  abstract = {Large Language Models ({LLMs}) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction ({IE}), lagging behind task-specific models. Typically, {IE} tasks are characterized by complex annotation guidelines which describe the task and give examples to humans. We propose {GoLLIE} (Guideline-following Large Language Model for {IE}), a model able to improve zero-shot results on unseen {IE} tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that {GoLLIE} is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction.},
  archiveprefix = {arXiv},
  author = {Oscar Sainz and Iker Garc√≠a-Ferrero and Rodrigo Agerri and Oier Lopez de Lacalle and German Rigau and Eneko Agirre},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.03668},
  file = {:/home/b/documents/inproceedings/sainz2024gollie.pdf:pdf},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=Y3wpuxd7u9},
  primaryclass = {cs.CL},
  publisher = {OpenReview.net},
  title = {{GoLLIE}: Annotation Guidelines improve Zero-Shot Information-Extraction},
  url = {https://openreview.net/forum?id=Y3wpuxd7u9},
  year = {2024}
}

@inproceedings{sakos2024beating,
  abstract = {Arguably one of the thorniest problems in game theory is that of equilibrium selection. Specifically, in the presence of multiple equilibria do self-interested learning dynamics typically select the socially optimal ones? We study a rich class of continuous-time no-regret dynamics in potential games ({PGs}). Our class of dynamics, Q-Replicator Dynamics ({QRD}), include gradient descent ({GD}), log-barrier and replicator dynamics ({RD}) as special cases. We start by establishing pointwise convergence of all {QRD} to Nash equilibria in almost all {PGs}. In the case of {GD}, we show a tight average case performance within a factor of two of optimal, for a class of symmetric 2 √ó 2 potential games with unbounded Price of Anarchy ({PoA}).},
  author = {Iosif Sakos and Stefanos Leonardos and Stelios Andrew Stavroulakis and Will Overman and Ioannis Panageas and Georgios Piliouras},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sakos2024beating.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=36L7W3ri4U},
  publisher = {OpenReview.net},
  title = {Beating Price of Anarchy and Gradient Descent without Regret in Potential Games},
  url = {https://openreview.net/forum?id=36L7W3ri4U},
  year = {2024}
}

@inproceedings{salihu2024deepspf,
  abstract = {Recently, {SO}(3)-equivariant methods have been explored for 3D reconstruction via Scan-to-{CAD}. Despite significant advancements attributed to the unique characteristics of 3D data, existing {SO}(3)-equivariant approaches often fall short in seamlessly integrating local and global contextual information in a widely generalizable manner. We introduce Spherical Patch Fields, a representation technique designed for patch-wise, {SO}(3)-equivariant 3D point clouds, anchored theoretically on the principles of Spherical Gaussians. We present the Patch Gaussian Layer, designed for the adaptive extraction of local and global contextual information from resizable point cloud patches. Culminating our contributions, we present Learnable Spherical Patch Fields ({DeepSPF}) ‚Äì a versatile and easily integrable backbone suitable for instance-based point networks.},
  author = {Driton Salihu and Adam Misik and Yuankai Wu and Constantin Patsch and Fabian Seguel and Eckehard G. Steinbach},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/salihu2024deepspf.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=Dnc3paMqDE},
  publisher = {OpenReview.net},
  title = {{DeepSPF}: Spherical {SO}(3)-Equivariant Patches for Scan-to-{CAD} Estimation},
  url = {https://openreview.net/forum?id=Dnc3paMqDE},
  year = {2024}
}

@inproceedings{salvatori2024stable,
  abstract = {Predictive coding networks are neuroscience-inspired models with roots in both Bayesian statistics and neuroscience. Training such models, however, is quite inefficient and unstable. In this work, we show how by simply changing the temporal scheduling of the update rule for the synaptic weights leads to an algorithm that is much more efficient and stable than the original one, and has theoretical guarantees in terms of convergence. The proposed algorithm, that we call incremental predictive coding (iPC) is also more biologically plausible than the original one, as it is fully automatic. In an extensive set of experiments, we show that iPC constantly performs better than the original formulation on a large number of benchmarks for image classification, as well as for the training of both conditional and masked language models.},
  archiveprefix = {arXiv},
  author = {Tommaso Salvatori and Yuhang Song and Yordan Yordanov and Beren Millidge and Lei Sha and Cornelius Emde and Zhenghua Xu and Rafal Bogacz and Thomas Lukasiewicz},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2212.00720},
  file = {:/home/b/documents/inproceedings/salvatori2024stable.pdf:pdf},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=RyUvzda8GH},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {A Stable, Fast, and Fully Automatic Learning Algorithm for Predictive Coding Networks},
  url = {https://openreview.net/forum?id=RyUvzda8GH},
  year = {2024}
}

@inproceedings{samsami2024mastering,
  abstract = {Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method. We also show that R2I is faster than the state-of-the-art MBRL method, DreamerV3, resulting in faster wall-time convergence.},
  author = {Mohammad Reza Samsami and Artem Zholus and Janarthanan Rajendran and Sarath Chandar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/samsami2024mastering.pdf:pdf},
  note = {ICLR 2024 Oral Presentation (top 1.2\%)},
  pdf = {https://openreview.net/pdf?id=1vDArHJ68h},
  publisher = {OpenReview.net},
  title = {Mastering Memory Tasks with World Models},
  url = {https://openreview.net/forum?id=1vDArHJ68h},
  year = {2024}
}

@inproceedings{saremi2024chain,
  abstract = {This paper introduces a theoretical framework for sampling from unnormalized densities based on a smoothing scheme that uses an isotropic Gaussian kernel with a single fixed noise scale. We prove one can decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. Our construction is unique in that it keeps track of a history of samples, making it non-Markovian as a whole, but it is lightweight algorithmically as the history only shows up in the form of a running empirical mean of samples. The sampling algorithm generalizes walk-jump sampling, where the "walk" phase becomes a (non-Markovian) chain of (log-concave) Markov chains. A remarkable capacity of our sampling algorithm is its ability to "tunnel" between modes of a distribution.},
  author = {Saeed Saremi and Ji Won Park and Francis R. Bach},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/saremi2024chain.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=yiMB2DOjsR},
  publisher = {OpenReview.net},
  title = {Chain of Log-Concave Markov Chains},
  url = {https://openreview.net/forum?id=yiMB2DOjsR},
  year = {2024}
}

@inproceedings{sarthi2024raptor,
  abstract = {Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic understanding of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over traditional retrieval-augmented LMs on several tasks.},
  author = {Parth Sarthi and Salman Abdullah and Aditi Tuli and Shubh Khanna and Anna Goldie and Christopher D. Manning},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sarthi2024raptor.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=GN921JHCRw},
  publisher = {OpenReview.net},
  title = {{RAPTOR}: Recursive Abstractive Processing for Tree-Organized Retrieval},
  url = {https://openreview.net/forum?id=GN921JHCRw},
  year = {2024}
}

@inproceedings{sarti2024quantifying,
  abstract = {Establishing whether language models can use contextual information in a human-plausible way is important to ensure their safe adoption in real-world settings. However, the questions of when and which parts of the context affect model generations are typically tackled separately, and current plausibility evaluations are practically limited to a handful of artificial benchmarks. To address these limitations, we introduce Plausibility Evaluation of Context Reliance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. PECoRe leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We benchmark PECoRe by comparing model rationales with human annotations across several discourse-level phenomena. Finally, we apply PECoRe to unannotated model translations to identify context-mediated predictions and highlight instances of (im)plausible context usage throughout generation.},
  author = {Gabriele Sarti and Grzegorz Chrupala and Malvina Nissim and Arianna Bisazza},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sarti2024quantifying.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=XTHfNGI3zT},
  publisher = {OpenReview.net},
  title = {Quantifying the Plausibility of Context Reliance in Neural Machine Translation},
  url = {https://openreview.net/forum?id=XTHfNGI3zT},
  year = {2024}
}

@inproceedings{scalbert2024towards,
  abstract = {In Self-Supervised Learning (SSL), models are typically pretrained, fine-tuned, and evaluated on the same domains. However, they tend to perform poorly when evaluated on unseen domains, a challenge that Unsupervised Domain Generalization (UDG) seeks to address. Current UDG methods rely on domain labels, which are often challenging to collect, and domain-specific architectures that lack scalability when confronted with numerous domains, making the current methodology impractical and rigid. Inspired by contrastive-based UDG methods that mitigate spurious correlations by restricting comparisons to examples from the same domain, we hypothesize that eliminating style variability within a batch could provide a more convenient and flexible way to reduce spurious correlations. To this end, we introduce Batch Styles Standardization (BSS), a Fourier-based method to standardize the style of images in a batch, designed to be combined with existing SSL approaches to reduce spurious correlations and promote domain-invariance within SSL representations.},
  author = {Marin Scalbert and Maria Vakalopoulou and Florent Couzinie-Devy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/scalbert2024towards.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=qtE9K23ISq},
  publisher = {OpenReview.net},
  title = {Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization},
  url = {https://openreview.net/forum?id=qtE9K23ISq},
  year = {2024}
}

@inproceedings{scannell2024functionspace,
  abstract = {Sequential learning paradigms pose challenges for gradient-based deep learning due to difficulties incorporating new data and retaining prior knowledge. While Gaussian processes elegantly tackle these problems, they struggle with scalability and handling rich inputs, such as images. To address these issues, we introduce a technique that converts neural networks from weight space to function space, through a dual parameterization. Our parameterization offers: (i) a way to scale function-space methods to large data sets via sparsification, (ii) retention of prior knowledge when access to past data is limited, and (iii) a mechanism to incorporate new data without retraining. The experiments demonstrate that the method can retain knowledge in continual learning and incorporate new data efficiently, while showing strengths in uncertainty quantification and guiding exploration in model-based RL.},
  author = {Aidan Scannell and Riccardo Mereu and Paul Edmund Chang and Ella Tamir and Joni Pajarinen and Arno Solin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/scannell2024functionspace.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=2dhxxIKhqz},
  publisher = {OpenReview.net},
  title = {Function-space Parameterization of Neural Networks for Sequential Learning},
  url = {https://openreview.net/forum?id=2dhxxIKhqz},
  year = {2024}
}

@inproceedings{scarpellini20242vec,
  abstract = {This paper introduces $\pi$2vec, a method for representing black box policies as comparable feature vectors. Our method combines the strengths of foundation models that serve as generic and powerful state representations and successor features that can model the future occurrence of the states for a policy. $\pi$2vec represents the behavior of policies by capturing the statistics of the features from a pretrained model with the help of successor feature framework. We focus on the offline setting where policies and their representations are trained on a fixed dataset of trajectories. Finally, we employ linear regression on $\pi$2vec vector representations to predict the performance of held out policies. The synergy of these techniques results in a method for efficient policy evaluation in resource constrained environments.},
  author = {Gianluca Scarpellini and Ksenia Konyushkova and Claudio Fantacci and Thomas Paine and Yutian Chen and Misha Denil},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/scarpellini20242vec.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=o5Bqa4o5Mi},
  publisher = {OpenReview.net},
  title = {$\pi$2vec: Policy Representation with Successor Features},
  url = {https://openreview.net/forum?id=o5Bqa4o5Mi},
  year = {2024}
}

@inproceedings{schmidt2024learning,
  abstract = {Pre-training large models on vast amounts of web data has proven to be an effective approach for obtaining powerful, general models in domains such as language and vision. However, this paradigm has not yet taken hold in reinforcement learning. This is because videos, the most abundant form of embodied behavioral data on the web, lack the action labels required by existing methods for imitating behavior from demonstrations. We introduce Latent Action Policies (LAPO), a method for recovering latent action information‚Äîand thereby latent-action policies, world models, and inverse dynamics models‚Äîpurely from videos. LAPO is the first method able to recover the structure of the true action space just from observed dynamics, even in challenging procedurally-generated environments. LAPO enables training latent-action policies that can be rapidly fine-tuned into expert-level policies, either offline using a small action-labeled dataset, or online with rewards. LAPO takes a first step towards pre-training powerful, generalist policies and world models on the vast amounts of videos readily available on the web.},
  author = {Dominik Schmidt and Minqi Jiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/schmidt2024learning.pdf:pdf},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=rvUq3cxpDF},
  publisher = {OpenReview.net},
  title = {Learning to Act without Actions},
  url = {https://openreview.net/forum?id=rvUq3cxpDF},
  year = {2024}
}

@inproceedings{schneider2024universal,
  abstract = {Web-scraped datasets are vulnerable to data poisoning, which can be used for backdooring deep image classifiers during training. Since training on large datasets is expensive, a model is trained once and reused many times. Unlike adversarial examples, backdoor attacks often target specific classes rather than any class learned by the model. One might expect that targeting many classes through a na√Øve composition of attacks vastly increases the number of poison samples. We show this is not necessarily true and more efficient, universal data poisoning attacks exist that allow controlling misclassifications from any source class into any target class with a slight increase in poison samples. We create universal backdoor attacks against image classifiers, which can induce misclassifications between any two classes with a handful of poison samples.},
  author = {Benjamin Schneider and Nils Lukas and Florian Kerschbaum},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/schneider2024universal.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=3QkzYBSWqL},
  publisher = {OpenReview.net},
  title = {Universal Backdoor Attacks},
  url = {https://openreview.net/forum?id=3QkzYBSWqL},
  year = {2024}
}

@inproceedings{schneider2024identifying,
  abstract = {Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. We find that the dimensionality of these subspaces is problem-dependent and that they can be identified as early as 30\% into training.},
  author = {Jan Schneider and Pierre Schumacher and Simon Guist and Le Chen and Daniel F. B. H{\"a}ufle and Bernhard Sch{\"o}lkopf and Dieter B{\"u}chler},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/schneider2024identifying.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=iPWxqnt2ke},
  publisher = {OpenReview.net},
  title = {Identifying Policy Gradient Subspaces},
  url = {https://openreview.net/forum?id=iPWxqnt2ke},
  year = {2024}
}

@inproceedings{schnell2024stabilizing,
  abstract = {Of all the vector fields surrounding the minima of recurrent learning setups, the gradient field with its exploding and vanishing updates appears a poor choice for optimization, offering little beyond efficient computability. We seek to improve this suboptimal practice in the context of physics simulations, where backpropagating feedback through many unrolled time steps is considered crucial to acquiring temporally coherent behavior. The alternative vector field we propose follows from two principles: physics simulators, unlike neural networks, have a balanced gradient flow, and certain modifications to the backpropagation pass leave the positions of the original minima unchanged. As any modification of backpropagation decouples forward and backward pass, the rotation-free character of the gradient field is lost. Therefore, we discuss the negative implications of using such a rotational vector field for optimization and how to counteract them. Our final procedure is easily implementable via a sequence of gradient stopping and component-wise comparison operations, which do not negatively affect scalability. Our experiments on three control problems show that especially as we increase the complexity of each task, the unbalanced updates from the gradient can no longer provide the precise control signals necessary while our method still solves the tasks.},
  author = {Patrick Schnell and Nils Thuerey},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/schnell2024stabilizing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bozbTTWcaw},
  publisher = {OpenReview.net},
  title = {Stabilizing Backpropagation Through Time to Learn Complex Physics},
  url = {https://openreview.net/forum?id=bozbTTWcaw},
  year = {2024}
}

@inproceedings{schro2024causal,
  abstract = {Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framework in a series of experiments, including a real-world case study about predicting prison sentences. To the best of our knowledge, ours is the first work to study causal fairness under unobserved confounding. To this end, our work is of direct practical value as a refutation strategy to ensure the fairness of predictions in high-stakes applications.},
  author = {Maresa Schr{\"o}der and Dennis Frauen and Stefan Feuerriegel},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/schro2024causal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DqD59dQP37},
  publisher = {OpenReview.net},
  title = {Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework},
  url = {https://openreview.net/forum?id=DqD59dQP37},
  year = {2024}
}

@inproceedings{schug2024discovering,
  abstract = {Many complex tasks can be decomposed into simpler, independent parts. Discovering such underlying compositional structure has the potential to enable compositional generalization. Despite progress, our most powerful systems struggle to compose flexibly. It therefore seems natural to make models more modular to help capture the compositional nature of many tasks. However, it is unclear under which circumstances modular systems can discover hidden compositional structure. To shed light on this question, we study a teacher-student setting with a modular teacher where we have full control over the composition of ground truth modules. This allows us to relate the problem of compositional generalization to that of identification of the underlying modules. In particular we study modularity in hypernetworks representing a general class of multiplicative interactions. We show theoretically that identification up to linear transformation purely from demonstrations is possible without having to learn an exponential number of module combinations. We further demonstrate empirically that under the theoretically identified conditions, meta-learning from finite data can discover modular policies that generalize compositionally in a number of complex environments.},
  author = {Simon Schug and Seijin Kobayashi and Yassir Akram and Maciej Wo{\\l}czyk and Alexandra Proca and Johannes von Oswald and Razvan Pascanu and Jo{\\~a}o Sacramento and Angelika Steger},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/schug2024discovering.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=H98CVcX1eh},
  publisher = {OpenReview.net},
  title = {Discovering modular solutions that generalize compositionally},
  url = {https://openreview.net/forum?id=H98CVcX1eh},
  year = {2024}
}

@inproceedings{schultheis2024consistent,
  abstract = {We consider the optimization of complex performance metrics in multi-label classification under the population utility framework. We mainly focus on metrics linearly decomposable into a sum of binary classification utilities applied separately to each label with an additional requirement of exactly $k$ labels predicted for each instance. These macro-at-$k$ metrics possess desired properties for extreme classification problems with long tail labels. Unfortunately, the at-$k$ constraint couples the otherwise independent binary classification tasks, leading to a much more challenging optimization problem than standard macro-averages. We provide a statistical framework to study this problem, prove the existence and the form of the optimal classifier, and propose a statistically consistent and practical learning algorithm based on the Frank-Wolfe method. Interestingly, our main results concern even more general metrics being non-linear functions of label-wise confusion matrices.},
  author = {Erik Schultheis and Wojciech Kot{\\l}owski and Marek Wydmuch and Rohit Babbar and Strom Borman and Krzysztof Dembczy{\\'n}ski},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/schultheis2024consistent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=XOnya9gSdF},
  publisher = {OpenReview.net},
  title = {Consistent algorithms for multi-label classification with macro-at-k metrics},
  url = {https://openreview.net/forum?id=XOnya9gSdF},
  year = {2024}
}

@inproceedings{schwarz2024wildfusion,
  abstract = {Modern learning-based approaches to 3D-aware image synthesis achieve high photorealism and 3D-consistent viewpoint changes for the generated images. Existing approaches represent instances in a shared canonical space. However, for in-the-wild datasets a shared canonical system can be difficult to define or might not even exist. In this work, we instead model instances in view space, alleviating the need for posed images and learned camera distributions. We find that in this setting, existing GAN-based methods are prone to generating flat geometry and struggle with distribution coverage. We hence propose WildFusion, a new approach to 3D-aware image synthesis based on latent diffusion models (LDMs). WildFusion is trained without multiview or 3D geometry supervision and relies neither on posed images nor on learned pose or camera distributions. Key to our framework is an image autoencoder with a 3D-aware latent space that simultaneously enables not only novel view synthesis but also compression. This allows us to efficiently train a diffusion model in the autoencoder's latent space. WildFusion outperforms recent state-of-the-art GAN-based methods when training on diverse data without camera poses.},
  author = {Katja Schwarz and Seung Wook Kim and Jun Gao and Sanja Fidler and Andreas Geiger and Karsten Kreis},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/schwarz2024wildfusion.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=VdwVOREDZM},
  publisher = {OpenReview.net},
  title = {WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space},
  url = {https://openreview.net/forum?id=VdwVOREDZM},
  year = {2024}
}

@inproceedings{sclar2024quantifying,
  abstract = {As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.},
  author = {Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/sclar2024quantifying.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RIu5lyNXjT},
  publisher = {OpenReview.net},
  title = {Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting},
  url = {https://openreview.net/forum?id=RIu5lyNXjT},
  year = {2024}
}

@inproceedings{scott2024pefll,
  abstract = {We present PeFLL, a new personalized federated learning algorithm that improves over the state-of-the-art in three aspects: 1) it produces more accurate models, especially in the low-data regime, and not only for clients present during its training phase, but also for any that may emerge in the future; 2) it reduces the amount of on-client computation and client-server communication by providing future clients with ready-to-use personalized models that require no additional finetuning or optimization; 3) it comes with theoretical guarantees that establish generalization from the observed clients to future ones. At the core of PeFLL lies a learning-to-learn approach that jointly trains an embedding network and a hypernetwork. The embedding network is used to represent clients in a latent descriptor space in a way that reflects their similarity to each other. The hypernetwork takes as input such descriptors and outputs the parameters of fully personalized client models.},
  author = {Jonathan Scott and Hossein Zakerinia and Christoph H. Lampert},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/scott2024pefll.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MrYiwlDRQO},
  publisher = {OpenReview.net},
  title = {PeFLL: Personalized Federated Learning by Learning to Learn},
  url = {https://openreview.net/forum?id=MrYiwlDRQO},
  year = {2024}
}

@inproceedings{seedat2024dissecting,
  abstract = {Characterizing samples that are difficult to learn from is crucial to developing highly performant ML models. This has led to numerous Hardness Characterization Methods (HCMs) that aim to identify hard samples. However, there is a lack of consensus regarding the definition and evaluation of hardness. Unfortunately, current HCMs have only been evaluated on specific types of hardness and often only qualitatively or with respect to downstream performance, overlooking the fundamental quantitative identification task. We address this gap by presenting a fine-grained taxonomy of hardness types. Additionally, we propose the Hardness Characterization Analysis Toolkit (H-CAT), which supports comprehensive and quantitative benchmarking of HCMs across the hardness taxonomy and can easily be extended to new HCMs, hardness types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8 hardness types. This comprehensive evaluation encompassing over 14K setups uncovers strengths and weaknesses of different HCMs, leading to practical tips to guide HCM selection and future development.},
  author = {Nabeel Seedat and Fergus Imrie and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/seedat2024dissecting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=icTZCUbtD6},
  publisher = {OpenReview.net},
  title = {Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness Characterization Methods for Data-Centric AI},
  url = {https://openreview.net/forum?id=icTZCUbtD6},
  year = {2024}
}

@inproceedings{segert2024flat,
  abstract = {We consider the problem of linear estimation and establish an extension of the Gauss-Markov theorem, in which the bias operator is allowed to be non-zero but bounded with respect to a matrix norm of Schatten type. We derive simple and explicit formulas for the optimal estimator in the cases of Nuclear and Spectral norms (with the Frobenius case recovering ridge regression). Additionally, we analytically derive the generalization error in multiple random matrix ensembles, and compare with Ridge regression. Our work provides theoretical insights into the connection between flat minima and generalization in linear estimation problems.},
  author = {Simon N. Segert},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/segert2024flat.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nxnbPPVvOG},
  publisher = {OpenReview.net},
  title = {Flat Minima in Linear Estimation and an Extended Gauss Markov Theorem},
  url = {https://openreview.net/forum?id=nxnbPPVvOG},
  year = {2024}
}

@inproceedings{segev2024align,
  abstract = {Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose Align With Purpose, a general Plug-and-Play framework for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect alignments. We apply our framework in the domain of Automatic Speech Recognition (ASR) and show its generality in terms of property selection, architectural choice, and scale of training dataset (up to 280,000 hours).},
  author = {Eliya Segev and Maya Alroy and Ronen Katsir and Noam Wies and Ayana Shenhav and Yael Ben-Oren and David Zar and Oren Tadmor and Jacob Bitterman and Amnon Shashua and Tal Rosenwein},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/segev2024align.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fUGhVYPVRM},
  publisher = {OpenReview.net},
  title = {Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework},
  url = {https://openreview.net/forum?id=fUGhVYPVRM},
  year = {2024}
}

@inproceedings{sehanobish2024scalable,
  abstract = {We introduce the concept of scalable neural network kernels (SNNKs), the replacements of regular feedforward layers (FFLs), capable of approximating the latter, but with favorable computational properties. SNNKs effectively disentangle the inputs from the parameters of the neural network in the FFL, only to connect them in the final computation via the dot-product kernel. They are also strictly more expressive, as allowing to model complicated relationships beyond the functions of the dot-products of parameter-input vectors. We also introduce the neural network bundling process that applies SNNKs to compactify deep neural network architectures, resulting in additional compression gains. In its extreme version, it leads to the fully bundled network whose optimal parameters can be expressed via explicit formulae for several loss functions (e.g. mean squared error), opening a possibility to bypass backpropagation. As a by-product of our analysis, we introduce the mechanism of the universal random features (or URFs), applied to instantiate several SNNK variants, and interesting on its own in the context of scalable kernel methods. We provide rigorous theoretical analysis of all these concepts as well as an extensive empirical evaluation, ranging from point-wise kernel estimation to Transformers' fine-tuning with novel adapter layers inspired by SNNKs. Our mechanism provides up to 5x reduction in the number of trainable parameters, while maintaining competitive accuracy.},
  author = {Arijit Sehanobish and Krzysztof Marcin Choromanski and Yunfan Zhao and Kumar Avinava Dubey and Valerii Likhosherstov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sehanobish2024scalable.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4iPw1klFWa},
  publisher = {OpenReview.net},
  title = {Scalable Neural Network Kernels},
  url = {https://openreview.net/forum?id=4iPw1klFWa},
  year = {2024}
}

@inproceedings{sehgal2024neurosymbolic,
  abstract = {We introduce Cosmos, a framework for object-centric world modeling that is designed for compositional generalization (CompGen), i.e., high performance on unseen input scenes obtained through the composition of known visual 'atoms.' The central insight behind Cosmos is the use of a novel form of neurosymbolic grounding. Specifically, the framework introduces two new tools: (i) neurosymbolic scene encodings, which represent each entity in a scene using a real vector computed using a neural encoder, as well as a vector of composable symbols describing attributes of the entity, and (ii) a neurosymbolic attention mechanism that binds these entities to learned rules of interaction. Cosmos is end-to-end differentiable; also, unlike traditional neurosymbolic methods that require representations to be manually mapped to symbols, it computes an entity's symbolic attributes using vision-language foundation models. Through an evaluation that considers two different forms of CompGen on an established blocks-pushing domain, we show that the framework establishes a new state-of-the-art for CompGen in world modeling.},
  author = {Atharva Sehgal and Arya Grayeli and Jennifer J. Sun and Swarat Chaudhuri},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sehgal2024neurosymbolic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4KZpDGD4Nh},
  publisher = {OpenReview.net},
  title = {Neurosymbolic Grounding for Compositional World Models},
  url = {https://openreview.net/forum?id=4KZpDGD4Nh},
  year = {2024}
}

@inproceedings{seitzer2024dyst,
  abstract = {Visual understanding of the world goes beyond the semantics and flat structure of individual images. In this work, we aim to capture both the 3D structure and dynamics of real-world scenes from monocular real-world videos. Our Dynamic Scene Transformer (DyST) model leverages recent work in neural scene representation to learn a latent decomposition of monocular real-world videos into scene content, per-view scene dynamics, and camera pose. This separation is achieved through a novel co-training scheme on monocular videos and our new synthetic dataset DySO. DyST learns tangible latent representations for dynamic scenes that enable view generation with separate control over the camera and the content of the scene.},
  author = {Maximilian Seitzer and Sjoerd van Steenkiste and Thomas Kipf and Klaus Greff and Mehdi S. M. Sajjadi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/seitzer2024dyst.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=MnMWa94t12},
  publisher = {OpenReview.net},
  title = {{DyST}: Towards Dynamic Neural Scene Representations on Real-World Videos},
  url = {https://openreview.net/forum?id=MnMWa94t12},
  year = {2024}
}

@inproceedings{sekikawa2024sas,
  abstract = {Wide networks usually yield better accuracy than their narrower counterpart at the expense of the massive mult cost. To break this tradeoff, we advocate a novel concept of Structured Activation Sparsification, dubbed SAS, which boosts accuracy without increasing computation by utilizing the projected sparsity in activation maps with a specific structure. Concretely, the projected sparse activation is allowed to have N nonzero value among M consecutive activations. Owing to the local structure in sparsity, the wide matmul between a dense weight and the sparse activation is executed as an equivalent narrow matmul between a dense weight and dense activation, which is compatible with NVIDIA's SparseTensorCore developed for the N:M structured sparse weight. In extensive experiments, we demonstrate that increasing sparsity monotonically improves accuracy (up to 7% on CIFAR10) without increasing the mult count. Furthermore, we show that structured sparsification of activation scales better than that of weight given the same computational budget.},
  author = {Yusuke Sekikawa and Shingo Yashima},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sekikawa2024sas.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vZfi5to2Xl},
  publisher = {OpenReview.net},
  title = {{SAS}: Structured Activation Sparsification},
  url = {https://openreview.net/forum?id=vZfi5to2Xl},
  year = {2024}
}

@inproceedings{seng2024learning,
  abstract = {Structure learning is a crucial task in science, especially in fields such as medicine and biology, where the wrong identification of (in)dependencies among random variables can have significant implications. The primary objective of structure learning is to learn a Directed Acyclic Graph (DAG) that represents the underlying probability distribution of the data. Many prominent DAG learners rely on least square losses or log-likelihood losses for optimization. It is well-known from regression models that least square losses are heavily influenced by the scale of the variables. Recently it has been demonstrated that the scale of data also affects performance of structure learning algorithms, though with a strong focus on linear 2-node systems and simulated data. Moving beyond these results, we provide conditions under which square-based losses are minimal for wrong DAGs in $d$-dimensional cases. Furthermore, we also show that scale can impair performance of structure learners if relations among variables are non-linear for both square based and log-likelihood based losses. We confirm our theoretical findings through extensive experiments on synthetic and real-world data.},
  author = {Jonas Seng and Matej Zecevic and Devendra Singh Dhami and Kristian Kersting},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/seng2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gwbQ2YwLhD},
  publisher = {OpenReview.net},
  title = {Learning Large {DAGs} is Harder than you Think: Many Losses are Minimal for the Wrong {DAG}},
  url = {https://openreview.net/forum?id=gwbQ2YwLhD},
  year = {2024}
}

@inproceedings{sengupta2024good,
  abstract = {Knowledge distillation (KD) is a technique used to transfer knowledge from a larger ''teacher'' model into a smaller ''student'' model. Recent advancements in meta-learning-based knowledge distillation (MetaKD) emphasize that the fine-tuning of teacher models should be aware of the student's need to achieve better knowledge distillation. However, existing MetaKD methods often lack incentives for the teacher model to improve itself. In this study, we introduce MPDistil, a meta-policy distillation technique, that utilizes novel optimization strategies to foster both *collaboration* and *competition* during the fine-tuning of the teacher model in the meta-learning step. Additionally, we propose a curriculum learning framework for the student model in a competitive setup, in which the student model aims to outperform the teacher model by self-training on various tasks. Exhaustive experiments on SuperGLUE and GLUE benchmarks demonstrate the efficacy of MPDistil compared to 20 conventional KD and advanced MetaKD baselines, showing significant performance enhancements in the student model -- e.g., a distilled 6-layer BERT model outperforms a 12-layer BERT model on five out of six SuperGLUE tasks. Furthermore, MPDistil, while applied to a large language teacher model (DeBERTa-v2-xxlarge), significantly narrows the performance gap of its smaller student counterpart (DeBERTa-12) by just 4.6% on SuperGLUE. We further demonstrate how higher rewards and customized training curricula strengthen the student model and enhance generalizability.},
  author = {Ayan Sengupta and Shantanu Dixit and Md. Shad Akhtar and Tanmoy Chakraborty},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sengupta2024good.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Ixi4j6LtdX},
  publisher = {OpenReview.net},
  title = {A Good Learner can Teach Better: Teacher-Student Collaborative Knowledge Distillation},
  url = {https://openreview.net/forum?id=Ixi4j6LtdX},
  year = {2024}
}

@inproceedings{seo2024let,
  abstract = {Text-to-3D generation has shown rapid progress in recent days with the advent of score distillation sampling (SDS), a methodology of using pretrained text-to-2D diffusion models to optimize a neural radiance field (NeRF) in a zero-shot setting. However, the lack of 3D awareness in the 2D diffusion model often destabilizes previous methods from generating a plausible 3D scene. To address this issue, we propose 3DFuse, a novel framework that incorporates 3D awareness into the pretrained 2D diffusion model, enhancing the robustness and 3D consistency of score distillation-based methods. Specifically, we introduce a consistency injection module which constructs a 3D point cloud from the text prompt and utilizes its projected depth map at given view as a condition for the diffusion model. The 2D diffusion model, through its generative capability, robustly infers dense structure from the sparse point cloud depth map and generates a geometrically consistent and coherent 3D scene. We also introduce a new technique called semantic coding that reduces semantic ambiguity of the text prompt for improved results. Our method can be easily adapted to various text-to-3D baselines, and we experimentally demonstrate how our method notably enhances the 3D consistency of generated scenes in comparison to previous baselines, achieving state-of-the-art performance in geometric robustness and fidelity.},
  author = {Junyoung Seo and Wooseok Jang and Minseop Kwak and Ins Hyeonsu Kim and Jaehoon Ko and Junho Kim and Jin-Hwa Kim and Jiyoung Lee and Seungryong Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/seo2024let.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=UbxWjq0UO2},
  publisher = {OpenReview.net},
  title = {Let {2D} Diffusion Model Know {3D}-Consistency for Robust Text-to-{3D} Generation},
  url = {https://openreview.net/forum?id=UbxWjq0UO2},
  year = {2024}
}

@inproceedings{seo2024teddy,
  abstract = {Since the pioneering work on the lottery ticket hypothesis for graph neural networks (GNNs) was proposed in Chen et al. (2021), the study on finding graph lottery tickets (GLT) has become one of the pivotal focus in the GNN community, inspiring researchers to discover sparser GLT while achieving comparable performance to original dense networks. In parallel, the graph structure has gained substantial attention as a crucial factor in GNN training dynamics, also elucidated by several recent studies. Despite this, contemporary studies on GLT, in general, have not fully exploited inherent pathways in the graph structure and identified tickets in an iterative manner, which is time-consuming and inefficient. To address these limitations, we introduce **TEDDY**, a one-shot edge sparsification framework that leverages structural information by incorporating *edge-degree* statistics. Following the edge sparsification, we encourage the parameter sparsity during training via simple projected gradient descent on the ‚Ñì0 ball. Given the target sparsity levels for both the graph structure and the model parameters, our TEDDY facilitates efficient and rapid realization of GLT within a *single* training. Remarkably, our experimental results demonstrate that TEDDY significantly surpasses conventional iterative approaches in generalization, even when conducting one-shot sparsification that solely utilizes graph structures, without taking feature information into account.},
  author = {Hyunjin Seo and Jihun Yun and Eunho Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/seo2024teddy.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5RUf9nEdyC},
  publisher = {OpenReview.net},
  title = {{TEDDY}: Trimming Edges with Degree-based Discrimination Strategy},
  url = {https://openreview.net/forum?id=5RUf9nEdyC},
  year = {2024}
}

@inproceedings{sergazinov2024glucobench,
  abstract = {The rising rates of diabetes necessitate innovative methods for its management. Continuous glucose monitors (CGM) are small medical devices that measure blood glucose levels at regular intervals providing insights into daily patterns of glucose variation. Forecasting of glucose trajectories based on CGM data holds the potential to substantially improve diabetes management, by both refining artificial pancreas systems and enabling individuals to make adjustments based on predictions to maintain optimal glycemic range. Despite numerous methods proposed for CGM-based glucose trajectory prediction, these methods are typically evaluated on small, private datasets, impeding reproducibility, further research, and practical adoption. The absence of standardized prediction tasks and systematic comparisons between methods has led to uncoordinated research efforts, obstructing the identification of optimal tools for tackling specific challenges. As a result, only a limited number of prediction methods have been implemented in clinical practice. To address these challenges, we present a comprehensive resource that provides: (1) a consolidated repository of curated publicly available CGM datasets to foster reproducibility and accessibility; (2) a standardized task list to unify research objectives and facilitate coordinated efforts; (3) a set of benchmark models with established baseline performance, enabling the research community to objectively gauge new methods' efficacy; (4) a detailed analysis of performance-influencing factors for model development. We anticipate these resources to propel collaborative research endeavors in the critical domain of CGM-based glucose predictions. Our code is available online at github.com/IrinaStatsLab/GlucoBench.},
  author = {Renat Sergazinov and Elizabeth Chun and Valeriya Rogovchenko and Nathaniel J. Fernandes and Nicholas Kasman and Irina Gaynanova},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sergazinov2024glucobench.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cUSNs8nGaV},
  publisher = {OpenReview.net},
  title = {{GlucoBench}: Curated List of Continuous Glucose Monitoring Datasets with Prediction Benchmarks},
  url = {https://openreview.net/forum?id=cUSNs8nGaV},
  year = {2024}
}

@inproceedings{seyyedsalehi2024sointer,
  abstract = {This paper proposes a novel interpretation technique to explain the behavior of structured output models, which simultaneously learn mappings between an input vector and a set of output variables. As a result of the complex relationships between the computational path of output variables in structured models, a feature may impact an output value via other output variables. We focus on one of the outputs as the target and try to find the most important features adopted by the structured model to decide on the target in each locality of the input space. We consider an arbitrary structured output model available as a black-box and argue that considering correlations among output variables can improve explanation quality. The goal is to train a function as an interpreter for the target output variable over the input space. We introduce an energy-based training process for the interpreter function, which effectively considers the structural information incorporated into the model to be explained. The proposed method's effectiveness is confirmed using various simulated and real data sets.},
  author = {Seyyede Fatemeh Seyyedsalehi and Mahdieh Soleymani Baghshah and Hamid R. Rabiee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/seyyedsalehi2024sointer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Fn655mJ4bv},
  publisher = {OpenReview.net},
  title = {{SOInter}: A Novel Deep Energy-Based Interpretation Method for Explaining Structured Output Models},
  url = {https://openreview.net/forum?id=Fn655mJ4bv},
  year = {2024}
}

@inproceedings{shafir2024human,
  abstract = {Recent work has demonstrated the significant potential of denoising diffusion models for generating human motion, including text-to-motion capabilities. However, these methods are restricted by the paucity of annotated motion data, a focus on single-person motions, and a lack of detailed control. In this paper, we introduce three forms of composition based on diffusion priors: sequential, parallel, and model composition. Using sequential composition, we tackle the challenge of long sequence generation. We introduce DoubleTake, an inference-time method with which we generate long animations consisting of sequences of prompted intervals and their transitions, using a prior trained only for short clips. Using parallel composition, we show promising steps toward two-person generation. Beginning with two fixed priors as well as a few two-person training examples, we learn a slim communication block, ComMDM, to coordinate interaction between the two resulting motions. Using model composition, we first train individual priors to complete motions that realize a prescribed motion for a given joint. We then introduce DiffusionBlending, an interpolation mechanism to effectively blend several such models to enable flexible and efficient fine-grained joint and trajectory-level control and editing.},
  author = {Yoni Shafir and Guy Tevet and Roy Kapon and Amit Haim Bermano},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shafir2024human.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dTpbEdN9kr},
  publisher = {OpenReview.net},
  title = {Human Motion Diffusion as a Generative Prior},
  url = {https://openreview.net/forum?id=dTpbEdN9kr},
  year = {2024}
}

@inproceedings{shah2024spder,
  abstract = {We present a neural network architecture designed to naturally learn a positional embedding and overcome the spectral bias towards lower frequencies faced by conventional implicit neural representation networks. Our proposed architecture, {SPDER}, is a simple {MLP} that uses an activation function composed of a sinusoidal multiplied by a sublinear function, called the damping function. The sinusoidal enables the network to automatically learn the positional embedding of an input coordinate while the damping passes on the actual coordinate value by preventing it from being projected down to within a finite range of values. {SPDERs} speed up training by 10 times and converge to losses 1,500 to 50,000 times lower than that of the state-of-the-art for image representation. {SPDER} is also state-of-the-art in audio representation. The superior representation capability allows {SPDER} to also excel on multiple downstream tasks such as image super-resolution and video frame interpolation.},
  author = {Kathan Shah and Chawin Sitawarin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shah2024spder.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=92btneN9Wm},
  publisher = {OpenReview.net},
  title = {{SPDER}: Semiperiodic Damping-Enabled Object Representation},
  url = {https://openreview.net/forum?id=92btneN9Wm},
  year = {2024}
}

@inproceedings{shamsabadi2024confidentialdpproof,
  abstract = {Post hoc privacy auditing techniques can be used to test the privacy guarantees of a model, but come with several limitations: (i) they can only establish lower bounds on the privacy loss, (ii) the intermediate model updates and some data must be shared with the auditor to get a better approximation of the privacy loss, and (iii) the auditor typically faces a steep computational cost to run a large number of attacks. In this paper, we propose to proactively generate a cryptographic certificate of privacy during training to forego such auditing limitations. We introduce Confidential-{DPproof}, a framework for Confidential Proof of Differentially Private Training, which enhances training with a certificate of the ({$\varepsilon$},{$\delta$})-{DP} guarantee. To obtain this certificate without revealing information about the training data or model, we design a customized zero-knowledge proof protocol tailored to the requirements introduced by differentially private training, including random noise addition and privacy amplification by subsampling. In experiments on {CIFAR-10}, Confidential-{DPproof} trains a model achieving state-of-the-art 91\% test accuracy with a certified privacy guarantee of ({$\varepsilon$}=0.55,{$\delta$}=10$^{-5}$)-{DP} in approximately 100 hours.},
  author = {Ali Shahin Shamsabadi and Gefei Tan and Tudor Ioan Cebere and Aurlien Bellet and Hamed Haddadi and Nicolas Papernot and Xiao Wang and Adrian Weller},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shamsabadi2024confidentialdpproof.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=PQY2v6VtGe},
  publisher = {OpenReview.net},
  title = {Confidential-{DPproof}: Confidential Proof of Differentially Private Training},
  url = {https://openreview.net/forum?id=PQY2v6VtGe},
  year = {2024}
}

@inproceedings{shamsi2024graphpulse,
  abstract = {Many real-world networks evolve over time, and predicting the evolution of such networks remains a challenging task. Graph Neural Networks ({GNNs}) have shown empirical success for learning on static graphs, but they lack the ability to effectively learn from nodes and edges with different timestamps. Consequently, the prediction of future properties in temporal graphs remains a relatively under-explored area. In this paper, we aim to bridge this gap by introducing a principled framework, named {GraphPulse}. The framework combines two important techniques for the analysis of temporal graphs within a Newtonian framework. First, we employ the Mapper method, a key tool in topological data analysis, to extract essential clustering information from graph nodes. Next, we harness the sequential modeling capabilities of Recurrent Neural Networks ({RNNs}) for temporal reasoning regarding the graph's evolution. Through extensive experimentation, we demonstrate that our model enhances the {ROC-AUC} metric by 10.2\% in comparison to the top-performing state-of-the-art method across various temporal networks.},
  author = {Kiarash Shamsi and Farimah Poursafaei and Shenyang Huang and Bao Tran Gia Ngo and Baris Coskunuzer and Cuneyt Gurcan Akcora},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shamsi2024graphpulse.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DZqic2sPTY},
  publisher = {OpenReview.net},
  title = {{GraphPulse}: Topological representations for temporal graph property prediction},
  url = {https://openreview.net/forum?id=DZqic2sPTY},
  year = {2024}
}

@inproceedings{shao2024omniquant,
  abstract = {Large language models ({LLMs}) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization ({PTQ}) methods are effective in reducing memory footprint and improving the computational efficiency of {LLM}, they hand-craft quantization parameters, leading to low performance, especially in extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization ({OmniQuant}) technique for {LLMs}, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of {PTQ} by efficiently optimizing various quantization parameters. {OmniQuant} comprises two innovative components including Learnable Weight Clipping ({LWC}) and Learnable Equivalent Transformation ({LET}). {LWC} modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, {LET} tackles activation outliers by shifting the challenge of quantization from activations to weights through a learnable equivalent transformation. Operating within a differentiable framework using block-wise error minimization, {OmniQuant} can optimize the quantization process efficiently for both weight-only and weight-activation quantization. For instance, the {LLaMA-2} model family size 7-70B can be processed with {OmniQuant} on a single A100-40G {GPU} within 1-16 hours using 128 samples. Extensive experiments validate {OmniQuant}'s superior performance across diverse quantization configurations such as W4A4 (4-bit weight, 4-bit activation), W6A6, W4A16, W3A16, and W2A16. Additionally, {OmniQuant} demonstrates effectiveness in instruction-tuned models and delivers notable improvements in inference speed and memory reduction on real devices.},
  author = {Wenqi Shao and Mengzhao Chen and Zhaoyang Zhang and Peng Xu and Lirui Zhao and Zhiqian Li and Kaipeng Zhang and Peng Gao and Yu Qiao and Ping Luo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shao2024omniquant.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; {ICLR} 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=8Wuvhh0LYW},
  publisher = {OpenReview.net},
  title = {{OmniQuant}: Omnidirectionally Calibrated Quantization for Large Language Models},
  url = {https://openreview.net/forum?id=8Wuvhh0LYW},
  year = {2024}
}

@inproceedings{sharma2024truth,
  abstract = {Transformer-based Large Language Models ({LLMs}) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of {LLMs} by selectively removing higher-order components of their weight matrices. This simple intervention, which we call {LAyer-SElective} Rank reduction ({LASER}), can be done on a model after training has completed, and requires no additional parameters or data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when {LASER} is effective and the mechanism by which it operates.},
  author = {Pratyusha Sharma and Jordan T. Ash and Dipendra Misra},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sharma2024truth.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ozX92bu8VA},
  publisher = {OpenReview.net},
  title = {The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction},
  url = {https://openreview.net/forum?id=ozX92bu8VA},
  year = {2024}
}

@inproceedings{sharma2024auccl,
  abstract = {Self-supervised learning through contrastive representations is an emergent and promising avenue, aiming at alleviating the availability of labeled data. Recent research in the field also demonstrates its viability for several downstream tasks, henceforth leading to works that implement the contrastive principle through innovative loss functions and methods. However, despite achieving impressive progress, most methods depend on prohibitively large batch sizes and compute requirements for good performance. In this work, we propose the {AUC-Contrastive} Learning, a new approach to contrastive learning that demonstrates robust and competitive performance in compute-limited regimes. We propose to incorporate the contrastive objective within the {AUC-maximization} framework, by noting that the {AUC} metric is maximized upon enhancing the probability of the network's binary prediction difference between positive and negative samples which inspires adequate embedding space arrangements in representation learning. Unlike standard contrastive methods, when performing stochastic optimization, our method maintains unbiased stochastic gradients and thus is more robust to batchsizes as opposed to standard stochastic optimization problems. Remarkably, our method with a batch size of 256, outperforms several state-of-the-art methods that may need much larger batch sizes (e.g., 4096), on {ImageNet} and other standard datasets.},
  author = {Rohan Sharma and Kaiyi Ji and Zhiqiang Xu and Changyou Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sharma2024auccl.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=YgMdDQB09U},
  publisher = {OpenReview.net},
  title = {{AUC-CL}: A Batchsize-Robust Framework for Self-Supervised Contrastive Representation Learning},
  url = {https://openreview.net/forum?id=YgMdDQB09U},
  year = {2024}
}

@inproceedings{sharma2024towards,
  abstract = {Reinforcement learning from human feedback ({RLHF}) is a popular technique for training high-quality {AI} assistants. However, {RLHF} may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in {RLHF}-trained models and whether human preference judgments are responsible. We first demonstrate that five state-of-the-art {AI} assistants consistently exhibit sycophancy behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of {RLHF} models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models ({PMs}) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against {PMs} also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of {RLHF} models, likely driven in part by human preference judgments favoring sycophantic responses.},
  author = {Mrinank Sharma and Meg Tong and Tomasz Korbak and David Duvenaud and Amanda Askell and Samuel R. Bowman and Esin Durmus and Zac Hatfield-Dodds and Scott R. Johnston and Shauna Kravec and Timothy Maxwell and Sam McCandlish and Kamal Ndousse and Oliver Rausch and Nicholas Schiefer and Da Yan and Miranda Zhang and Ethan Perez},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sharma2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=tvhaxkMKAn},
  publisher = {OpenReview.net},
  title = {Towards Understanding Sycophancy in Language Models},
  url = {https://openreview.net/forum?id=tvhaxkMKAn},
  year = {2024}
}

@inproceedings{shaul2024bespoke,
  abstract = {Diffusion or flow-based models are powerful generative paradigms that are notoriously hard to sample as samples are defined as solutions to high-dimensional Ordinary or Stochastic Differential Equations ({ODEs}/{SDEs}) which require a large Number of Function Evaluations ({NFE}) to approximate well. Existing methods to alleviate the costly sampling process include model distillation and designing dedicated {ODE} solvers. However, distillation is costly to train and sometimes can deteriorate quality, while dedicated solvers still require relatively large {NFE} to produce high quality samples. In this paper we introduce ``Bespoke solvers'', a novel framework for constructing custom {ODE} solvers tailored to the {ODE} of a given pre-trained flow model. Our approach optimizes an order consistent and parameter-efficient solver (e.g., with 80 learnable parameters), is trained for roughly 1\% of the {GPU} time required for training the pre-trained model, and significantly improves approximation and generation quality compared to dedicated solvers. For example, a Bespoke solver for a {CIFAR10} model produces samples with Frchet Inception Distance ({FID}) of 2.73 with 10 {NFE}, and gets to 1\% of the Ground Truth ({GT}) {FID} (2.59) for this model with only 20 {NFE}. On the more challenging {ImageNet}-64√ó64, Bespoke samples at 2.2 {FID} with 10 {NFE}, and gets within 2\% of {GT} {FID} (1.71) with 20 {NFE}.},
  author = {Neta Shaul and Juan C. Prez and Ricky T. Q. Chen and Ali K. Thabet and Albert Pumarola and Yaron Lipman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shaul2024bespoke.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; {ICLR} 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=1PXEY7ofFX},
  publisher = {OpenReview.net},
  title = {Bespoke Solvers for Generative Flow Models},
  url = {https://openreview.net/forum?id=1PXEY7ofFX},
  year = {2024}
}

@inproceedings{shayegani2024jailbreak,
  abstract = {We introduce new jailbreak attacks on vision language models ({VLMs}), which use aligned {LLMs} and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the {LLM} draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the {LLM} model.},
  author = {Erfan Shayegani and Yue Dong and Nael B. Abu-Ghazaleh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shayegani2024jailbreak.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; {ICLR} 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=plmBsXHxgR},
  publisher = {OpenReview.net},
  title = {Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models},
  url = {https://openreview.net/forum?id=plmBsXHxgR},
  year = {2024}
}

@inproceedings{shayegh2024ensemble,
  abstract = {The paper investigates unsupervised constituency parsing, proposing a tree averaging approach and an ensemble method for parsing. The authors aim to enhance parsing performance by leveraging different aspects of parsing structures across existing unsupervised parsers, with contributions including a novel ensemble method for unsupervised parsing and knowledge distillation to improve inference efficiency.},
  author = {Behzad Shayegh and Yanshuai Cao and Xiaodan Zhu and Jackie C. K. Cheung and Lili Mou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shayegh2024ensemble.pdf:pdf},
  keywords = {Constituency Parsing, Unsupervised Grammar Induction, Knowledge Distillation},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=RR8y0WKrFv},
  publisher = {OpenReview.net},
  title = {Ensemble Distillation for Unsupervised Constituency Parsing},
  url = {https://openreview.net/forum?id=RR8y0WKrFv},
  year = {2024}
}

@inproceedings{shen2024gim,
  abstract = {We propose the first framework to learn a single generalizable image matching model from internet videos. GIM is a self-training framework that improves image matching generalization across different domains using internet videos. We introduce ZEB, a zero-shot evaluation benchmark for image matching, and demonstrate performance improvements across different architectures in pose estimation and 3D reconstruction tasks.},
  author = {Xuelun Shen and Zhipeng Cai and Wei Yin and Matthias M{\"u}ller and Zijun Li and Kaixuan Wang and Xiaozhi Chen and Cheng Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024gim.pdf:pdf},
  keywords = {Image Matching, Pose Estimation, 3D Reconstruction},
  note = {Spotlight presentation. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=NYN1b8GRGS},
  publisher = {OpenReview.net},
  title = {{GIM}: Learning Generalizable Image Matcher From Internet Videos},
  url = {https://openreview.net/forum?id=NYN1b8GRGS},
  year = {2024}
}

@inproceedings{shen2024multiresolution,
  abstract = {The diffusion model has been successfully used in many computer vision applications. We leverage the multi-resolution temporal structure in time series and propose the multi-resolution diffusion model (mr-Diff). The model uses seasonal-trend decomposition to extract fine-to-coarse trends through a non-autoregressive denoising process. We demonstrate effectiveness on nine real-world time series datasets.},
  author = {Lifeng Shen and Weiyu Chen and James T. Kwok},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024multiresolution.pdf:pdf},
  keywords = {Diffusion model, Time series, Multiscale},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mmjnr0G8ZY},
  publisher = {OpenReview.net},
  title = {Multi-Resolution Diffusion Models for Time Series Forecasting},
  url = {https://openreview.net/forum?id=mmjnr0G8ZY},
  year = {2024}
}

@inproceedings{shen2024trickledown,
  abstract = {We examine inconsistency in Reward Models (RMs) in Reinforcement Learning from Human Feedback (RLHF) and its trickle-down impact. We propose the Contrast Instruction benchmarking strategy and introduce techniques ConvexDA and RewardFusion to improve reward consistency and overall RLHF performance.},
  author = {Lingfeng Shen and Sihao Chen and Linfeng Song and Lifeng Jin and Baolin Peng and Haitao Mi and Daniel Khashabi and Dong Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024trickledown.pdf:pdf},
  keywords = {Large language model, reward model, RLHF, consistency},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MeHmwCDifc},
  publisher = {OpenReview.net},
  title = {The Trickle-down Impact of Reward Inconsistency on {RLHF}},
  url = {https://openreview.net/forum?id=MeHmwCDifc},
  year = {2024}
}

@inproceedings{shen2024finetuning,
  abstract = {We address biases in text-to-image diffusion models by proposing a distributional alignment loss to steer image generation and reduce gender, racial, and intersectional biases. Our approach supports diverse perspectives of fairness through adjusted direct finetuning (DFT) of the sampling process.},
  author = {Xudong Shen and Chao Du and Tianyu Pang and Min Lin and Yongkang Wong and Mohan S. Kankanhalli},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024finetuning.pdf:pdf},
  keywords = {Fairness, Alignment, Diffusion Models, Text-to-Image Generation},
  note = {Oral presentation. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=hnrB5YHoYu},
  publisher = {OpenReview.net},
  title = {Finetuning Text-to-Image Diffusion Models for Fairness},
  url = {https://openreview.net/forum?id=hnrB5YHoYu},
  year = {2024}
}

@inproceedings{shen2024mixtureofexperts,
  abstract = {We explore combining Sparse Mixture-of-Experts (MoE) with instruction tuning for large language models. We study three experimental setups and demonstrate that FLAN-MoE-32B surpasses Flan-PaLM-62B on four benchmark tasks, showing that Mixture-of-Expert models benefit more from instruction tuning than dense models.},
  author = {Sheng Shen and Le Hou and Yanqi Zhou and Nan Du and Shayne Longpre and Jason Wei and Hyung Won Chung and Barret Zoph and William Fedus and Xinyun Chen and Tu Vu and Yuexin Wu and Wuyang Chen and Albert Webson and Yunxuan Li and Vincent Y. Zhao and Hongkun Yu and Kurt Keutzer and Trevor Darrell and Denny Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024mixtureofexperts.pdf:pdf},
  keywords = {MoE, Instruction Tuning},
  note = {DBLP last modified: 2025-02-18},
  pdf = {https://openreview.net/pdf?id=6mLjDwYte5},
  publisher = {OpenReview.net},
  title = {Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models},
  url = {https://openreview.net/forum?id=6mLjDwYte5},
  year = {2024}
}

@inproceedings{shen2024n,
  abstract = {We focus on scaling text-to-speech (TTS) to large-scale, multi-speaker datasets using neural audio codec with residual vector quantizers. We implement a diffusion model for latent vector generation and introduce speech prompting mechanism for zero-shot capability. The model is trained on 44K hours of speech and singing data.},
  author = {Kai Shen and Zeqian Ju and Xu Tan and Eric Liu and Yichong Leng and Lei He and Tao Qin and Sheng Zhao and Jiang Bian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024n.pdf:pdf},
  keywords = {text-to-speech, large-scale corpus, non-autoregressive, diffusion},
  note = {Spotlight presentation. Audio samples: https://naturalspeech2.github.io/. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Rc7dAwVL3v},
  publisher = {OpenReview.net},
  title = {{N}atural{S}peech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers},
  url = {https://openreview.net/forum?id=Rc7dAwVL3v},
  year = {2024}
}

@inproceedings{shen2024advancing,
  abstract = {Recent work has showcased the significant potential of diffusion models in pose-guided person image synthesis. However, owing to the inconsistency in pose between the source and target images, synthesizing an image with a distinct pose remains a formidable challenge. We propose Progressive Conditional Diffusion Models (PCDMs) that incrementally bridge pose differences through a three-stage approach.},
  author = {Fei Shen and Hu Ye and Jun Zhang and Cong Wang and Xiao Han and Yang Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024advancing.pdf:pdf},
  keywords = {Diffusion Model, Pose-Guided Image Synthesis},
  note = {Code: https://github.com/tencent-ailab/PCDMs. DBLP last modified: 2024-08-23},
  pdf = {https://openreview.net/pdf?id=rHzapPnCgT},
  publisher = {OpenReview.net},
  title = {Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models},
  url = {https://openreview.net/forum?id=rHzapPnCgT},
  year = {2024}
}

@inproceedings{shen2024measuring,
  abstract = {We introduce a challenge to test neural models' STEM skills with a dataset including 448 skills and 1,073,146 questions across STEM subjects. We focus on K-12 curriculum fundamental skills and test models like CLIP and GPT-3.5-Turbo. Models performed poorly, mastering only 2.5\% of third-grade skills.},
  author = {Jianhao Shen and Ye Yuan and Srbuhi Mirzoyan and Ming Zhang and Chenguang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shen2024measuring.pdf:pdf},
  keywords = {Benchmark, STEM, Multimodal, Vision-language models, Language models},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=spvaV5LELF},
  publisher = {OpenReview.net},
  title = {Measuring Vision-Language {STEM} Skills of Neural Models},
  url = {https://openreview.net/forum?id=spvaV5LELF},
  year = {2024}
}

@inproceedings{shenbin2024i,
  abstract = {We present ImplicitSLIM, a novel unsupervised learning approach for sparse high-dimensional data, with applications to collaborative filtering. Sparse linear methods (SLIM) and their variations show outstanding performance, but they are memory-intensive and hard to scale. Our approach improves embedding-based models by extracting embeddings from SLIM-like models in a computationally cheap and memory-efficient manner.},
  author = {Ilya Shenbin and Sergey I. Nikolenko},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shenbin2024i.pdf:pdf},
  keywords = {Collaborative filtering, Representation learning},
  note = {Code: https://github.com/ilya-shenbin/ImplicitSLIM. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6vF0ZJGor4},
  publisher = {OpenReview.net},
  title = {{I}mplicit{SLIM} and How it Improves Embedding-based Collaborative Filtering},
  url = {https://openreview.net/forum?id=6vF0ZJGor4},
  year = {2024}
}

@inproceedings{sherborne2024tram,
  abstract = {Sharpness-aware minimization (SAM) reports improving domain generalization by reducing the loss surface curvature in the parameter space. However, generalization during fine-tuning is often more dependent on the transferability of representations in the function space. Trust-region methods (TR) target this goal by regularizing representation curvature to reduce catastrophic forgetting of pre-trained task-agnostic information while adopting task-specific skills. We consider unifying these strategies for low curvature in both parameter space and function space to improve out-of-domain (OOD) generalization. We propose Trust Region Aware Minimization (TRAM), a SAM algorithm fine-tuning for low parameter sharpness and smooth, informative representations preserving pre-trained structure. TRAM uses a trust region bound to inform the SAM adversarial neighborhood, introducing an awareness of function curvature within optimization for flatter minima.},
  author = {Tom Sherborne and Naomi Saphra and Pradeep Dasigi and Hao Peng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sherborne2024tram.pdf:pdf},
  note = {Spotlight paper; ArXiv: 2310.03646},
  pdf = {https://openreview.net/pdf?id=kxebDHZ7b7},
  publisher = {OpenReview.net},
  title = {{TRAM}: Bridging Trust Regions and Sharpness Aware Minimization},
  url = {https://openreview.net/forum?id=kxebDHZ7b7},
  year = {2024}
}

@inproceedings{shetty2024generalized,
  abstract = {Control of dynamic systems involving hybrid actions is a challenging task in robotics. To address this, we present a novel algorithm called Generalized Policy Iteration using Tensor Train (TTPI) that belongs to the class of Approximate Dynamic Programming (ADP). We use a low-rank tensor approximation technique called Tensor Train (TT) to approximate the state-value and advantage function which enables us to efficiently handle hybrid systems. We demonstrate the superiority of our approach over previous baselines for some benchmark problems with hybrid action spaces. Additionally, the robustness and generalization of the policy for hybrid systems are showcased through a real-world robotics experiment.},
  author = {Suhan Shetty and Teng Xue and Sylvain Calinon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shetty2024generalized.pdf:pdf},
  note = {Also published in Journal of Field Robotics 2024},
  pdf = {https://openreview.net/pdf?id=csukJcpYDe},
  publisher = {OpenReview.net},
  title = {Generalized Policy Iteration using Tensor Approximation for Hybrid Control},
  url = {https://openreview.net/forum?id=csukJcpYDe},
  year = {2024}
}

@inproceedings{shi2024towards,
  abstract = {Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models will analyze the underlying rules and select one image from candidates to complete the image matrix. Participators of RPM tests can show powerful reasoning ability by inferring and combining attribute-changing rules and imagining the missing images at arbitrary positions of a matrix. However, existing solvers can hardly manifest such an ability in realistic RPM tests. In this paper, we propose a deep latent variable model for answer generation problems through Rule AbstractIon and SElection (RAISE). RAISE can encode image attributes into latent concepts and abstract atomic rules that act on the latent concepts.},
  author = {Fan Shi and Bin Li and Xiangyang Xue},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024towards.pdf:pdf},
  note = {ArXiv: 2401.09966; Code: github.com/FudanVI/generative-abstract-reasoning},
  pdf = {https://openreview.net/pdf?id=IcR1OOFzxm},
  publisher = {OpenReview.net},
  title = {Towards Generative Abstract Reasoning: Completing {Raven's} Progressive Matrix via Rule Abstraction and Selection},
  url = {https://openreview.net/forum?id=IcR1OOFzxm},
  year = {2024}
}

@inproceedings{shi2024detecting,
  abstract = {Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method MIN-K PROB based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities.},
  author = {Weijia Shi and Anirudh Ajith and Mengzhou Xia and Yangsibo Huang and Daogao Liu and Terra Blevins and Danqi Chen and Luke Zettlemoyer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024detecting.pdf:pdf},
  note = {ArXiv: 2310.16789; Code: github.com/swj0419/detect-pretrain-code},
  pdf = {https://openreview.net/pdf?id=zWqr3MQuNs},
  publisher = {OpenReview.net},
  title = {Detecting Pretraining Data from Large Language Models},
  url = {https://openreview.net/forum?id=zWqr3MQuNs},
  year = {2024}
}

@inproceedings{shi2024towards,
  abstract = {Spiking Neural Networks (SNNs) have emerged as energy-efficient alternatives to Artificial Neural Networks (ANNs) when deployed on neuromorphic chips. However, while recent studies have demonstrated the impressive performance of deep SNNs on challenging tasks, their energy efficiency advantage has been diminished. Existing methods targeting energy consumption reduction do not fully exploit sparsity, whereas powerful pruning methods can achieve high sparsity but are not directly targeted at energy efficiency, limiting their effectiveness in energy saving. In this work, we combine unstructured weight pruning with unstructured neuron pruning to maximize the utilization of the sparsity of neuromorphic computing, thereby enhancing energy efficiency.},
  author = {Xinyu Shi and Jianhao Ding and Zecheng Hao and Zhaofei Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024towards.pdf:pdf},
  note = {Code: github.com/xyshi2000/Unstructured-Pruning},
  pdf = {https://openreview.net/pdf?id=eoSeaK4QJo},
  publisher = {OpenReview.net},
  title = {Towards Energy Efficient Spiking Neural Networks: An Unstructured Pruning Framework},
  url = {https://openreview.net/forum?id=eoSeaK4QJo},
  year = {2024}
}

@inproceedings{shi2024analysis,
  abstract = {Image-to-image (I2I) translation is vital in computer vision tasks like style transfer and domain adaptation. While recent advances in GAN have enabled high-quality sample generation, real-world challenges such as noise and distortion remain significant obstacles. Although Gaussian noise injection during training has been utilized, its theoretical underpinnings have been unclear. This work provides a robust theoretical framework elucidating the role of Gaussian noise injection in I2I translation models. We address critical questions on the influence of noise variance on distribution divergence. Our contributions include connecting f-divergence and score matching, unveiling insights into the impact of Gaussian noise on aligning probability distributions, and demonstrating generalized robustness implications.},
  author = {Chaohua Shi and Kexin Huang and Lu Gan and Hongqing Liu and Mingrui Zhu and Nannan Wang and Xinbo Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024analysis.pdf:pdf},
  note = {Code: github.com/Alan0693/Noise-Injection},
  pdf = {https://openreview.net/pdf?id=sLregLuXpn},
  publisher = {OpenReview.net},
  title = {On the Analysis of {GAN}-based Image-to-Image Translation with {Gaussian} Noise Injection},
  url = {https://openreview.net/forum?id=sLregLuXpn},
  year = {2024}
}

@inproceedings{shi2024exedec,
  abstract = {When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step.},
  author = {Kensen Shi and Joey Hong and Yinlin Deng and Pengcheng Yin and Manzil Zaheer and Charles Sutton},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024exedec.pdf:pdf},
  note = {Oral presentation; ArXiv: 2307.13883; Code: github.com/google-deepmind/exedec},
  pdf = {https://openreview.net/pdf?id=oTRwljRgiv},
  publisher = {OpenReview.net},
  title = {{ExeDec}: Execution Decomposition for Compositional Generalization in Neural Program Synthesis},
  url = {https://openreview.net/forum?id=oTRwljRgiv},
  year = {2024}
}

@inproceedings{shi2024multiresolution,
  abstract = {Existing Self-Supervised Learning (SSL) models for speech typically process speech signals at a fixed resolution of 20 milliseconds. This approach overlooks the varying informational content present at different resolutions in speech signals. In contrast, this paper aims to incorporate multi-resolution information into speech self-supervised representation learning. We introduce a SSL model that leverages a hierarchical Transformer architecture, complemented by HuBERT-style masked prediction objectives, to process speech at multiple resolutions. Experimental results indicate that the proposed model not only achieves more efficient inference but also exhibits superior or comparable performance to the original HuBERT model over various tasks.},
  author = {Jiatong Shi and Hirofumi Inaguma and Xutai Ma and Ilia Kulikov and Anna Y. Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024multiresolution.pdf:pdf},
  note = {ArXiv: 2310.02720},
  pdf = {https://openreview.net/pdf?id=kUuKFW7DIF},
  publisher = {OpenReview.net},
  title = {Multi-resolution {HuBERT}: Multi-resolution Speech Self-Supervised Learning with Masked Unit Prediction},
  url = {https://openreview.net/forum?id=kUuKFW7DIF},
  year = {2024}
}

@inproceedings{shi2024dept,
  abstract = {Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates.},
  author = {Zhengxiang Shi and Aldo Lipani},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024dept.pdf:pdf},
  note = {ArXiv: 2309.05173; Code: github.com/ZhengxiangShi/DePT},
  pdf = {https://openreview.net/pdf?id=KjegfPGRde},
  publisher = {OpenReview.net},
  title = {{DePT}: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning},
  url = {https://openreview.net/forum?id=KjegfPGRde},
  year = {2024}
}

@inproceedings{shi2024unleashing,
  abstract = {Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces Language Models for Motion Control (LaMo), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Empirical results indicate LaMo achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks.},
  author = {Ruizhe Shi and Yuyao Liu and Yanjie Ze and Simon Shaolei Du and Huazhe Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shi2024unleashing.pdf:pdf},
  note = {ArXiv: 2310.20587; Code: github.com/srzer/LaMo-2023},
  pdf = {https://openreview.net/pdf?id=AY6aM13gGF},
  publisher = {OpenReview.net},
  title = {Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=AY6aM13gGF},
  year = {2024}
}

@inproceedings{shi2024incontext,
  abstract = {Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. In-Context Pretraining offers a simple and scalable approach to significantly enhance LMs' performance, and is seen to see notable improvements in tasks that require more complex contextual reasoning, including in-context learning.},
  author = {Weijia Shi and Sewon Min and Maria Lomeli and Chunting Zhou and Margaret Li and Gergely Szilvasy and Rich James and Xi Victoria Lin and Noah A. Smith and Luke Zettlemoyer and Scott Yih and Mike Lewis},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shi2024incontext.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=LXVswInHOo},
  publisher = {OpenReview.net},
  title = {{In-Context Pretraining: Language Modeling Beyond Document Boundaries}},
  url = {https://openreview.net/forum?id=LXVswInHOo},
  year = {2024}
}

@inproceedings{shi2024graphconstrained,
  abstract = {Path planning underpins various applications such as transportation, logistics, and robotics. Conventionally, path planning is formulated with explicit optimization objectives such as distance or time. However, real-world data reveals that user intentions are hard-to-model, suggesting a need for data-driven path planning that implicitly incorporates the complex user intentions. In this paper, we propose GDP, a diffusion-based model for end-to-end data-driven path planning. It effectively learns path patterns via a novel diffusion process that incorporates constraints from road networks.},
  author = {Dingyuan Shi and Yongxin Tong and Zimu Zhou and Ke Xu and Zheng Wang and Jieping Ye},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shi2024graphconstrained.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vuK8MhVtuu},
  publisher = {OpenReview.net},
  title = {{Graph-constrained Diffusion for End-to-End Path Planning}},
  url = {https://openreview.net/forum?id=vuK8MhVtuu},
  year = {2024}
}

@inproceedings{shi2024toss,
  abstract = {TOSS presents a novel approach that introduces text to the task of novel view synthesis (NVS) from just a single RGB image. While Zero-1-to-3 has demonstrated impressive zero-shot open-set NVS capability, it treats NVS as a pure image-to-image translation problem. This approach suffers from the challengingly under-constrained nature of single-view NVS: the process lacks means of explicit user control and often results in implausible NVS generations. To address this limitation, TOSS uses text as high-level semantic information to constrain the NVS solution space. TOSS fine-tunes text-to-image Stable Diffusion pre-trained on large-scale text-image pairs and introduces modules specifically tailored to image and camera pose conditioning, as well as dedicated training for pose correctness and preservation of fine details. Comprehensive experiments are conducted with results showing that the proposed TOSS outperforms Zero-1-to-3 with more plausible, controllable and multiview-consistent NVS results. The results are further supported with comprehensive ablations that underscore the effectiveness and potential of the introduced semantic guidance and architecture design.},
  author = {Yukai Shi and Jianan Wang and He Cao and Boshi Tang and Xianbiao Qi and Tianyu Yang and Yukun Huang and Shilong Liu and Lei Zhang and Heung-Yeung Shum},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shi2024toss.pdf:pdf},
  note = {DBLP last modified: 2024-10-11},
  pdf = {https://openreview.net/pdf?id=9ZUYJpvIys},
  publisher = {OpenReview.net},
  title = {{TOSS: High-quality Text-guided Novel View Synthesis from a Single Image}},
  url = {https://openreview.net/forum?id=9ZUYJpvIys},
  year = {2024}
}

@inproceedings{shi2024mvdream,
  abstract = {We introduce MVDream, a diffusion model that is able to generate consistent multi-view images from a given text prompt. Learning from both 2D and 3D data, a multi-view diffusion model can achieve the generalizability of 2D diffusion models and the consistency of 3D renderings. We demonstrate that such a multi-view diffusion model is implicitly a generalizable 3D prior agnostic to 3D representations. It can be applied to 3D generation via Score Distillation Sampling, significantly enhancing the consistency and stability of existing 2D-lifting methods. It can also learn new concepts from a few 2D examples, akin to DreamBooth, but for 3D generation.},
  author = {Yichun Shi and Peng Wang and Jianglong Ye and Long Mai and Kejie Li and Xiao Yang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shi2024mvdream.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FUgrjq2pbB},
  publisher = {OpenReview.net},
  title = {{MVDream: Multi-view Diffusion for 3D Generation}},
  url = {https://openreview.net/forum?id=FUgrjq2pbB},
  year = {2024}
}

@inproceedings{shi2024prompt,
  abstract = {Multimodal pre-trained models have shown impressive potential in enhancing performance on downstream tasks. However, existing fusion strategies for modalities primarily rely on explicit interaction structures that fail to capture the diverse aspects and patterns inherent in input data. This yields limited performance in zero-shot contexts, especially when fine-grained classifications and abstract interpretations are required. To address this, we propose an effective approach, namely Prompt Learning with Quaternion Networks (QNet), for semantic alignment across diverse modalities.},
  author = {Boya Shi and Zhengqin Xu and Shuai Jia and Chao Ma},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shi2024prompt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dKlxDx2SoS},
  publisher = {OpenReview.net},
  title = {{Prompt Learning with Quaternion Networks}},
  url = {https://openreview.net/forum?id=dKlxDx2SoS},
  year = {2024}
}

@inproceedings{shi2024devil,
  abstract = {Foundation models, pre-trained on a large amount of data have demonstrated impressive zero-shot capabilities in various downstream tasks. However, in object detection and instance segmentation, two fundamental computer vision tasks heavily reliant on extensive human annotations, foundation models such as SAM and DINO struggle to achieve satisfactory performance. In this study, we reveal that the devil is in the object boundary, i.e., these foundation models fail to discern boundaries between individual objects. For the first time, we probe that CLIP, which has never accessed any instance-level annotations, can provide a highly beneficial and strong instance-level boundary prior in the clustering results of its particular intermediate layer. Following this surprising observation, we propose Zip which Zips up CLip and SAM in a novel classification-first-then-discovery pipeline, enabling annotation-free, complex-scene-capable, open-vocabulary object detection and instance segmentation. Our Zip significantly boosts SAM's mask AP on COCO dataset by 12.5% and establishes state-of-the-art performance in various settings, including training-free, self-training, and label-efficient finetuning. Furthermore, annotation-free Zip even achieves comparable performance to the best-performing open-vocabulary object detecters using base annotations.},
  author = {Cheng Shi and Sibei Yang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shi2024devil.pdf:pdf},
  note = {DBLP last modified: 2024-11-13},
  pdf = {https://openreview.net/pdf?id=4JbrdrHxYy},
  publisher = {OpenReview.net},
  title = {{The Devil is in the Object Boundary: Towards Annotation-free Instance Segmentation using Foundation Models}},
  url = {https://openreview.net/forum?id=4JbrdrHxYy},
  year = {2024}
}

@inproceedings{shi2024hybrid,
  abstract = {As two prominent strategies for representation learning, Contrastive Learning (CL) and Masked Image Modeling (MIM) have witnessed significant progress. Previous studies have demonstrated the advantages of each approach in specific scenarios. CL, resembling supervised pre-training, excels at capturing longer-range global patterns and enhancing feature discrimination, while MIM is adept at introducing local and diverse attention across transformer layers. Considering the respective strengths, previous studies utilize feature distillation to inherit both discrimination and diversity. In this paper, we thoroughly examine previous feature distillation methods and observe that the increase in diversity mainly stems from asymmetric designs, which may in turn compromise discrimination. Hybrid Distill emulates the token relations of the MIM teacher at intermediate layers for diversity, while simultaneously distilling the final features of the CL teacher to enhance discrimination. A progressive redundant token masking strategy is employed to reduce the expenses associated with distillation and aid in preventing the model from converging to local optima. Experimental results demonstrate that Hybrid Distill achieves superior performance on various benchmark datasets.},
  author = {Bowen Shi and Xiaopeng Zhang and Yaoming Wang and Jin Li and Wenrui Dai and Junni Zou and Hongkai Xiong and Qi Tian},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shi2024hybrid.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=jUWktnsplU},
  publisher = {OpenReview.net},
  title = {{Hybrid Distillation: Connecting Masked Autoencoders with Contrastive Learners}},
  url = {https://openreview.net/forum?id=jUWktnsplU},
  year = {2024}
}

@inproceedings{shidani2024polyview,
  abstract = {Contrastive learning typically matches pairs of related views among a number of unrelated negative views. We investigate matching when there are more than two related views which we call poly-view tasks, and derive new representation learning objectives using information maximization and sufficient statistics. We show that with unlimited computation, one should maximize the number of related views, and with a fixed compute budget, it is beneficial to decrease the number of unique samples whilst increasing the number of views of those samples. In particular, poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k, challenging the belief that contrastive models require large batch sizes and many training epochs.},
  author = {Amitis Shidani and R. Devon Hjelm and Jason Ramapuram and Russell Webb and Eeshan Gunesh Dhekane and Dan Busbridge},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shidani2024polyview.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iHcTLIor0m},
  publisher = {OpenReview.net},
  title = {{Poly-View Contrastive Learning}},
  url = {https://openreview.net/forum?id=iHcTLIor0m},
  year = {2024}
}

@inproceedings{shih2024fast,
  abstract = {Reinforcement learning (RL) tackles sequential decision-making problems by creating agents that interacts with their environment. However, existing algorithms often view these problem as static, focusing on point estimates for model parameters to maximize expected rewards, neglecting the stochastic dynamics of agent-environment interactions and the critical role of uncertainty quantification. Our research leverages the Kalman filtering paradigm to introduce a novel and scalable sampling algorithm called Langevinized Kalman Temporal-Difference (LKTD) for deep reinforcement learning. This algorithm, grounded in Stochastic Gradient Markov Chain Monte Carlo (SGMCMC), efficiently draws samples from the posterior distribution of deep neural network parameters. Under mild conditions, we prove that the posterior samples generated by the LKTD algorithm converge to a stationary distribution. This convergence not only enables us to quantify uncertainties associated with the value function and model parameters but also allows us to monitor these uncertainties during policy updates throughout the training phase. The LKTD algorithm paves the way for more robust and adaptable reinforcement learning approaches.},
  author = {Frank Shih and Faming Liang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shih2024fast.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LZIOBA2oDU},
  publisher = {OpenReview.net},
  title = {{Fast Value Tracking for Deep Reinforcement Learning}},
  url = {https://openreview.net/forum?id=LZIOBA2oDU},
  year = {2024}
}

@inproceedings{shimizu2024improved,
  abstract = {We show how to obtain improved active learning methods in the agnostic (adversarial noise) setting by combining marginal leverage score sampling with non-independent sampling strategies that promote spatial coverage. In particular, we propose an easily implemented method based on the pivotal sampling algorithm, which we test on problems motivated by learning-based methods for parametric PDEs and uncertainty quantification. In comparison to independent sampling, our method reduces the number of samples needed to reach a given target accuracy by up to 50%. We support our findings with two theoretical results. First, we show that any non-independent leverage score sampling method that obeys a weak one-sided ‚Ñì‚àû independence condition (which includes pivotal sampling) can actively learn d dimensional linear functions with O(d log d) samples, matching independent sampling.},
  author = {Atsushi Shimizu and Xiaoou Cheng and Christopher Musco and Jonathan Weare},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/shimizu2024improved.pdf:pdf},
  note = {Oral presentation. DBLP last modified: 2025-06-17},
  pdf = {https://openreview.net/pdf?id=IYxDy2jDFL},
  publisher = {OpenReview.net},
  title = {{Improved Active Learning via Dependent Leverage Score Sampling}},
  url = {https://openreview.net/forum?id=IYxDy2jDFL},
  year = {2024}
}

@inproceedings{shin2024unknown,
  abstract = {The objective of domain generalization (DG) is to enhance the transferability of the model learned from a source domain to unobserved domains. To prevent overfitting to a specific domain, Sharpness-Aware Minimization (SAM) reduces source domain's loss sharpness. However, we point out that there is still room for SAM to improve generalization to unknown domain. While SAM can reduce the source domain's loss sharpness, we highlight that there is potential for further improvement in generalizing to unknown domains. As unknown domains are inaccessible, these domains are empirically crafted by perturbing instances from the source domain dataset. We then propose Unknown Domain Inconsistency Minimization (UDIM), which seeks to reduce the loss landscape inconsistency between source and unknown domains. Our approach aims to align loss landscapes between source and perturbed domains to achieve better generalization.},
  author = {Seungjae Shin and HeeSun Bae and Byeonghu Na and Yoon-Yeong Kim and Il-Chul Moon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shin2024unknown.pdf:pdf},
  keywords = {Robustness, Domain generalization, Sharpness-Aware Minimization, Loss Sharpness, Inconsistency},
  note = {Primary Area: transfer learning, meta learning, and lifelong learning. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eNoiRal5xi},
  publisher = {OpenReview.net},
  title = {Unknown Domain Inconsistency Minimization for Domain Generalization},
  url = {https://openreview.net/forum?id=eNoiRal5xi},
  year = {2024}
}

@inproceedings{shocher2024idempotent,
  abstract = {We propose a new approach for generative modeling based on training a neural network to be idempotent. An idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, namely f(f(z))=f(z). The proposed model f is trained to map a source distribution (e.g, Gaussian noise) to a target distribution (e.g. realistic images) using the following objectives: (1) Instances from the target distribution should map to themselves, namely f(x)=x. The target manifold is defined as the set of all instances that f maps to themselves. (2) Instances from the source distribution should map onto the defined target manifold, achieved by optimizing the idempotence term f(f(z))=f(z) which encourages the range of f(z) to be on the target manifold.},
  author = {Assaf Shocher and Amil Dravid and Yossi Gandelsman and Inbar Mosseri and Michael Rubinstein and Alexei A. Efros},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shocher2024idempotent.pdf:pdf},
  keywords = {Generative model, idempotent, energy based models},
  note = {Primary Area: generative models. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=XIaS66XkNA},
  publisher = {OpenReview.net},
  title = {Idempotent Generative Network},
  url = {https://openreview.net/forum?id=XIaS66XkNA},
  year = {2024}
}

@inproceedings{shoghi2024molecules,
  abstract = {Foundation models have been transformational in machine learning fields such as natural language processing and computer vision. Similar success in atomic property prediction has been limited due to the challenges of training effective models across multiple chemical domains. To address this, we introduce Joint Multi-domain Pre-training (JMP), a supervised pre-training strategy that simultaneously trains on multiple datasets from different chemical domains, treating each dataset as a unique pre-training task within a multi-task framework. Our combined training dataset consists of ~120M systems from OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and generalization by fine-tuning over a diverse set of downstream tasks and datasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP demonstrates an average improvement of 59% over training from scratch and matches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights the potential of pre-training strategies that utilize diverse data to advance property prediction across chemical domains, especially for low-data tasks.},
  author = {Nima Shoghi and Adeesh Kolluru and John R. Kitchin and Zachary W. Ulissi and C. Lawrence Zitnick and Brandon M. Wood},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shoghi2024molecules.pdf:pdf},
  keywords = {atomic property prediction, pre-training, 3D atomic pre-training, graph neural networks, multi-task learning, molecules, materials},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PfPnugdxup},
  publisher = {OpenReview.net},
  title = {From Molecules to Materials: Pre-training Large Generalizable Models for Atomic Property Prediction},
  url = {https://openreview.net/forum?id=PfPnugdxup},
  year = {2024}
}

@inproceedings{shrestha2024towards,
  abstract = {Unsupervised domain translation (UDT) aims to find functions that convert samples from one domain (e.g., sketches) to another domain (e.g., photos) without changing the high-level semantic meaning (also referred to as "content"). The translation functions are often sought by probability distribution matching of the transformed source domain and target domain. CycleGAN stands as arguably the most representative approach among this line of work. However, it was noticed in the literature that CycleGAN and variants could fail to identify the desired translation functions and produce content-misaligned translations. This limitation arises due to the presence of multiple translation functions‚Äîreferred to as "measure-preserving automorphism" (MPA)‚Äîin the solution space of the learning criteria. This study delves into the core identifiability inquiry and introduces an MPA elimination theory. Our analysis shows that MPA is unlikely to exist, if multiple pairs of diverse cross-domain conditional distributions are matched by the learning function. Based on this analysis, we introduce a UDT learning approach using distribution matching over auxiliary variable-induced subsets of the domains, which is theoretically more appealing than prior work. To our best knowledge, the identifiability result stands as the first of its kind, without using restrictive conditions on the structure of the desired translation functions.},
  author = {Sagar Shrestha and Xiao Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shrestha2024towards.pdf:pdf},
  keywords = {unsupervised domain translation, CycleGAN, identifiability, distribution matching, measure-preserving automorphism},
  note = {Primary Area: computer vision and pattern recognition. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=55uj7mU7Cv},
  publisher = {OpenReview.net},
  title = {Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach},
  url = {https://openreview.net/forum?id=55uj7mU7Cv},
  year = {2024}
}

@inproceedings{shrivastava2024video,
  abstract = {We present Video Decomposition Prior (VDP), a novel inference-time optimization framework that decomposes a video sequence into multiple RGB layers with associated opacity levels, enabling layer-by-layer video editing. Our approach derives inspiration from professional video editing practices and addresses the limitations of traditional deep learning video editing techniques that rely on extensive paired datasets. VDP utilizes the motion and appearance of the input video itself, without requiring task-specific external data collection. The framework performs three primary vision tasks: video object segmentation, video dehazing, and video relighting. For video relighting, we introduce a novel logarithmic video decomposition formulation that sets a new benchmark over existing methodologies. Our approach enables advanced video manipulation across challenging scenarios where ground truth data is difficult to obtain, demonstrating successful results on datasets like DAVIS, REVIDE, and SDSD with improved edit propagation and reduced flickering compared to existing methods.},
  author = {Gaurav Shrivastava and Ser-Nam Lim and Abhinav Shrivastava},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shrivastava2024video.pdf:pdf},
  keywords = {video decomposition, video editing, layer-based editing, video relighting, video dehazing, video object segmentation},
  note = {Primary Area: computer vision and pattern recognition. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nfMyERXNru},
  publisher = {OpenReview.net},
  title = {Video Decomposition Prior: Editing Videos Layer by Layer},
  url = {https://openreview.net/forum?id=nfMyERXNru},
  year = {2024}
}

@inproceedings{shukor2024beyond,
  abstract = {Large Multimodal Models (LMMs) have shown impressive progress in multimodal tasks. However, LMMs still struggle with hallucinations and flawed reasoning, limiting their applicability in real-world scenarios. This paper introduces EvALign-ICL, an evaluation framework that assesses LMMs on five critical dimensions beyond traditional task performance: hallucinations, abstention, compositionality, explainability, and instruction following. We evaluate 10 open-source LMMs ranging from 3B to 80B parameters, and explore in-context learning (ICL) as a training-free solution to address model limitations. Our findings reveal that scaling alone does not resolve fundamental model flaws, and ICL's effects are nuanced - while improving explainability and abstention capabilities, ICL slightly improves instruction following, does not enhance compositional abilities, and may even amplify hallucinations. To address these issues, we propose novel ICL variants including Multitask-ICL, Chain-of-Hindsight-ICL, and Self-Correcting-ICL, which show promise as post-hoc approaches to efficiently tackle some limitations of current LMMs.},
  author = {Mustafa Shukor and Alexandre Ram and Corentin Dancette and Matthieu Cord},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shukor2024beyond.pdf:pdf},
  keywords = {multimodal models, in-context learning, hallucination, evaluation framework, explainability, instruction following},
  note = {Primary Area: multimodal learning. Code available: https://github.com/mshukor/EvALign-ICL. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mMaQvkMzDi},
  publisher = {OpenReview.net},
  title = {Beyond task performance: evaluating and reducing the flaws of large multimodal models with in-context-learning},
  url = {https://openreview.net/forum?id=mMaQvkMzDi},
  year = {2024}
}

@inproceedings{shypula2024learning,
  abstract = {With the decline of Moore's law, optimizing program performance has become a major focus of software research. However, high-level optimizations such as API and algorithm changes remain elusive due to the difficulty of understanding the semantics of code. We introduce a framework for adapting Large Language Models (LLMs) to high-level program optimization by learning from human programmers' performance-improving edits. We curate PIE, a dataset of over 77,000 performance-improving edits made by competitive C++ programmers, accompanied by extensive unit tests. To reliably evaluate performance improvements, we develop a deterministic evaluation environment based on the gem5 full system simulator. We investigate the ability of LLMs to suggest functionally correct, performance-improving code edits, introducing performance conditioning and self-play techniques for model improvement. Our best model achieves an 87.63% optimization rate with 6.86x aggregate speedup and 95.09% functional correctness, demonstrating the potential of LLMs for automated program optimization.},
  author = {Alexander Shypula and Aman Madaan and Yimeng Zeng and Uri Alon and Jacob R. Gardner and Yiming Yang and Milad Hashemi and Graham Neubig and Parthasarathy Ranganathan and Osbert Bastani and Amir Yazdanbakhsh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/shypula2024learning.pdf:pdf},
  keywords = {code optimization, large language models, program performance, competitive programming, self-play learning},
  note = {Spotlight paper. Project website: https://pie4perf.com/. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ix7rLVHXyY},
  publisher = {OpenReview.net},
  title = {Learning Performance-Improving Code Edits},
  url = {https://openreview.net/forum?id=ix7rLVHXyY},
  year = {2024}
}

@inproceedings{si2024pac,
  abstract = {Prediction sets capture uncertainty by predicting sets of labels rather than individual labels, enabling downstream decisions to conservatively account for all plausible outcomes. While conformal inference algorithms construct prediction sets guaranteed to contain the true label with high probability, these guarantees fail to hold in the face of distribution shift, which is precisely when reliable uncertainty quantification can be most useful. We propose a novel algorithm for constructing prediction sets with PAC guarantees in the label shift setting, where the probabilities of labels can differ between the source and target distributions. Our algorithm relies on estimating the predicted probabilities of classes in the target domain and constructing confidence intervals for importance weights by propagating uncertainty through a Gaussian elimination algorithm. We evaluate our approach on five diverse datasets including CIFAR-10, ChestX-Ray, Entity-13, CDC Heart, and AGNews, demonstrating that our algorithm satisfies the PAC guarantee while producing smaller, more informative prediction sets compared to several baselines.},
  author = {Wenwen Si and Sangdon Park and Insup Lee and Edgar Dobriban and Osbert Bastani},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/si2024pac.pdf:pdf},
  keywords = {prediction sets, PAC guarantees, label shift, distribution shift, conformal inference, uncertainty quantification},
  note = {Primary Area: probabilistic methods. DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=4vPVBh3fhz},
  publisher = {OpenReview.net},
  title = {{PAC} Prediction Sets Under Label Shift},
  url = {https://openreview.net/forum?id=4vPVBh3fhz},
  year = {2024}
}

@inproceedings{si2024difftactile,
  abstract = {We present DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. The system incorporates several key components including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (elastic, elastoplastic, cables) under manipulation, and a penalty-based contact model for handling contact dynamics. The differentiable nature of the system facilitates gradient-based optimization for both refining physical properties in simulation using real-world data and efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of tactile sensors to contact using an efficient pixel-based neural module.},
  author = {Zilin Si and Gu Zhang and Qingwei Ben and Branden Romero and Zhou Xian and Chao Liu and Chuang Gan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/si2024difftactile.pdf:pdf},
  keywords = {tactile simulation, differentiable physics, robotic manipulation, contact modeling, FEM simulation, soft body dynamics},
  note = {Project website: https://difftactile.github.io/. Code: https://github.com/Genesis-Embodied-AI/DiffTactile. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eJHnSg783t},
  publisher = {OpenReview.net},
  title = {{DIFFTACTILE}: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation},
  url = {https://openreview.net/forum?id=eJHnSg783t},
  year = {2024}
}

@inproceedings{sikchi2024score,
  abstract = {Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. We propose SMORE, a novel approach that casts GCRL under a new lens of mixture-distribution matching, leading to a discriminator-free method for performant offline goal-conditioned RL. SMORE learns scores or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. We show that SMORE can outperform state-of-the-art baselines by a significant margin on robot manipulation and locomotion tasks, demonstrating the effectiveness of our discriminator-free occupancy matching approach.},
  author = {Harshit Sikchi and Rohan Chitnis and Ahmed Touati and Alborz Geramifard and Amy Zhang and Scott Niekum},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sikchi2024score.pdf:pdf},
  keywords = {goal-conditioned reinforcement learning, offline RL, occupancy matching, score models, discriminator-free learning, robot manipulation},
  note = {Project website: https://hari-sikchi.github.io/smore/. Code: https://github.com/hari-sikchi/SMORe. DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oXjnwQLcTA},
  publisher = {OpenReview.net},
  title = {Score Models for Offline Goal-Conditioned Reinforcement Learning},
  url = {https://openreview.net/forum?id=oXjnwQLcTA},
  year = {2024}
}

@inproceedings{sikchi2024dual,
  abstract = {The goal of reinforcement learning ({RL}) is to find a policy that maximizes the expected cumulative return. It has been shown that this objective can be represented as an optimization problem of state-action visitation distribution under linear constraints. The dual problem of this formulation, which we refer to as dual {RL}, is unconstrained and easier to optimize. In this work, we first cast several state-of-the-art offline {RL} and offline imitation learning ({IL}) algorithms as instances of dual {RL} approaches with shared structures. Such unification allows us to identify the root cause of the shortcomings of prior methods. For offline {IL}, our analysis shows that prior methods are based on a restrictive coverage assumption that greatly limits their performance in practice. To fix this limitation, we propose a new discriminator-free method {ReCOIL} that learns to imitate from arbitrary off-policy data to obtain near-expert performance. For offline {RL}, our analysis frames a recent offline {RL} method {XQL} in the dual framework, and we further propose a new method f-{DVL} that provides alternative choices to the {Gumbel} regression loss that fixes the known training instability issue of {XQL}. The performance improvements by both of our proposed methods, {ReCOIL} and f-{DVL}, in {IL} and {RL} are validated on an extensive suite of simulated robot locomotion and manipulation tasks.},
  author = {Harshit Sikchi and Qinqing Zheng and Amy Zhang and Scott Niekum},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sikchi2024dual.pdf:pdf},
  keywords = {Robot Learning, Offline Imitation Learning, Offline Reinforcement Learning, Deep Reinforcement Learning},
  month = {5},
  note = {Spotlight (top 5\% of accepted papers)},
  pdf = {https://openreview.net/pdf?id=xt9Bu66rqv},
  publisher = {OpenReview.net},
  title = {Dual {RL}: Unification and New Methods for Reinforcement and Imitation Learning},
  url = {https://openreview.net/forum?id=xt9Bu66rqv},
  year = {2024}
}

@inproceedings{simon2024more,
  abstract = {In our era of enormous neural networks, empirical progress has been driven by the philosophy that *more is better*. Recent deep learning practice has found repeatedly that larger model size, more data, and more computation (resulting in lower training loss) optimizing to near-interpolation improves performance. In this paper, we give theoretical backing to these empirical observations by showing that these three properties hold in random feature ({RF}) regression, a class of models equivalent to shallow networks with only the last layer trained. Concretely, we first show that the test risk of {RF} regression decreases monotonically with both the number of features and samples, provided the ridge penalty is tuned optimally. In particular, this implies that infinite width {RF} architectures are preferable to those of any finite width. We then proceed to demonstrate that, for a large class of tasks characterized by powerlaw eigenstructure, training to near-zero training loss is *obligatory*: near-optimal performance can *only* be achieved when the training error is much smaller than the test error. Grounding our theory in real-world data, we find empirically that standard computer vision tasks with convolutional neural kernels clearly fall into this class. Taken together, our results tell a simple, testable story of the benefits of overparameterization and overfitting in random feature models.},
  author = {James B. Simon and Dhruva Karkada and Nikhil Ghosh and Mikhail Belkin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/simon2024more.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=OdpIjS0vkO},
  publisher = {OpenReview.net},
  title = {More is Better: when Infinite Overparameterization is Optimal and Overfitting is Obligatory},
  url = {https://openreview.net/forum?id=OdpIjS0vkO},
  year = {2024}
}

@inproceedings{singh2024learning,
  abstract = {Recent advances in the theory of Neural Operators ({NO}s) have enabled fast and accurate computation of the solutions to complex systems described by partial differential equations ({PDE}s). Despite their great success, current {NO}-based solutions face important challenges when dealing with spatio-temporal {PDE}s over long time scales. Specifically, the current theory of {NO}s does not present a systematic framework to perform data assimilation and efficiently correct the evolution of {PDE} solutions over time based on sparsely sampled noisy measurements. In this paper, we propose a learning-based state-space approach to compute the solution operators to infinite-dimensional semilinear {PDE}s. We extend neural operator theory by exploiting the structure of semilinear {PDE}s and the theory of nonlinear observers in function spaces, developing a flexible recursive method that allows for both prediction and data assimilation by combining prediction and correction operations. We propose {NODA} (Neural Operators with Data Assimilation), which is a recursive {NO} approach that can be used for both estimation, when noisy measurement data is available, and prediction. The method demonstrates better prediction performance when compared with other closely related {NO} approaches, while also being able to assimilate data with arbitrary sampling rates.},
  author = {Ashutosh Singh and Ricardo Augusto Borsoi and Deniz Erdogmus and Tales Imbiriba},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/singh2024learning.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=ZMv6zKYYUs},
  publisher = {OpenReview.net},
  title = {Learning semilinear neural operators: A unified recursive framework for prediction and data assimilation},
  url = {https://openreview.net/forum?id=ZMv6zKYYUs},
  year = {2024}
}

@inproceedings{sitawarin2024pubdef,
  abstract = {Adversarial attacks have been a looming and unaddressed threat in the industry. However, through a decade-long history of the robustness evaluation literature, we have learned that mounting a strong or optimal attack is challenging. It requires both machine learning and domain expertise. In other words, the white-box threat model, religiously assumed by a large majority of the past literature, is unrealistic. In this work, we propose a new practical threat model where the adversary relies on transfer attacks through publicly available surrogate models. We argue that this setting will become the most prevalent for security-sensitive applications in the future. We evaluate the transfer attacks in this setting and propose a specialized defense method based on a game-theoretic perspective. Under 24 public models and 11 attack algorithms across three datasets ({CIFAR}-10, {CIFAR}-100, and {ImageNet}), our defense, {PubDef}, outperforms state-of-the-art white-box adversarial training by a large margin with almost no loss in normal accuracy. For instance, on {ImageNet}, our defense achieves 62\% accuracy under the strongest transfer attack vs only 36\% of the best adversarially trained model.},
  author = {Chawin Sitawarin and Jaewon Chang and David Huang and Wesson Altoyan and David A. Wagner},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sitawarin2024pubdef.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=Tvwf4Vsi5F},
  publisher = {OpenReview.net},
  title = {{PubDef}: Defending Against Transfer Attacks From Public Models},
  url = {https://openreview.net/forum?id=Tvwf4Vsi5F},
  year = {2024}
}

@inproceedings{siththaranjan2024distributional,
  abstract = {In practice, preference learning from human feedback depends on incomplete data with hidden context. Hidden context refers to data that affects the feedback received, but which is not represented in the data used to train a preference model. This captures common issues of data collection, such as having human annotators with varied preferences, cognitive processes that result in seemingly irrational behavior, and combining data labeled according to different criteria. We prove that standard applications of preference learning, including reinforcement learning from human feedback ({RLHF}), implicitly aggregate over hidden contexts according to a well-known voting rule called *{Borda} count*. We show this can produce counter-intuitive results that are very different from other methods which implicitly aggregate via expected utility. Furthermore, our analysis formalizes the way that preference learning from users with diverse values tacitly implements a social choice function. A key implication of this result is that annotators have an incentive to misreport their preferences in order to influence the learned model, leading to vulnerabilities in the deployment of {RLHF}. As a step towards mitigating these problems, we introduce a class of methods called *distributional preference learning* ({DPL}). {DPL} methods estimate a distribution of possible score values for each alternative in order to better account for hidden context. Experimental results indicate that applying {DPL} to {RLHF} for {LLM} chatbots identifies hidden context in the data and significantly reduces subsequent jailbreak vulnerability.},
  author = {Anand Siththaranjan and Cassidy Laidlaw and Dylan Hadfield-Menell},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/siththaranjan2024distributional.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=0tWTxYYPnW},
  publisher = {OpenReview.net},
  title = {Distributional Preference Learning: Understanding and Accounting for Hidden Context in {RLHF}},
  url = {https://openreview.net/forum?id=0tWTxYYPnW},
  year = {2024}
}

@inproceedings{siuzdak2024vocos,
  abstract = {Recent advancements in neural vocoding are predominantly driven by Generative Adversarial Networks ({GAN}s) operating in the time-domain. While effective, this approach neglects the inductive bias offered by time-frequency representations, resulting in redundant and computationally-intensive upsampling operations. {Fourier}-based time-frequency representation is an appealing alternative, aligning more accurately with human auditory perception, and benefitting from well-established fast algorithms for its computation. Nevertheless, direct reconstruction of complex-valued spectrograms has been historically problematic, primarily due to phase recovery issues. This study seeks to close this gap by presenting Vocos, a new model that directly generates {Fourier} spectral coefficients. Vocos not only matches the state-of-the-art in audio quality, as demonstrated in our evaluations, but it also substantially improves computational efficiency, achieving an order of magnitude increase in speed compared to prevailing time-domain neural vocoding approaches. The source code and model weights have been open-sourced and are available on {GitHub}.},
  author = {Hubert Siuzdak},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/siuzdak2024vocos.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=vY9nzQmQBw},
  publisher = {OpenReview.net},
  title = {Vocos: Closing the gap between time-domain and {Fourier}-based neural vocoders for high-quality audio synthesis},
  url = {https://openreview.net/forum?id=vY9nzQmQBw},
  year = {2024}
}

@inproceedings{sivan2024fosi,
  abstract = {Popular machine learning approaches forgo second-order information due to the difficulty of computing curvature in high dimensions. We present {FOSI}, a novel meta-algorithm that improves the performance of any base first-order optimizer by efficiently incorporating second-order information during the optimization process. In each iteration, {FOSI} implicitly splits the function into two quadratic functions defined on orthogonal subspaces, then uses a second-order method to minimize the first, and the base optimizer to minimize the other. We provide theoretical guarantees for the convergence of {FOSI} when the base optimizer is gradient descent, and empirically demonstrate significant improvements over popular first-order optimizers on a variety of machine learning tasks.},
  author = {Hadar Sivan and Moshe Gabel and Assaf Schuster},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sivan2024fosi.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=NvbeD9Ttkx},
  publisher = {OpenReview.net},
  title = {{FOSI}: Hybrid First and Second Order Optimization},
  url = {https://openreview.net/forum?id=NvbeD9Ttkx},
  year = {2024}
}

@inproceedings{siyao2024duolando,
  abstract = {We introduce a novel task within the field of human motion generation, termed dance accompaniment, which necessitates the generation of responsive movements from a dance partner, the ``follower'', synchronized with the lead dancer's movements and the underlying musical rhythm. Unlike existing solo or group dance generation tasks, a duet dance scenario entails a heightened degree of interaction between the two participants, requiring delicate coordination in both pose and position. To support this task, we first built a large-scale and diverse duet interactive dance dataset, {DD100}, by recording about 117 minutes of professional dancers' performances. To address the challenges inherent in this task, we propose a {GPT}-based model, Duolando, which autoregressively predicts the subsequent tokenized motion conditioned on the coordinated information of the music, the leader's and the follower's movements. To further enhance the {GPT}'s capabilities of generating stable results on unseen conditions (music and leader motions), we devise an off-policy reinforcement learning strategy that allows the model to explore viable trajectories from out-of-distribution samplings, guided by human-defined rewards. Based on the collected dataset and proposed method, we establish a benchmark with several carefully designed metrics.},
  author = {Li Siyao and Tianpei Gu and Zhitao Yang and Zhengyu Lin and Ziwei Liu and Henghui Ding and Lei Yang and Chen Change Loy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/siyao2024duolando.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=GW4j4n2cjH},
  publisher = {OpenReview.net},
  title = {Duolando: Follower {GPT} with Off-Policy Reinforcement Learning for Dance Accompaniment},
  url = {https://openreview.net/forum?id=GW4j4n2cjH},
  year = {2024}
}

@inproceedings{skalse2024quantifying,
  abstract = {Inverse reinforcement learning ({IRL}) aims to infer an agent's preferences (represented as a reward function $R$) from their behaviour (represented as a policy $\pi$). To do this, we need a behavioural model of how $\pi$ relates to $R$. In the current literature, the most common behavioural models are optimality, {Boltzmann}-rationality, and causal entropy maximisation. However, the true relationship between a human's preferences and their behaviour is much more complex than any of these behavioural models. This means that the behavioural models are misspecified, which raises the concern that they may lead to systematic errors if applied to real data. In this paper, we analyse how sensitive the {IRL} problem is to misspecification of the behavioural model. Specifically, we provide necessary and sufficient conditions that completely characterise how the observed data may differ from the assumed behavioural model without incurring an error above a given threshold. In addition to this, we also characterise the conditions under which a behavioural model is robust to small perturbations of the observed policy, and we analyse how robust many behavioural models are to misspecification of their parameter values (such as e.g. the discount rate). Our analysis suggests that the {IRL} problem is highly sensitive to misspecification, in the sense that very mild misspecification can lead to very large errors in the inferred reward function.},
  author = {Joar Max Viktor Skalse and Alessandro Abate},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/skalse2024quantifying.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=pz2E1Q9Wni},
  publisher = {OpenReview.net},
  title = {Quantifying the Sensitivity of Inverse Reinforcement Learning to Misspecification},
  url = {https://openreview.net/forum?id=pz2E1Q9Wni},
  year = {2024}
}

@inproceedings{skalse2024starc,
  abstract = {In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to anticipate in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call {STARC} ({STAndardised} Reward Comparison) metrics. We show that {STARC} metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy.},
  author = {Joar Max Viktor Skalse and Lucy Farnik and Sumeet Ramesh Motwani and Erik Jenner and Adam Gleave and Alessandro Abate},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/skalse2024starc.pdf:pdf},
  month = {5},
  pdf = {https://openreview.net/pdf?id=wPhbtwlCDa},
  publisher = {OpenReview.net},
  title = {{STARC}: A General Framework For Quantifying Differences Between Reward Functions},
  url = {https://openreview.net/forum?id=wPhbtwlCDa},
  year = {2024}
}

@inproceedings{sokota2024updateequivalence,
  abstract = {The process of revising (or constructing) a policy at execution time---known as decision-time planning---has been key to achieving superhuman performance in perfect-information games like chess and Go. A recent line of work has extended decision-time planning to imperfect-information games, leading to superhuman performance in poker. However, these methods involve solving subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on solving subgames, but rather on update equivalence. In this update-equivalence framework, decision-time planning algorithms replicate the updates of last-iterate algorithms, which need not rely on public information. This facilitates scalability to games with large amounts of non-public information. Using this framework, we derive a provably sound search algorithm for fully cooperative games based on mirror descent and a search algorithm for adversarial games based on magnetic mirror descent. We validate the performance of these algorithms in cooperative and adversarial domains, notably in Hanabi, the standard benchmark for search in fully cooperative imperfect-information games. Here, our mirror descent approach exceeds or matches the performance of public information-based search while using two orders of magnitude less search time. This is the first instance of a non-public-information-based algorithm outperforming public-information-based approaches in a domain they have historically dominated.},
  author = {Samuel Sokota and Gabriele Farina and David J. Wu 0002 and Hengyuan Hu and Kevin A. Wang and J. Zico Kolter and Noam Brown},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/sokota2024updateequivalence.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JXGph215fL},
  publisher = {OpenReview.net},
  title = {The Update-Equivalence Framework for Decision-Time Planning},
  url = {https://openreview.net/forum?id=JXGph215fL},
  year = {2024}
}

@inproceedings{solonets2024analytical,
  abstract = {Direct image alignment is a widely used technique for relative 6DoF pose estimation between two images, but its accuracy strongly depends on pose initialization. Therefore, recent end-to-end frameworks increase the convergence basin of the learned feature descriptors with special training objectives, such as the Gauss-Newton loss. However, the training data may exhibit bias toward a specific type of motion and pose initialization, thus limiting the generalization of these methods. In this work, we derive a closed-form solution to the expected optimum of the Gauss-Newton loss. The solution is agnostic to the underlying feature representation and allows us to dynamically adjust the basin of convergence according to our assumptions about the uncertainty in the current estimates. These properties allow for effective control over the convergence in the alignment process. Despite using self-supervised feature embeddings, our solution achieves compelling accuracy w.r.t. the state-of-the-art direct image alignment methods trained end-to-end with pose supervision, and demonstrates improved robustness to pose initialization. Our analytical solution exposes some inherent limitations of end-to-end learning with the Gauss-Newton loss, and establishes an intriguing connection between direct image alignment and feature-matching approaches.},
  author = {Sergei Solonets and Daniil Sinitsyn and Lukas von Stumberg and Nikita Araslanov and Daniel Cremers},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/solonets2024analytical.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=mE52zURNGc},
  publisher = {OpenReview.net},
  title = {An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment},
  url = {https://openreview.net/forum?id=mE52zURNGc},
  year = {2024}
}

@inproceedings{somerstep2024learning,
  abstract = {Motivated by equilibrium models of labor markets, we develop a formulation of causal strategic classification in which strategic agents can directly manipulate their outcomes. As an application, we consider employers that seek to anticipate the strategic response of a labor force when developing a hiring policy. We show theoretically that employers with performatively optimal hiring policies improve employer reward, labor force skill level, and labor force equity (compared to employers that do not anticipate the strategic labor force response) in the classic Coate-Loury labor market model. Empirically, we show that these desirable properties of performative hiring policies do generalize to our own formulation of a general equilibrium labor market. On the other hand, we also observe that the benefits of performatively optimal hiring policies are brittle in some aspects. We demonstrate that in our formulation a performative employer both harms workers by reducing their aggregate welfare and fails to prevent discrimination when more sophisticated wage and cost structures are introduced.},
  author = {Seamus Somerstep and Yuekai Sun and Yaacov Ritov},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/somerstep2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vEfmVS5ywF},
  publisher = {OpenReview.net},
  title = {Learning in reverse causal strategic environments with ramifications on two sided markets},
  url = {https://openreview.net/forum?id=vEfmVS5ywF},
  year = {2024}
}

@inproceedings{son2024intuitive,
  author = {Dongwon Son and Jaehyung Kim 0001 and Sanghyeon Son and Beomjoon Kim},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/son2024intuitive.pdf:pdf},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=5JWAOLBxwp},
  publisher = {OpenReview.net},
  title = {An Intuitive Multi-Frequency Feature Representation for SO(3)-Equivariant Networks},
  url = {https://openreview.net/forum?id=5JWAOLBxwp},
  year = {2024}
}

@inproceedings{song2024graph,
  abstract = {Graph pooling compresses graph information into a compact representation. State-of-the-art graph pooling methods follow a hierarchical approach, which reduces the graph size step-by-step. These methods must balance memory efficiency with preserving node information, depending on whether they use node dropping or node clustering. Additionally, fixed pooling ratios or numbers of pooling layers are predefined for all graphs, which prevents personalized pooling structures from being captured for each individual graph. To address these limitations, we draw inspiration from bottom-up grammar induction and propose an efficient parsing algorithm to infer the graph pooling structure. This structure is then used to enable the model to learn personalized pooling structure for each individual graph. We demonstrate that our proposed method consistently outperforms existing graph pooling methods across diverse graph learning tasks, including graph classification, node classification, and graph reconstruction. Further analysis shows that our method can learn meaningful hierarchical graph structures that reflect the underlying graph properties.},
  author = {Yunchong Song and Siyuan Huang 0003 and Xinbing Wang and Chenghu Zhou and Zhouhan Lin},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/song2024graph.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hv3SklibkL},
  publisher = {OpenReview.net},
  title = {Graph Parsing Networks},
  url = {https://openreview.net/forum?id=hv3SklibkL},
  year = {2024}
}

@inproceedings{song2024improved,
  abstract = {Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We identify a previously overlooked flaw and address it by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. We introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet 64√ó64 respectively in a single sampling step. These scores mark a 3.5√ó and 4√ó improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation.},
  author = {Yang Song and Prafulla Dhariwal},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/song2024improved.pdf:pdf},
  note = {DBLP last modified: 2024-07-29; Oral presentation},
  pdf = {https://openreview.net/pdf?id=WNzy9bRDvG},
  publisher = {OpenReview.net},
  title = {Improved Techniques for Training Consistency Models},
  url = {https://openreview.net/forum?id=WNzy9bRDvG},
  year = {2024}
}

@inproceedings{song2024unified,
  abstract = {Generating 3D molecules with desired properties requires simultaneous generation of molecular graphs and 3D coordinates, posing challenges due to the discrete-continuous hybrid nature and inherent variability in molecule geometry. Existing approaches often decompose this joint distribution into separate models, which can lead to inconsistencies. We introduce Geometric Bayesian Flow Networks (GeoBFN), a generative model that naturally handles the multi-modal nature of molecular data by modeling diverse modalities in the differentiable parameter space of distributions. GeoBFN maintains SE(3) invariant density modeling by incorporating equivariant inter-dependency modeling on parameters of distributions and unifying the probabilistic modeling of different modalities. Our approach achieves state-of-the-art performance on multiple 3D molecule generation benchmarks, with 90.87% molecule stability in QM9 and 85.6% atom stability in GEOM-DRUGS datasets, while enabling conditional generation on various molecular properties.},
  author = {Yuxuan Song and Jingjing Gong and Hao Zhou 0012 and Mingyue Zheng and Jingjing Liu and Wei-Ying Ma},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/song2024unified.pdf:pdf},
  note = {DBLP last modified: 2024-08-02; Oral presentation},
  pdf = {https://openreview.net/pdf?id=NSVtmmzeRB},
  publisher = {OpenReview.net},
  title = {Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks},
  url = {https://openreview.net/forum?id=NSVtmmzeRB},
  year = {2024}
}

@inproceedings{song2024towards,
  abstract = {Current deep learning approaches for learning physics are restricted to single system domains and cannot be applied to new physical systems governed by different laws. We propose to overcome such limitations by using graph neural networks (GNNs) to learn cross-domain generalizable Hamiltonian dynamics. Our approach aims to learn a unified Hamiltonian representation that is generalizable across multiple system domains using meta learning algorithms. The meta-trained model can capture generalized Hamiltonian representations consistent across different physical domains. We demonstrate that our method achieves cross domain generalization capabilities, providing a step toward a unified model for understanding dynamical systems via deep learning. We validate our approach on various Hamiltonian systems including pendulum, spring-mass, and gravitational systems, showing superior performance compared to existing methods in terms of generalization to new physical domains.},
  author = {Yeongwoo Song and Hawoong Jeong},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/song2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AZGIwqCyYY},
  publisher = {OpenReview.net},
  title = {Towards Cross Domain Generalization of Hamiltonian Representation via Meta Learning},
  url = {https://openreview.net/forum?id=AZGIwqCyYY},
  year = {2024}
}

@inproceedings{song2024solving,
  abstract = {Diffusion models have emerged as a powerful tool for solving inverse problems. However, most existing methods are restricted to pixel-space diffusion models and face challenges when extended to latent diffusion models due to the nonlinearity of the encoder and decoder. We propose ReSample, an algorithm that can solve general inverse problems with pre-trained latent diffusion models. Our algorithm incorporates data consistency by solving an optimization problem during the reverse sampling process, a concept we term as hard data consistency. We introduce a novel resampling scheme to map the measurement-consistent sample back onto the noisy data manifold, and theoretically demonstrate its benefits. We apply our algorithm to solve a wide range of linear and nonlinear inverse problems in both natural and medical images, demonstrating that our approach outperforms existing state-of-the-art approaches, including those based on pixel-space diffusion models. Our experimental results showcase the versatility and effectiveness of ReSample for solving diverse inverse problems using latent diffusion models.},
  author = {Bowen Song and Soo Min Kwon and Zecheng Zhang and Xinyu Hu and Qing Qu 0001 and Liyue Shen},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/song2024solving.pdf:pdf},
  note = {DBLP last modified: 2024-08-04},
  pdf = {https://openreview.net/pdf?id=j8hdRqOUhN},
  publisher = {OpenReview.net},
  title = {Solving Inverse Problems with Latent Diffusion Models via Hard Data Consistency},
  url = {https://openreview.net/forum?id=j8hdRqOUhN},
  year = {2024}
}

@inproceedings{song2024compositional,
  abstract = {Offline reinforcement learning learns optimal policies from past experiences without additional environment interaction, but faces distributional shift problems where states and actions during policy execution may not match the training dataset distribution. A common solution involves incorporating conservatism into the policy or value function to safeguard against uncertainties. We propose COmpositional COnservatism with Anchor-seeking (COCOA) for offline RL, an approach that pursues conservatism in a compositional manner on top of transductive reparameterization, which decomposes the input variable (the state) into an anchor and its difference from the original input. COCOA seeks both in-distribution anchors and differences by utilizing a learned reverse dynamics model, encouraging conservatism in the compositional input space for the policy or value function. Such compositional conservatism is independent of and agnostic to the prevalent behavioral conservatism in offline RL. We applied COCOA to four state-of-the-art offline RL algorithms and evaluated them on the D4RL benchmark, where COCOA generally improves the performance of each algorithm.},
  author = {Yeda Song and Dongwook Lee and Gunhee Kim},
  booktitle = {12th International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/song2024compositional.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HRkyLbBRHI},
  publisher = {OpenReview.net},
  title = {Compositional Conservatism: A Transductive Approach in Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=HRkyLbBRHI},
  year = {2024}
}

@inproceedings{song2024decoding,
  abstract = {Electroencephalography (EEG) signals, known for convenient non-invasive acquisition but low signal-to-noise ratio, have recently gained substantial attention due to the potential to decode natural images. This paper presents a self-supervised framework to demonstrate the feasibility of learning image representations from EEG signals, particularly for object recognition.},
  author = {Yonghao Song and Bingchuan Liu and Xiang Li and Nanlin Shi and Yijun Wang and Xiaorong Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/song2024decoding.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=dhLIno8FmH},
  publisher = {OpenReview.net},
  title = {Decoding Natural Images from {EEG} for Object Recognition},
  url = {https://openreview.net/forum?id=dhLIno8FmH},
  year = {2024}
}

@inproceedings{song2024decoupled,
  abstract = {A Marked Temporal Point Process (MTPP) is a stochastic process whose realization is a set of event-time data. MTPP is often used to understand complex dynamics of asynchronous temporal events such as money transaction, social media, healthcare, etc.},
  author = {Yujee Song and Donghyun Lee and Rui Meng and Won Hwa Kim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/song2024decoupled.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=BuFNoKBiMs},
  publisher = {OpenReview.net},
  title = {Decoupled Marked Temporal Point Process using Neural Ordinary Differential Equations},
  url = {https://openreview.net/forum?id=BuFNoKBiMs},
  year = {2024}
}

@inproceedings{song2024hierarchical,
  abstract = {Large language models (LLMs) have shown remarkable performance in various natural language processing tasks. However, a primary constraint they face is the context limit, i.e., the maximum number of tokens they can process.},
  author = {Woomin Song and Seunghyuk Oh and Sangwoo Mo and Jaehyung Kim and Sukmin Yun and Jung-Woo Ha and Jinwoo Shin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/song2024hierarchical.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=ulaUJFd96G},
  publisher = {OpenReview.net},
  title = {Hierarchical Context Merging: Better Long Context Understanding for Pre-trained {LLM}s},
  url = {https://openreview.net/forum?id=ulaUJFd96G},
  year = {2024}
}

@inproceedings{song2024amortized,
  abstract = {Excitatory point processes (i.e., event flows) occurring over dynamic graphs (i.e., evolving topologies) provide a fine-grained model to capture how discrete events may spread over time and space.},
  author = {Zitao Song and Wendi Ren and Shuang Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/song2024amortized.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=8g26Yv1EOu},
  publisher = {OpenReview.net},
  title = {Amortized Network Intervention to Steer the Excitatory Point Processes},
  url = {https://openreview.net/forum?id=8g26Yv1EOu},
  year = {2024}
}

@inproceedings{song2024pose,
  abstract = {It is now possible to reconstruct dynamic human motion and shape from a sparse set of cameras using Neural Radiance Fields (NeRF) driven by an underlying skeleton. However, a challenge remains to model the deformation of cloth and skin in relation to skeleton pose.},
  author = {Chunjin Song and Bastian Wandt and Helge Rhodin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/song2024pose.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=5t44vPlv9x},
  publisher = {OpenReview.net},
  title = {Pose Modulated Avatars from Video},
  url = {https://openreview.net/forum?id=5t44vPlv9x},
  year = {2024}
}

@inproceedings{song2024increasing,
  abstract = {Fine-tuning large pre-trained foundation models has attracted attention. Parameter-efficient fine-tuning methods have limitations in model capacity, especially under constrained parameter budgets.},
  author = {Haobo Song and Hao Zhao and Soumajit Majumder and Tao Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/song2024increasing.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=H3IUunLy8s},
  publisher = {OpenReview.net},
  title = {Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning},
  url = {https://openreview.net/forum?id=H3IUunLy8s},
  year = {2024}
}

@inproceedings{sorrenson2024lifting,
  abstract = {Normalizing Flows explicitly maximize a full-dimensional likelihood on training data. However, real data is typically only supported on a lower-dimensional manifold leading the model to expend significant compute on modeling noise. Injective Flows fix this by jointly learning a manifold and the distribution on it.},
  author = {Peter Sorrenson and Felix Draxler and Armand Rousselot and Sander Hummerich and Lea Zimmermann and Ullrich K{\"o}the},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sorrenson2024lifting.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=kBNIx4Biq4},
  publisher = {OpenReview.net},
  title = {Lifting Architectural Constraints of Injective Flows},
  url = {https://openreview.net/forum?id=kBNIx4Biq4},
  year = {2024}
}

@inproceedings{soto2024fewshot,
  abstract = {The significant risk posed by instruction-tuned language models that convincingly mimic human writing, which could be used for abuse. However, such abuse may be counteracted with the ability to detect whether text was composed by a language model rather than a human author.},
  author = {Rafael A. Rivera Soto and Kailin Koch and Aleem Khan and Barry Y. Chen and Marcus Bishop and Nicholas Andrews},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/soto2024fewshot.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=cWiEN1plhJ},
  publisher = {OpenReview.net},
  title = {Few-Shot Detection of Machine-Generated Text using Style Representations},
  url = {https://openreview.net/forum?id=cWiEN1plhJ},
  year = {2024}
}

@inproceedings{sow2024doubly,
  abstract = {Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification.},
  author = {Daouda Sow and Sen Lin and Zhangyang Wang and Yingbin Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sow2024doubly.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=OF5x1dzWSS},
  publisher = {OpenReview.net},
  title = {Doubly Robust Instance-Reweighted Adversarial Training},
  url = {https://openreview.net/forum?id=OF5x1dzWSS},
  year = {2024}
}

@inproceedings{spieler2024expressive,
  abstract = {By exploiting slowly decaying memory-like hidden states and two-layered nonlinear integration of synaptic input, the ELM neuron can accurately match input-output relationships with under ten thousand trainable parameters.},
  author = {Aaron Spieler and Nasim Rahaman and Georg Martius and Bernhard Sch{\"o}lkopf and Anna Levina},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/spieler2024expressive.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=vE1e1mLJ0U},
  publisher = {OpenReview.net},
  title = {The Expressive Leaky Memory Neuron: an Efficient and Expressive Phenomenological Neuron Model Can Solve Long-Horizon Tasks},
  url = {https://openreview.net/forum?id=vE1e1mLJ0U},
  year = {2024}
}

@inproceedings{sprague2024musr,
  abstract = {While large language models (LLMs) equipped with techniques like chain-of-thought prompting have demonstrated impressive capabilities, they still fall short in their ability to reason robustly in complex settings. However, evaluating LLM reasoning is challenging because system capabilities continue to grow while benchmark datasets for tasks like logical deduction have remained static. We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released. Second, our dataset instances are free text narratives corresponding to real-world domains of reasoning; this makes it simultaneously much more challenging than other synthetically-crafted benchmarks while remaining realistic and tractable for human annotators to solve with high accuracy. We evaluate a range of LLMs and prompting techniques on this dataset and characterize the gaps that remain for techniques like chain-of-thought to perform robust reasoning.},
  author = {Zayne Sprague and Xi Ye and Kaj Bostrom and Swarat Chaudhuri and Greg Durrett},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sprague2024musr.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://arxiv.org/pdf/2310.16049.pdf},
  publisher = {OpenReview.net},
  title = {{MuSR}: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning},
  url = {https://openreview.net/forum?id=jenyYQzue1},
  year = {2024}
}

@inproceedings{springer2024sharpnessaware,
  abstract = {Sharpness-Aware Minimization (SAM) has emerged as a promising alternative optimizer to stochastic gradient descent (SGD). The originally-proposed motivation behind SAM was to bias neural networks towards flatter minima that are believed to generalize better. However, recent studies have shown conflicting evidence on the relationship between flatness and generalization, suggesting that flatness does fully explain SAM's success. Sidestepping this debate, we identify an orthogonal effect of SAM that is beneficial out-of-distribution: we argue that SAM implicitly balances the quality of diverse features. SAM achieves this effect by adaptively suppressing well-learned features which gives remaining features opportunity to be learned. We show that this mechanism is beneficial in datasets that contain redundant or spurious features where SGD falls for the simplicity bias and would not otherwise learn all available features. Our insights are supported by experiments on real data: we demonstrate that SAM improves the quality of features in datasets containing redundant or spurious features, including CelebA, Waterbirds, CIFAR-MNIST, and DomainBed.},
  author = {Jacob Mitchell Springer and Vaishnavh Nagarajan and Aditi Raghunathan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/springer2024sharpnessaware.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2405.20439.pdf},
  publisher = {OpenReview.net},
  title = {Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning},
  url = {https://openreview.net/forum?id=3xDaj4pRna},
  year = {2024}
}

@inproceedings{sridhar2024memoryconsistent,
  abstract = {Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised 'behavior cloning' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our 'memory-consistent neural network' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical 'memory' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 10 imitation learning tasks, with MLP, Transformer, and Diffusion backbones, spanning dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data, we find large and consistent gains in performance, validating that MCNNs are better-suited than vanilla deep neural networks for imitation learning applications.},
  author = {Kaustubh Sridhar and Souradeep Dutta and Dinesh Jayaraman and James Weimer and Insup Lee 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sridhar2024memoryconsistent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2310.06171.pdf},
  publisher = {OpenReview.net},
  title = {Memory-Consistent Neural Networks for Imitation Learning},
  url = {https://openreview.net/forum?id=R3Tf7LDdX4},
  year = {2024}
}

@inproceedings{srinivasan2024forward,
  abstract = {"Forward-only" algorithms, which train neural networks while avoiding a backward pass, have recently gained attention as a way of solving the biologically unrealistic aspects of backpropagation. Here, we first address compelling challenges related to the "forward-only" rules, which include reducing the performance gap with backpropagation and providing an analytical understanding of their dynamics. To this end, we show that the forward-only algorithm with top-down feedback is well-approximated by an "adaptive-feedback-alignment" algorithm, and we analytically track its performance during learning in a prototype high-dimensional setting. Then, we compare different versions of forward-only algorithms, focusing on the Forward-Forward and PEPITA frameworks, and we show that they share the same learning principles. Overall, our work unveils the connections between three key neuro-inspired learning rules, providing a link between "forward-only" algorithms, i.e., Forward-Forward and PEPITA, and an approximation of backpropagation, i.e., Feedback Alignment.},
  author = {Ravi Francesco Srinivasan and Francesca Mignacco and Martino Sorbaro 0001 and Maria Refinetti and Avi Cooper and Gabriel Kreiman and Giorgia Dellaferrera},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/srinivasan2024forward.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://arxiv.org/pdf/2302.05440.pdf},
  publisher = {OpenReview.net},
  title = {Forward Learning with Top-Down Feedback: Empirical and Analytical Characterization},
  url = {https://openreview.net/forum?id=My7lkRNnL9},
  year = {2024}
}

@inproceedings{srivatsan2024alttext,
  abstract = {In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. More than just a special case of image captioning, alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly leveraged can be informative. We address this task with a multimodal model that conditions on both textual information from the associated social media post as well as visual signal from the image, and demonstrate that the utility of these two information sources stacks. We put forward a new dataset of 371k images paired with alt-text and tweets scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation. We show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work, by more than 2x on BLEU@4.},
  author = {Nikita Srivatsan and Sofa Samaniego and Omar Florez and Taylor Berg-Kirkpatrick},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/srivatsan2024alttext.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2305.14779.pdf},
  publisher = {OpenReview.net},
  title = {Alt-Text with Context: Improving Accessibility for Images on Twitter},
  url = {https://openreview.net/forum?id=97Dl82avFs},
  year = {2024}
}

@inproceedings{staab2024beyond,
  abstract = {Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\%$ top-1 and $95\%$ top-3 accuracy at a fraction of the cost ($100\times$) and time ($240\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.},
  author = {Robin Staab and Mark Vero and Mislav Balunovic and Martin T. Vechev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/staab2024beyond.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://arxiv.org/pdf/2310.07298.pdf},
  publisher = {OpenReview.net},
  title = {Beyond Memorization: Violating Privacy via Inference with Large Language Models},
  url = {https://openreview.net/forum?id=kmn0BhQk7p},
  year = {2024}
}

@inproceedings{steinberg2024motor,
  abstract = {We present a self-supervised, time-to-event (TTE) foundation model called MOTOR (Many Outcome Time Oriented Representations) which is pretrained on timestamped sequences of events in electronic health records (EHR) and health insurance claims. TTE models are used for estimating the probability distribution of the time until a specific event occurs, which is an important task in medical settings. TTE models provide many advantages over classification using fixed time horizons, including naturally handling censored observations, but are challenging to train with limited labeled data. MOTOR addresses this challenge by pretraining on up to 55M patient records (9B clinical events). We evaluate MOTOR's transfer learning performance on 19 tasks, across 3 patient databases (a private EHR system, MIMIC-IV, and Merative claims data). Task-specific models adapted from MOTOR improve time-dependent C statistics by 4.6% over state-of-the-art, improve label efficiency by up to 95%, and are more robust to temporal distributional shifts. We further evaluate cross-site portability by adapting our MOTOR foundation model for six prediction tasks on the MIMIC-IV dataset, where it outperforms all baselines. MOTOR is the first foundation model for medical TTE predictions and we release a 143M parameter pretrained model for research use.},
  author = {Ethan Steinberg and Jason Alan Fries and Yizhe Xu and Nigam Shah},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/steinberg2024motor.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://arxiv.org/pdf/2301.03150.pdf},
  publisher = {OpenReview.net},
  title = {{MOTOR}: A Time-to-Event Foundation Model For Structured Medical Records},
  url = {https://openreview.net/forum?id=NialiwI2V6},
  year = {2024}
}

@inproceedings{stengeleskin2024zero,
  abstract = {Despite the frequent challenges posed by ambiguity when representing meaning via natural language, it is often ignored or deliberately removed in tasks mapping language to formally-designed representations, which generally assume a one-to-one mapping between linguistic and formal representations. We attempt to address this shortcoming by introducing AmP, a framework, dataset, and challenge for translating ambiguous natural language to formal representations like logic and code. We define templates and generate data for five well-documented linguistic ambiguities. Using AmP, we investigate how several few-shot text-to-code systems handle ambiguity, introducing three new metrics. We find that large pre-trained models perform poorly at capturing the distribution of possible meanings without deliberate instruction. However, models are able to capture the distribution well when ambiguity is attested in their inputs. These results motivate a call for including ambiguity explicitly in datasets and promote considering the distribution of possible outputs when evaluating systems.},
  author = {Elias Stengel-Eskin and Kyle Rawlins and Benjamin Van Durme},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/stengeleskin2024zero.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2306.00824.pdf},
  publisher = {OpenReview.net},
  title = {Zero and Few-shot Semantic Parsing with Ambiguous Inputs},
  url = {https://openreview.net/forum?id=qL9gogRepu},
  year = {2024}
}

@inproceedings{stoian2024how,
  abstract = {Deep Generative Models (DGMs) have been shown to be powerful tools for generating tabular data, as they have been increasingly able to capture the complex distributions that characterize them. However, to generate realistic synthetic data, it is often not enough to have a good approximation of their distribution, as it also requires compliance with constraints that encode essential background knowledge on the problem at hand. In this paper, we address this limitation and show how DGMs for tabular data can be transformed into Constrained Deep Generative Models (C-DGMs), whose generated samples are guaranteed to be compliant with the given constraints. This is achieved by automatically parsing the constraints and transforming them into a Constraint Layer (CL) seamlessly integrated with the DGM. Our extensive experimental analysis with various DGMs and tasks reveals that standard DGMs often violate constraints, some exceeding 95% non-compliance, while their corresponding C-DGMs are never non-compliant. Then, we quantitatively demonstrate that, at training time, C-DGMs are able to exploit the background knowledge expressed by the constraints to outperform their standard counterparts with up to 6.5% improvement in utility and detection. Further, we show how our CL does not necessarily need to be integrated at training time, as it can be also used as a guardrail at inference time, still producing some improvements in the overall performance of the models. Finally, we show that our CL does not hinder the sample generation time of the models.},
  author = {Mihaela C. Stoian and Salijona Dyrmishi and Maxime Cordy and Thomas Lukasiewicz and Eleonora Giunchiglia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/stoian2024how.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2402.04823.pdf},
  publisher = {OpenReview.net},
  title = {How Realistic Is Your Synthetic Data? Constraining Deep Generative Models for Tabular Data},
  url = {https://openreview.net/forum?id=tBROYsEz9G},
  year = {2024}
}

@inproceedings{stoica2024zipit,
  abstract = {Typical deep visual recognition models are capable of performing the one task they were trained on. In this paper, we tackle the extremely difficult problem of combining distinct models with different initializations, each solving a separate task, into one multi-task model without any additional training. Prior work in model merging permutes one model to the space of the other then averages them together. While this works for models trained on the same task, we find that this fails to account for the differences in models trained on disjoint tasks. Thus, we introduce "ZipIt!", a general method for merging two arbitrary models of the same architecture that incorporates two simple strategies. First, in order to account for features that aren't shared between models, we expand the model merging problem to allow for merging features within each model by defining a general "zip" operation. Second, we add support for partially zipping the models up until a specified layer, naturally creating a multi-head model. Compared to prior work, these two changes combined account for 20-60% improvement over prior work, and can zip models that couldn't previously be merged at all.},
  author = {George Stoica and Daniel Bolya and Jakob Bjorner and Pratik Ramesh and Taylor Hearn and Judy Hoffman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/stoica2024zipit.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2305.03053.pdf},
  publisher = {OpenReview.net},
  title = {{ZipIt}! Merging Models from Different Tasks without Training},
  url = {https://openreview.net/forum?id=LEYUkvdUhq},
  year = {2024}
}

@inproceedings{struppek2024careful,
  abstract = {Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks ({MIAs}), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters {MIAs}, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhancing model resilience against {MIAs}.},
  author = {Lukas Struppek and Dominik Hintersdorf and Kristian Kersting},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/struppek2024careful.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1SbkubNdbW},
  publisher = {OpenReview.net},
  title = {Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks},
  url = {https://openreview.net/forum?id=1SbkubNdbW},
  year = {2024}
}

@inproceedings{su2024saprot,
  abstract = {Large-scale protein language models ({PLMs}), such as the {ESM} family, have achieved remarkable performance in various downstream tasks related to protein structure and function by undergoing unsupervised training on residue sequences. However, a limitation of vanilla {PLMs} is their lack of explicit consideration for protein structure information. To address this, we introduce {SaProt}, a structure-aware vocabulary that integrates residue tokens with structure tokens derived by encoding 3{D} protein structures using {Foldseek}. {SaProt} is a large-scale general-purpose {PLM} trained on approximately 40 million protein sequences and structures. Through extensive evaluation across 10 significant downstream tasks, {SaProt} surpasses well-established baselines, demonstrating exceptional capacity and broad applicability in protein-related tasks.},
  author = {Jin Su and Chenchen Han and Yuyang Zhou and Junjie Shan and Xibin Zhou and Fajie Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/su2024saprot.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=6MRm3G4NiU},
  publisher = {OpenReview.net},
  title = {SaProt: Protein Language Modeling with Structure-aware Vocabulary},
  url = {https://openreview.net/forum?id=6MRm3G4NiU},
  year = {2024}
}

@inproceedings{su2024pres,
  abstract = {Memory-based Dynamic Graph Neural Networks ({MDGNNs}) are a family of dynamic graph neural networks that leverage a memory module to extract, distill, and memorize long-term temporal dependencies, leading to superior performance compared to memory-less counterparts. However, training {MDGNNs} faces the challenge of handling entangled temporal and structural dependencies, requiring sequential and chronological processing of data sequences to capture accurate temporal patterns. During batch training, temporal data points within the same batch are processed in parallel, while their temporal dependencies are neglected, leading to temporal discontinuity that restricts the effective temporal batch size. We propose {PRES}, an iterative prediction-correction scheme combined with a memory coherence learning objective to mitigate the effect of temporal discontinuity, enabling {MDGNNs} to be trained with significantly larger temporal batches without sacrificing generalization performance. Experimental results demonstrate that our approach enables up to 4√ó larger temporal batch sizes with 3.4√ó training speed-up.},
  author = {Junwei Su and Difan Zou and Chuan Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/su2024pres.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gjXor87Xfy},
  publisher = {OpenReview.net},
  title = {{PRES}: Toward Scalable Memory-Based Dynamic Graph Neural Networks},
  url = {https://openreview.net/forum?id=gjXor87Xfy},
  year = {2024}
}

@inproceedings{subramani2024expressivity,
  abstract = {Most reinforcement learning algorithms require objectives to be formalized with {Markovian} reward functions, but certain tasks cannot be expressed through {Markov} rewards formalism. This has motivated study of alternative objective-specification formalisms like Linear Temporal Logic and Multi-Objective Reinforcement Learning. However, there has not been a thorough analysis of how these formalisms relate to each other in terms of expressivity. We fill this gap by providing a comprehensive comparison of 17 salient objective-specification formalisms. We place these formalisms in a preorder based on their expressive power, and present this preorder as a {Hasse} diagram. We prove that each of Regularised {RL}, Outer Nonlinear {Markov} Rewards, Reward Machines, Linear Temporal Logic, and Limit Average Rewards can express a task that the others cannot. Our results have two key implications: we identify important expressivity limitations to consider when specifying objectives for policy optimization, and highlight the need for future research which adapts reward learning to work with a greater variety of formalisms.},
  author = {Rohan Subramani and Marcus Williams and Max Heitmann and Halfdan Holm and Charlie Griffin and Joar Max Viktor Skalse},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/subramani2024expressivity.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qr4ECbGcSj},
  publisher = {OpenReview.net},
  title = {On the Expressivity of Objective-Specification Formalisms in Reinforcement Learning},
  url = {https://openreview.net/forum?id=qr4ECbGcSj},
  year = {2024}
}

@inproceedings{sui2024selfsupervised,
  abstract = {We propose a new domain-agnostic self-supervised learning framework, Learning from Random projectors ({LFR}), that can be applied to any data modality and network architecture. {LFR} does not rely on augmentations or masking and shows that high-quality data representations can be learned by reconstructing random data projections. The proposed approach has several advantages: it does not require domain-specific knowledge or specific model architecture, can be applied to any data modality and application domains, and outperforms multiple state-of-the-art {SSL} baselines on a wide range of data modalities (image, sequential, and tabular) and real-world applications (banking, healthcare and natural sciences).},
  author = {Yi Sui and Tongzi Wu and Jesse C. Cresswell and Ga Wu and George Stein and Xiao Shi Huang and Xiaochen Zhang and Maksims Volkovs},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sui2024selfsupervised.pdf:pdf},
  note = {DBLP last modified: 2025-05-19},
  pdf = {https://openreview.net/pdf?id=EpYnZpDpsQ},
  publisher = {OpenReview.net},
  title = {Self-supervised Representation Learning from Random Data Projectors},
  url = {https://openreview.net/forum?id=EpYnZpDpsQ},
  year = {2024}
}

@inproceedings{sukovic2024reward,
  abstract = {Equipping agents with the capacity to justify made decisions using supporting evidence represents a cornerstone of accountable decision-making. Ensuring justifications align with human expectations and societal norms is vital, especially in high-stakes situations such as healthcare. We propose the use of a debate-based reward model for reinforcement learning agents, where the outcome of a zero-sum debate game quantifies the justifiability of a decision in a particular state. This reward model is then used to train a justifiable policy, whose decisions can be more easily corroborated with supporting evidence. In the debate game, two argumentative agents take turns providing supporting evidence for two competing decisions. Given the proposed evidence, a proxy of a human judge evaluates which decision is better justified.},
  author = {Aleksa Sukovic and Goran Radanovic},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sukovic2024reward.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OUkZXbbwQr},
  publisher = {OpenReview.net},
  title = {Reward Design for Justifiable Sequential Decision-Making},
  url = {https://openreview.net/forum?id=OUkZXbbwQr},
  year = {2024}
}

@inproceedings{sun2024mmd,
  abstract = {This paper presents a class of maximum mean discrepancy ({MMD}) based graph kernels, called {MMD-GK}, which are computed by applying {MMD} to the node representations of two graphs with message-passing propagation. We make three main contributions: a class of deep {MMD-GKs} that are able to learn graph kernels and implicit graph features adaptively in an unsupervised manner, a class of supervised deep {MMD-GKs} that utilize label information of graphs to yield more discriminative metrics, and demonstrated effectiveness in clustering and classification tasks across diverse graph datasets.},
  author = {Yan Sun and Jicong Fan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024mmd.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=GZ6AcZwA8r},
  publisher = {OpenReview.net},
  title = {{MMD} Graph Kernel: Effective Metric Learning for Graphs via Maximum Mean Discrepancy},
  url = {https://openreview.net/forum?id=GZ6AcZwA8r},
  year = {2024}
}

@inproceedings{sun2024simple,
  abstract = {The recent trend of large language models ({LLMs}) has sparked great interest in pruning methods for {LLMs}. Existing methods require either retraining, which is rarely affordable for billion-scale {LLMs}, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. We propose {Wanda} (Pruning by Weights and activations), a straightforward yet effective pruning method that induces sparsity in pretrained {LLMs}. Motivated by the observation of emergent large magnitude features in {LLMs}, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. {Wanda} requires no retraining or weight update, and the pruned {LLM} can be used as is. We conduct a thorough evaluation on {LLaMA} and {LLaMA-2} across various language benchmarks, demonstrating that {Wanda} significantly outperforms magnitude pruning and performs competitively against recent methods involving intensive weight updates.},
  author = {Mingjie Sun and Zhuang Liu and Anna Bair and J. Zico Kolter},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024simple.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PxoFut3dWW},
  publisher = {OpenReview.net},
  title = {A Simple and Effective Pruning Approach for Large Language Models},
  url = {https://openreview.net/forum?id=PxoFut3dWW},
  year = {2024}
}

@inproceedings{sun2024test,
  abstract = {Time series analysis has been a long-standing task, and researchers have used various methods to accomplish it. Recent advances in large language models ({LLMs}) have inspired a new approach to time series analysis: the {TS}-for-{LLM} approach, which converts time series into model-friendly representations to enable pretrained {LLMs} to handle time series data. We focus on the {TS}-for-{LLM} approach and introduce {TEST}, which first tokenizes time series, builds an encoder to embed time series via instance-wise, feature-wise, and text-prototype-aligned contrast, where the time series embedding space is aligned to {LLM} embedding layer space, then creates soft prompts to make {LLM} more open to that embeddings, and finally implements time series tasks using the frozen {LLM}. Experiments demonstrate that the pretrained {LLM} with {TEST} strategy can achieve better or comparable performance than today's state-of-the-art time series models and offer benefits for few-shot and generalization.},
  author = {Chenxi Sun and Hongyan Li and Yaliang Li and Shenda Hong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024test.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Tuh4nZVb0g},
  publisher = {OpenReview.net},
  title = {{TEST}: Text Prototype Aligned Embedding to Activate {LLM}'s Ability for Time Series},
  url = {https://openreview.net/forum?id=Tuh4nZVb0g},
  year = {2024}
}

@inproceedings{sun2024querydependent,
  abstract = {We study enhancing the arithmetic reasoning ability of Large Language Models ({LLMs}) through zero-shot prompt optimization. We identify a previously overlooked objective of query dependency in such optimization and elucidate two ensuing challenges: the absence of an effective method to evaluate prompts during inference when the golden answer is unavailable, and the learning via interactions with {LLMs} to navigate the expansive natural language prompting space proves to be resource-intensive. To address this, we introduce Prompt-{OIRL}, which harnesses offline inverse reinforcement learning to draw insights from offline prompting demonstration data. This approach seeks to bridge the gap between effective prompt evaluation and affordability by drawing on offline datasets from expert evaluations, employing Inverse-{RL} to derive a reward model for offline, query-dependent prompt evaluations.},
  author = {Hao Sun and Alihan Hyk and Mihaela van der Schaar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024querydependent.pdf:pdf},
  note = {DBLP last modified: 2024-12-11},
  pdf = {https://openreview.net/pdf?id=N6o0ZtPzTg},
  publisher = {OpenReview.net},
  title = {Query-Dependent Prompt Evaluation and Optimization with Offline Inverse {RL}},
  url = {https://openreview.net/forum?id=N6o0ZtPzTg},
  year = {2024}
}

@inproceedings{sun2024cifar10warehouse,
  abstract = {Analyzing model performance in various unseen environments is a critical research problem in the machine learning community. To study this problem, it is important to construct a testbed with out-of-distribution test sets that have broad coverage of environmental discrepancies. We introduce CIFAR-10-Warehouse with 180 datasets collected by prompting image search engines and diffusion models. Dataset sizes range from 300 to 8,000 images, focusing on domain generalization and model accuracy prediction.},
  author = {Xiaoxiao Sun and Xingjian Leng and Zijian Wang and Yang Yang and Zi Huang and Liang Zheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024cifar10warehouse.pdf:pdf},
  note = {DBLP last modified: 2025-01-13},
  pdf = {https://openreview.net/pdf?id=pw2ssoOTpo},
  publisher = {OpenReview.net},
  title = {{CIFAR-10-Warehouse}: Broad and More Realistic Testbeds in Model Generalization Analysis},
  url = {https://openreview.net/forum?id=pw2ssoOTpo},
  year = {2024}
}

@inproceedings{sun2024interpgnn,
  abstract = {Transductive node prediction has been a popular learning setting in Graph Neural Networks (GNNs). It has been widely observed that the shortage of information flow between the distant nodes and intra-batch nodes (for large-scale graphs) often hurt the generalization of GNNs which overwhelmingly adopt message-passing. This paper proposes a PAC-Bayesian bound for GNNs and introduces a Graph Global Workspace module to enhance interplay between training and testing nodes.},
  author = {Jiawei Sun and Kailai Li and Ruoxin Chen and Jie Li and Chentao Wu and Yue Ding and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024interpgnn.pdf:pdf},
  note = {DBLP last modified: 2024-11-11},
  pdf = {https://openreview.net/pdf?id=pwW807WJ9G},
  publisher = {OpenReview.net},
  title = {{InterpGNN}: Understand and Improve Generalization Ability of Transductive {GNNs} through the Lens of Interplay between Train and Test Nodes},
  url = {https://openreview.net/forum?id=pwW807WJ9G},
  year = {2024}
}

@inproceedings{sun2024improving,
  abstract = {This paper proposes FFA-LoRA (Federated Freeze A LoRA), a modification to LoRA in privacy-preserved federated learning with better performance, reliability and efficiency. The paper addresses the instability of LoRA in privacy-preserving federated learning due to effects of data heterogeneity, multi-step local updates, and additive noise enforced on updating gradients. The core idea is to fix the randomly initialized non-zero matrices and only fine-tune the zero-initialized matrices, which alleviates these challenges and further halves the communication cost.},
  author = {Youbang Sun and Zitao Li and Yaliang Li and Bolin Ding},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024improving.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NLPzL6HWNl},
  publisher = {OpenReview.net},
  title = {Improving {LoRA} in Privacy-preserving Federated Learning},
  url = {https://openreview.net/forum?id=NLPzL6HWNl},
  year = {2024}
}

@inproceedings{sun2024co2,
  abstract = {This paper proposes CO2, a new approach that introduces local-updating and asynchronous communication to distributed data-parallel training, thereby facilitating the full overlap of communication with computation. CO2 enables high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. The paper demonstrates CO2's capabilities in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs, achieving perfect 100% scalability on clusters with both 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.},
  author = {Weigao Sun and Zhen Qin and Weixuan Sun and Shidi Li and Dong Li and Xuyang Shen and Yu Qiao and Yiran Zhong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024co2.pdf:pdf},
  note = {DBLP last modified: 2025-04-16},
  pdf = {https://openreview.net/pdf?id=ZO5cn4IfaN},
  publisher = {OpenReview.net},
  title = {{CO2}: Efficient Distributed Training with Full Communication-Computation Overlap},
  url = {https://openreview.net/forum?id=ZO5cn4IfaN},
  year = {2024}
}

@inproceedings{sun2024importance,
  abstract = {This paper investigates whether differentially private stochastic gradient descent (DPSGD) alone is sufficient for finding good minimizers under privacy constraints. For linear classification, the authors show theoretically that feature preprocessing is vital for differentially private optimization, unlike non-private optimization. Without feature preprocessing, DPSGD incurs an optimality gap proportional to the maximum Euclidean norm of features over all samples, demonstrating that popular DP algorithms require additional preprocessing steps for optimal performance.},
  author = {Ziteng Sun and Ananda Theertha Suresh and Aditya Krishna Menon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024importance.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=XlTDBZFXWp},
  publisher = {OpenReview.net},
  title = {The importance of feature preprocessing for differentially private linear optimization},
  url = {https://openreview.net/forum?id=XlTDBZFXWp},
  year = {2024}
}

@inproceedings{sun2024evaluating,
  abstract = {This paper investigates instruction fine-tuning as a promising approach for improving the zero-shot capabilities of Large Language Models on new tasks. The authors ask two key questions: How sensitive are instruction-tuned models to particular phrasings of instructions, and how can we make them more robust to such natural language variation? They collected 319 instructions manually written by NLP practitioners for over 80 unique tasks and found that using novel but appropriate instruction phrasings consistently degrades model performance. The paper proposes introducing soft prompt embedding parameters optimized to maximize similarity between representations of semantically equivalent instructions to improve robustness.},
  author = {Jiuding Sun and Chantal Shaib and Byron C. Wallace},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024evaluating.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=g9diuvxN6D},
  publisher = {OpenReview.net},
  title = {Evaluating the Zero-shot Robustness of Instruction-tuned Language Models},
  url = {https://openreview.net/forum?id=g9diuvxN6D},
  year = {2024}
}

@inproceedings{sun2024salmon,
  abstract = {This paper introduces SALMON, a new AI alignment paradigm where an instructable reward model is trained to effectively and flexibly align language models with human values and intentions. SALMON presents a novel approach to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. The approach addresses the dependency of traditional SFT+RLHF approaches on high-quality human annotations, making alignment more practical for intricate tasks by training an instructable reward model on synthetic preference data that can generate reward scores based on arbitrary human-defined principles.},
  author = {Zhiqing Sun and Yikang Shen and Hongxin Zhang and Qinhong Zhou and Zhenfang Chen and David Daniel Cox and Yiming Yang and Chuang Gan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024salmon.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xJbsmB8UMx},
  publisher = {OpenReview.net},
  title = {{SALMON}: Self-Alignment with Instructable Reward Models},
  url = {https://openreview.net/forum?id=xJbsmB8UMx},
  year = {2024}
}

@inproceedings{sun2024inner,
  abstract = {Classifier-free guidance (CFG) is a pivotal technique for balancing the diversity and fidelity of samples in conditional diffusion models. This approach involves utilizing a single model to jointly optimize the conditional score predictor and unconditional score predictor, eliminating the need for additional classifiers. This paper proposes inner classifier-free guidance (ICFG) as an alternative perspective on the CFG method when the condition has a specific structure, demonstrating that CFG represents a first-order case of ICFG. The method provides a more general framework for understanding and improving guidance in diffusion models.},
  author = {Shikun Sun and Longhui Wei and Zhicai Wang and Zixuan Wang and Junliang Xing and Jia Jia and Qi Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024inner.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0QAzIMq32X},
  publisher = {OpenReview.net},
  title = {Inner Classifier-Free Guidance and Its Taylor Expansion for Diffusion Models},
  url = {https://openreview.net/forum?id=0QAzIMq32X},
  year = {2024}
}

@inproceedings{sun2024program,
  abstract = {Test-time adaptation (TTA) aims to adapt a pre-trained model from a source domain to a target domain using only online unlabeled target data during testing, without accessing source data or modifying the original training process. This paper proposes PROGRAM, a novel TTA method consisting of two key components: (1) Prototype Graph Model (PGM) for reliable pseudo-label generation by constructing graphs using prototypes and test samples to facilitate effective message passing; (2) Robust Self-Training (RST) for test-time adaptation with noisy pseudo-labels by combining consistency regularization and pseudo-labeling. PROGRAM can be easily integrated into existing baselines and outperforms existing TTA methods on multiple domain generalization and image corruption benchmarks.},
  author = {Haopeng Sun and Lumin Xu and Sheng Jin and Ping Luo and Chen Qian and Wentao Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024program.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=x5LvBK43wg},
  publisher = {OpenReview.net},
  title = {{PROGRAM}: {PROtotype} {GRAph} Model based Pseudo-Label Learning for Test-Time Adaptation},
  url = {https://openreview.net/forum?id=x5LvBK43wg},
  year = {2024}
}

@inproceedings{sun2024thinkongraph,
  abstract = {This paper addresses hallucination problems in large language models (LLMs), especially in scenarios requiring deep and responsible reasoning, by introducing external knowledge graphs (KG) in LLM reasoning. The authors propose a new LLM-KG integrating paradigm where the LLM is treated as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. The paper implements Think-on-Graph (ToG), where the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. ToG achieves state-of-the-art results in 6 out of 9 datasets as a training-free method with lower computational cost and better generality.},
  author = {Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Saizhuo Wang and Chen Lin and Yeyun Gong and Lionel M. Ni and Heung-Yeung Shum and Jian Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024thinkongraph.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nnVO1PvbTv},
  publisher = {OpenReview.net},
  title = {Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph},
  url = {https://openreview.net/forum?id=nnVO1PvbTv},
  year = {2024}
}

@inproceedings{sun2024copula,
  abstract = {Accurate uncertainty measurement is a key step in building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification framework popular for its ease of implementation, finite-sample coverage guarantees, and generality for underlying prediction algorithms. However, existing conformal prediction approaches for time series are limited to single-step prediction without considering the temporal dependency. In this paper, we propose the Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. The key insight of our algorithm is that we can model the joint probability of uncertainty for multiple predicted time steps with a copula, hence better capturing the confidence regions. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.},
  author = {Sophia Huiwen Sun and Rose Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024copula.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ojIJZDNIBj},
  publisher = {OpenReview.net},
  title = {Copula Conformal prediction for multi-step time series prediction},
  url = {https://openreview.net/forum?id=ojIJZDNIBj},
  year = {2024}
}

@inproceedings{sun2024emu,
  abstract = {We present Emu, a Transformer-based multimodal foundation model, which can seamlessly generate images and texts in multimodal context. This omnivore model can take in any single-modality or multimodal data input indiscriminately (e.g., interleaved image, text and video) through a one-model-for-all autoregressive training process. First, visual signals are encoded into embeddings, and together with text tokens form an interleaved input sequence. Emu is then end-to-end trained with a unified objective of classifying the next text token or regressing the next visual embedding in the multimodal sequence. This versatile multimodality empowers the exploration of diverse pretraining data sources at scale, such as videos with interleaved frames and text, webpages with interleaved images and text, as well as web-scale image-text pairs and video-text pairs. Emu can serve as a generalist multimodal interface for both image-to-text and text-to-image tasks, and supports in-context image and text generation. Across a broad range of zero-shot/few-shot tasks including image captioning, visual question answering, video question answering and text-to-image generation, Emu demonstrates superb performance compared to state-of-the-art large multimodal models.},
  author = {Quan Sun and Qiying Yu and Yufeng Cui and Fan Zhang and Xiaosong Zhang and Yueze Wang and Hongcheng Gao and Jingjing Liu and Tiejun Huang 0003 and Xinlong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024emu.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mL8Q9OOamV},
  publisher = {OpenReview.net},
  title = {Emu: Generative Pretraining in Multimodality},
  url = {https://openreview.net/forum?id=mL8Q9OOamV},
  year = {2024}
}

@inproceedings{2024alice,
  abstract = {For object re-identification (re-ID), learning from synthetic data has become a promising strategy to cheaply acquire large-scale annotated datasets and effective models, with few privacy concerns. Many interesting research problems arise from this strategy, e.g., how to reduce the domain gap between synthetic source and real-world target. To facilitate developing more new approaches in learning from synthetic data, we introduce the Alice benchmarks, large-scale datasets providing benchmarks as well as evaluation protocols to the research community. Within the Alice benchmarks, two object re-ID tasks are offered: person and vehicle re-ID. We collected and annotated two challenging real-world target datasets: AlicePerson and AliceVehicle, captured under various illuminations, image resolutions, etc. As an important feature of our real target, the clusterability of its training set is not manually guaranteed to make it closer to a real domain adaptation test scenario. Correspondingly, we reuse existing PersonX and VehicleX as synthetic source domains. The primary goal is to train models from synthetic data that can work effectively in the real world.},
  author = {Xiaoxiao Sun 0002 and Yue Yao and Shengjin Wang and Hongdong Li and Liang Zheng 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024alice.pdf:pdf},
  note = {DBLP last modified: 2025-01-13},
  pdf = {https://openreview.net/pdf?id=vkkHqoerLV},
  publisher = {OpenReview.net},
  title = {Alice Benchmarks: Connecting Real World Re-Identification with the Synthetic},
  url = {https://openreview.net/forum?id=vkkHqoerLV},
  year = {2024}
}

@inproceedings{sun2024backdoor,
  abstract = {Contrastive Learning (CL) has attracted enormous attention due to its remarkable capability in unsupervised representation learning. However, recent works have revealed the vulnerability of CL to backdoor attacks: the feature extractor could be misled to embed backdoored data close to an attack target class, thus fooling the downstream predictor to misclassify it as the target. Existing attacks usually adopt a fixed trigger pattern and poison the training set with trigger-injected data, hoping for the feature extractor to learn the association between trigger and target class. However, we find that such fixed trigger design fails to effectively associate trigger-injected data with target class in the embedding space due to special CL mechanisms, leading to a limited attack success rate (ASR). This phenomenon motivates us to find a better backdoor trigger design tailored for CL framework. In this paper, we propose a bi-level optimization approach to achieve this goal, where the inner optimization simulates the CL dynamics of a surrogate victim, and the outer optimization enforces the backdoor trigger to stay close to the target throughout the surrogate CL procedure. Extensive experiments show that our attack can achieve a higher attack success rate (e.g., 99% ASR on ImageNet-100) with a very low poisoning rate (1%). Besides, our attack can effectively evade existing state-of-the-art defenses.},
  author = {Weiyu Sun and Xinyu Zhang and Hao Lu 0009 and Ying-Cong Chen and Ting Wang and Jinghui Chen and Lu Lin 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024backdoor.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oxjeePpgSP},
  publisher = {OpenReview.net},
  title = {Backdoor Contrastive Learning via Bi-level Trigger Optimization},
  url = {https://openreview.net/forum?id=oxjeePpgSP},
  year = {2024}
}

@inproceedings{2024beliefenriched,
  abstract = {Reinforcement learning (RL) has achieved phenomenal success in various domains. However, its data-driven nature also introduces new vulnerabilities that can be exploited by malicious opponents. Recent work shows that a well-trained RL agent can be easily manipulated by strategically perturbing its state observations at the test stage. Existing solutions either introduce a regularization term to improve the smoothness of the trained policy against perturbations or alternatively train the agent's policy and the attacker's policy. However, the former does not provide sufficient protection against strong attacks, while the latter is computationally prohibitive for large environments. In this work, we propose a new robust RL algorithm for deriving a pessimistic policy to safeguard against an agent's uncertainty about true states. This approach is further enhanced with belief state inference and diffusion-based state purification to reduce uncertainty.},
  author = {Xiaolin Sun 0002 and Zizhan Zheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024beliefenriched.pdf:pdf},
  note = {DBLP last modified: 2025-05-27},
  pdf = {https://openreview.net/pdf?id=7gDENzTzw1},
  publisher = {OpenReview.net},
  title = {Belief-Enriched Pessimistic Q-Learning against Adversarial State Perturbations},
  url = {https://openreview.net/forum?id=7gDENzTzw1},
  year = {2024}
}

@inproceedings{sun2024eventrpg,
  abstract = {Event camera, a novel bio-inspired vision sensor, has drawn a lot of attention for its low latency, low power consumption, and high dynamic range. Currently, overfitting remains a critical problem in event-based classification tasks for Spiking Neural Network (SNN) due to its relatively weak spatial representation capability. Data augmentation is a simple but efficient method to alleviate overfitting and improve the generalization ability of neural networks, and saliency-based augmentation methods are proven to be effective in the image processing field. However, there is no approach available for extracting saliency maps from SNNs. Therefore, for the first time, we present Spiking Layer-Time-wise Relevance Propagation rule (SLTRP) and Spiking Layer-wise Relevance Propagation rule (SLRP) in order for SNN to generate stable and accurate CAMs and saliency maps. Based on this, we propose EventRPG, which leverages relevance propagation on the spiking neural network for more efficient augmentation. Our proposed method has been evaluated on several SNN structures, achieving state-of-the-art performance in object recognition tasks including N-Caltech101, CIFAR10-DVS, with accuracies of 85.62% and 85.55%, as well as action recognition task SL-Animals with an accuracy of 91.59%.},
  author = {Mingyuan Sun and Donghao Zhang and Zongyuan Ge and Jiaxu Wang and Jia Li and Zheng Fang 0001 and Renjing Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024eventrpg.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=i7LCsDMcZ4},
  publisher = {OpenReview.net},
  title = {EventRPG: Event Data Augmentation with Relevance Propagation Guidance},
  url = {https://openreview.net/forum?id=i7LCsDMcZ4},
  year = {2024}
}

@inproceedings{sun2024dreamcraft3d,
  abstract = {We present DreamCraft3D, a hierarchical 3D content generation method that produces high-fidelity and coherent 3D objects. We tackle the problem by leveraging a 2D reference image to guide the stages of geometry sculpting and texture boosting. A central focus of this work is to address the consistency issue that existing works encounter. To sculpt geometries that render coherently, we perform score distillation sampling via a view-dependent diffusion model. This 3D prior, alongside several training strategies, prioritizes the geometry consistency but compromises the texture fidelity. We further propose bootstrapped score distillation to specifically boost the texture. We train a personalized diffusion model, Dreambooth, on the augmented renderings of the scene, imbuing it with 3D knowledge of the scene being optimized. The score distillation from this 3D-aware diffusion prior provides view-consistent guidance for the scene. Notably, through an alternating optimization of the diffusion prior and 3D scene representation, we achieve mutually reinforcing improvements: the optimized 3D scene aids in training the scene-specific diffusion model, which offers increasingly view-consistent guidance for 3D optimization. The optimization is thus bootstrapped and leads to substantial texture boosting. With tailored 3D priors throughout the hierarchical generation, DreamCraft3D generates coherent 3D objects with photorealistic renderings, advancing the state-of-the-art in 3D content generation.},
  author = {Jingxiang Sun and Bo Zhang and Ruizhi Shao and Lizhen Wang 0002 and Wen Liu and Zhenda Xie and Yebin Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024dreamcraft3d.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DDX1u29Gqr},
  publisher = {OpenReview.net},
  title = {DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior},
  url = {https://openreview.net/forum?id=DDX1u29Gqr},
  year = {2024}
}

@inproceedings{sun2024calico,
  abstract = {Perception is crucial in the realm of autonomous driving systems, where bird's eye view (BEV)-based architectures have recently reached state-of-the-art performance. The desirability of self-supervised representation learning stems from the expensive and laborious process of annotating 2D and 3D data. Although previous research has investigated pretraining methods for both LiDAR and camera-based 3D object detection, a unified pretraining framework for multimodal BEV perception is missing. In this study, we introduce CALICO, a novel framework that applies contrastive objectives to both LiDAR and camera backbones. Specifically, CALICO incorporates two stages: point-region contrast (PRC) and region-aware distillation (RAD). PRC better balances the region- and scene-level representation learning on the LiDAR modality and offers significant performance improvement compared to existing methods. RAD effectively achieves contrastive distillation on our self-trained teacher model. CALICO's efficacy is substantiated by extensive evaluations on 3D object detection and BEV map segmentation tasks, where it delivers significant performance improvements. Notably, CALICO outperforms the baseline method by 10.5% and 8.6% on NDS and mAP. Moreover, CALICO boosts the robustness of multimodal 3D object detection against adversarial attacks and corruption. Additionally, our framework can be tailored to different backbones and heads, positioning it as a promising approach for multimodal BEV perception.},
  author = {Jiachen Sun and Haizhong Zheng and Qingzhao Zhang and Atul Prakash 0001 and Zhuoqing Mao 0001 and Chaowei Xiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sun2024calico.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=U7iiF79kI3},
  publisher = {OpenReview.net},
  title = {CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception},
  url = {https://openreview.net/forum?id=U7iiF79kI3},
  year = {2024}
}

@inproceedings{sung2024robust,
  abstract = {We introduce $\mathcal{L}_1$-MBRL, a control-theoretic augmentation scheme for Model-Based Reinforcement Learning (MBRL) algorithms. Unlike model-free approaches, MBRL algorithms learn a model of the transition function using data and use it to design a control input. Our approach generates a series of approximate control-affine models of the learned transition function according to the proposed switching law. Using the approximate model, control input produced by the underlying MBRL is perturbed by the $\mathcal{L}_1$ adaptive control, which is designed to enhance the robustness of the system against uncertainties. Importantly, this approach is agnostic to the choice of MBRL algorithm, enabling the use of the scheme with various MBRL algorithms. MBRL algorithms with $\mathcal{L}_1$ augmentation exhibit enhanced performance and sample efficiency across multiple MuJoCo environments, outperforming the original MBRL algorithms, both with and without system noise.},
  author = {Minjun Sung and Sambhu H. Karumanchi and Aditya Gahlawat and Naira Hovakimyan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sung2024robust.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=GaLCLvJaoF},
  publisher = {OpenReview.net},
  title = {Robust Model Based Reinforcement Learning Using L1 Adaptive Control},
  url = {https://openreview.net/forum?id=GaLCLvJaoF},
  year = {2024}
}

@inproceedings{sung2024ecoflap,
  abstract = {Large Vision-Language Models (LVLMs) can understand the world comprehensively by integrating rich information from different modalities, achieving remarkable advancements on various multimodal downstream tasks. However, deploying LVLMs is often problematic due to their massive computational/energy costs and carbon consumption. Such issues make it infeasible to adopt conventional iterative global pruning, which is costly due to computing the Hessian matrix of the entire large model for sparsification. Alternatively, several studies have recently proposed layer-wise pruning approaches to avoid the expensive computation of global pruning and efficiently compress model weights according to their importance within a layer. However, they often suffer from suboptimal model compression due to their lack of a global perspective. To address this limitation in recent efficient pruning methods for large models, we propose Efficient Coarse-to-Fine LayerWise Pruning (ECoFLaP), a two-stage coarse-to-fine weight pruning approach for LVLMs. We first determine the sparsity ratios of different layers or blocks by leveraging the global importance score, which is efficiently computed based on the zeroth-order approximation of the global model gradients. Then, the model performs local layer-wise unstructured weight pruning based on globally-informed sparsity ratios. We validate our proposed method across various multimodal and unimodal models and datasets, demonstrating significant performance improvements over prevalent pruning techniques in the high-sparsity regime.},
  author = {Yi-Lin Sung and Jaehong Yoon and Mohit Bansal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sung2024ecoflap.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iIT02bAKzv},
  publisher = {OpenReview.net},
  title = {ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models},
  url = {https://openreview.net/forum?id=iIT02bAKzv},
  year = {2024}
}

@inproceedings{sussex2024adversarial,
  abstract = {In Causal Bayesian Optimization (CBO), an agent intervenes on a structural causal model with known graph but unknown mechanisms to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-world data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users' demand patterns in a shared mobility system and reposition vehicles in strategic areas.},
  author = {Scott Sussex and Pier Giuseppe Sessa and Anastasia Makarova and Andreas Krause},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/sussex2024adversarial.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf/70ae95d2b3d8a042800cb337082f488983922e18.pdf},
  publisher = {OpenReview.net},
  title = {Adversarial Causal Bayesian Optimization},
  url = {https://openreview.net/forum?id=YcW8i9VCf5},
  year = {2024}
}

@inproceedings{suzuki2024optimal,
  abstract = {Deep neural networks with feature learning have shown surprising generalization performance in high dimensional settings, but it has not been fully understood how and when they enjoy the benefit of feature learning. In this paper, we theoretically analyze the statistical properties of the benefits from feature learning in a two-layer linear neural network with multiple outputs in a high-dimensional setting. For that purpose, we propose a new criterion that allows feature learning of a two-layer linear neural network in a high-dimensional setting. Interestingly, we can show that models with smaller values of the criterion generalize even in situations where normal ridge regression fails to generalize. This is because the proposed criterion contains a proper regularization for the feature mapping and acts as an upper bound on the predictive risk. As an important characterization of the criterion, the two-layer linear neural network that minimizes this criterion can achieve the optimal Bayes risk that is determined by the distribution of the true signals across the multiple outputs. To the best of our knowledge, this is the first study to specifically identify the conditions under which a model obtained by proper feature learning can outperform normal ridge regression in a high-dimensional multiple-output linear regression problem.},
  author = {Keita Suzuki and Taiji Suzuki},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/suzuki2024optimal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Jc0FssXh2R},
  publisher = {OpenReview.net},
  title = {Optimal criterion for feature learning of two-layer linear neural network in high dimensional interpolation regime},
  url = {https://openreview.net/forum?id=Jc0FssXh2R},
  year = {2024}
}

@inproceedings{szot2024large,
  abstract = {We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42\% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement.},
  author = {Andrew Szot and Max Schwarzer and Harsh Agrawal and Bogdan Mazoure and Rin Metcalf and Walter Talbott and Natalie Mackraz and R. Devon Hjelm and Alexander T. Toshev},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/szot2024large.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=u6imHU4Ebu},
  publisher = {OpenReview.net},
  title = {Large Language Models as Generalizable Policies for Embodied Tasks},
  url = {https://openreview.net/forum?id=u6imHU4Ebu},
  year = {2024}
}

@inproceedings{takida2024san,
  abstract = {Generative adversarial networks (GANs) learn a target probability distribution by optimizing a generator and a discriminator with minimax objectives. This paper addresses the question of whether such optimization actually provides the generator with gradients that make its distribution close to the target distribution. We derive metrizable conditions, sufficient conditions for the discriminator to serve as the distance between the distributions by connecting the GAN formulation with the concept of sliced optimal transport. Furthermore, by leveraging these theoretical results, we propose a novel GAN training scheme called the Slicing Adversarial Network (SAN). With only simple modifications, a broad class of existing GANs can be converted to SANs. Experiments on synthetic and image datasets support our theoretical results and the effectiveness of SAN as compared to the usual GANs.},
  author = {Yuhta Takida and Masaaki Imaizumi and Takashi Shibuya and Chieh-Hsin Lai and Toshimitsu Uesaka and Naoki Murata and Yuki Mitsufuji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/takida2024san.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf/e24f00c0e961a02784afb5aaa4e6f670cb1640ab.pdf},
  publisher = {OpenReview.net},
  title = {SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear Layer},
  url = {https://openreview.net/forum?id=eiF7TU1E8E},
  year = {2024}
}

@inproceedings{tan2024boosting,
  abstract = {Large-scale Vision Transformers have achieved promising performance on downstream tasks through feature pre-training. However, the performance of vanilla lightweight Vision Transformers (ViTs) is still far from satisfactory compared to that of recent lightweight CNNs or hybrid networks. In this paper, we aim to unlock the potential of vanilla lightweight ViTs by exploring the adaptation of the widely-used re-parameterization technology to ViTs for improving learning ability during training without increasing the inference cost. The main challenge comes from the fact that CNNs perfectly complement the re-parameterization technology whereas ViTs present different architectural characteristics. We propose RepViT, which integrates structural re-parameterization into Vision Transformers to enhance their training dynamics while maintaining their lightweight inference profile.},
  author = {Zhentao Tan and Xiaodan Li and Yue Wu and Qi Chu and Le Lu and Nenghai Yu and Jieping Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tan2024boosting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3rmpixOjPS},
  publisher = {OpenReview.net},
  title = {Boosting Vanilla Lightweight Vision Transformers via Re-parameterization},
  url = {https://openreview.net/forum?id=3rmpixOjPS},
  year = {2024}
}

@inproceedings{tan2024massive,
  abstract = {While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. We conduct comprehensive experiments on both the COUNTERFACT and zsRE datasets with different-size language models, and MALMEN demonstrates superior performance and efficiency.},
  author = {Chenmien Tan and Ge Zhang and Jie Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tan2024massive.pdf:pdf},
  note = {DBLP last modified: 2025-05-23},
  pdf = {https://openreview.net/pdf?id=L6L1CJQ2PE},
  publisher = {OpenReview.net},
  title = {Massive Editing for Large Language Models via Meta Learning},
  url = {https://openreview.net/forum?id=L6L1CJQ2PE},
  year = {2024}
}

@inproceedings{tan2024true,
  abstract = {Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring a pre-trained policy. Specifically, we formulate the LLM as the policy network and initialize it with pre-trained weights. The policy network is optimized to maximize the return through RL training. We additionally propose a structured prompting approach that converts environmental observations to natural language to help with LLM training. Experiments on 6 RL environments ranging from classic control to robotics manipulation demonstrate that TWOSOME exhibits significantly better sample efficiency and performance compared to conventional RL methods like PPO and prompt tuning methods like SayCan. Moreover, benefiting from the open-vocabulary nature of LLMs, TWOSOME shows superior generalization ability to unseen tasks.},
  author = {Weihao Tan and Wentao Zhang and Shanqi Liu and Longtao Zheng and Xinrun Wang and Bo An},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tan2024true.pdf:pdf},
  note = {DBLP last modified: 2025-02-12},
  pdf = {https://openreview.net/pdf?id=hILVmJ4Uvu},
  publisher = {OpenReview.net},
  title = {True Knowledge Comes from Practice: Aligning Large Language Models with Embodied Environments via Reinforcement Learning},
  url = {https://openreview.net/forum?id=hILVmJ4Uvu},
  year = {2024}
}

@inproceedings{tan2024contrastive,
  abstract = {Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the Kernel-InfoNCE loss, incorporating mixtures of kernel functions that outperform the standard Gaussian kernel on several vision datasets. Our theoretical analysis provides new perspectives on understanding contrastive learning and opens up new avenues for improving contrastive learning methods.},
  author = {Zhiquan Tan and Yifan Zhang and Jingqin Yang and Yang Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tan2024contrastive.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hLZQTFGToA},
  publisher = {OpenReview.net},
  title = {Contrastive Learning is Spectral Clustering on Similarity Graph},
  url = {https://openreview.net/forum?id=hLZQTFGToA},
  year = {2024}
}

@inproceedings{tang2024parameterefficient,
  abstract = {Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outperforming standard adapter tuning and task arithmetic alone. Experimental results demonstrate the capabilities of our proposed partial linearization technique to effectively construct unified multi-task models via the fusion of fine-tuned task vectors.},
  author = {Anke Tang and Li Shen and Yong Luo and Yibing Zhan and Han Hu and Bo Du and Yixin Chen and Dacheng Tao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024parameterefficient.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iynRvVVAmH},
  publisher = {OpenReview.net},
  title = {Parameter-Efficient Multi-Task Model Fusion with Partial Linearization},
  url = {https://openreview.net/forum?id=iynRvVVAmH},
  year = {2024}
}

@inproceedings{tang2024towards,
  abstract = {Estimating the properties of quantum systems such as quantum phase has been critical in addressing the essential quantum many-body problems in physics and chemistry. Deep learning models have been recently introduced to property estimation, surpassing conventional statistical approaches. However, these methods are tailored to the specific task and quantum data at hand. It remains an open and attractive question for devising a more universal task-agnostic pretraining model for quantum property estimation. In this paper, we propose LLM4QPE, a large language model style quantum task-agnostic pretraining and finetuning paradigm that 1) performs unsupervised pretraining on diverse quantum systems with different physical conditions; 2) uses the pretrained model for supervised finetuning and delivers high performance with limited training data, on downstream tasks. We develop a comprehensive benchmark with 22 different quantum tasks to evaluate our approach. Our experimental results demonstrate that LLM4QPE significantly outperforms conventional methods and task-specific models across various quantum property estimation tasks.},
  author = {Yehui Tang and Hao Xiong and Nianzu Yang and Tailong Xiao and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=vrBVFXwAmi},
  publisher = {OpenReview.net},
  title = {Towards LLM4QPE: Unsupervised Pretraining of Quantum Property Estimation and A Benchmark},
  url = {https://openreview.net/forum?id=vrBVFXwAmi},
  year = {2024}
}

@inproceedings{tang2024learning,
  abstract = {Personalized federated learning (PFL) has gained great success in tackling the scenarios where target datasets are heterogeneous across the local clients. However, the application of the existing PFL methods to real-world setting is hindered by the common assumption that the test data on each client is in-distribution (IND) with respect to its training data.},
  author = {Xueyang Tang and Song Guo and Jie Zhang and Jingcai Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=8FHWkY0SwF},
  publisher = {OpenReview.net},
  title = {{Learning Personalized Causally Invariant Representations for Heterogeneous Federated Clients}},
  url = {https://openreview.net/forum?id=8FHWkY0SwF},
  year = {2024}
}

@inproceedings{tang2024augkd,
  abstract = {Due to privacy or patent concerns, a growing number of large models are released without granting access to their training data, making transferring their knowledge inefficient and problematic. In response, Data-Free Knowledge Distillation (DFKD) methods have emerged as direct solutions.},
  author = {Zihao Tang and Zheqi Lv and Shengyu Zhang and Yifan Zhou and Xinyu Duan and Fei Wu and Kun Kuang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024augkd.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fcqWJ8JgMR},
  publisher = {OpenReview.net},
  title = {{AuG-KD}: {Anchor-Based} {Mixup} {Generation} for {Out-of-Domain} {Knowledge} {Distillation}},
  url = {https://openreview.net/forum?id=fcqWJ8JgMR},
  year = {2024}
}

@inproceedings{tang2024zerothorder,
  abstract = {In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance.},
  author = {Zhiwei Tang and Dmitry Rybin and Tsung-Hui Chang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024zerothorder.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TVDUVpgu9s},
  publisher = {OpenReview.net},
  title = {{Zeroth-Order} {Optimization} {Meets} {Human} {Feedback}: {Provable} {Learning} via {Ranking} {Oracles}},
  url = {https://openreview.net/forum?id=TVDUVpgu9s},
  year = {2024}
}

@inproceedings{tang2024dreamgaussian,
  abstract = {Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS). Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously.},
  author = {Jiaxiang Tang and Jiawei Ren and Hang Zhou and Ziwei Liu and Gang Zeng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024dreamgaussian.pdf:pdf},
  note = {DBLP last modified: 2025-03-06; ICLR 2024 Oral},
  pdf = {https://openreview.net/pdf?id=UyNXMqnN3c},
  publisher = {OpenReview.net},
  title = {{DreamGaussian}: {Generative} {Gaussian} {Splatting} for {Efficient} {3D} {Content} {Creation}},
  url = {https://openreview.net/forum?id=UyNXMqnN3c},
  year = {2024}
}

@inproceedings{tang2024privacypreserving,
  abstract = {We study the problem of in-context learning (ICL) with large language models (LLMs) on private datasets. This scenario poses privacy risks, as LLMs may leak or regurgitate the private examples demonstrated in the prompt.},
  author = {Xinyu Tang and Richard Shin and Huseyin A. Inan and Andre Manoel and Fatemehsadat Mireshghallah and Zinan Lin and Sivakanth Gopi and Janardhan Kulkarni and Robert Sim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024privacypreserving.pdf:pdf},
  note = {DBLP last modified: 2025-02-21},
  pdf = {https://openreview.net/pdf?id=oZtt0pRnOl},
  publisher = {OpenReview.net},
  title = {{Privacy-Preserving} {In-Context} {Learning} with {Differentially} {Private} {Few-Shot} {Generation}},
  url = {https://openreview.net/forum?id=oZtt0pRnOl},
  year = {2024}
}

@inproceedings{tang2024accelerating,
  abstract = {Computing the optimal transport distance between statistical distributions is a fundamental task in machine learning. One remarkable recent advancement is entropic regularization and the Sinkhorn algorithm, which utilizes only matrix scaling and guarantees an approximated solution with near-linear runtime. Despite the success of the Sinkhorn algorithm, its runtime may still be slow due to the potentially large number of iterations needed for convergence.},
  author = {Xun Tang and Michael Shavlovsky and Holakou Rahmanian and Elisa Tardini and Kiran Koshy Thekumparampil and Tesi Xiao and Lexing Ying},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024accelerating.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Kuj5gVp5GQ},
  publisher = {OpenReview.net},
  title = {{Accelerating} {Sinkhorn} algorithm with sparse {Newton} iterations},
  url = {https://openreview.net/forum?id=Kuj5gVp5GQ},
  year = {2024}
}

@inproceedings{tang2024layoutnuwa,
  abstract = {Graphic layout generation, a growing research field, plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element.},
  author = {Zecheng Tang and Chenfei Wu and Juntao Li and Nan Duan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024layoutnuwa.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qCUWVT0Ayy},
  publisher = {OpenReview.net},
  title = {{LayoutNUWA}: {Revealing} the {Hidden} {Layout} {Expertise} of {Large} {Language} {Models}},
  url = {https://openreview.net/forum?id=qCUWVT0Ayy},
  year = {2024}
}

@inproceedings{tang2024procedural,
  abstract = {We reveal and address the frequently overlooked yet important issue of disguised procedural unfairness, namely, the potentially inadvertent alterations on the behavior of neutral (i.e., not problematic) aspects of data generating process, and/or the lack of procedural assurance of the greatest benefit of the least advantaged individuals.},
  author = {Zeyu Tang and Jialu Wang and Yang Liu and Peter Spirtes and Kun Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024procedural.pdf:pdf},
  note = {DBLP last modified: 2024-11-13},
  pdf = {https://openreview.net/pdf?id=cxfPefbu1s},
  publisher = {OpenReview.net},
  title = {{Procedural} {Fairness} {Through} {Decoupling} {Objectionable} {Data} {Generating} {Components}},
  url = {https://openreview.net/forum?id=cxfPefbu1s},
  year = {2024}
}

@inproceedings{tang2024salmonn,
  abstract = {SALMONN is a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. The work addresses hearing as an essential ability of AI agents, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music.},
  author = {Changli Tang and Wenyi Yu and Guangzhi Sun and Xianzhao Chen and Tian Tan and Wei Li and Lu Lu and Zejun Ma and Chao Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024salmonn.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=14rn7HpKVk},
  publisher = {OpenReview.net},
  title = {{SALMONN}: {Towards} {Generic} {Hearing} {Abilities} for {Large} {Language} {Models}},
  url = {https://openreview.net/forum?id=14rn7HpKVk},
  year = {2024}
}

@inproceedings{tang2024fedimpro,
  abstract = {Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models.},
  author = {Zhenheng Tang and Yonggang Zhang and Shaohuai Shi and Xinmei Tian and Tongliang Liu and Bo Han and Xiaowen Chu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tang2024fedimpro.pdf:pdf},
  note = {DBLP last modified: 2025-01-29},
  pdf = {https://openreview.net/pdf?id=giU9fYGTND},
  publisher = {OpenReview.net},
  title = {{FedImpro}: {Measuring} and {Improving} {Client} {Update} in {Federated} {Learning}},
  url = {https://openreview.net/forum?id=giU9fYGTND},
  year = {2024}
}

@inproceedings{tang2024adversarial,
  abstract = {Solving partial differential equations (PDEs) is a central task in scientific computing. Recently, neural network approximation of PDEs has received increasing attention due to its flexible meshless discretization and its potential for high-dimensional problems. One fundamental numerical difficulty is that random samples in the training set introduce statistical errors into the discretization of the loss functional which may become the dominant error in the final approximation, and therefore overshadow the modeling capability of the neural network. In this work, we propose a new minmax formulation to optimize simultaneously the approximate solution, given by a neural network model, and the random samples in the training set. The key idea is to use a deep generative model to adjust the random samples in the training set such that the residual induced by the neural network model can maintain a smooth profile in the training process.},
  author = {Kejun Tang and Jiayu Zhai and Xiaoliang Wan and Chao Yang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tang2024adversarial.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7QI7tVrh2c},
  publisher = {OpenReview.net},
  title = {Adversarial Adaptive Sampling: Unify {PINN} and Optimal Transport for the Approximation of {PDEs}},
  url = {https://openreview.net/forum?id=7QI7tVrh2c},
  year = {2024}
}

@inproceedings{taniai2024crystalformer,
  abstract = {Predicting physical properties of materials from their crystal structures is a fundamental problem in materials science. In peripheral areas such as the prediction of molecular properties, fully connected attention networks have been shown to be successful. However, unlike these finite atom arrangements, crystal structures are infinitely repeating, periodic arrangements of atoms, whose fully connected attention results in infinitely connected attention. In this work, we show that this infinitely connected attention can lead to a computationally tractable formulation, interpreted as neural potential summation, that performs infinite interatomic potential summations in a deeply learned feature space. We then propose a simple yet effective Transformer-based encoder architecture for crystal structures called Crystalformer. Compared to an existing Transformer-based model, the proposed model requires only 29.4\% of the number of parameters, with minimal modifications to the original Transformer architecture. Despite the architectural simplicity, the proposed method outperforms state-of-the-art methods for various property regression tasks on the Materials Project and JARVIS-DFT datasets.},
  author = {Tatsunori Taniai and Ryo Igarashi and Yuta Suzuki and Naoya Chiba and Kotaro Saito and Yoshitaka Ushiku and Kanta Ono},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/taniai2024crystalformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fxQiecl9HB},
  publisher = {OpenReview.net},
  title = {Crystalformer: Infinitely Connected Attention for Periodic Structure Encoding},
  url = {https://openreview.net/forum?id=fxQiecl9HB},
  year = {2024}
}

@inproceedings{tanzer2024benchmark,
  abstract = {Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -- using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human performance, achieving 44.7 chrF on Kalamang to English translation and 45.8 chrF on English to Kalamang translation, compared to 51.6 and 57.0 chrF by a human who learned Kalamang from the same reference materials.},
  author = {Garrett Tanzer and Mirac Suzgun and Eline Visser and Dan Jurafsky and Luke Melas-Kyriazi},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tanzer2024benchmark.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=tbVWug9f2h},
  publisher = {OpenReview.net},
  title = {A Benchmark for Learning to Translate a New Language from One Grammar Book},
  url = {https://openreview.net/forum?id=tbVWug9f2h},
  year = {2024}
}

@inproceedings{tao2024reverse,
  abstract = {Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards. One direction includes augmenting RL with offline data demonstrating desired tasks, but past work often require a lot of high-quality demonstration data that is difficult to obtain, especially for domains such as robotics. Our approach consists of a reverse curriculum followed by a forward curriculum. Unique to our approach compared to past work is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum generated via state resets. The result of our reverse curriculum is an initial policy that performs well on a narrow initial state distribution and helps overcome difficult exploration problems. A forward curriculum is then used to accelerate the training of the initial policy to perform well on the full initial state distribution of the task and improve demonstration and sample efficiency. We show how the combination of a reverse curriculum and forward curriculum in our method, RFCL, enables significant improvements in demonstration and sample efficiency compared against various state-of-the-art learning-from-demonstration baselines, even solving previously unsolvable tasks that require high precision and control.},
  author = {Stone Tao and Arth Shukla and Tse-kai Chan and Hao Su},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tao2024reverse.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=w4rODxXsmM},
  publisher = {OpenReview.net},
  title = {Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency},
  url = {https://openreview.net/forum?id=w4rODxXsmM},
  year = {2024}
}

@inproceedings{tao2024benchmark,
  abstract = {Deep neural networks are increasingly utilized in various machine learning tasks. However, as these models grow in complexity, they often face calibration issues, despite enhanced prediction accuracy. Many studies have endeavored to improve calibration performance through the use of specific loss functions, data preprocessing and training frameworks. Yet, investigations into calibration properties have been somewhat overlooked. This study leverages the Neural Architecture Search (NAS) search space, offering an exhaustive model architecture space for thorough calibration properties exploration. We create a model calibration dataset that evaluates 90 bin-based and 12 additional calibration measurements across 117,702 unique neural networks within the widely employed NATS-Bench search space. This comprehensive analysis provides insights into calibration properties of neural network architectures and establishes a benchmark for future calibration research in safety-critical applications such as autonomous driving and medical diagnosis.},
  author = {Linwei Tao and Younan Zhu and Haolan Guo and Minjing Dong and Chang Xu},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tao2024benchmark.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=GzNhzX9kVa},
  publisher = {OpenReview.net},
  title = {A Benchmark Study on Calibration},
  url = {https://openreview.net/forum?id=GzNhzX9kVa},
  year = {2024}
}

@inproceedings{tasse2024skill,
  abstract = {It is desirable for an agent to be able to solve a rich variety of problems that can be specified through language in the same environment. A popular approach towards obtaining such agents is to reuse skills learned in prior tasks to generalise compositionally to new ones. However, this is a challenging problem due to the curse of dimensionality induced by the combinatorially large number of ways high-level goals can be combined both logically and temporally in language. To address this problem, we propose a framework where an agent first learns a sufficient set of skill primitives to achieve all high-level goals in its environment. The agent can then flexibly compose them both logically and temporally to provably achieve temporal logic specifications in any regular language, such as regular fragments of linear temporal logic. This provides the agent with the ability to map from complex temporal logic task specifications to near-optimal behaviours zero-shot.},
  author = {Geraud Nangue Tasse and Devon Jarvis and Steven James and Benjamin Rosman},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tasse2024skill.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qiduMcw3CU},
  publisher = {OpenReview.net},
  title = {Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning},
  url = {https://openreview.net/forum?id=qiduMcw3CU},
  year = {2024}
}

@inproceedings{tay2024unified,
  abstract = {Bayesian optimization under contextual uncertainty (BOCU) is a family of BO problems in which the learner makes a decision prior to observing the context and must manage the risks involved. Distributionally robust BO (DRBO) is a subset of BOCU that affords robustness against context distribution shift, and includes the optimization of expected values and worst-case values as special cases. By considering the first derivatives of the DRBO objective, we generalize DRBO to one that includes several other uncertainty objectives studied in the BOCU literature such as worst-case sensitivity (and thus notions of risk such as variance, range, and conditional value-at-risk) and mean-risk tradeoffs. We develop a general Thompson sampling algorithm that is able to optimize any objective within the BOCU framework, analyze its theoretical properties, and compare it to suitable baselines across different experimental settings and uncertainty objectives.},
  author = {Sebastian Shenghong Tay and Chuan-Sheng Foo and Daisuke Urano and Richalynn Leong and Bryan Kian Hsiang Low},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tay2024unified.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oMNkj4ER7V},
  publisher = {OpenReview.net},
  title = {A Unified Framework for Bayesian Optimization under Contextual Uncertainty},
  url = {https://openreview.net/forum?id=oMNkj4ER7V},
  year = {2024}
}

@inproceedings{tec2024space,
  abstract = {Spatial confounding poses a significant challenge in scientific studies involving spatial data, where unobserved spatial variables can influence both treatment and outcome, possibly leading to spurious associations. To address this problem, we introduce SpaCE: The Spatial Confounding Environment, the first toolkit to provide realistic benchmark datasets and tools for systematically evaluating causal inference methods designed to alleviate spatial confounding. Each dataset includes training data, true counterfactuals, a spatial graph with coordinates, and smoothness and confounding scores characterizing the effect of a missing spatial confounder. It also includes realistic semi-synthetic outcomes and counterfactuals, generated using state-of-the-art machine learning ensembles, following best practices for causal inference benchmarks. The datasets cover real treatment and covariates from diverse domains, including climate, health and social sciences. SpaCE facilitates an automated end-to-end pipeline, simplifying data loading, experimental setup, and evaluating machine learning and causal inference models. The SpaCE project provides several dozens of datasets of diverse sizes and spatial complexity. It is publicly available as a Python package, encouraging community feedback and contributions.},
  author = {Mauricio Tec and Ana Trisovic and Michelle Audirac and Sophie Woodward and Jie Kate Hu and Naeem Khoshnevis and Francesca Dominici},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tec2024space.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=D9rJdtmIG6},
  publisher = {OpenReview.net},
  title = {{SpaCE}: The Spatial Confounding Environment},
  url = {https://openreview.net/forum?id=D9rJdtmIG6},
  year = {2024}
}

@inproceedings{teng2024relay,
  abstract = {Diffusion models achieved great success in image synthesis, but still face challenges in high-resolution generation. Through the lens of discrete cosine transformation, we find the main reason is that the same noise level on a higher resolution results in a higher Signal-to-Noise Ratio in the frequency domain. In this work, we present Relay Diffusion Model (RDM), which transfers a low-resolution image or noise into an equivalent high-resolution one for diffusion model via blurring diffusion and block noise. Therefore, the diffusion process can continue seamlessly in any new resolution or model without restarting from pure noise or low-resolution conditioning. RDM achieves state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256√ó256, surpassing previous works such as ADM, LDM and DiT by a large margin.},
  author = {Jiayan Teng and Wendi Zheng and Ming Ding and Wenyi Hong and Jianqiao Wangni and Zhuoyi Yang and Jie Tang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/teng2024relay.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=qTlcbLSm4p},
  publisher = {OpenReview.net},
  title = {Relay Diffusion: Unifying diffusion process across resolutions for image synthesis},
  url = {https://openreview.net/forum?id=qTlcbLSm4p},
  year = {2024}
}

@inproceedings{tennenholtz2024demystifying,
  abstract = {Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing large language models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decoding user preferences in recommender systems.},
  author = {Guy Tennenholtz and Yinlam Chow and Chih-Wei Hsu and Jihwan Jeong and Lior Shani and Azamat Tulepbergenov and Deepak Ramachandran and Martin Mladenov and Craig Boutilier},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/tennenholtz2024demystifying.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qoYogklIPz},
  publisher = {OpenReview.net},
  title = {Demystifying Embedding Spaces using Large Language Models},
  url = {https://openreview.net/forum?id=qoYogklIPz},
  year = {2024}
}

@inproceedings{theodoropoulos2024robust,
  abstract = {Neural networks and neural ODEs tend to be vulnerable to adversarial attacks, rendering robust optimizers critical to curb the success of such attacks. In this regard, the key insight of this work is to interpret Neural ODE optimization as a min-max optimal control problem.},
  author = {Panagiotis Theodoropoulos and Guan-Horng Liu and Tianrong Chen and Augustinos D. Saravanos and Evangelos A. Theodorou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/theodoropoulos2024robust.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=zbOSJ3CATY},
  publisher = {OpenReview.net},
  title = {A Robust Differential Neural {ODE} Optimizer},
  url = {https://openreview.net/forum?id=zbOSJ3CATY},
  year = {2024}
}

@inproceedings{theus2024towards,
  abstract = {Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm.},
  author = {Alexander Theus and Olin Geimer and Friedrich Wicke and Thomas Hofmann and Sotiris Anagnostidis and Sidak Pal Singh},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/theus2024towards.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=sMoifbuxjB},
  publisher = {OpenReview.net},
  title = {Towards Meta-Pruning via Optimal Transport},
  url = {https://openreview.net/forum?id=sMoifbuxjB},
  year = {2024}
}

@inproceedings{thilak2024lidar,
  abstract = {Joint embedding architectures have emerged as a promising avenue for acquiring transferable data representations. A key obstacle to using joint embedding methods is the inherent challenge of evaluating learned representations without access to a downstream task and an annotated dataset. We introduce LiDAR (Linear Discriminant Analysis Rank), a metric designed to measure the quality of representations within joint embedding architectures by discriminating between informative and uninformative features.},
  author = {Vimal Thilak and Chen Huang and Omid Saremi and Laurent Dinh and Hanlin Goh and Preetum Nakkiran and Joshua M. Susskind and Etai Littwin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/thilak2024lidar.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=f3g5XpL9Kb},
  publisher = {OpenReview.net},
  title = {{LiDAR}: Sensing Linear Probing Performance in Joint Embedding {SSL} Architectures},
  url = {https://openreview.net/forum?id=f3g5XpL9Kb},
  year = {2024}
}

@inproceedings{tian2024multiscale,
  abstract = {Mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. We propose a Multiscale Positive-Unlabeled (MPU) training framework to address the difficulty of short-text detection by acknowledging the human-resemblance property of short machine texts and rephrasing AI text detection as a partial Positive-Unlabeled problem.},
  author = {Yuchuan Tian and Hanting Chen and Xutao Wang and Zheyuan Bai and Qinghua Zhang and Ruifeng Li and Chao Xu and Yunhe Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024multiscale.pdf:pdf},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=5Lp6qU9hzV},
  publisher = {OpenReview.net},
  title = {Multiscale Positive-Unlabeled Detection of {AI}-Generated Texts},
  url = {https://openreview.net/forum?id=5Lp6qU9hzV},
  year = {2024}
}

@inproceedings{tian2024semantic,
  abstract = {This work pioneers Semantic Flow, a neural semantic representation of dynamic scenes from monocular videos. In contrast to previous NeRF methods that reconstruct dynamic scenes from colors and volume densities, Semantic Flow learns semantics from continuous flows that contain rich 3D motion information. The method addresses 2D-to-3D ambiguity by considering volume densities as opacity priors that describe the contributions of flow features to semantics.},
  author = {Fengrui Tian and Yueqi Duan and Angtian Wang and Jianfei Guo and Shaoyi Du},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024semantic.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=A2mRcRyGdl},
  publisher = {OpenReview.net},
  title = {Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos},
  url = {https://openreview.net/forum?id=A2mRcRyGdl},
  year = {2024}
}

@inproceedings{tian2024finetuning,
  abstract = {The factual accuracy of large language models (LLMs) is often compromised by their tendency to generate hallucinated content. This work fine-tunes language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage automatically generated factuality preference rankings and show that learning from these significantly improves the factuality of Llama-2, with 58\% and 40\% reduction in factual error rate when generating biographies and answering medical questions, respectively.},
  author = {Katherine Tian and Eric Mitchell and Huaxiu Yao and Christopher D. Manning and Chelsea Finn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024finetuning.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=WPZ2yPag4K},
  publisher = {OpenReview.net},
  title = {Fine-Tuning Language Models for Factuality},
  url = {https://openreview.net/forum?id=WPZ2yPag4K},
  year = {2024}
}

@inproceedings{tian2024freedyg,
  abstract = {Unlike previous works that primarily focus on the time domain, FreeDyG delves into the frequency domain, allowing a deeper and more nuanced extraction of interaction patterns, revealing periodic and shift behaviors. The key innovation lies in using the Fast Fourier Transform to convert time-domain data into the frequency domain, allowing the model to identify key interaction patterns and address challenges such as overfitting and optimization difficulties in continuous-time dynamic graph models.},
  author = {Yuxing Tian and Yiyan Qi and Fan Guo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024freedyg.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=82Mc5ilInM},
  publisher = {OpenReview.net},
  title = {{FreeDyG}: Frequency Enhanced Continuous-Time Dynamic Graph Model for Link Prediction},
  url = {https://openreview.net/forum?id=82Mc5ilInM},
  year = {2024}
}

@inproceedings{tian2024addp,
  abstract = {Image recognition and generation have long been developed independently. This work proposes an Alternating Denoising Diffusion Process (ADDP) that integrates pixel and VQ token spaces within a single representation learning framework. In each denoising step, the method first decodes pixels from previous VQ tokens, then generates new VQ tokens from the decoded pixels. The learned representations can generate diverse high-fidelity images and demonstrate excellent transfer performance on recognition tasks.},
  author = {Changyao Tian and Chenxin Tao and Jifeng Dai and Hao Li and Ziheng Li and Lewei Lu and Xiaogang Wang and Hongsheng Li and Gao Huang and Xizhou Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024addp.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=cMPm8YFXZe},
  publisher = {OpenReview.net},
  title = {{ADDP}: Learning General Representations for Image Recognition and Generation with Alternating Denoising Diffusion Process},
  url = {https://openreview.net/forum?id=cMPm8YFXZe},
  year = {2024}
}

@inproceedings{tian2024joma,
  abstract = {We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. JoMA integrates out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. The framework predicts that attention first becomes sparse to learn salient tokens, then dense to learn less salient tokens, and leverages JoMA to qualitatively explain how tokens are combined to form hierarchies in multilayer Transformers.},
  author = {Yuandong Tian and Yiping Wang and Zhenyu Zhang and Beidi Chen and Simon Shaolei Du},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024joma.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=LbJqRGNYCf},
  publisher = {OpenReview.net},
  title = {{JoMA}: Demystifying Multilayer Transformers via Joint Dynamics of {MLP} and Attention},
  url = {https://openreview.net/forum?id=LbJqRGNYCf},
  year = {2024}
}

@inproceedings{tian2024what,
  abstract = {When operating in service of people, robots need to optimize rewards aligned with end-user preferences. We introduce Representation-Aligned Preference-based Learning (RAPL), a method for solving the visual representation alignment problem and visual reward learning problem through preference-based learning and optimal transport. RAPL's reward consistently generates preferred robot behaviors with high sample efficiency and shows strong zero-shot generalization across experiments in X-MAGICAL and robotic manipulation.},
  author = {Thomas Tian and Chenfeng Xu and Masayoshi Tomizuka and Jitendra Malik and Andrea Bajcsy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tian2024what.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=CTlUHIKF71},
  publisher = {OpenReview.net},
  title = {What Matters to You? Towards Visual Representation Alignment for Robot Learning},
  url = {https://openreview.net/forum?id=CTlUHIKF71},
  year = {2024}
}

@inproceedings{tiapkin2024demonstrationregularized,
  abstract = {Incorporating expert demonstrations has empirically helped to improve the sample efficiency of reinforcement learning (RL). This paper quantifies theoretically to what extent this extra information reduces RL's sample complexity. The research specifically studies demonstration-regularized RL using KL-regularization, revealing that with expert demonstrations, an optimal policy can be identified with reduced sample complexity in both finite and linear Markov decision processes. The authors provide tight convergence guarantees for the behaviour cloning procedure and demonstrate that demonstration-regularized methods can be effective for reinforcement learning from human feedback (RLHF).},
  author = {Daniil Tiapkin and Denis Belomestny and Daniele Calandriello and Eric Moulines and Alexey Naumov and Pierre Perrault and Michal Valko and Pierre M{\'e}nard},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tiapkin2024demonstrationregularized.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2310.17303},
  publisher = {OpenReview.net},
  title = {Demonstration-Regularized {RL}},
  url = {https://openreview.net/forum?id=lF2aip4Scn},
  year = {2024}
}

@inproceedings{tiboni2024domain,
  abstract = {Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL), but DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. The authors propose DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high, providing a novel approach for sim-to-real transfer.},
  author = {Gabriele Tiboni and Pascal Klink and Jan Peters and Tatiana Tommasi and Carlo D'Eramo and Georgia Chalvatzaki},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tiboni2024domain.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2311.01885},
  publisher = {OpenReview.net},
  title = {Domain Randomization via Entropy Maximization},
  url = {https://openreview.net/forum?id=GXtmuiVrOM},
  year = {2024}
}

@inproceedings{tirumala2024replay,
  abstract = {Replaying data is a principal mechanism underlying the stability and data efficiency of off-policy reinforcement learning (RL). This paper presents an effective yet simple framework to extend the use of replays across multiple experiments, minimally adapting the RL workflow for sizeable improvements in controller performance and research iteration times. At its core, Replay Across Experiments (RaE) involves reusing experience from previous experiments to improve exploration and bootstrap learning while reducing required changes to a minimum in comparison to prior work.},
  author = {Dhruva Tirumala and Thomas Lampe and Jos\'{e} Enrique Chen and Tuomas Haarnoja and Sandy H. Huang and Guy Lever and Ben Moran and Tim Hertweck and Leonard Hasenclever and Martin A. Riedmiller and Nicolas Heess and Markus Wulfmeier},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tirumala2024replay.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://arxiv.org/pdf/2311.15951},
  publisher = {OpenReview.net},
  title = {Replay across Experiments: {A} Natural Extension of Off-Policy {RL}},
  url = {https://openreview.net/forum?id=Nf4Lm6fXN8},
  year = {2024}
}

@inproceedings{titsias2024kalman,
  abstract = {In Online Continual Learning (OCL) a learning system receives a stream of data and sequentially performs prediction and training steps. Key challenges in OCL include automatic adaptation to the specific non-stationary structure of the data and maintaining appropriate predictive uncertainty. This paper introduces a probabilistic Bayesian online learning approach that utilizes a (possibly pretrained) neural representation and a state space model over the linear predictor weights. Non-stationarity over the linear predictor weights is modelled using a parameter drift transition density, parametrized by a coefficient that quantifies forgetting. Inference in the model is implemented with efficient Kalman filter recursions which track the posterior distribution over the linear weights, while online SGD updates over the transition dynamics coefficient allows to adapt to the non-stationarity seen in data.},
  author = {Michalis K. Titsias and Alexandre Galashov and Amal Rannen-Triki and Razvan Pascanu and Yee Whye Teh and J\"{o}rg Bornschein},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/titsias2024kalman.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2306.08448},
  publisher = {OpenReview.net},
  title = {Kalman Filter for Online Classification of Non-Stationary Data},
  url = {https://openreview.net/forum?id=ZzmKEpze8e},
  year = {2024}
}

@inproceedings{todd2024function,
  abstract = {This paper reports the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models. Using causal mediation analysis on various in-context-learning tasks, the authors discover that a small number attention heads transport a compact representation of the demonstrated task, which they call a function vector (FV). Function vectors are robust to changes in context and trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. The study investigates the internal structure of FVs and finds that while they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. They also test semantic vector composition in FVs, and find that to some extent they can be summed to create vectors that trigger new complex tasks.},
  author = {Eric Todd and Millicent L. Li and Arnab Sen Sharma and Aaron Mueller and Byron C. Wallace and David Bau},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/todd2024function.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2310.15213},
  publisher = {OpenReview.net},
  title = {Function Vectors in Large Language Models},
  url = {https://openreview.net/forum?id=AwyxtyMwaG},
  year = {2024}
}

@inproceedings{togashi2024safe,
  abstract = {This study introduces a safe collaborative filtering method that prioritizes recommendation quality for less-satisfied users rather than focusing on the average performance. The approach minimizes the conditional value at risk (CVaR), which represents the average risk over the tails of users' loss. Excellent tail performance is crucial for modern machine learning tasks, such as algorithmic fairness, class imbalance, and risk-sensitive decision making, as it ensures the effective handling of challenging samples within a dataset. To overcome computational challenges for web-scale recommender systems, the authors developed a robust yet practical algorithm that extends the most scalable method, implicit alternating least squares (iALS). Empirical evaluation on real-world datasets demonstrates the excellent tail performance of their approach while maintaining competitive computational efficiency.},
  author = {Riku Togashi and Tatsushi Oka and Naoto Ohsaka and Tetsuro Morimura},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/togashi2024safe.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2306.05292},
  publisher = {OpenReview.net},
  title = {Safe Collaborative Filtering},
  url = {https://openreview.net/forum?id=yarUvgEXq3},
  year = {2024}
}

@inproceedings{toloubidokhti2024dats,
  abstract = {Physics-informed neural networks (PINNs) have been developed for solving partial differential equations (PDEs) without supervised PDE solutions. While recent works show potential for meta-learning PINNs across PDE configurations, PINN training has different difficulty levels depending on PDE configurations or residual sampling points, yet existing meta-learning approaches treat all PINN tasks equally. This paper introduces a novel difficulty-aware task sampler (DATS) for meta-learning of PINNs. The authors derive an optimal analytical solution to optimize the probability for sampling individual PINN tasks in order to minimize their validation loss across tasks. They present two alternative strategies to utilize this sampling probability to either adaptively weigh PINN tasks, or dynamically allocate optimal residual points across tasks. Experimental evaluation demonstrates that DATS improved the accuracy of meta-learned PINN solutions while reducing performance disparity across PDE configurations, using only a fraction of residual sampling budgets required by baselines.},
  author = {Maryam Toloubidokhti and Yubo Ye and Ryan Missel and Xiajun Jiang and Nilesh Kumar and Ruby Shrestha and Linwei Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/toloubidokhti2024dats.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EvyYFSxdgB},
  publisher = {OpenReview.net},
  title = {{DATS}: Difficulty-Aware Task Sampler for Meta-Learning Physics-Informed Neural Networks},
  url = {https://openreview.net/forum?id=EvyYFSxdgB},
  year = {2024}
}

@inproceedings{tomar2024unmixing,
  abstract = {Recent test-time adaptation methods heavily rely on nuanced adjustments of batch normalization (BN) parameters. However, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This oversight leads to skewed BN statistics and undermines the reliability of the model under non-i.i.d. scenarios. This paper presents Un-Mixing Test-Time Normalization Statistics (UnMix-TNS), which re-calibrates the statistics for each instance within a test batch by mixing it with multiple distinct statistics components, thus inherently simulating the i.i.d. scenario. The core of this method hinges on a distinctive online unmixing procedure that continuously updates these statistics components by incorporating the most similar instances from new test batches. Remarkably generic in its design, UnMix-TNS seamlessly integrates with a wide range of leading test-time adaptation methods and pre-trained architectures equipped with BN layers.},
  author = {Devavrat Tomar and Guillaume Vray and Jean-Philippe Thiran and Behzad Bozorgtabar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tomar2024unmixing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://arxiv.org/pdf/2401.08328},
  publisher = {OpenReview.net},
  title = {Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation},
  url = {https://openreview.net/forum?id=xyxU99Nutg},
  year = {2024}
}

@inproceedings{torres2024lagrangian,
  abstract = {This paper introduces Lagrangian Flow Networks (LFlows) for modeling fluid densities and velocities continuously in space and time. By construction, the proposed LFlows satisfy the continuity equation, a PDE describing mass conservation in its differentiable form. The model is based on the insight that solutions to the continuity equation can be expressed as time-dependent density transformations via differentiable and invertible maps, following from classical theory of the existence and uniqueness of Lagrangian flows for smooth vector fields. The method models fluid densities by transforming a base density with parameterized diffeomorphisms conditioned on time. The key benefit compared to methods relying on numerical ODE solvers or PINNs is that the analytic expression of the velocity is always consistent with changes in density. Furthermore, they require neither expensive numerical solvers, nor additional penalties to enforce the PDE. Experimental results show improved predictive accuracy on synthetic density modeling tasks in both 2D and 3D, with a real-world application of modeling bird migration based on sparse weather radar measurements.},
  author = {Fabricio Arend Torres and Marcello Massimo Negri and Marco Inversi and Jonathan Aellen and Volker Roth},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/torres2024lagrangian.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://arxiv.org/pdf/2305.16846},
  publisher = {OpenReview.net},
  title = {Lagrangian Flow Networks for Conservation Laws},
  url = {https://openreview.net/forum?id=Nshk5YpdWE},
  year = {2024}
}

@inproceedings{toyer2024tensor,
  abstract = {While Large Language Models (LLMs) are increasingly being used in real-world applications, they remain vulnerable to prompt injection attacks: malicious third party prompts that subvert the intent of the system designer. To help researchers study this problem, this paper presents a dataset of over 563,000 prompt injection attacks and 118,000 prompt-based defenses against prompt injection, all created by players of an online game called Tensor Trust. To the best of the authors' knowledge, this is the first dataset that includes both human-generated attacks and defenses for instruction-following LLMs. The attacks in the dataset have easily interpretable structure, and shed light on the weaknesses of LLMs. The dataset is used to create a benchmark for resistance to two types of prompt injection: prompt extraction and prompt hijacking. Benchmark results show that many models are vulnerable to the attack strategies in the Tensor Trust dataset. Furthermore, some attack strategies from the dataset generalize to deployed LLM-based applications, even though they have a very different set of constraints to the game.},
  author = {Sam Toyer and Olivia Watkins and Ethan Adrian Mendes and Justin Svegliato and Luke Bailey and Tiffany Wang and Isaac Ong and Karim Elmaaroufi and Pieter Abbeel and Trevor Darrell and Alan Ritter and Stuart Russell},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/toyer2024tensor.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://arxiv.org/pdf/2311.01011},
  publisher = {OpenReview.net},
  title = {Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game},
  url = {https://openreview.net/forum?id=fsW7wJGLBd},
  year = {2024}
}

@inproceedings{trabucco2024effective,
  abstract = {Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances in computer vision, natural language processing, and other domains. Despite its ubiquity, formal theoretical understanding of the principles of data augmentation remains limited, and practitioners rely on rules of thumb when applying augmentation policies. In this paper, we take a step toward formalizing data augmentation by studying its distributional effects. We argue that the diversity of the augmentation policy, defined as the range of transformations that a sample may undergo, is a key factor in the success of augmentation. In particular, we show that when the augmentation policy is more diverse, standard generalization bounds become tighter, and the augmentation-regularization trade-off is improved. Our method, called effective data augmentation (EDA), is a simple technique for generating diverse augmentations. Our method edits images to change their semantics using an off-the-shelf diffusion model, while preserving the original class label. We evaluate EDA on several computer vision benchmarks and show consistent improvements over baseline augmentation strategies.},
  author = {Brandon Trabucco and Kyle Doherty and Max A. Gurinas and Ruslan Salakhutdinov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/trabucco2024effective.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=ZWzUA9zeAg},
  publisher = {OpenReview.net},
  title = {Effective Data Augmentation With Diffusion Models},
  url = {https://openreview.net/forum?id=ZWzUA9zeAg},
  year = {2024}
}

@inproceedings{tran2024learning,
  abstract = {Recommender systems aim to model user preferences from their behavior data to predict user interactions with items. However, representing user interests based on their interactions with items is challenging due to the complexity and diversity of user preferences. Recent work has made significant progress in developing sophisticated sequential models for capturing user preferences, but has paid less attention to understanding what constitutes user interests. In this paper, we propose to uncover the latent interest units from behavioral data to better learn user preferences under the VAE framework. Our approach learns multi-faceted prototypical user interests by utilizing the item characteristics through a prototype-based representation learning method. We design a binding mechanism to effectively align user interests with multi-faceted item characteristics, enabling better user preference modeling. Experimental results on real-world datasets demonstrate the effectiveness of our approach in learning comprehensive user representations for recommendation tasks.},
  author = {Nhu-Thuat Tran and Hady W. Lauw},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tran2024learning.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=MzjiMxlWab},
  publisher = {OpenReview.net},
  title = {Learning Multi-Faceted Prototypical User Interests},
  url = {https://openreview.net/forum?id=MzjiMxlWab},
  year = {2024}
}

@inproceedings{trinh2024inputgradient,
  abstract = {Deep Ensembles demonstrate improved accuracy and robustness partly due to their functional diversity. Recent works have studied the connection between ensemble diversity and generalization, but there is limited understanding of how to explicitly encourage diversity during training. In this work, we propose a particle-based variational inference approach that explicitly promotes diversity among ensemble members by operating in the space of input gradients. Our method, called input-gradient space particle inference, leverages the geometry of the input-gradient space to encourage diverse neural network solutions. We show that our approach improves ensemble performance on standard benchmarks, achieving better accuracy-diversity trade-offs and improved robustness to distribution shift. Experimental results demonstrate that our method produces more diverse ensembles that generalize better than conventional approaches while maintaining computational efficiency.},
  author = {Trung Q. Trinh and Markus Heinonen and Luigi Acerbi and Samuel Kaski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/trinh2024inputgradient.pdf:pdf},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=nLWiR5P3wr},
  publisher = {OpenReview.net},
  title = {Input-gradient space particle inference for neural network ensembles},
  url = {https://openreview.net/forum?id=nLWiR5P3wr},
  year = {2024}
}

@inproceedings{tripp2024retrofallback,
  abstract = {Retrosynthesis is the task of planning a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by algorithms may not work in a laboratory. In this work, we propose Retro-fallback, a retrosynthetic planning algorithm that considers uncertainty in reaction predictions and creates robust plans that contain backup options. Our approach uses Monte Carlo tree search with a fallback mechanism that generates alternative pathways when the primary synthesis route fails. We evaluate our method on synthetic benchmarks and show that it produces more reliable synthesis plans under uncertainty while maintaining competitive performance in terms of traditional metrics.},
  author = {Austin Tripp and Krzysztof Maziarz and Sarah Lewis and Marwin H. S. Segler and Jos{\'e} Miguel Hern{\'a}ndez{-}Lobato},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tripp2024retrofallback.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=dl0u4ODCuW},
  publisher = {OpenReview.net},
  title = {Retro-fallback: retrosynthetic planning in an uncertain world},
  url = {https://openreview.net/forum?id=dl0u4ODCuW},
  year = {2024}
}

@inproceedings{trivedi2024accurate,
  abstract = {Safe deployment of graph neural networks (GNNs) under distribution shift requires models to provide accurate confidence indicators (CI). However, while it is well-known in computer vision that CI quality diminishes under distribution shift, this behavior remains understudied for GNNs. Hence, we begin with a case study on CI calibration under controlled structural and feature distribution shifts and demonstrate that increased expressivity or model size do not always lead to improved CI performance. Consequently, we instead advocate for the use of epistemic uncertainty quantification (UQ) methods to modulate CIs. To this end, we propose G-ŒîUQ, a new single model UQ method that extends the recently proposed stochastic centering framework to support structured data and partial stochasticity. Evaluated across covariate, concept, and graph size shifts, G-ŒîUQ not only outperforms several popular UQ methods in obtaining calibrated CIs, but also outperforms alternatives when CIs are used for generalization gap prediction or OOD detection. Overall, our work not only introduces a new, flexible GNN UQ method, but also provides novel insights into GNN CIs on safety-critical tasks.},
  author = {Puja Trivedi and Mark Heimann and Rushil Anirudh and Danai Koutra and Jayaraman J. Thiagarajan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/trivedi2024accurate.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=ZL6yd6N1S2},
  publisher = {OpenReview.net},
  title = {Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks},
  url = {https://openreview.net/forum?id=ZL6yd6N1S2},
  year = {2024}
}

@inproceedings{tsai2024dual,
  abstract = {Restoring facial details from low-quality (LQ) images has remained a challenging problem due to its ill-posedness induced by various degradations in the wild. The existing codebook prior mitigates the ill-posedness by leveraging an autoencoder and learned codebook of high-quality (HQ) features, achieving remarkable quality. However, existing approaches in this paradigm frequently depend on a single encoder pre-trained on HQ data for restoring HQ images, disregarding the domain gap between LQ and HQ images. As a result, the encoding of LQ inputs may be insufficient, resulting in suboptimal performance. To tackle this problem, we propose a novel dual-branch framework named DAEFR. Our method introduces an auxiliary LQ branch that extracts crucial information from the LQ inputs. Additionally, we incorporate association training to promote effective synergy between the two branches, enhancing code prediction and output quality. We evaluate the effectiveness of DAEFR on both synthetic and real-world datasets, demonstrating its superior performance in restoring facial details.},
  author = {Yu-Ju Tsai and Yu-Lun Liu and Lu Qi and Kelvin C. K. Chan and Ming-Hsuan Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tsai2024dual.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=gwDuW7Ok5f},
  publisher = {OpenReview.net},
  title = {Dual Associated Encoder for Face Restoration},
  url = {https://openreview.net/forum?id=gwDuW7Ok5f},
  year = {2024}
}

@inproceedings{tsai2024ringabell,
  abstract = {Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, the authors aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. They introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first performs concept extraction to obtain holistic representations for sensitive and inappropriate concepts. Subsequently, by leveraging the extracted concept, Ring-A-Bell automatically identifies problematic prompts for diffusion models with the corresponding generation of inappropriate content, allowing the user to assess the reliability of deployed safety mechanisms. Finally, they empirically validate their method by testing online services such as Midjourney and various methods of concept removal. Their results show that Ring-A-Bell, by manipulating safe prompting benchmarks, can transform prompts that were originally regarded as safe to evade existing safety mechanisms, thus revealing the defects of the so-called safety mechanisms which could practically lead to the generation of harmful contents.},
  author = {Yu-Lin Tsai and Chia-Yi Hsu and Chulin Xie and Chih-Hsun Lin and Jia-You Chen and Bo Li and Pin-Yu Chen and Chia-Mu Yu and Chun-Ying Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tsai2024ringabell.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=lm7MRcsFiS},
  publisher = {OpenReview.net},
  title = {Ring-A-Bell! How Reliable are Concept Removal Methods For Diffusion Models?},
  url = {https://openreview.net/forum?id=lm7MRcsFiS},
  year = {2024}
}

@inproceedings{tsao2024autovp,
  abstract = {Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach to adapting pre-trained vision models to solve various downstream image-classification tasks. However, there has hitherto been little systematic study of the design space of VP and no clear benchmark for evaluating its performance. To bridge this gap, we propose AutoVP, an end-to-end expandable framework for automating VP design choices, along with 12 downstream image-classification tasks that can serve as a holistic VP-performance benchmark. Our design space covers 1) the joint optimization of the prompts, 2) the selection of pre-trained models, including image classifiers and text-image encoders, and 3) model output mapping strategies, including nonparametric and trainable label mapping. Our extensive experimental results show that AutoVP outperforms the best-known current VP methods by a substantial margin, having up to 6.7% improvement in accuracy; and attains a maximum performance increase of 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold contribution: serving both as an efficient tool for hyperparameter tuning on VP design choices, and as a comprehensive benchmark that can reasonably be expected to accelerate VP's development.},
  author = {Hsi-Ai Tsao and Lei Hsiung and Pin-Yu Chen and Si Liu and Tsung-Yi Ho},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tsao2024autovp.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=wR9qVlPh0P},
  publisher = {OpenReview.net},
  title = {AutoVP: An Automated Visual Prompting Framework and Benchmark},
  url = {https://openreview.net/forum?id=wR9qVlPh0P},
  year = {2024}
}

@inproceedings{tsypin2024gradual,
  abstract = {Molecular conformation optimization is crucial to computer-aided drug discovery and materials design. Traditional energy minimization techniques rely on iterative optimization methods that use molecular forces calculated by a physical simulator (oracle) as anti-gradients. However, this is a computationally expensive approach that requires many interactions with a physical simulator. One way to accelerate this procedure is to replace the physical simulator with a neural network. Despite recent progress in neural networks for molecular conformation energy prediction, such models are prone to distribution shift, leading to inaccurate energy minimization. We find that the quality of energy minimization with neural networks can be improved by providing optimization trajectories as additional training data. Still, it takes around $5 \times 10^5$ additional conformations to match the physical simulator's optimization quality. In this work, we present the Gradual Optimization Learning Framework (GOLF) for energy minimization with neural networks that significantly reduces the required additional data. The framework consists of an efficient data-collecting scheme and an external optimizer. The external optimizer utilizes gradients from the energy prediction model to generate optimization trajectories, and the data-collecting scheme selects additional training data to be processed by the physical simulator. Our results demonstrate that the neural network trained with GOLF performs on par with the oracle on a benchmark of diverse drug-like molecules using $50$x less additional data.},
  author = {Artem Tsypin and Leonid Ugadiarov and Kuzma Khrabrov and Alexander Telepov and Egor Rumiantsev and Alexey Skrynnik and Aleksandr Panov and Dmitry P. Vetrov and Elena Tutubalina and Artur Kadurin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tsypin2024gradual.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=FMMF1a9ifL},
  publisher = {OpenReview.net},
  title = {Gradual Optimization Learning for Conformational Energy Minimization},
  url = {https://openreview.net/forum?id=FMMF1a9ifL},
  year = {2024}
}

@inproceedings{tu2024guaranteed,
  abstract = {Neural operators, such as Fourier Neural Operators (FNO), form a principled approach for learning solution operators for PDEs and other mappings between function spaces. However, many real-world problems require high-resolution training data, and the training time and limited GPU memory pose big barriers. One solution is to train neural operators in mixed precision to reduce the memory requirement and increase training speed. However, existing mixed-precision training techniques are designed for standard neural networks, and we find that their direct application to FNO leads to numerical overflow and poor memory efficiency. Further, at first glance, it may appear that mixed precision in FNO will lead to drastic accuracy degradation since reducing the precision of the Fourier transform yields poor results in classical numerical solvers. We show that this is not the case; in fact, we prove that reducing the precision in FNO still guarantees a good approximation bound, when done in a targeted manner. Specifically, we build on the intuition that neural operator learning inherently induces an approximation error, arising from discretizing the infinite-dimensional ground-truth input function, implying that training in full precision is not needed. We formalize this intuition by rigorously characterizing the approximation and precision errors of FNO and bounding these errors for general input functions. We prove that the precision error is asymptotically comparable to the approximation error. Based on this, we design a simple method to optimize the memory-intensive half-precision tensor contractions by greedily finding the optimal contraction order. Through extensive experiments on different state-of-the-art neural operators, datasets, and GPUs, we demonstrate that our approach reduces GPU memory usage by up to 50% and improves throughput by 58% with little or no reduction in accuracy.},
  author = {Renbo Tu and Colin White and Jean Kossaifi and Boris Bonev and Gennady Pekhimenko and Kamyar Azizzadenesheli and Anima Anandkumar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tu2024guaranteed.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=QJGj07PD9C},
  publisher = {OpenReview.net},
  title = {Guaranteed Approximation Bounds for Mixed-Precision Neural Operators},
  url = {https://openreview.net/forum?id=QJGj07PD9C},
  year = {2024}
}

@inproceedings{tumma2024leveraging,
  abstract = {Developing autonomous agents that can interact with changing environments is an open challenge in machine learning. Robustness is particularly important in these settings as agents are often fit offline on expert demonstrations but deployed online where they must generalize to the closed feedback loop within the environment. In this work, we explore the application of recurrent neural networks to tasks of this nature and understand how a parameterization of their recurrent connectivity influences robustness in closed-loop settings. Specifically, we represent the recurrent connectivity as a function of rank and sparsity and show both theoretically and empirically that modulating these two variables has desirable effects on network dynamics. The proposed low-rank, sparse connectivity induces an interpretable prior on the network that proves to be most amenable for a class of models known as closed-form continuous-time neural networks (CfCs). We find that CfCs with fewer parameters can outperform their full-rank, fully-connected counterparts in the online setting under distribution shift. This yields memory-efficient and robust agents while opening a new perspective on how we can modulate network dynamics through connectivity.},
  author = {Neehal Tumma and Mathias Lechner and Noel Loo and Ramin M. Hasani and Daniela Rus},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tumma2024leveraging.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=EriR6Ec69a},
  publisher = {OpenReview.net},
  title = {Leveraging Low-Rank and Sparse Recurrent Connectivity for Robust Closed-Loop Control},
  url = {https://openreview.net/forum?id=EriR6Ec69a},
  year = {2024}
}

@inproceedings{tuo2024anytext,
  abstract = {Diffusion model based Text-to-Image has achieved impressive achievements recently. Although current technology for synthesizing images is highly advanced and capable of generating images with high fidelity, it is still possible to give the show away when focusing on the text area in the generated image. To address this issue, we introduce AnyText, a diffusion-based multilingual visual text generation and editing model that focuses on rendering accurate and coherent text in images. AnyText comprises a diffusion pipeline with two primary elements: an auxiliary latent module and a text embedding module. The former uses inputs like text glyph, position, and masked image to generate latent features for text generation or editing. The latter employs an OCR model for encoding stroke data as embeddings, which blend with image caption embeddings from the tokenizer to generate texts that seamlessly integrate with the background. AnyText can write characters in multiple languages and to the best of our knowledge, this is the first work to address multilingual visual text generation. We make three key contributions: (1) the first work to address multilingual visual text generation; (2) the first large-scale multilingual text images dataset, AnyWord-3M, containing 3 million image-text pairs with OCR annotations; and (3) AnyText-benchmark for evaluating visual text generation accuracy and quality.},
  author = {Yuxiang Tuo and Wangmeng Xiang and Jun-Yan He and Yifeng Geng and Xuansong Xie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/tuo2024anytext.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ezBH9WE9s2},
  publisher = {OpenReview.net},
  title = {{AnyText}: Multilingual Visual Text Generation and Editing},
  url = {https://openreview.net/forum?id=ezBH9WE9s2},
  year = {2024}
}

@inproceedings{udandarao2024visual,
  abstract = {Recent advances in vision-language models (VLMs) are yielding remarkable success in recognizing visual semantic content. Yet, these models still struggle to understand visual data-types and their intrinsic challenges. We introduce Visual Data-Type Identification, a fundamental perceptual skill with implications for data curation (e.g., noisy data-removal from large datasets, domain-specific retrieval) and autonomous vision (e.g., distinguishing changing weather conditions from camera lens staining). We develop two datasets consisting of animal images altered across a diverse set of 27 visual data-types, spanning four broad categories. An extensive zero-shot evaluation of 39 VLMs, ranging from 100M to 80B parameters, shows a nuanced performance landscape. VLMs are reasonably good at identifying stylistic data-types like cartoons and sketches but struggle with simpler data-types arising from basic manipulations like image rotations or additive noise. Scaling model size yields marginal gains for contrastively-trained models, while the largest auto-regressively trained VLMs (e.g., GPT-4V) show a performance drop. We conclude that visual data-type understanding is an open problem that requires a novel approach beyond simply scaling VLMs.},
  author = {Vishaal Udandarao and Max F. Burg and Samuel Albanie and Matthias Bethge},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/udandarao2024visual.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WyEdX2R4er},
  publisher = {OpenReview.net},
  title = {Visual Data-Type Understanding does not emerge from scaling Vision-Language Models},
  url = {https://openreview.net/forum?id=WyEdX2R4er},
  year = {2024}
}

@inproceedings{ueda2024lewiss,
  abstract = {As a sub-discipline of evolutionary and computational linguistics, emergent communication (EC) studies communication protocols, called emergent languages, arising in simulations where agents communicate. A key goal of EC is to give rise to languages that share statistical properties with natural languages. In this paper, we reinterpret Lewis's signaling game, a frequently used setting in EC, as beta-VAE and reformulate its objective function as ELBO. Consequently, we clarify the existence of prior distributions of emergent languages and show that the choice of the priors can influence their statistical properties. Specifically, we address the properties of word lengths and segmentation, known as Zipf's law of abbreviation (ZLA) and Harris's articulation scheme (HAS), respectively. It has been reported that the emergent languages do not follow them when using the conventional objective. We experimentally demonstrate that by selecting an appropriate prior distribution, more natural segments emerge, while suggesting that the conventional one prevents the languages from following ZLA and HAS.},
  author = {Ryo Ueda and Tadahiro Taniguchi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ueda2024lewiss.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HC0msxE3sf},
  publisher = {OpenReview.net},
  title = {Lewis's Signaling Game as beta-{VAE} For Natural Word Lengths and Segments},
  url = {https://openreview.net/forum?id=HC0msxE3sf},
  year = {2024}
}

@inproceedings{ugare2024incremental,
  abstract = {Randomized smoothing-based certification is an effective approach for obtaining robustness certificates of deep neural networks (DNNs) against adversarial attacks. This method constructs a smoothed DNN model and certifies its robustness through statistical sampling, but it is computationally expensive, especially when certifying with a large number of samples. Furthermore, when the smoothed model is modified (e.g., quantized or pruned), certification guarantees may not hold for the modified DNN, and recertifying from scratch can be prohibitively expensive. We present the first approach for incremental robustness certification for randomized smoothing, IRS. We show how to reuse the certification guarantees for the original smoothed model to certify an approximated model with very few samples. IRS significantly reduces the computational cost of certifying modified DNNs while maintaining strong robustness guarantees. We experimentally demonstrate the effectiveness of our approach, showing up to 3x certification speedup over the certification that applies randomized smoothing of the approximate model from scratch.},
  author = {Shubham Ugare and Tarun Suresh and Debangshu Banerjee 0001 and Gagandeep Singh 0001 and Sasa Misailovic},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ugare2024incremental.pdf:pdf},
  note = {DBLP last modified: 2025-05-14},
  pdf = {https://openreview.net/pdf?id=SdeAPV1irk},
  publisher = {OpenReview.net},
  title = {Incremental Randomized Smoothing Certification},
  url = {https://openreview.net/forum?id=SdeAPV1irk},
  year = {2024}
}

@inproceedings{um2024dont,
  abstract = {We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data. However, the conventional generation process of diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves ineffective and time-consuming for the minority generating task. In this work, we present a novel framework that can make the generation process of the diffusion models focus on the minority samples. We first highlight that Tweedie's denoising formula yields favorable results for majority samples. The observation motivates us to introduce a metric that describes the uniqueness of a given sample. To address the inherent preference of the diffusion models w.r.t. the majority samples, we further develop minority guidance, a sampling technique that can guide the generation process toward regions with desired likelihood levels. Experiments on benchmark real datasets demonstrate that our minority guidance can greatly improve the capability of generating high-quality minority samples over existing generative samplers. We showcase that the performance benefit of our framework persists even in demanding real-world scenarios such as medical imaging, further underscoring the practical significance of our work.},
  author = {Soobin Um and Suhyeon Lee 0004 and Jong Chul Ye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/um2024dont.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3NmO9lY4Jn},
  publisher = {OpenReview.net},
  title = {Don't Play Favorites: Minority Guidance for Diffusion Models},
  url = {https://openreview.net/forum?id=3NmO9lY4Jn},
  year = {2024}
}

@inproceedings{ustimenko2024ito,
  abstract = {In this work, we consider rather general and broad class of Markov chains, Ito chains, that look like Euler-Maryama discretization of some Stochastic Differential Equation. The chain we study is a unified framework for theoretical analysis. It comes with almost arbitrary isotropic and state-dependent noise instead of normal and state-independent one as in most related papers. Moreover, in our chain the drift and diffusion coefficient can be inexact in order to cover wide range of applications as Stochastic Gradient Langevin Dynamics, sampling, Stochastic Gradient Descent or Stochastic Gradient Boosting. We prove the bound in $W_{2}$-distance between the laws of our Ito chain and corresponding differential equation. These results improve or cover most of the known estimates. And for some particular cases, our analysis is the first.},
  author = {Aleksei Ustimenko and Aleksandr Beznosikov},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ustimenko2024ito.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fjpfCOV4ru},
  publisher = {OpenReview.net},
  title = {Ito Diffusion Approximation of Universal Ito Chains for Sampling, Optimization and Boosting},
  url = {https://openreview.net/forum?id=fjpfCOV4ru},
  year = {2024}
}

@inproceedings{uzan2024representationlearning,
  abstract = {We propose a game-based formulation for learning dimensionality-reducing representations of feature vectors, when only a prior knowledge on future prediction tasks is available. In this game, the first player chooses a representation, and then the second player adversarially chooses a prediction task from a given class, representing the prior knowledge. The first player aims to minimize, and the second player to maximize, the regret: The minimal prediction loss using the representation, compared to the same loss using the original features. We consider the canonical setting in which the representation, the response to predict and the predictors are all linear functions, and the loss function is the mean squared error. We derive the theoretically optimal representation in pure strategies, which shows the effectiveness of the prior knowledge, and the optimal regret in mixed strategies, which shows the usefulness of randomizing the representation. We further analyze two important cases: The agnostic case, where the class of tasks consists of all possible prediction tasks, and the isotropic case, where the covariance matrix of the feature vector is the identity matrix.},
  author = {Neria Uzan and Nir Weinberger},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/uzan2024representationlearning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Uw8xvFqVAE},
  publisher = {OpenReview.net},
  title = {A representation-learning game for classes of prediction tasks},
  url = {https://openreview.net/forum?id=Uw8xvFqVAE},
  year = {2024}
}

@inproceedings{vacher2024perceptual,
  abstract = {Perception is often viewed as a process that transforms physical variables, external to an observer, into internal psychological variables. Such a process can be modeled by a function coined perceptual scale. The perceptual scale can be deduced from psychophysical measurements that consist in comparing the relative differences between stimuli (i.e. difference scaling experiments). However, this approach is often overlooked by the modeling and experimentation communities. In this work, we demonstrate the value of measuring the perceptual scale of classical (spatial frequency, orientation) and less classical physical variables (interpolation between textures) by embedding it in recent probabilistic modeling of perception. We show that the assumption that an observer has an internal representation of univariate parameters such as spatial frequency or orientation, while stimuli are high-dimensional, does not lead to contradictory predictions when following the theoretical framework. We show that it is related to the Fisher information of the generative model that underlies perception and we test the predictions given by the generative model of different stimuli in a set of difference scaling experiments. Our main conclusion is that the perceptual scale is mostly driven by the stimulus power spectrum.},
  author = {Jonathan Vacher and Pascal Mamassian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/vacher2024perceptual.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=z7K2faBrDG},
  publisher = {OpenReview.net},
  title = {Perceptual Scales Predicted by Fisher Information Metrics},
  url = {https://openreview.net/forum?id=z7K2faBrDG},
  year = {2024}
}

@inproceedings{vaduguru2024generating,
  abstract = {Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one's intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose PraX, a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample. We then use the informative dataset to train models to improve the synthesizer's ability to disambiguate user-provided examples without human supervision. We validate PraX on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.},
  author = {Saujas Vaduguru and Daniel Fried and Yewen Pu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/vaduguru2024generating.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=yxKZGQLzOP},
  publisher = {OpenReview.net},
  title = {Generating Pragmatic Examples to Train Neural Program Synthesizers},
  url = {https://openreview.net/forum?id=yxKZGQLzOP},
  year = {2024}
}

@inproceedings{vahidi2024probabilistic,
  abstract = {Self-supervised learning methods have shown promising results across a wide range of tasks in computer vision, natural language processing, and multimodal analysis. However, self-supervised approaches come with a notable limitation, dimensional collapse, where a model doesn't fully utilize its capacity to encode information optimally.},
  author = {Amirhossein Vahidi and Simon Schosser and Lisa Wimmer and Yawei Li and Bernd Bischl and Eyke H{\"u}llermeier and Mina Rezaei},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/vahidi2024probabilistic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=skcTCdJz0f},
  publisher = {OpenReview.net},
  title = {Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization},
  url = {https://openreview.net/forum?id=skcTCdJz0f},
  year = {2024}
}

@inproceedings{vaitl2024fast,
  abstract = {Recent work shows that path gradient estimators for normalizing flows have lower variance compared to standard estimators, resulting in improved training. However, they are often prohibitively more expensive from a computational point of view and cannot be applied to maximum likelihood training in a scalable manner, which severely hinders their widespread adoption.},
  author = {Lorenz Vaitl and Ludwig Winkler and Lorenz Richter and Pan Kessel},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/vaitl2024fast.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=zlkXLb3wpF},
  publisher = {OpenReview.net},
  title = {Fast and unified path gradient estimators for normalizing flows},
  url = {https://openreview.net/forum?id=zlkXLb3wpF},
  year = {2024}
}

@inproceedings{valensi2024tree,
  abstract = {This paper addresses a fundamental limitation in standard Markov decision processes (MDPs) which assume immediate execution of agent decisions. However, in realistic applications such as robotics or healthcare, actions are performed with delays that can be stochastic. The authors introduce stochastic delayed execution MDPs, a new formalism addressing random delays without resorting to state augmentation.},
  author = {David Valensi and Esther Derman and Shie Mannor and Gal Dalal},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/valensi2024tree.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RaqZX9LSGA},
  publisher = {OpenReview.net},
  title = {Tree Search-Based Policy Optimization under Stochastic Execution Delay},
  url = {https://openreview.net/forum?id=RaqZX9LSGA},
  year = {2024}
}

@inproceedings{valkov2024probabilistic,
  abstract = {Modular approaches that use a different composition of modules for each problem are a promising direction in continual learning (CL). However, searching through the large, discrete space of module compositions is challenging, especially because evaluating a composition's performance requires a round of neural network training. We address this challenge through a modular CL framework, PICLE, that uses a probabilistic model to cheaply compute the fitness of each composition, allowing PICLE to achieve both perceptual, few-shot and latent transfer.},
  author = {Lazar Valkov and Akash Srivastava and Swarat Chaudhuri and Charles Sutton},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/valkov2024probabilistic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MVe2dnWPCu},
  publisher = {OpenReview.net},
  title = {A Probabilistic Framework for Modular Continual Learning},
  url = {https://openreview.net/forum?id=MVe2dnWPCu},
  year = {2024}
}

@inproceedings{vedder2024zeroflow,
  abstract = {Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds to process full-size point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. To address both limitations, we propose Scene Flow via Distillation, a simple, scalable distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feedforward model.},
  author = {Kyle Vedder and Neehar Peri and Nathaniel Chodosh and Ishan Khatri and Eric Eaton and Dinesh Jayaraman and Yang Liu and Deva Ramanan and James Hays},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/vedder2024zeroflow.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FRCHDhbxZF},
  publisher = {OpenReview.net},
  title = {ZeroFlow: Scalable Scene Flow via Distillation},
  url = {https://openreview.net/forum?id=FRCHDhbxZF},
  year = {2024}
}

@inproceedings{velikanov2024generalization,
  abstract = {The asymptotically precise estimation of the generalization of kernel methods has recently received attention due to the parallels between neural networks and their associated kernels. However, prior works derive such estimates for training by kernel ridge regression (KRR), whereas neural networks are typically trained with gradient descent (GD). In the present work, we consider the training of kernels with a family of spectral algorithms specified by profile h({\lambda}), and including KRR and GD as special cases.},
  author = {Maksim Velikanov and Maxim Panov and Dmitry Yarotsky},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/velikanov2024generalization.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=3SJE1WLB4M},
  publisher = {OpenReview.net},
  title = {Generalization error of spectral algorithms},
  url = {https://openreview.net/forum?id=3SJE1WLB4M},
  year = {2024}
}

@inproceedings{venkataramanan2024skipattention,
  abstract = {This work aims to improve the efficiency of vision transformers (ViTs). While ViTs use computationally expensive self-attention operations in every layer, the authors identify that these operations are highly correlated across layers -- a key redundancy that causes unnecessary computations. Based on this observation, they propose SkipAt, a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers.},
  author = {Shashanka Venkataramanan and Amir Ghodrati and Yuki M. Asano and Fatih Porikli and Amirhossein Habibian},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/venkataramanan2024skipattention.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vI95kcLAoU},
  publisher = {OpenReview.net},
  title = {Skip-Attention: Improving Vision Transformers by Paying Less Attention},
  url = {https://openreview.net/forum?id=vI95kcLAoU},
  year = {2024}
}

@inproceedings{venkataramanan2024imagenet,
  abstract = {This paper addresses the question of whether self-supervised learning can be more data-efficient by exploring novel approaches to visual representation learning. The authors introduce DoRA (DiscOver and tRAck), a novel self-supervised image pretraining method that leads to attention maps that discover and track objects over time in an end-to-end manner, using transformer cross-attention. Using their novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.},
  author = {Shashanka Venkataramanan and Mamshad Nayeem Rizve and Jo{\~a}o Carreira and Yuki M. Asano and Yannis Avrithis},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/venkataramanan2024imagenet.pdf:pdf},
  note = {Outstanding Paper Award; DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Yen1lGns2o},
  publisher = {OpenReview.net},
  title = {Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video},
  url = {https://openreview.net/forum?id=Yen1lGns2o},
  year = {2024}
}

@inproceedings{venkatraman2024reasoning,
  abstract = {This work leverages latent diffusion models to learn skill representations with which we learn high value policies from offline datasets. The paper proposes a novel approach that leverages the expressiveness of latent diffusion to model in-support trajectory sequences as compressed latent skills. This facilitates learning a Q-function while avoiding extrapolation error via batch-constraining.},
  author = {Siddarth Venkatraman and Shivesh Khaitan and Ravi Tej Akella and John M. Dolan and Jeff Schneider and Glen Berseth},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/venkatraman2024reasoning.pdf:pdf},
  note = {DBLP last modified: 2025-04-12},
  pdf = {https://openreview.net/pdf?id=tGQirjzddO},
  publisher = {OpenReview.net},
  title = {Reasoning with Latent Diffusion in Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=tGQirjzddO},
  year = {2024}
}

@inproceedings{verma2024climode,
  abstract = {ClimODE introduces a novel climate and weather modeling approach, inspired by physics, using ODEs that capture underlying inductive biases and allow for uncertainty quantification in predictions. ClimODE addresses the limitations of traditional deep learning approaches as a spatiotemporal continuous-time process that implements a key principle of advection from statistical mechanics, namely, weather changes due to a spatial movement of quantities over time.},
  author = {Yogesh Verma and Markus Heinonen and Vikas Garg},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR} 2024)},
  file = {:/home/b/documents/inproceedings/verma2024climode.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=xuY33XhEGR},
  publisher = {OpenReview.net},
  title = {ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs},
  url = {https://openreview.net/forum?id=xuY33XhEGR},
  year = {2024}
}

@inproceedings{verma2024hindsight,
  abstract = {Preference based Reinforcement Learning (PbRL) removes the need to hand specify a reward function by learning one from preference feedback over policy behaviors. Current approaches to PbRL do not address the credit assignment problem inherent in determining which parts of a behavior most contributed to a preference resulting in data intensive approaches and subpar reward models. We address such limitations by introducing a credit assignment strategy (Hindsight PRIOR) that uses a world model to approximate state importance within a trajectory and then guides rewards to be proportional to state importance through an auxiliary predicted return redistribution objective. Incorporating state importance into reward learning improves the speed of policy learning, overall policy performance, and reward recovery on both locomotion and manipulation tasks. For example, Hindsight PRIOR recovers on average significantly (p<0.05) more reward on MetaWorld (20\%) and DMC (15\%). The performance gains and our ablations demonstrate the benefits even a simple credit assignment strategy can have on reward learning and that state importance in forward dynamics prediction is a strong proxy for a state's contribution to a preference decision.},
  author = {Mudit Verma and Katherine Metcalf},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/verma2024hindsight.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NLevOah0CJ},
  publisher = {OpenReview.net},
  title = {Hindsight {PRIOR}s for Reward Learning from Human Preferences},
  url = {https://openreview.net/forum?id=NLevOah0CJ},
  year = {2024}
}

@inproceedings{vermani2024leveraging,
  abstract = {Large scale inference models are widely used in neuroscience to extract latent representations from high-dimensional neural recordings. Due to the statistical heterogeneities between sessions and animals, a new model is trained from scratch to infer the underlying dynamics for each new dataset. This is computationally expensive and does not fully leverage all the available data. Moreover, as these models get more complex, they can be challenging to train. In parallel, it is becoming common to use pre-trained models in the machine learning community for few shot and transfer learning. One major hurdle that prevents the re-use of generative models in neuroscience is the complex spatio-temporal structure of neural dynamics within and across animals. Interestingly, the underlying dynamics identified from different datasets on the same task are qualitatively similar. In this work, we exploit this observation and propose a source-free and unsupervised alignment approach that utilizes the learnt dynamics and enables the re-use of trained generative models. We validate our approach on simulations and show the efficacy of the alignment on neural recordings from the motor cortex obtained during a reaching task.},
  author = {Ayesha Vermani and Il Memming Park and Josue Nassar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/vermani2024leveraging.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=9zhHVyLY4K},
  publisher = {OpenReview.net},
  title = {Leveraging Generative Models for Unsupervised Alignment of Neural Time Series Data},
  url = {https://openreview.net/forum?id=9zhHVyLY4K},
  year = {2024}
}

@inproceedings{vo2024brusleattack,
  abstract = {We study the unique, less-well understood problem of generating sparse adversarial samples simply by observing the score-based replies to model queries. Sparse attacks aim to discover a minimum number-the $l_0$ bounded-perturbations to model inputs to craft adversarial examples and misguide model decisions. But, in contrast to query-based dense attack counterparts against black-box models, constructing sparse adversarial perturbations, even when models serve confidence score information to queries in a score-based setting, is non-trivial. Because, such an attack leads to i) an NP-hard problem; and ii) a non-differentiable search space. We develop the BruSLeAttack-a new, faster (more query-efficient) Bayesian algorithm for the problem. The proposed attack scales to achieve state-of-the-art attack success rates and query efficiency on standard computer vision tasks such as ImageNet across different model architectures.},
  author = {Viet Quoc Vo and Ehsan Abbasnejad and Damith C. Ranasinghe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/vo2024brusleattack.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PAfnMGXief},
  publisher = {OpenReview.net},
  title = {{BRUSLEATTACK}: A Query-Efficient Score-Based Black-Box Sparse Adversarial Attack},
  url = {https://openreview.net/forum?id=PAfnMGXief},
  year = {2024}
}

@inproceedings{wabartha2024piecewise,
  abstract = {Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. Linear policies can justify their decisions while interacting in a dynamic environment, but their reduced expressivity prevents them from solving hard tasks. Instead, we argue for the use of piecewise-linear policies. We carefully study to what extent they can retain the interpretable properties of linear policies while reaching competitive performance with neural baselines. In particular, we propose the HyperCombinator (HC), a piecewise-linear neural architecture expressing a policy with a controllably small number of sub-policies. Each sub-policy is linear with respect to interpretable features, shedding light on the decision process of the agent without requiring an additional explanation model.},
  author = {Maxime Wabartha and Joelle Pineau},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wabartha2024piecewise.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hOMVq57Ce0},
  publisher = {OpenReview.net},
  title = {Piecewise Linear Parametrization of Policies: Towards Interpretable Deep Reinforcement Learning},
  url = {https://openreview.net/forum?id=hOMVq57Ce0},
  year = {2024}
}

@inproceedings{wan2024knowledge,
  abstract = {While training large language models (LLMs) from scratch can generate models with distinct functionalities and strengths, it comes at significant costs and may result in redundant capabilities. Alternatively, a cost-effective and compelling approach is to merge existing pre-trained LLMs into a more potent model. However, due to the varying architectures of these LLMs, directly blending their weights is impractical. In this paper, we introduce the notion of knowledge fusion for LLMs, aimed at combining the capabilities of existing LLMs and transferring them into a single LLM. By leveraging the generative distributions of source LLMs, we externalize their collective knowledge and unique strengths, thereby potentially elevating the capabilities of the target model beyond those of any individual source LLM. We validate our approach using three popular LLMs with different architectures--Llama-2, MPT, and OpenLLaMA--across various benchmarks and tasks.},
  author = {Fanqi Wan and Xinting Huang and Deng Cai and Xiaojun Quan and Wei Bi and Shuming Shi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wan2024knowledge.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jiDsk12qcz},
  publisher = {OpenReview.net},
  title = {Knowledge Fusion of Large Language Models},
  url = {https://openreview.net/forum?id=jiDsk12qcz},
  year = {2024}
}

@inproceedings{wang2024structural,
  abstract = {In contrast to prior methods, our approach leverages variational inference to encode node dynamics within latent variables, and structural reconstruction relies on the calculation of partial correlation coefficients derived from these latent variables. This unique design endows our method with scalability and extends its applicability to both one-dimensional and multi-dimensional feature spaces. Furthermore, by reorganizing latent variables according to temporal steps, our approach can effectively reconstruct directed graph structures. We validate our method through extensive experimentation on twenty datasets from a benchmark dataset and biological networks. The results demonstrate the superior scalability, accuracy, and versatility of our proposed approach compared to existing methods. Additionally, experiments conducted on noisy data affirm the robustness of our method.},
  author = {Aoran Wang and Jun Pang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024structural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TKnzPdyeJu},
  publisher = {OpenReview.net},
  title = {Structural Inference with Dynamics Encoding and Partial Correlation Coefficients},
  url = {https://openreview.net/forum?id=TKnzPdyeJu},
  year = {2024}
}

@inproceedings{wang2024sparsedff,
  abstract = {We present SparseDFF, a novel DFF for 3D scenes utilizing large 2D vision models to extract semantic features from sparse RGBD images, a domain where research is limited despite its relevance to many tasks with fixed-camera setups. SparseDFF generates view-consistent 3D DFFs, enabling efficient one-shot learning of dexterous manipulations by mapping image features to a 3D point cloud. Central to SparseDFF is a feature refinement network, optimized with a contrastive loss between views and a point-pruning mechanism for feature continuity. The method addresses the challenge of transferring manipulation abilities across objects of varying shapes, poses, and appearances, a capability rooted in their understanding of semantic correspondences between different instances. Validated in real-world scenarios with a dexterous hand, SparseDFF proves effective in manipulating both rigid and deformable objects, demonstrating significant generalization capabilities across object and scene variations.},
  author = {Qianxu Wang and Haotong Zhang and Congyue Deng and Yang You and Hao Dong and Yixin Zhu and Leonidas J. Guibas},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024sparsedff.pdf:pdf},
  note = {DBLP last modified: 2024-12-18},
  pdf = {https://openreview.net/pdf?id=HHWlwxDeRn},
  publisher = {OpenReview.net},
  title = {{SparseDFF}: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation},
  url = {https://openreview.net/forum?id=HHWlwxDeRn},
  year = {2024}
}

@inproceedings{wang2024exploring,
  abstract = {Visual prostheses are potential devices to restore vision for blind people, which highly depends on the quality of stimulation patterns of the implanted electrode array. However, existing processing frameworks prioritize the generation of stimulation while disregarding the potential impact of restoration effects and fail to assess the quality of the generated stimulation properly. To address this challenge, we introduce an end-to-end visual prosthesis framework (StimuSEE) that generates stimulation patterns with proper quality verification using V1 neuron spike patterns as supervision. The framework employs a vision system modeling that simulates the signal processing from the retina to the primary visual cortex (V1) and assesses the quality of generated stimulation patterns by comparing the elicited V1 responses with those of natural vision.},
  author = {Chuanqing Wang and Di Wu and Chaoming Fang and Jie Yang and Mohamad Sawan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024exploring.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cKAUvMePUN},
  publisher = {OpenReview.net},
  title = {Exploring Effective Stimulus Encoding via Vision System Modeling for Visual Prostheses},
  url = {https://openreview.net/forum?id=cKAUvMePUN},
  year = {2024}
}

@inproceedings{wang2024scalability,
  abstract = {Lipschitz constant estimation plays an important role in understanding generalization, robustness, and fairness in deep learning. Unlike naive bounds based on the network weight norm product, semidefinite programs (SDPs) have shown great promise in providing less conservative Lipschitz bounds with polynomial-time complexity guarantees. However, due to the memory consumption and running speed, standard SDP algorithms cannot scale to modern neural network architectures. We transform the SDPs for Lipschitz constant estimation into an eigenvalue optimization problem, which aligns with the modern large-scale optimization paradigms based on first-order methods. This is amenable to autodiff frameworks such as PyTorch and TensorFlow, requiring significantly less memory than standard SDP algorithms. The essential technique of our eigenvalue-problem transformation is to introduce redundant quadratic constraints and then utilize both Lagrangian and Shor's SDP relaxations under a certain trace constraint. Our numerical study successfully scales the SDP-based Lipschitz constant estimation to address large neural networks on ImageNet.},
  author = {Zi Wang and Bin Hu and Aaron J. Havens and Alexandre Araujo and Yang Zheng and Yudong Chen and Somesh Jha},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024scalability.pdf:pdf},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=dwzLn78jq7},
  publisher = {OpenReview.net},
  title = {On the Scalability and Memory Efficiency of Semidefinite Programs for {Lipschitz} Constant Estimation of Neural Networks},
  url = {https://openreview.net/forum?id=dwzLn78jq7},
  year = {2024}
}

@inproceedings{wang2024exgraph,
  abstract = {EX-Graph is a novel dataset that authentically links Ethereum and X, marking the first and largest dataset of its kind. While numerous public blockchain datasets are available, their utility is constrained by an exclusive focus on blockchain data. This constraint limits the incorporation of relevant social network data into blockchain analysis, thereby diminishing the breadth and depth of insight that can be derived. EX-Graph combines Ethereum transaction records (2 million nodes and 30 million edges) and X following data (1 million nodes and 3 million edges), bonding 30,667 Ethereum addresses with verified X accounts sourced from OpenSea. The dataset is specifically designed to facilitate various tasks including Ethereum link prediction, wash-trading addresses detection, and matching link prediction between Ethereum addresses and X accounts. Extensive experiments include Ethereum link prediction, wash-trading Ethereum addresses detection, and X-Ethereum matching link prediction.},
  author = {Qian Wang and Zhen Zhang and Zemin Liu and Shengliang Lu and Bingqiao Luo and Bingsheng He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024exgraph.pdf:pdf},
  note = {DBLP last modified: 2025-05-23},
  pdf = {https://openreview.net/pdf?id=juE0rWGCJW},
  publisher = {OpenReview.net},
  title = {{EX-Graph}: A Pioneering Dataset Bridging {Ethereum} and {X}},
  url = {https://openreview.net/forum?id=juE0rWGCJW},
  year = {2024}
}

@inproceedings{wang2024diagnosis,
  abstract = {The paper addresses concerns about unauthorized data usage in text-to-image diffusion models. The authors propose a method to detect such usage by planting injected memorization into protected datasets using stealthy image warping functions that are imperceptible to humans but detectable by models.},
  author = {Zhenting Wang and Chen Chen 0043 and Lingjuan Lyu and Dimitris N. Metaxas and Shiqing Ma},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024diagnosis.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=f8S3aLm0Vp},
  publisher = {OpenReview.net},
  title = {{DIAGNOSIS}: Detecting Unauthorized Data Usages in Text-to-image Diffusion Models},
  url = {https://openreview.net/forum?id=f8S3aLm0Vp},
  year = {2024}
}

@inproceedings{wang2024polynomial,
  abstract = {We prove that polynomial dimension of the embedding space are sufficient for DeepSet like architecture to represent any high-dimensional continuous permutation-invariant functions. Set representation has become ubiquitous in deep learning for modeling the inductive bias of neural networks that are insensitive to the input order.},
  author = {Peihao Wang and Shenghao Yang and Shu Li and Zhangyang Wang and Pan Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024polynomial.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=34STseLBrQ},
  publisher = {OpenReview.net},
  title = {Polynomial Width is Sufficient for Set Representation with High-dimensional Features},
  url = {https://openreview.net/forum?id=34STseLBrQ},
  year = {2024}
}

@inproceedings{wang2024allseeing,
  abstract = {We present the All-Seeing (AS) project: a large-scale dataset and model for recognizing and understanding everything in the open world. Using a scalable data engine that incorporates human feedback and efficient models in the loop, we create a new dataset (AS-1B) with over 1.2 billion regions annotated with semantic tags, question-answering pairs, and detailed captions.},
  author = {Weiyun Wang and Min Shi and Qingyun Li and Wenhai Wang and Zhenhang Huang and Linjie Xing and Zhe Chen and Hao Li and Xizhou Zhu and Zhiguo Cao and Yushi Chen and Tong Lu and Jifeng Dai and Yu Qiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024allseeing.pdf:pdf},
  note = {DBLP last modified: 2024-11-18},
  pdf = {https://openreview.net/pdf?id=c2R7ajodcI},
  publisher = {OpenReview.net},
  title = {The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World},
  url = {https://openreview.net/forum?id=c2R7ajodcI},
  year = {2024}
}

@inproceedings{wang2024optimal,
  abstract = {The paper resolves the open question regarding the sample complexity of policy learning for maximizing the long-run average reward in Markov decision processes. The key contribution is developing an estimator with optimal sample complexity, bridging a previous gap in the literature.},
  author = {Shengbo Wang and Jos H. Blanchet and Peter W. Glynn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024optimal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jOm5p3q7c7},
  publisher = {OpenReview.net},
  title = {Optimal Sample Complexity for Average Reward {M}arkov Decision Processes},
  url = {https://openreview.net/forum?id=jOm5p3q7c7},
  year = {2024}
}

@inproceedings{wang2024faithful,
  abstract = {The paper explores methods for extracting interpretable rules from ML models trained to solve a wide range of tasks over knowledge graphs. It focuses on addressing the lack of formal guarantees in rule extraction, particularly for safety-critical applications. The authors propose a novel algorithm for rule extraction using an extended Datalog approach that ensures soundness and completeness.},
  author = {Xiaxia Wang and David Jaime Tena Cucala and Bernardo Cuenca Grau and Ian Horrocks},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024faithful.pdf:pdf},
  note = {DBLP last modified: 2024-08-09},
  pdf = {https://openreview.net/pdf?id=kBTzlxM2J1},
  publisher = {OpenReview.net},
  title = {Faithful Rule Extraction for Differentiable Rule Learning Models},
  url = {https://openreview.net/forum?id=kBTzlxM2J1},
  year = {2024}
}

@inproceedings{wang2024combinatorial,
  abstract = {We investigate the combinatorial multi-armed bandit problem where an action is to select k arms from a set of base arms, and its reward is the maximum of the sample values of these k arms, under a weak feedback structure that only returns the value and index of the arm with the maximum value.},
  author = {Yiliu Wang and Wei Chen and Milan Vojnovic},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024combinatorial.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eMHn77ZKOp},
  publisher = {OpenReview.net},
  title = {Combinatorial Bandits for Maximum Value Reward Function under Value-Index Feedback},
  url = {https://openreview.net/forum?id=eMHn77ZKOp},
  year = {2024}
}

@inproceedings{wang2024tackling,
  abstract = {Asynchronous federated learning, which enables local clients to send their model update asynchronously to the server without waiting for others, has recently emerged for its improved efficiency and scalability over traditional synchronized federated learning. In this paper, we study how the asynchronous delay affects the convergence of asynchronous federated learning under non-i.i.d. distributed data across clients.},
  author = {Yujia Wang and Yuanpu Cao and Jingcheng Wu and Ruoyu Chen and Jinghui Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024tackling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4aywmeb97I},
  publisher = {OpenReview.net},
  title = {Tackling the Data Heterogeneity in Asynchronous Federated Learning with Cached Update Calibration},
  url = {https://openreview.net/forum?id=4aywmeb97I},
  year = {2024}
}

@inproceedings{wang2024efficient,
  abstract = {Sampling-based algorithms eliminate unimportant computations during forward and/or backpropagation, offering potential solutions to accelerate neural network training. We introduce Variance Controlled Adaptive Sampling (VCAS) method that reduces computational load of backpropagation while preserving original training loss trajectory and validation accuracy.},
  author = {Ziteng Wang and Jianfei Chen and Jun Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024efficient.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gEwKAZZmSw},
  publisher = {OpenReview.net},
  title = {Efficient Backpropagation with Variance Controlled Adaptive Sampling},
  url = {https://openreview.net/forum?id=gEwKAZZmSw},
  year = {2024}
}

@inproceedings{wang2024openchat,
  abstract = {Nowadays, open-source large language models like LLaMA have emerged. Recent developments have incorporated supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to align these models with human goals. However, SFT methods treat all training data with mixed quality equally, while RLFT methods require high-quality pairwise or ranking-based preference data. The paper proposes a novel framework called OpenChat to advance open-source language models using mixed-quality data.},
  author = {Guan Wang and Sijie Cheng and Xianyuan Zhan and Xiangang Li and Sen Song and Yang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024openchat.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=AOJyfhWYHf},
  publisher = {OpenReview.net},
  title = {{OpenChat}: Advancing Open-source Language Models with Mixed-Quality Data},
  url = {https://openreview.net/forum?id=AOJyfhWYHf},
  year = {2024}
}

@inproceedings{wang2024removing,
  abstract = {High-throughput drug screening using cell imaging or gene expression measurements as readouts of drug effect is a critical tool in biotechnology to assess and understand the relationship between the chemical structure and biological activity of a drug. Since large-scale screens have to be divided into multiple experiments, a key difficulty is dealing with batch effects, which can introduce systematic errors and non-biological associations in the data. We propose InfoCORE to deal with batch effects in drug screens and obtain refined molecular representations.},
  author = {Chenyu Wang and Sharut Gupta and Caroline Uhler and Tommi S. Jaakkola},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024removing.pdf:pdf},
  note = {DBLP last modified: 2025-05-22},
  pdf = {https://openreview.net/pdf?id=7TOs9gjAg1},
  publisher = {OpenReview.net},
  title = {Removing Biases from Molecular Representations via Information Maximization},
  url = {https://openreview.net/forum?id=7TOs9gjAg1},
  year = {2024}
}

@inproceedings{wang2024internvid,
  abstract = {This paper introduces {InternVid}, a large-scale video-centric multimodal dataset that enables learning powerful and transferable video-text representations for multimodal understanding and generation. {InternVid} contains over 7 million videos lasting nearly 760K hours, yielding 234M video clips accompanied by detailed descriptions of total 4.1B words. Our core contribution is to develop a scalable approach to autonomously build a high-quality video-text dataset with large language models ({LLM}), thereby showcasing its efficacy in learning video-language representation at scale. Specifically, we utilize a multi-scale approach to generate video-related descriptions. Furthermore, we introduce {ViCLIP}, a video-text representation learning model based on {ViT-L}. Learned on {InternVid} via contrastive learning, this model demonstrates leading zero-shot action recognition and competitive video retrieval performance. Beyond basic video understanding tasks like recognition and retrieval, our dataset and model have broad applications. They are particularly beneficial for generating interleaved video-text data for learning a video-centric dialogue system, advancing video-to-text and text-to-video generation research. These proposed resources provide a tool for researchers and practitioners interested in multimodal video understanding and generation.},
  author = {Yi Wang and Yinan He and Yizhuo Li and Kunchang Li and Jiashuo Yu and Xin Ma and Xinhao Li and Guo Chen and Xinyuan Chen and Yaohui Wang and Ping Luo and Ziwei Liu and Yali Wang and Limin Wang and Yu Qiao},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024internvid.pdf:pdf},
  note = {DBLP last modified: 2025-06-23},
  pdf = {https://openreview.net/pdf?id=MLBdiWu4Fw},
  publisher = {OpenReview.net},
  title = {{InternVid}: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation},
  url = {https://openreview.net/forum?id=MLBdiWu4Fw},
  year = {2024}
}

@inproceedings{wang2024accelerating,
  abstract = {Learning neural operators for solving partial differential equations ({PDEs}) has attracted great attention due to its high inference efficiency. However, training such operators requires generating a substantial amount of labeled data, i.e., {PDE} problems together with their solutions. The data generation process is exceptionally time-consuming, as it involves solving numerous systems of linear equations to obtain numerical solutions to the {PDEs}. Many existing methods solve these systems independently without considering their inherent similarities, resulting in extremely redundant computations. To tackle this problem, we propose a novel method, namely Sorting {Krylov} Recycling ({SKR}), to boost the efficiency of solving these systems, thus significantly accelerating data generation for neural operators training. To the best of our knowledge, {SKR} is the first attempt to address the time-consuming nature of data generation for learning neural operators. The working horse of {SKR} is {Krylov} subspace recycling, a powerful technique for solving a series of interrelated systems by leveraging their inherent similarities. Specifically, {SKR} employs a sorting algorithm to arrange these systems in a sequence, where adjacent systems exhibit high similarities. Then it equips a solver with {Krylov} subspace recycling to solve the systems sequentially instead of independently, thus effectively enhancing the solving efficiency. Both theoretical analysis and extensive experiments demonstrate that {SKR} can significantly accelerate neural operator data generation, achieving a remarkable speedup of up to 13.9 times.},
  author = {Hong Wang and Zhongkai Hao and Jie Wang and Zijie Geng and Zhen Wang and Bin Li and Feng Wu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024accelerating.pdf:pdf},
  note = {DBLP last modified: 2025-05-22},
  pdf = {https://openreview.net/pdf?id=UpgRVWexaD},
  publisher = {OpenReview.net},
  title = {Accelerating Data Generation for Neural Operators via {Krylov} Subspace Recycling},
  url = {https://openreview.net/forum?id=UpgRVWexaD},
  year = {2024}
}

@inproceedings{wang2024rethinking,
  abstract = {Theoretical and empirical comparisons have been made to assess the expressive power and performance of invariant and equivariant {GNNs}. However, there is currently no theoretical result comparing the expressive power of $k$-hop invariant {GNNs} and equivariant {GNNs}. Additionally, little is understood about whether the performance of equivariant {GNNs}, employing steerable features up to type-$L$, increases as $L$ grows -- especially when the feature dimension is held constant. In this study, we introduce a key lemma that allows us to analyze steerable features by examining their corresponding invariant features. The lemma facilitates us in understanding the limitations of $k$-hop invariant {GNNs}, which fail to capture the global geometric structure due to the loss of geometric information between local structures. Furthermore, we investigate the invariant features associated with different types of steerable features and demonstrate that the expressiveness of steerable features is primarily determined by their dimension -- independent of their irreducible decomposition. This suggests that when the feature dimension is constant, increasing $L$ does not lead to essentially improved performance in equivariant {GNNs} employing steerable features up to type-$L$. We substantiate our theoretical insights with numerical evidence.},
  author = {Shih-Hsin Wang and Yung-Chang Hsu and Justin M. Baker and Andrea L. Bertozzi and Jack Xin and Bao Wang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-09-11},
  pdf = {https://openreview.net/pdf?id=mGHJAyR8w0},
  publisher = {OpenReview.net},
  title = {Rethinking the Benefits of Steerable Features in {3D} Equivariant Graph Neural Networks},
  url = {https://openreview.net/forum?id=mGHJAyR8w0},
  year = {2024}
}

@inproceedings{wang2024addressing,
  abstract = {Despite the notable advancements in deep reinforcement learning ({DRL}) in recent years, a prevalent issue that is often overlooked is the impact of signal delay. Signal delay occurs when there is a lag between an agent's perception of the environment and its corresponding actions. In this paper, we first formalize delayed-observation {Markov} decision processes ({DOMDP}) by extending the standard {MDP} framework to incorporate signal delays. Next, we elucidate the challenges posed by the presence of signal delay in {DRL}, showing that trivial {DRL} algorithms and generic methods for partially observable tasks suffer greatly from delays. Lastly, we propose effective strategies to overcome these challenges. Our methods achieve remarkable performance in continuous robotic control tasks with large delays, yielding results comparable to those in non-delayed cases. Overall, our work contributes to a deeper understanding of {DRL} in the presence of signal delays and introduces novel approaches to address the associated challenges.},
  author = {William Wei Wang and Dongqi Han and Xufang Luo and Dongsheng Li},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024addressing.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Z8UfDs4J46},
  publisher = {OpenReview.net},
  title = {Addressing Signal Delay in Deep Reinforcement Learning},
  url = {https://openreview.net/forum?id=Z8UfDs4J46},
  year = {2024}
}

@inproceedings{wang2024neuroback,
  abstract = {Propositional satisfiability ({SAT}) is an {NP}-complete problem that impacts many research fields, such as planning, verification, and security. Mainstream modern {SAT} solvers are based on the Conflict-Driven Clause Learning ({CDCL}) algorithm. Recent work aimed to enhance {CDCL} {SAT} solvers using Graph Neural Networks ({GNNs}). However, so far this approach either has not made solving more effective, or required substantial {GPU} resources for frequent online model inferences. Aiming to make {GNN} improvements practical, this paper proposes an approach called {NeuroBack}, which builds on two insights: (1) predicting phases (i.e., values) of variables appearing in the majority (or even all) of the satisfying assignments are essential for {CDCL} {SAT} solving, and (2) it is sufficient to query the neural model only once for the predictions before the {SAT} solving starts. Once trained, the offline model inference allows {NeuroBack} to execute exclusively on the {CPU}, removing its reliance on {GPU} resources. To train {NeuroBack}, a new dataset called {DataBack} containing 120,286 data samples is created. Finally, {NeuroBack} is implemented as an enhancement to a state-of-the-art {SAT} solver called {Kissat}. As a result, it allowed {Kissat} to solve 5.2\% more problems on the recent {SAT} competition problem set, {SATCOMP-2022}. {NeuroBack} therefore shows how machine learning can be harnessed to improve {SAT} solving in an effective and practical manner.},
  author = {Wenxi Wang and Yang Hu and Mohit Tiwari and Sarfraz Khurshid and Kenneth L. McMillan and Risto Miikkulainen},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024neuroback.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=samyfu6G93},
  publisher = {OpenReview.net},
  title = {{NeuroBack}: Improving {CDCL} {SAT} Solving using Graph Neural Networks},
  url = {https://openreview.net/forum?id=samyfu6G93},
  year = {2024}
}

@inproceedings{wang2024neural,
  abstract = {Discovering the underlying relationships among variables from temporal observations has been a longstanding challenge in numerous scientific disciplines, including biology, finance, and climate science. The dynamics of such systems are often best described using continuous-time stochastic processes. Unfortunately, most existing structure learning approaches assume that the underlying process evolves in discrete-time and/or observations occur at regular time intervals. These mismatched assumptions can often lead to incorrect learned structures and models. In this work, we introduce a novel structure learning method, {SCOTCH}, which combines neural stochastic differential equations ({SDE}) with variational inference to infer a posterior distribution over possible structures. {SCOTCH} can handle learning from and predicting observations at arbitrary time points, addressing the limitations of previous discrete-time approaches.},
  author = {Benjie Wang and Joel Jennings and Wenbo Gong},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-11-30},
  pdf = {https://openreview.net/pdf?id=V1GM9xDvIY},
  publisher = {OpenReview.net},
  title = {Neural structure learning with stochastic differential equations},
  url = {https://openreview.net/forum?id=V1GM9xDvIY},
  year = {2024}
}

@inproceedings{wang2024implicit,
  abstract = {The $L_2$-regularized loss of Deep Linear Networks ({DLNs}) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can be avoided since they do not fit the data, {GD} might get stuck at rank-overestimating minima. We show that with {SGD}, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_1 \subset B_2 \subset \cdots \subset B_R$ so that $B_r$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: {SGD} has prob. 0 of leaving $B_r$, and from any starting point there is a non-zero prob. for {SGD} to go in $B_r$.},
  author = {Zihan Wang and Arthur Jacot},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024implicit.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=P1aobHnjjj},
  publisher = {OpenReview.net},
  title = {Implicit bias of {SGD} in $L_2$-regularized linear {DNNs}: One-way jumps from high to low rank},
  url = {https://openreview.net/forum?id=P1aobHnjjj},
  year = {2024}
}

@inproceedings{wang2024beyond,
  abstract = {The increasing capabilities of large language models ({LLMs}) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of {AI} systems, necessitating effective {AI} alignment. Reinforcement Learning from Human Feedback ({RLHF}) has emerged as a promising pathway towards {AI} alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization ({DPO}) has been proposed as an alternative; and it remains equivalent to {RLHF} under the reverse {KL} regularization constraint. This paper presents $f$-{DPO}, a generalized approach to {DPO} by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including {Jensen-Shannon} divergence, forward {KL} divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the {Karush-Kuhn-Tucker} conditions.},
  author = {Chaoqi Wang and Yibo Jiang and Chenghao Yang and Han Liu and Yuxin Chen},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024beyond.pdf:pdf},
  note = {DBLP last modified: 2024-07-30},
  pdf = {https://openreview.net/pdf?id=2cRzmWXK9N},
  publisher = {OpenReview.net},
  title = {Beyond Reverse {KL}: Generalizing Direct Preference Optimization with Diverse Divergence Constraints},
  url = {https://openreview.net/forum?id=2cRzmWXK9N},
  year = {2024}
}

@inproceedings{wang2024graphs,
  abstract = {This paper studies the implications of using graphs instead of hypergraphs to represent real-world interconnected systems whose relationships are naturally higher-order. Such modeling involves a projection process mapping hypergraphs to graphs, which is common in graph-based analysis. While hypergraph projection can lead to loss of higher-order relations, there are limited studies on the consequences and remediation. The work addresses this gap through two contributions: (1) developing analysis based on graph and set theory to show two patterns of hyperedges that cause structural information loss in projections, and quantifying the combinatorial impossibility of recovery without extra help; (2) developing a learning-based hypergraph reconstruction method using hyperedge distribution statistics, with reasonable assumptions allowing domain knowledge assistance. The reconstruction method is evaluated on 8 real-world datasets and demonstrates consistently top performance, with benefits shown through protein rankings and link prediction use cases.},
  author = {Yanbang Wang and Jon M. Kleinberg},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024graphs.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qwYKE3VB2h},
  publisher = {OpenReview.net},
  title = {From Graphs to Hypergraphs: Hypergraph Projection and its Reconstruction},
  url = {https://openreview.net/forum?id=qwYKE3VB2h},
  year = {2024}
}

@inproceedings{wang2024memorization,
  abstract = {Self-supervised learning ({SSL}) has recently received significant attention due to its ability to train high-performance encoders purely on unlabeled data---often scraped from the internet. This data can still be sensitive and empirical evidence suggests that {SSL} encoders memorize private information of their training data and can disclose them at inference time. Since existing theoretical definitions of memorization from supervised learning rely on labels, they do not transfer to {SSL}. To address this gap, we propose {SSLMem}, a framework for defining memorization within {SSL}. Our definition compares the difference in alignment of representations for data points and their augmented views returned by both encoders that were trained on these data points and encoders that were not. We show that this memorization is essential for encoders to achieve higher generalization performance on different downstream tasks.},
  author = {Wenhao Wang and Muhammad Ahmad Kaleem and Adam Dziedzic and Michael Backes and Nicolas Papernot and Franziska Boenisch},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wang2024memorization.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=KSjPaXtxP8},
  publisher = {OpenReview.net},
  title = {Memorization in Self-Supervised Learning Improves Downstream Generalization},
  url = {https://openreview.net/forum?id=KSjPaXtxP8},
  year = {2024}
}

@inproceedings{wang2024towards,
  abstract = {Gradient inversion attacks aim to reconstruct local training data from intermediate gradients exposed in the federated learning framework. Despite successful attacks, all previous methods, starting from reconstructing a single data point and then relaxing the single-image limit to batch level, are only tested under hard label constraints. Even for single-image reconstruction, we still lack an analysis-based algorithm to recover augmented soft labels. In this work, we change the focus from enlarging batchsize to investigating the hard label constraints, considering a more realistic circumstance where label smoothing and mixup techniques are used in the training process. In particular, we are the first to initiate a novel algorithm to simultaneously recover the ground-truth augmented label and the input feature of the last fully-connected layer from single-input gradients, and provide a necessary condition for any analytical-based label recovery methods. Extensive experiments testify to the label recovery accuracy, as well as the benefits to the following image reconstruction. We believe soft labels in classification tasks are worth further attention in gradient inversion attacks.},
  author = {Yanbo Wang and Jian Liang and Ran He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024towards.pdf:pdf},
  note = {DBLP last modified: 2025-01-14},
  pdf = {https://openreview.net/pdf?id=s8cMuxI5gu},
  publisher = {OpenReview.net},
  title = {Towards Eliminating Hard Label Constraints in Gradient Inversion Attacks},
  url = {https://openreview.net/forum?id=s8cMuxI5gu},
  year = {2024}
}

@inproceedings{wang2024unified,
  abstract = {Continual Learning (CL) focuses on learning from dynamic and changing data distributions while retaining previously acquired knowledge. Various methods have been developed to address the challenge of catastrophic forgetting, including regularization-based, Bayesian-based, and memory-replay-based techniques. However, these methods lack a unified framework and common terminology for describing their approaches. This research aims to bridge this gap by introducing a comprehensive and overarching framework that encompasses and reconciles these existing methodologies. Notably, this new framework is capable of encompassing established CL approaches as special instances within a unified and general optimization objective. Building upon our unified framework, we derive a new refresh learning mechanism with an unlearn-relearn scheme to more effectively combat the forgetting issue. The proposed method is a simple plug-in and can be seamlessly integrated with existing CL methods. We provide in-depth theoretical analysis to prove the generalization ability of the proposed refresh learning mechanism. Extensive experiments on several representative datasets demonstrate the effectiveness and efficiency of refresh learning.},
  archiveprefix = {arXiv},
  author = {Zhenyi Wang and Yan Li and Li Shen and Heng Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2403.13249},
  file = {:/home/b/documents/inproceedings/wang2024unified.pdf:pdf},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=BE5aK0ETbp},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {A Unified and General Framework for Continual Learning},
  url = {https://openreview.net/forum?id=BE5aK0ETbp},
  year = {2024}
}

@inproceedings{wang2024barleria,
  abstract = {Pre-training followed by full fine-tuning has gradually been substituted by Parameter-Efficient Tuning (PET) in the field of computer vision. PET has gained popularity, especially in the context of large-scale models, due to its ability to reduce transfer learning costs and conserve hardware resources. However, existing PET approaches primarily focus on recognition tasks and typically support uni-modal optimization, while neglecting dense prediction tasks and vision language interactions. To address this limitation, we propose a novel PET framework called Bi-directional Intertwined Vision Language Efficient Tuning for Referring Image Segmentation (BarLeRIa), which leverages bi-directional intertwined vision language adapters to fully exploit the frozen pre-trained models' potential. In BarLeRIa, two different tuning modules are employed for efficient attention, one for global, and the other for local, along with an intertwined vision language tuning module for efficient modal fusion. Extensive experiments conducted on RIS benchmarks demonstrate the superiority of BarLeRIa over prior PET methods with a significant margin, i.e., achieving an average improvement of 5.6\%. Remarkably, without requiring additional training datasets, BarLeRIa even surpasses SOTA full fine-tuning approaches.},
  author = {Yaoming Wang and Jin Li and Xiaopeng Zhang and Bowen Shi and Chenglin Li and Wenrui Dai and Hongkai Xiong and Qi Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024barleria.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=wHLDHRkmEu},
  publisher = {OpenReview.net},
  title = {{BarLeRIa}: An Efficient Tuning Framework for Referring Image Segmentation},
  url = {https://openreview.net/forum?id=wHLDHRkmEu},
  year = {2024}
}

@inproceedings{wang2024beno,
  abstract = {Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically cannot handle complex geometries and inhomogeneous boundary values present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green's function, BENO consists of two branches of Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model extensively in elliptic PDEs with various boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96\%.},
  archiveprefix = {arXiv},
  author = {Haixin Wang and Jiaxin Li and Anubhav Dwivedi and Kentaro Hara and Tailin Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2401.09323},
  file = {:/home/b/documents/inproceedings/wang2024beno.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZZTkLDRmkg},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {{BENO}: Boundary-embedded Neural Operators for Elliptic {PDEs}},
  url = {https://openreview.net/forum?id=ZZTkLDRmkg},
  year = {2024}
}

@inproceedings{wang2024inverse,
  abstract = {We prove an inverse approximation theorem for the approximation of nonlinear sequence-to-sequence relationships using recurrent neural networks (RNNs). This is a so-called Bernstein-type result in approximation theory, which deduces properties of a target function under the assumption that it can be effectively approximated by a hypothesis space. In particular, we show that nonlinear sequence relationships that can be stably approximated by nonlinear RNNs must have an exponential decaying memory structure -- a notion that can be made precise. This extends the previously identified curse of memory in linear RNNs into the general nonlinear setting, and quantifies the essential limitations of the RNN architecture for learning sequential relationships with long-term memory.},
  archiveprefix = {arXiv},
  author = {Shida Wang and Zhong Li and Qianxiao Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2305.19190},
  file = {:/home/b/documents/inproceedings/wang2024inverse.pdf:pdf},
  note = {DBLP last modified: 2025-05-14},
  pdf = {https://openreview.net/pdf?id=yC2waD70Vj},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {Inverse Approximation Theory for Nonlinear Recurrent Neural Networks},
  url = {https://openreview.net/forum?id=yC2waD70Vj},
  year = {2024}
}

@inproceedings{wang2024hardtobeat,
  abstract = {Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches.},
  archiveprefix = {arXiv},
  author = {Zhengbo Wang and Jian Liang and Lijun Sheng and Ran He and Zilei Wang and Tieniu Tan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2402.04087},
  file = {:/home/b/documents/inproceedings/wang2024hardtobeat.pdf:pdf},
  note = {DBLP last modified: 2024-10-15},
  pdf = {https://openreview.net/pdf?id=Js5PJPHDyY},
  primaryclass = {cs.CV},
  publisher = {OpenReview.net},
  title = {A Hard-to-Beat Baseline for Training-free {CLIP}-based Adaptation},
  url = {https://openreview.net/forum?id=Js5PJPHDyY},
  year = {2024}
}

@inproceedings{wang2024promptagent,
  abstract = {Expert-level prompts, carefully engineered by human experts who have a deep understanding of both large language models (LLMs) and domain knowledge, are the future of prompting and pivotal to harnessing the full power of advanced LLMs. Discovering such prompts with an automated process remains a sought-after and unresolved challenge. Existing prompt optimization techniques, though automated through iterative sampling, often fall short in injecting domain knowledge and exploring the vast prompt space for complex expert-level prompts efficiently. To address this pressing need and achieve expert-level prompting, we introduce PromptAgent, which autonomously discovers prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm (rooted in Monte Carlo Tree Search) to strategically explore the vast expert-level prompt space. PromptAgent interacts with the LLM in a human-like trial-and-error manner during the planning, and injects expert-level knowledge by reflecting on model errors and generating insightful error feedback. This novel formulation allows it to iteratively evaluate intermediate prompts, refine them based on errors, simulate future rewards, and search for high-reward paths leading to expert-level prompts.},
  archiveprefix = {arXiv},
  author = {Xinyuan Wang and Chenxi Li and Zhen Wang and Fan Bai and Haotian Luo and Jiayou Zhang and Nebojsa Jojic and Eric P. Xing and Zhiting Hu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.16427},
  file = {:/home/b/documents/inproceedings/wang2024promptagent.pdf:pdf},
  note = {DBLP last modified: 2025-04-24},
  pdf = {https://openreview.net/pdf?id=22pyNMuIoa},
  primaryclass = {cs.CL},
  publisher = {OpenReview.net},
  title = {{PromptAgent}: Strategic Planning with Language Models Enables Expert-level Prompt Optimization},
  url = {https://openreview.net/forum?id=22pyNMuIoa},
  year = {2024}
}

@inproceedings{wang2024gensim,
  abstract = {Collecting large amounts of real-world interaction data to train general robotic policies is often prohibitively expensive, thus motivating the use of simulation data. However, existing methods for data generation have generally focused on scene-level diversity (e.g., object instances and poses) rather than task-level diversity, due to the human effort required to come up with and verify novel tasks. This has made it challenging for policies trained on simulation data to demonstrate significant task-level generalization. In this paper, we propose to automatically generate rich simulation environments and expert demonstrations by exploiting a large language model's (LLM) grounding and coding ability. Our approach, dubbed GenSim, has two modes: goal-directed generation, wherein a target task is given to the LLM and the LLM proposes a task curriculum to solve the target task, and exploratory generation, wherein the LLM bootstraps from previous tasks and iteratively proposes novel tasks that would be helpful in solving more complex tasks. We show that LLM-generated simulation programs can enhance task-level generalization significantly when used for multitask policy training, and with minimal sim-to-real adaptation, the multitask policies pretrained on GPT4-generated simulation tasks exhibit stronger transfer to unseen long-horizon tasks in the real world and outperform baselines by 25\%.},
  archiveprefix = {arXiv},
  author = {Lirui Wang and Yiyang Ling and Zhecheng Yuan and Mohit Shridhar and Chen Bao and Yuzhe Qin and Bailin Wang and Huazhe Xu and Xiaolong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.01361},
  file = {:/home/b/documents/inproceedings/wang2024gensim.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=OI3RoHoWAN},
  primaryclass = {cs.RO},
  publisher = {OpenReview.net},
  title = {{GenSim}: Generating Robotic Simulation Tasks via Large Language Models},
  url = {https://openreview.net/forum?id=OI3RoHoWAN},
  year = {2024}
}

@inproceedings{wang2024marginal,
  abstract = {Momentum is known to accelerate gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization like neural network training, folklore suggests momentum may help by reducing variance of stochastic gradient updates, but previous theoretical analyses don't find provable acceleration. In this work, we revisit the problem and focus on the small learning rate regime. We first prove that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where optimal learning rate isn't very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks. Our theoretical results clarify momentum's role in stochastic settings where learning rate is small and gradient noise dominates instability, suggesting SGD with and without momentum behave similarly in short and long time horizons. Experiments show momentum indeed has limited benefits for both optimization and generalization in practical training regimes where optimal learning rate isn't very large.},
  archiveprefix = {arXiv},
  author = {Runzhe Wang and Sadhika Malladi and Tianhao Wang and Kaifeng Lyu and Zhiyuan Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2307.15196},
  file = {:/home/b/documents/inproceedings/wang2024marginal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3JjJezzVkT},
  primaryclass = {cs.LG},
  publisher = {OpenReview.net},
  title = {The Marginal Value of Momentum for Small Learning Rate {SGD}},
  url = {https://openreview.net/forum?id=3JjJezzVkT},
  year = {2024}
}

@inproceedings{wang2024wholesong,
  abstract = {Recent deep music generation studies have put much emphasis on long-term generation with structures. However, we are yet to see high-quality, well-structured whole-song generation. In this paper, we make the first attempt to model a full music piece under the realization of compositional hierarchy. With a focus on symbolic representations of pop songs, we define a hierarchical language, in which each level of hierarchy focuses on the semantics and context dependency at a certain music scope. The high-level languages reveal whole-song form, phrase, and cadence, whereas the low-level languages focus on notes, chords, and their local patterns. A cascaded diffusion model is trained to model the hierarchical language, where each level is conditioned on its upper levels. We introduce 4-levels of hierarchical music languages, where each level of the hierarchy focuses on the context dependency at certain music scope, from the high-level whole-song form, phrase, and cadence to low-level notes, chords, and their local patterns. Our approach makes the first attempt to model full pop songs (melody and piano accompaniment) under the realization of compositional hierarchy.},
  archiveprefix = {arXiv},
  author = {Ziyu Wang and Lejun Min and Gus Xia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2405.09901},
  file = {:/home/b/documents/inproceedings/wang2024wholesong.pdf:pdf},
  note = {DBLP last modified: 2025-02-17},
  pdf = {https://openreview.net/pdf?id=sn7CYWyavh},
  primaryclass = {cs.SD},
  publisher = {OpenReview.net},
  title = {Whole-Song Hierarchical Generation of Symbolic Music Using Cascaded Diffusion Models},
  url = {https://openreview.net/forum?id=sn7CYWyavh},
  year = {2024}
}

@inproceedings{wang2024learning,
  abstract = {We study the problem of learning hierarchical polynomials over the standard Gaussian distribution with three-layer neural networks. We specifically consider target functions of the form $h = g \circ p$ where $p : \mathbb{R}^d \rightarrow \mathbb{R}$ is a degree $k$ polynomial and $g: \mathbb{R} \rightarrow \mathbb{R}$ is a degree $q$ polynomial. This function class generalizes the single-index model, which corresponds to $k=1$, and is a natural class of functions possessing an underlying hierarchical structure. Our main result shows that for a large subclass of degree $k$ polynomials $p$, a three-layer neural network trained via layerwise gradient descent on the square loss learns the target $h$ up to vanishing test error in $\widetilde{O}(d^k)$ samples and polynomial time. This is a strict improvement over kernel methods, which require $\widetilde{\Theta}(d^{kq})$ samples, as well as existing guarantees for two-layer networks, which require the target function to be low-rank. Our result also generalizes prior works on three-layer neural networks, which were restricted to the case of $p$ being a quadratic. When $p$ is indeed a quadratic, we achieve the information-theoretically optimal sample complexity $\widetilde{O}(d^2)$, which is an improvement over prior work requiring a sample size of $\widetilde{\Theta}(d^4)$. Our proof proceeds by showing that during the initial stage of training the network performs feature learning to recover the feature $p$ with $\widetilde{\mathcal{O}}(d^k)$ samples. This work demonstrates the ability of three-layer neural networks to learn complex features and as a result, learn a broad class of hierarchical functions.},
  author = {Zihao Wang and Eshaan Nichani and Jason D. Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024learning.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QgwAYFrh9t},
  publisher = {OpenReview.net},
  title = {Learning Hierarchical Polynomials with Three-Layer Neural Networks},
  url = {https://openreview.net/forum?id=QgwAYFrh9t},
  year = {2024}
}

@inproceedings{wang2024zero,
  abstract = {Zero Redundancy Optimizer ({ZeRO}) has been used to train a wide range of large language models on massive {GPU}s clusters due to its ease of use, efficiency, and good scalability. However, when training on low-bandwidth clusters, or at scale which forces batch size per {GPU} to be small, {ZeRO}'s effective throughput is limited because of high communication volume from gathering weights in forward pass, backward pass, and averaging gradients. This paper introduces three communication volume reduction techniques, which we collectively refer to as {ZeRO}++, targeting each of the communication collectives in {ZeRO}. First is block-quantization based all-gather. Second is data remapping that trades-off communication for more memory. Third is a novel all-to-all based quantized gradient averaging paradigm as replacement of reduce-scatter collective, which preserves accuracy despite communicating low precision data.},
  author = {Guanhua Wang and Heyang Qin and Sam Ade Jacobs and Xiaoxia Wu and Connor Holmes and Zhewei Yao and Samyam Rajbhandari and Olatunji Ruwase and Feng Yan and Lei Yang and Yuxiong He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024zero.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gx2BT0a9MQ},
  publisher = {OpenReview.net},
  title = {{ZeRO}++: Extremely Efficient Collective Communication for Large Model Training},
  url = {https://openreview.net/forum?id=gx2BT0a9MQ},
  year = {2024}
}

@inproceedings{wang2024mathcoder,
  abstract = {The recently released {GPT}-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as {MathCodeInstruct}. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the {MathCoder} models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the {MathCoder} models achieve state-of-the-art scores among open-source {LLM}s on the {MATH} (45.2\%) and {GSM}8{K} (83.9\%) datasets, substantially outperforming other open-source alternatives. Notably, the {MathCoder} model not only surpasses {ChatGPT}-3.5 and {PaLM}-2 on {GSM}8{K} and {MATH} but also outperforms {GPT}-4 on the competition-level {MATH} dataset.},
  author = {Ke Wang and Houxing Ren and Aojun Zhou and Zimu Lu and Sichun Luo and Weikang Shi and Renrui Zhang and Linqi Song and Mingjie Zhan and Hongsheng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024mathcoder.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-04-08},
  pdf = {https://openreview.net/pdf?id=z8TW0ttBPp},
  publisher = {OpenReview.net},
  title = {{MathCoder}: Seamless Code Integration in {LLM}s for Enhanced Mathematical Reasoning},
  url = {https://openreview.net/forum?id=z8TW0ttBPp},
  year = {2024}
}

@inproceedings{wang2024gnnboundary,
  abstract = {While Graph Neural Networks ({GNN}s) have achieved remarkable performance on various machine learning tasks on graph data, they also raised questions regarding their transparency and interpretability. Recently, there have been extensive research efforts in explaining the decision-making process of {GNN}s. These efforts often focus on explaining why a certain prediction is being made for a particular instance, or what discriminative features the {GNN}s try to detect for each class. However, to the best of our knowledge, there is no existing study on understanding the decision boundaries of {GNN}s. The paper introduces {GNNBoundary} as a novel approach to understanding {GNN} behavior. For an adjacent class pair, the near-boundary graphs between them are effectively generated by optimizing a novel objective function specifically designed for the purpose of boundary graph generation. Thus, by analyzing the near-boundary graphs, the important characteristics of decision boundaries can be uncovered. To evaluate the efficacy of {GNNBoundary}, we conduct experiments on both synthetic datasets and public real-world datasets. The results have demonstrated that, through the analysis of faithful near-boundary graphs generated by {GNNBoundary}, we can thoroughly assess the robustness and generalizability of the explained {GNN}s.},
  author = {Xiaoqi Wang and Han-Wei Shen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024gnnboundary.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=WIzzXCVYiH},
  publisher = {OpenReview.net},
  title = {{GNNBoundary}: Towards Explaining Graph Neural Networks through the Lens of Decision Boundaries},
  url = {https://openreview.net/forum?id=WIzzXCVYiH},
  year = {2024}
}

@inproceedings{wang2024customizable,
  abstract = {Modular and composable transfer learning is an emerging direction in the field of Parameter Efficient Fine-Tuning, as it enables neural networks to better organize various aspects of knowledge, leading to improved cross-task generalization. In this paper, we introduce a novel approach Customized Polytropon (C-Poly) that combines task-common skills and task-specific skills, while the skill parameters being highly parameterized using low-rank techniques. Each task is associated with a customizable number of exclusive specialized skills and also benefits from skills shared with peer tasks. A skill assignment matrix is jointly learned. To evaluate our approach, we conducted extensive experiments on the Super-NaturalInstructions and the {SuperGLUE} benchmarks. Our findings demonstrate that C-Poly outperforms fully-shared, task-specific, and skill-indistinguishable baselines, significantly enhancing the sample efficiency in multi-task learning scenarios.},
  author = {Haowen Wang and Tao Sun and Congyun Jin and Yingbo Wang and Yibo Fan and Yunqi Xu and Yuliang Du and Cong Fan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024customizable.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=G1Hlubz1fR},
  publisher = {OpenReview.net},
  title = {Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning},
  url = {https://openreview.net/forum?id=G1Hlubz1fR},
  year = {2024}
}

@inproceedings{wang2024twostage,
  abstract = {Pretrained large language models ({LLM}s) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases {LLM}s' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with {MO}del Tuning ({ProMoT}), a simple yet effective two-stage fine-tuning framework that reduces format specialization and improves generalization. {ProMoT} offloads task-specific format learning into additional and removable parameters by first doing prompt tuning and then fine-tuning the model itself with this soft prompt attached. With experiments on several fine-tuning tasks and 8 in-context evaluation tasks, we show that {ProMoT} achieves comparable performance on fine-tuned tasks to standard fine-tuning, but with much less loss of in-context learning performances across a board range of out-of-domain evaluation tasks.},
  author = {Yihan Wang and Si Si and Daliang Li and Michal Lukasik and Felix X. Yu and Cho-Jui Hsieh and Inderjit S. Dhillon and Sanjiv Kumar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024twostage.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-04-13},
  pdf = {https://openreview.net/pdf?id=pCEgna6Qco},
  publisher = {OpenReview.net},
  title = {Two-stage {LLM} Fine-tuning with Less Specialization and More Generalization},
  url = {https://openreview.net/forum?id=pCEgna6Qco},
  year = {2024}
}

@inproceedings{wang2024lemon,
  abstract = {Scaling of deep neural networks, especially Transformers, is pivotal for their surging performance and has further led to the emergence of sophisticated reasoning capabilities in foundation models. Such scaling generally requires training large models from scratch with random initialization, failing to leverage the knowledge acquired by their smaller counterparts, which are already resource-intensive to obtain. To tackle this inefficiency, we present Lossless Model Expansion ({LEMON}), a recipe to initialize scaled models using the weights of their smaller but pre-trained counterparts. This is followed by model training with an optimized learning rate scheduler tailored explicitly for the scaled models, substantially reducing the training time compared to training from scratch. Notably, {LEMON} is versatile, ensuring compatibility with various network structures, including models like Vision Transformers and {BERT}. Our empirical results demonstrate that {LEMON} reduces computational costs by 56.7\% for Vision Transformers and 33.2\% for {BERT} when compared to training from scratch.},
  author = {Yite Wang and Jiahao Su and Hanlin Lu and Cong Xie and Tianyi Liu and Jianbo Yuan and Haibin Lin and Ruoyu Sun and Hongxia Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024lemon.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3Vw7DQqq7U},
  publisher = {OpenReview.net},
  title = {{LEMON}: Lossless Model Expansion},
  url = {https://openreview.net/forum?id=3Vw7DQqq7U},
  year = {2024}
}

@inproceedings{wang2024sptnet,
  abstract = {Generalized Category Discovery ({GCD}) aims to classify unlabelled images from both 'seen' and 'unseen' classes by transferring knowledge from a set of labelled 'seen' class images. A key theme in existing {GCD} approaches is adapting large-scale pre-trained models for the {GCD} task. An alternate perspective, however, is to adapt the data representation itself for better alignment with the pre-trained model. As such, in this paper, we introduce a two-stage adaptation approach termed {SPTNet}, which iteratively optimizes model parameters (i.e., model-finetuning) and data parameters (i.e., prompt learning). Furthermore, we propose a novel spatial prompt tuning method ({SPT}) which considers the spatial property of image data, enabling the method to better focus on object parts, which can transfer between seen and unseen classes. We thoroughly evaluate our {SPTNet} on standard benchmarks and demonstrate that our method outperforms existing {GCD} methods. Notably, we find our method achieves an average accuracy of 61.4\% on the {SSB}, surpassing prior state-of-the-art methods by approximately 10\%. The improvement is particularly remarkable as our method yields extra parameters amounting to only 0.117\% of those in the backbone architecture.},
  author = {Hongjun Wang and Sagar Vaze and Kai Han},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024sptnet.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3QLkwU40EE},
  publisher = {OpenReview.net},
  title = {{SPTNet}: An Efficient Alternative Framework for Generalized Category Discovery with Spatial Prompt Tuning},
  url = {https://openreview.net/forum?id=3QLkwU40EE},
  year = {2024}
}

@inproceedings{wang2024rotation,
  abstract = {One-class classification ({OCC}) involves predicting whether a new data is normal or anomalous based solely on the data from a single class during training. Various attempts have been made to learn suitable representations for {OCC} within a self-supervised framework. Notably, discriminative methods that use geometric visual transformations, such as rotation, to generate pseudo-anomaly samples have exhibited impressive detection performance. Although rotation is commonly viewed as a distribution-shifting transformation and is widely used in the literature, the cause of its effectiveness has not been thoroughly investigated. The root cause of this phenomenon can be attributed to the transformation bias in the dataset, where representations learned from transformations already present in the dataset tend to be less effective, making it essential to accurately estimate the transformation distribution before utilizing pretext tasks involving these transformations for reliable self-supervised representation learning. To this end, we propose a novel two-stage method to estimate the transformation distribution within the dataset. In the first stage, we learn general representations through standard contrastive pre-training. In the second stage, we select potentially semantics-preserving samples from the entire augmented dataset, which includes all rotations, by employing density matching with the provided reference distribution. By sorting samples based on semantics-preserving versus shifting transformations, we achieve improved performance on {OCC} benchmarks.},
  author = {Guodong Wang and Yunhong Wang and Xiuguo Bao and Di Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024rotation.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-03-10},
  pdf = {https://openreview.net/pdf?id=Ad81awoBVS},
  publisher = {OpenReview.net},
  title = {Rotation Has Two Sides: Evaluating Data Augmentation for Deep One-class Classification},
  url = {https://openreview.net/forum?id=Ad81awoBVS},
  year = {2024}
}

@inproceedings{wang2024pretraining,
  abstract = {Recently, it has been shown that for offline deep reinforcement learning ({DRL}), pre-training Decision Transformer with a large language corpus can improve downstream performance. A natural question to ask is whether this performance gain can only be achieved with language pre-training, or can be achieved with simpler pre-training schemes which do not involve language. In this paper, we first show that language is not essential for improved performance, and indeed pre-training with synthetic {IID} data for a small number of updates can match the performance gains from pre-training with a large language corpus; moreover, pre-training with data generated by a one-step Markov chain can further improve the performance. Inspired by these experimental results, we then consider pre-training Conservative Q-Learning ({CQL}), a popular offline {DRL} algorithm, which is Q-learning-based and typically employs a Multi-Layer Perceptron ({MLP}) backbone. We show that synthetic data pre-training can significantly improve the performance of {CQL} on several offline {RL} benchmarks. Our work demonstrates that synthetic data can be a simple yet effective alternative to language-based pre-training for offline reinforcement learning.},
  author = {Zecheng Wang and Che Wang and Zixuan Dong and Keith W. Ross},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024pretraining.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PcxQgtHGj2},
  publisher = {OpenReview.net},
  title = {Pre-training with Synthetic Data Helps Offline Reinforcement Learning},
  url = {https://openreview.net/forum?id=PcxQgtHGj2},
  year = {2024}
}

@inproceedings{wang2024nuwadynamics,
  abstract = {Spatio-temporal ({ST}) prediction plays a pivotal role in earth sciences, such as meteorological prediction, urban computing. Adequate high-quality data, coupled with deep models capable of inference, are both indispensible and prerequisite for achieving meaningful results. However, the sparsity of data and the high costs associated with deploying sensors lead to significant data imbalances. Models that are overly tailored and lack causal relationships further compromise the generalizabilities of inference methods. Towards this end, we first establish a causal concept for {ST} predictions, named {NuwaDynamics}, which targets to identify causal regions in data and endow model with causal reasoning ability in a two-stage process. Concretely, we initially leverage upstream self-supervision to discern causal important patches, imbuing the model with generalized information and conducting informed interventions on complementary trivial patches to extrapolate potential test distributions. This phase is referred to as the discovery step. Advancing beyond discovery step, we transfer the data to downstream tasks for targeted {ST} objectives, aiding the model in recognizing a broader potential distribution and fostering its causal perceptual capabilities (refer as Update step). Our concept aligns seamlessly with the contemporary backdoor adjustment mechanism in causality theory. {NuwaDynamics} also can significantly benefit a wide range of changeable {ST} tasks like extreme weather and long temporal step super-resolution predictions.},
  author = {Kun Wang and Hao Wu and Yifan Duan and Guibin Zhang and Kai Wang and Xiaojiang Peng and Yu Zheng and Yuxuan Liang and Yang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024nuwadynamics.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=sLdVl0q68X},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{NuwaDynamics}: Discovering and Updating in Causal Spatio-Temporal Modeling},
  url = {https://openreview.net/forum?id=sLdVl0q68X},
  year = {2024}
}

@inproceedings{wang2024fedhyper,
  abstract = {The theoretical landscape of federated learning ({FL}) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of {FL} systems. In response to this critical need, this paper presents {FedHyper}, a novel hypergradient-based learning rate adaptation algorithm specifically designed for {FL}. {FedHyper} serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. Meticulous theoretical analysis for the hypergradients of local and global learning rates with regard to the gradients of local and global models is provided. {FedHyper} is a universal and robust learning rate scheduling algorithm, which can adjust both global and local learning rates and enhance the robustness against random initial learning rates. Extensive experiments show that {FedHyper} converges 1.1--3x faster than {FedAvg} and most competing baselines, and increases accuracy by up to 15\% in various initial learning rate settings.},
  author = {Ziyao Wang and Jianyu Wang and Ang Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024fedhyper.pdf:pdf},
  note = {DBLP last modified: 2024-11-25},
  pdf = {https://openreview.net/pdf?id=Kl9CqKf7h6},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{FedHyper}: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent},
  url = {https://openreview.net/forum?id=Kl9CqKf7h6},
  year = {2024}
}

@inproceedings{wang2024grounding,
  abstract = {Grounding the common-sense reasoning of Large Language Models ({LLMs}) in physical domains remains a pivotal yet unsolved problem for embodied {AI}. Whereas prior works have focused on leveraging {LLMs} directly for planning in symbolic spaces, this work uses {LLMs} to guide the search of task structures and constraints implicit in multi-step demonstrations. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural network to predict successful trajectories from failures and as a by-product learns classifiers that ground low-level states and images in mode families without dense labeling. The learned grounding classifiers can further be used to translate language plans into reactive policies in the physical domain in an interpretable manner. We show our approach improves the interpretability and reactivity of imitation learning through {2D} navigation and simulated and real robot manipulation tasks. The paper specifically borrows from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an {LLM} and the low-level physical trajectories of a robot.},
  author = {Yanwei Wang and Tsun-Hsuan Wang and Jiayuan Mao and Michael Hagenow and Julie Shah},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024grounding.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=qoHeuRAcSl},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Grounding Language Plans in Demonstrations Through Counterfactual Perturbations},
  url = {https://openreview.net/forum?id=qoHeuRAcSl},
  year = {2024}
}

@inproceedings{wang2024timemixer,
  abstract = {Time series forecasting is widely used in extensive applications, such as traffic planning and weather forecasting. However, real-world time series usually present intricate temporal variations, making forecasting extremely challenging. Going beyond the mainstream paradigms of plain decomposition and multiperiodicity analysis, we analyze temporal variations in a novel view of multiscale-mixing, where time series present distinct patterns in different sampling scales. Specifically, the microscopic and the macroscopic information are reflected in fine and coarse scales, respectively, and thereby complex variations are inherently disentangled. Based on this observation, we propose {TimeMixer} as a fully {MLP}-based architecture with Past-Decomposable-Mixing ({PDM}) and Future-Multipredictor-Mixing ({FMM}) blocks to take full advantage of disentangled multiscale series in both past extraction and future prediction phases. Concretely, {PDM} applies the decomposition to multiscale series and further mixes the decomposed seasonal and trend components in fine-to-coarse and coarse-to-fine directions separately, which successively aggregates the microscopic seasonal and macroscopic trend information. {FMM} further ensembles multiple predictors to utilize complementary forecasting capabilities in multiscale observations. Consequently, our proposed {TimeMixer} is able to achieve consistent state-of-the-art performances in both long-term and short-term forecasting tasks with favorable run-time efficiency.},
  author = {Shiyu Wang and Haixu Wu and Xiaoming Shi and Tengge Hu and Huakun Luo and Lintao Ma and James Y. Zhang and Jun Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024timemixer.pdf:pdf},
  note = {DBLP last modified: 2025-05-22},
  pdf = {https://openreview.net/pdf?id=7oLshfEIC2},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{TimeMixer}: Decomposable Multiscale Mixing for Time Series Forecasting},
  url = {https://openreview.net/forum?id=7oLshfEIC2},
  year = {2024}
}

@inproceedings{wang2024kill,
  abstract = {Real-world tasks are universally associated with training samples that exhibit a long-tailed class distribution, and traditional deep learning models are not suitable for fitting this distribution, thus resulting in a biased trained model. To surmount this dilemma, massive deep long-tailed learning studies have been proposed to achieve inter-class fairness models by designing sophisticated sampling strategies or improving existing model structures and loss functions. Habitually, these studies tend to apply data augmentation strategies to improve the generalization performance of their models. However, this augmentation strategy applied to balanced distributions may not be the best option for long-tailed distributions. For a profound understanding of data augmentation, we first theoretically analyze the gains of traditional augmentation strategies in long-tailed learning, and observe that augmentation methods cause the long-tailed distribution to be imbalanced again, resulting in an intertwined imbalance: inherent data-wise imbalance and extrinsic augmentation-wise imbalance, i.e., two 'birds' co-exist in long-tailed learning. Motivated by this observation, we propose an adaptive Dynamic Optional Data Augmentation ({DODA}) to address this intertwined imbalance, i.e., one 'stone' simultaneously 'kills' two 'birds', which allows each class to choose appropriate augmentation methods by maintaining a corresponding augmentation probability distribution for each class during training. Extensive experiments across mainstream long-tailed recognition benchmarks (e.g., {CIFAR}-100-{LT}, {ImageNet}-{LT}, and i{Naturalist} 2018) prove the effectiveness and flexibility of the {DODA} in overcoming the intertwined imbalance.},
  author = {Binwu Wang and Pengkun Wang and Wei Xu and Xu Wang and Yudong Zhang and Kun Wang and Yang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024kill.pdf:pdf},
  note = {DBLP last modified: 2025-04-17},
  pdf = {https://openreview.net/pdf?id=RzY9qQHUXy},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Kill Two Birds with One Stone: Rethinking Data Augmentation for Deep Long-tailed Learning},
  url = {https://openreview.net/forum?id=RzY9qQHUXy},
  year = {2024}
}

@inproceedings{wang2024demystifying,
  abstract = {Backdoor attacks pose a significant security risk to machine learning applications due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper derives a fundamental understanding of backdoor attacks that applies to both discriminative and generative models, including diffusion models and large language models. We evaluate the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. We demonstrate the theory by conducting experiments using benchmark datasets and state-of-the-art backdoor attack scenarios.},
  author = {Ganghua Wang and Xun Xian and Ashish Kundu and Jayanth Srinivasa and Xuan Bi and Mingyi Hong and Jie Ding},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024demystifying.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=BPHcEpGvF8},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Demystifying Poisoning Backdoor Attacks from a Statistical Perspective},
  url = {https://openreview.net/forum?id=BPHcEpGvF8},
  year = {2024}
}

@inproceedings{wang2024fedcda,
  abstract = {In Federated Learning ({FL}), model aggregation is pivotal. It involves a global server iteratively aggregating client local trained models in successive rounds without accessing private data. Traditional methods typically aggregate the local models from the current round alone. However, due to the statistical heterogeneity across clients, the local models from different clients may be greatly diverse, making the obtained global model incapable of maintaining the specific knowledge of each local model. In this paper, we introduce a novel method, {FedCDA}, which selectively aggregates cross-round local models, decreasing discrepancies between the global model and local models. Specifically, {FedCDA} maintains a model pool that stores local models from previous rounds and selects models from this pool based on their divergence from the current global model. The selected models are then aggregated with the current round's models to update the global model. This approach enables the global model to better retain knowledge from diverse clients across different rounds, leading to improved performance and convergence. Extensive experiments on various datasets and settings demonstrate the effectiveness of {FedCDA} compared to existing federated learning methods.},
  author = {Haozhao Wang and Haoran Xu and Yichen Li and Yuan Xu and Ruixuan Li and Tianwei Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024fedcda.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=nbPGqeH3lt},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{FedCDA}: Federated Learning with Cross-rounds Divergence-aware Aggregation},
  url = {https://openreview.net/forum?id=nbPGqeH3lt},
  year = {2024}
}

@inproceedings{wang2024unique,
  abstract = {Micro-expression spotting ({MES}) is challenging since the small magnitude of micro-expression ({ME}) makes them susceptible to global movements like head rotation. However, the unique movement pattern and inherent characteristics of {ME} allow them to be distinguished from other movements. Existing {MES} methods based on fixed reference frame degrade optical flow accuracy and are overly dependent on facial alignment. In this paper, we propose a skip-$k$-frame block-wise main directional mean optical flow ({MDMO}) feature for {MES} based on unfixed reference frame. By employing skip-$k$-frame strategy, we can obtain more obvious optical flow changes between frames, which is conducive to extracting the unique movement pattern of {ME}. The proposed {MDMO} feature can effectively represent the unique movement pattern of {ME} and improve the accuracy of micro-expression spotting in long videos.},
  author = {Jinxuan Wang and Shiting Xu and Tong Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024unique.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=H396R79GiQ},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {A unique {M}-pattern for micro-expression spotting in long videos},
  url = {https://openreview.net/forum?id=H396R79GiQ},
  year = {2024}
}

@inproceedings{wang2024lego,
  abstract = {Despite the success of large language models ({LLMs}), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present {LEGO}-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of {LLMs} used in theorem proving. {LEGO}-Prover advances the state-of-the-art pass rate on mini{F}2{F}-valid (48.0\% to 57.0\%) and mini{F}2{F}-test (45.5\% to 50.0\%). During the proving process, {LEGO}-Prover also generates over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in a 4.9\% improvement in success rate.},
  author = {Haiming Wang and Huajian Xin and Chuanyang Zheng and Zhengying Liu and Qingxing Cao and Yinya Huang and Jing Xiong and Han Shi and Enze Xie and Jian Yin and Zhenguo Li and Xiaodan Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024lego.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=3f5PALef5B},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {{LEGO}-Prover: Neural Theorem Proving with Growing Libraries},
  url = {https://openreview.net/forum?id=3f5PALef5B},
  year = {2024}
}

@inproceedings{wang2024towards,
  abstract = {As large language models ({LLMs}) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of {LLMs}. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by {LLMs} by injecting hidden patterns. However, we argue that existing {LLM} watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.). In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for {LLMs} ({CTWL}) that allows text watermarks to carry multi-bit customizable information. Additionally, we provide a comprehensive evaluation system for {CTWL}: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we follow the most prominent vocabulary partition-based watermarking direction, and devise an advanced {CTWL} method named Balance-Marking. The core idea of our method is to use a proxy language model to split the vocabulary into probability-balanced parts, thereby effectively maintaining the quality of the watermarked text.},
  author = {Lean Wang and Wenkai Yang and Deli Chen and Hao Zhou and Yankai Lin and Fandong Meng and Jie Zhou and Xu Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JYu5Flqm9D},
  publisher = {OpenReview.net},
  series = {{ICLR} 2024},
  title = {Towards Codable Watermarking for Injecting Multi-Bits Information to {LLMs}},
  url = {https://openreview.net/forum?id=JYu5Flqm9D},
  year = {2024}
}

@inproceedings{wang2024p2seg,
  abstract = {Point-level Supervised Instance Segmentation (PSIS) aims to enhance the applicability and scalability of instance segmentation by utilizing low-cost yet instance-informative annotations. Existing PSIS methods usually rely on positional information to distinguish objects, but predicting precise boundaries remains challenging due to the lack of contour annotations. Nevertheless, weakly supervised semantic segmentation methods are proficient in utilizing intra-class feature consistency to capture the boundary contours of the same semantic regions. In this paper, we design a Mutual Distillation Module (MDM) to leverage the complementary strengths of both instance position and semantic information and achieve accurate instance-level object perception. The MDM consists of Semantic to Instance (S2I) and Instance to Semantic (I2S). S2I is guided by the precise boundaries of semantic regions to learn the association between annotated points and instance contours. I2S leverages discriminative relationships between instances to facilitate the differentiation of various objects within the semantic map. Extensive experiments substantiate the efficacy of MDM in fostering the synergy between instance and semantic information, consequently improving the quality of instance-level object representations. Our method achieves 55.7 mAP50 and 17.6 mAP on the PASCAL VOC and MS COCO datasets, significantly outperforming recent PSIS methods and several box-supervised instance segmentation competitors.},
  author = {Zipeng Wang and Xuehui Yu and Xumeng Han and Wenwen Yu and Zhixun Huang and Jianbin Jiao and Zhenjun Han},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024p2seg.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=B4vzu2aokv},
  publisher = {OpenReview.net},
  title = {{P2Seg}: Pointly-supervised Segmentation via Mutual Distillation},
  url = {https://openreview.net/forum?id=B4vzu2aokv},
  year = {2024}
}

@inproceedings{wang2024pandalm,
  abstract = {Instruction tuning large language models (LLMs) remains a challenging task, owing to the complexity of hyperparameter selection and the difficulty involved in evaluating the tuned models. To determine the optimal hyperparameters, an automatic, robust, and reliable evaluation benchmark is essential. However, establishing such a benchmark is not a trivial task due to the challenges associated with evaluation accuracy and privacy protection. In response to these challenges, we introduce a judge large language model, named PandaLM, which is trained to distinguish the superior model given several LLMs. PandaLM's focus extends beyond just the objective correctness of responses, which is the main focus of traditional evaluation datasets. It addresses vital subjective factors such as relative conciseness, clarity, adherence to instructions, comprehensiveness, and formality. To ensure the reliability of PandaLM, we collect a diverse human-annotated test dataset, where all contexts are generated by humans and labels are aligned with human preferences. Our results indicate that PandaLM-7B achieves 93.75\% of GPT-3.5's evaluation ability and 88.28\% of GPT-4's in terms of F1-score on our test dataset. Our findings reveal that PandaLM-7B offers a performance comparable to both GPT-3.5 and GPT-4. Impressively, PandaLM-70B surpasses their performance. PandaLM enables the evaluation of LLM to be fairer but with less cost, evidenced by significant improvements achieved by models tuned through PandaLM compared to their counterparts trained with default Alpaca's hyperparameters. In addition, PandaLM does not depend on API-based evaluations, thus avoiding potential data leakage.},
  author = {Yidong Wang and Zhuohao Yu 0001 and Wenjin Yao and Zhengran Zeng and Linyi Yang and Cunxiang Wang and Hao Chen 0102 and Chaoya Jiang and Rui Xie 0003 and Jindong Wang 0001 and Xing Xie 0001 and Wei Ye 0004 and Shikun Zhang and Yue Zhang 0004},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024pandalm.pdf:pdf},
  note = {DBLP last modified: 2025-05-21},
  pdf = {https://openreview.net/pdf?id=5Nn2BLV7SB},
  publisher = {OpenReview.net},
  title = {{PandaLM}: An Automatic Evaluation Benchmark for {LLM} Instruction Tuning Optimization},
  url = {https://openreview.net/forum?id=5Nn2BLV7SB},
  year = {2024}
}

@inproceedings{wang2024neural,
  abstract = {In this work, we propose a novel link prediction model and further boost it by studying graph incompleteness. First, we introduce MPNN-then-SF, an innovative architecture leveraging structural feature (SF) to guide MPNN's representation pooling, with its implementation, namely Neural Common Neighbor (NCN). NCN exhibits superior expressiveness and scalability compared with existing models, which can be classified into two categories: SF-then-MPNN, augmenting MPNN's input with SF, and SF-and-MPNN, decoupling SF and MPNN. Second, we investigate the impact of graph incompleteness -- the phenomenon that some links are unobserved in the input graph -- on SF, like the common neighbor. Through dataset visualization, we observe that incompleteness reduces common neighbors and induces distribution shifts, significantly affecting model performance. To address this issue, we propose to use a link prediction model to complete the common neighbor structure.},
  author = {Xiyuan Wang and Haotong Yang and Muhan Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024neural.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=sNFLN3itAd},
  publisher = {OpenReview.net},
  title = {Neural Common Neighbor with Completion for Link Prediction},
  url = {https://openreview.net/forum?id=sNFLN3itAd},
  year = {2024}
}

@inproceedings{2024generated,
  abstract = {Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed ``data inflation''. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under data inflation. Drawing from these insights, we propose Adaptive Inflation (AdaInf), a purely data-centric strategy without introducing any extra computation cost. On benchmark datasets, AdaInf can bring significant improvements for various contrastive learning methods. Notably, without using external data, AdaInf obtains 94.70\% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods.},
  author = {Yifei Wang 0001 and Jizhe Zhang and Yisen Wang 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024generated.pdf:pdf},
  note = {DBLP last modified: 2024-08-09},
  pdf = {https://openreview.net/pdf?id=S5EqslEHnz},
  publisher = {OpenReview.net},
  title = {Do Generated Data Always Help Contrastive Learning?},
  url = {https://openreview.net/forum?id=S5EqslEHnz},
  year = {2024}
}

@inproceedings{2024thinshell,
  abstract = {In this work, we aim to teach robots to manipulate various thin-shell materials. Prior works studying thin-shell object manipulation mostly rely on heuristic policies or learn policies from real-world video demonstrations, and only focus on limited material types and tasks (e.g., cloth unfolding). However, these approaches face significant challenges when extended to a wider variety of thin-shell materials and a diverse range of tasks. To fill in this gap, we introduce ThinShellLab - a fully differentiable simulation platform tailored for robotic interactions with diverse thin-shell materials possessing varying material properties, enabling flexible thin-shell manipulation skill learning and evaluation. Our platform supports high-fidelity physics simulations of diverse thin-shell materials and provides a comprehensive benchmark including manipulation tasks, inverse design tasks, and real-world experiments to comprehensively evaluate the platform's capabilities.},
  author = {Yian Wang 0001 and Juntian Zheng and Zhehuan Chen and Zhou Xian and Gu Zhang and Chao Liu and Chuang Gan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024thinshell.pdf:pdf},
  note = {DBLP last modified: 2025-05-14},
  pdf = {https://openreview.net/pdf?id=KsUh8MMFKQ},
  publisher = {OpenReview.net},
  title = {Thin-Shell Object Manipulations With Differentiable Physics Simulations},
  url = {https://openreview.net/forum?id=KsUh8MMFKQ},
  year = {2024}
}

@inproceedings{2024nonnegative,
  abstract = {Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these advantages enable NCL to outperform CL significantly on feature disentanglement, feature selection, as well as downstream classification tasks. At last, we show that NCL can be easily extended to other learning scenarios and benefit supervised learning as well.},
  author = {Yifei Wang 0001 and Qi Zhang 0067 and Yaoyu Guo and Yisen Wang 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024nonnegative.pdf:pdf},
  note = {DBLP last modified: 2025-05-21},
  pdf = {https://openreview.net/pdf?id=lNCnZwcH5Z},
  publisher = {OpenReview.net},
  title = {Non-negative Contrastive Learning},
  url = {https://openreview.net/forum?id=lNCnZwcH5Z},
  year = {2024}
}

@inproceedings{wang2024differentiable,
  abstract = {Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to facilitate each other's development. However, none of the existing software in the fields can achieve this goal, because traditional brain simulators lack differentiability for training, while existing deep learning (DL) frameworks fail to capture the biophysical realism and complexity of brain dynamics. In this paper, we introduce BrainPy, a differentiable brain simulator developed using JAX and XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy expands upon the functionalities of JAX, a powerful AI framework, by introducing complete capabilities for flexible, efficient, and scalable brain simulation. It offers a range of sparse and event-driven operators for efficient and scalable brain simulation, an abstraction for managing the intricacies of synaptic computations, a modular and flexible interface for constructing multi-scale brain models, and an object-oriented just-in-time compilation approach to handle the memory-intensive nature of brain dynamics. We showcase the efficiency and scalability of BrainPy on benchmark tasks, highlight its differentiable simulation for biologically plausible spiking models, and discuss its potential to support research at the intersection of brain simulation and BIC.},
  author = {Chaoming Wang and Tianqiu Zhang and Sichao He and Hongyaoxing Gu and Shangyang Li and Si Wu 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024differentiable.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=AU2gS9ut61},
  publisher = {OpenReview.net},
  title = {A differentiable brain simulator bridging brain simulation and brain-inspired computing},
  url = {https://openreview.net/forum?id=AU2gS9ut61},
  year = {2024}
}

@inproceedings{wang2024nearoptimal,
  abstract = {The problem of minimizing the maximum of $N$ convex, Lipschitz functions plays significant roles in optimization and machine learning. It has a series of results, with the most recent one requiring $O(N\epsilon^{-2/3} + \epsilon^{-8/3})$ queries to a first-order oracle to compute an $\epsilon$-suboptimal point. On the other hand, quantum algorithms for optimization are rapidly advancing with speedups shown on many important optimization problems. In this paper, we conduct a systematic study of quantum algorithms and lower bounds for minimizing the maximum of $N$ convex, Lipschitz functions. On one hand, we develop quantum algorithms with an improved complexity bound of $\tilde{O}(\sqrt{N}\epsilon^{-5/3} + \epsilon^{-8/3})$. On the other hand, we prove that quantum algorithms must take $\tilde{\Omega}(\sqrt{N}\epsilon^{-2/3})$ queries to a first-order quantum oracle, showing that our dependence on $N$ is optimal up to poly-logarithmic factors.},
  author = {Hao Wang and Chenyi Zhang and Tongyang Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024nearoptimal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pB1FeRSQxh},
  publisher = {OpenReview.net},
  title = {Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss},
  url = {https://openreview.net/forum?id=pB1FeRSQxh},
  year = {2024}
}

@inproceedings{2024efficient,
  abstract = {Sharpness-aware minimization (SAM) has received increasing attention in computer vision since it can effectively eliminate the sharp local minima from the training trajectory and mitigate generalization degradation. However, SAM requires two sequential gradient computations during the optimization of each step: one to obtain the perturbation gradient and the other to obtain the updating gradient. Compared with the base optimizer (e.g., Adam), SAM doubles the time overhead due to the additional perturbation gradient. By dissecting the theory of SAM and observing the training gradient of the molecular graph transformer, we propose a new algorithm named GraphSAM, which reduces the training cost of SAM and improves the generalization performance of graph transformer models.},
  author = {Yili Wang 0004 and Kaixiong Zhou and Ninghao Liu and Ying Wang 0009 and Xin Wang 0035},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024efficient.pdf:pdf},
  note = {DBLP last modified: 2025-01-22},
  pdf = {https://openreview.net/pdf?id=Od39h4XQ3Y},
  publisher = {OpenReview.net},
  title = {Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models},
  url = {https://openreview.net/forum?id=Od39h4XQ3Y},
  year = {2024}
}

@inproceedings{wang2024hypothesis,
  abstract = {Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding ``in context learning.'' This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves 30\% accuracy, outperforming the direct prompting baseline (accuracy of 17\%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33\%.},
  author = {Ruocheng Wang and Eric Zelikman and Gabriel Poesia and Yewen Pu and Nick Haber and Noah D. Goodman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024hypothesis.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=G7UtIGQmjm},
  publisher = {OpenReview.net},
  title = {Hypothesis Search: Inductive Reasoning with Language Models},
  url = {https://openreview.net/forum?id=G7UtIGQmjm},
  year = {2024}
}

@inproceedings{wang2024coplanner,
  abstract = {Dyna-style model-based reinforcement learning contains two phases: model rollouts to generate sample for policy learning and real environment exploration using current policy for dynamics model learning. However, due to the complex real-world environment, it is inevitable to learn an imperfect dynamics model with model prediction error, which can further mislead policy learning and result in sub-optimal solutions. To address this issue, we propose {COPlanner}, a planning-driven framework for model-based methods that addresses the inaccurately learned dynamics model problem with conservative model rollouts and optimistic environment exploration. {COPlanner} leverages an uncertainty-aware policy-guided model predictive control ({UP-MPC}) component to plan for multi-step uncertainty estimation and can avoid model uncertain regions through conservative model rollouts, thereby alleviating the influence of model error. Simultaneously, it explores high-reward model uncertain regions to reduce model error actively through optimistic real environment exploration. The estimated uncertainty serves as a penalty during model rollouts and as a bonus during real environment exploration respectively, to choose actions. {COPlanner} is a plug-and-play framework that can be applied to any dyna-style model-based methods. Experimental results on a series of proprioceptive and visual continuous control tasks demonstrate that both sample efficiency and asymptotic performance of strong model-based methods are significantly improved combined with {COPlanner}.},
  author = {Xiyao Wang and Ruijie Zheng and Yanchao Sun and Ruonan Jia and Wichayaporn Wongkamjan and Huazhe Xu and Furong Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024coplanner.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jnFcKjtUPN},
  publisher = {OpenReview.net},
  title = {{COPlanner}: {Plan} to {Roll} {Out} {Conservatively} but to {Explore} {Optimistically} for {Model-Based} {RL}},
  url = {https://openreview.net/forum?id=jnFcKjtUPN},
  year = {2024}
}

@inproceedings{2024card,
  abstract = {Time series forecasting is a crucial task in various fields, and the use of Transformer-based models has shown significant promise in this area. However, one of the key challenges with these models is the channel-independent ({CI}) strategy, which limits their ability to capture the correlations among different channels effectively while still maintaining training robustness. To address this issue, we propose {CARD} (Channel Aligned Robust Dual Transformer), which introduces a channel-aligned attention structure that allows the model to capture both temporal correlations among signals and dynamical dependence among multiple variables over time. Additionally, we design a token blend module to generate tokens with different resolutions, enabling the model to efficiently utilize multi-scale knowledge. Through extensive experiments on several benchmark datasets, we demonstrate that {CARD} achieves state-of-the-art performance while maintaining computational efficiency.},
  author = {Xue Wang 0010 and Tian Zhou 0004 and Qingsong Wen and Jinyang Gao and Bolin Ding and Rong Jin 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024card.pdf:pdf},
  note = {DBLP last modified: 2025-01-23},
  pdf = {https://openreview.net/pdf?id=MJksrOhurE},
  publisher = {OpenReview.net},
  title = {{CARD}: {Channel} {Aligned} {Robust} {Blend} {Transformer} for {Time} {Series} {Forecasting}},
  url = {https://openreview.net/forum?id=MJksrOhurE},
  year = {2024}
}

@inproceedings{wang2024learning,
  abstract = {This paper introduces the first paradigm that constructs the generalizable neural field based on point-based rather than image-based rendering, which we call the Generalizable neural Point Field ({GPF}). The proposed approach addresses three main issues with previous generalizable {NeRF} approaches: occlusions often result in inconsistent feature matching, they deliver distortions and artifacts in geometric discontinuities and locally sharp shapes due to their individual process of sampled points and rough feature aggregation, and their image-based representations experience severe degradations when source views are not near enough to the target view. We explicitly model visibilities by geometric priors and augment them with neural features. We propose a novel nonuniform log sampling strategy to improve both rendering speed and reconstruction quality, and a learnable kernel spatially augmented with features for feature aggregations, mitigating distortions at places with drastically varying geometries. Experiments show that our model can deliver better geometries, view consistencies, and rendering quality than all counterparts and benchmarks on three datasets in both generalization and finetuning settings.},
  author = {Jiaxu Wang and Ziyi Zhang and Renjing Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=o4CLLlIaaH},
  publisher = {OpenReview.net},
  title = {Learning {Robust} {Generalizable} {Radiance} {Field} with {Visibility} and {Feature} {Augmented} {Point} {Representation}},
  url = {https://openreview.net/forum?id=o4CLLlIaaH},
  year = {2024}
}

@inproceedings{wang2024robot,
  abstract = {Fleets of robots ingest massive amounts of heterogeneous streaming data silos generated by interacting with their environments, far more than what can be stored or transmitted with ease. At the same time, teams of robots should co-acquire diverse skills through their heterogeneous experiences in varied settings. How can we enable such fleet-level learning without having to transmit or centralize fleet-scale data? In this paper, we investigate policy merging ({PoMe}) from such distributed heterogeneous datasets as a potential solution. To efficiently merge policies in the fleet setting, we propose {FLEET-MERGE}, an instantiation of distributed learning that accounts for the permutation invariance that arises when parameterizing the control policies with recurrent neural networks. We show that {FLEET-MERGE} consolidates the behavior of policies trained on 50 tasks in the {Meta-World} environment, with good performance on nearly all training tasks at test time.},
  author = {Lirui Wang and Kaiqing Zhang and Allan Zhou and Max Simchowitz and Russ Tedrake},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wang2024robot.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IL71c1z7et},
  publisher = {OpenReview.net},
  title = {{Robot} {Fleet} {Learning} via {Policy} {Merging}},
  url = {https://openreview.net/forum?id=IL71c1z7et},
  year = {2024}
}

@inproceedings{water2024yet,
  abstract = {We introduce Yet Another {ICU} Benchmark ({YAIB}), a flexible, holistic framework for the standardization of clinical prediction model experiments that allows researchers to define reproducible and comparable clinical {ML} experiments, offering an end-to-end solution from cohort definition to model evaluation. The framework natively supports most open-access {ICU} datasets ({MIMIC} {III}/{IV}, {eICU}, {HiRID}, {AUMCdb}) and is easily adaptable to future {ICU} datasets. Combined with a transparent preprocessing pipeline and extensible training code for multiple {ML} and deep learning models, {YAIB} enables unified model development, transfer, and evaluation. The benchmark comes with five predefined established prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and length of stay) developed in collaboration with clinicians.},
  author = {Robin Van De Water and Hendrik Schmidt and Paul W. G. Elbers and Patrick Thoral and Bert Arnrich and Patrick Rockenschaub},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/water2024yet.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ox2ATRM90I},
  publisher = {OpenReview.net},
  title = {{Yet} {Another} {ICU} {Benchmark}: {A} {Flexible} {Multi-Center} {Framework} for {Clinical} {ML}},
  url = {https://openreview.net/forum?id=ox2ATRM90I},
  year = {2024}
}

@inproceedings{wei2024bayesian,
  abstract = {We develop a bi-clustering method to cluster neural spiking activity spatially and temporally, according to their low-dimensional latent structures. The spatial (neuron) clusters are defined by the latent trajectories within each neural population, while the temporal (state) clusters are defined by (populationally) synchronous local linear dynamics shared with different periods. The model is built non-parametrically, with an efficient {Markov} chain {Monte} {Carlo} ({MCMC}) algorithm developed to sample the posterior. Validation through simulations shows that the method can recover unknown parameters and true bi-clustering structures successfully. The method was then applied to multi-regional neural recordings under different experiment settings, where simultaneously considering latent trajectories and spatial-temporal clustering structures provided more accurate and interpretable results. The proposed method provides scientific insights for large-scale counting time series with elongated recording periods, and has potential applications beyond neuroscience.},
  author = {Ganchao Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wei2024bayesian.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZYm1Ql6udy},
  publisher = {OpenReview.net},
  title = {{Bayesian} {Bi-clustering} of {Neural} {Spiking} {Activity} with {Latent} {Structures}},
  url = {https://openreview.net/forum?id=ZYm1Ql6udy},
  year = {2024}
}

@inproceedings{wei2024coeditor,
  abstract = {We explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. We introduce {Coeditor}, a fine-tuned language model specifically designed for code editing tasks. {Coeditor} represents code changes using a line diff format and employs static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collected a code editing dataset from the commit histories of 1650 open-source {Python} projects for training and evaluation. In a simplified single-round, single-edit task, {Coeditor} significantly outperforms {GPT-3.5} and {SOTA} open-source code completion models, bringing exact-match accuracy from 34.7\% up to 60.4\%. In the full multi-round setting, {Coeditor} automates editing 46.7\% of the changed lines, saving the user 28.6\% of keystrokes measured by an edit distance metric that accounts for cursor movement.},
  author = {Jiayi Wei and Greg Durrett and Isil Dillig},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wei2024coeditor.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=ALVwQjZRS8},
  publisher = {OpenReview.net},
  title = {{Coeditor}: {Leveraging} {Repo-level} {Diffs} for {Code} {Auto-editing}},
  url = {https://openreview.net/forum?id=ALVwQjZRS8},
  year = {2024}
}

@inproceedings{wei2024towards,
  abstract = {We develop a generic and extensible personalization generative framework that can handle a wide range of personalized needs including item recommendation, product search, preference prediction, explanation generation, and further user-guided image generation. We establish a {Unified} paradigm for {Multi-modal} {Personalization} systems ({UniMP}), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. Our methodology enhances the capabilities of foundational language models for personalized tasks by seamlessly ingesting interleaved cross-modal user history information, ensuring a more precise and customized experience for users. To train and evaluate the proposed multi-modal personalized tasks, we also introduce a novel and comprehensive benchmark covering a variety of user requirements. Our experiments on the real-world benchmark showcase the model's potential, outperforming competitive methods specialized for each task.},
  author = {Tianxin Wei and Bowen Jin and Ruirui Li 0002 and Hansi Zeng and Zhengyang Wang and Jianhui Sun and Qingyu Yin and Hanqing Lu and Suhang Wang and Jingrui He and Xianfeng Tang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wei2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=khAE1sTMdX},
  publisher = {OpenReview.net},
  title = {{Towards} {Unified} {Multi-Modal} {Personalization}: {Large} {Vision-Language} {Models} for {Generative} {Recommendation} and {Beyond}},
  url = {https://openreview.net/forum?id=khAE1sTMdX},
  year = {2024}
}

@inproceedings{wei2024incentivized,
  abstract = {We introduce the first truthful incentive mechanism for federated bandits, where clients are incentivized to truthfully report costs for their best interests. The proposed {Truth-FedBan} guarantees near-optimal regret, communication and social costs. Existing incentive mechanisms naively assume the clients are truthful: they all report their true cost and thus the higher cost one participating client claims, the more the server has to pay. Therefore, such mechanisms are vulnerable to strategic clients aiming to optimize their own utility by misreporting. Instead of simply paying a client by its claimed cost, we decouple the calculation of incentive from the target client's reported cost, while preserving individual rationality through a critical-value based payment design that depends on all other clients' report cost. {Truth-FedBan} still guarantees the sub-linear regret and communication cost without any overhead. Additionally, the proposed {Truth-FedBan} guarantees both sub-linear regret and near-optimal social cost, with only a constant-factor approximation ratio.},
  author = {Zhepei Wei and Chuanhao Li and Tianze Ren and Haifeng Xu and Hongning Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wei2024incentivized.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ykEixGIJYb},
  publisher = {OpenReview.net},
  title = {{Incentivized} {Truthful} {Communication} for {Federated} {Bandits}},
  url = {https://openreview.net/forum?id=ykEixGIJYb},
  year = {2024}
}

@inproceedings{2024nerm,
  author = {Dong Wei 0007 and Huaijiang Sun and Bin Li 0084 and Xiaoning Sun and Shengxiang Hu 0001 and Weiqing Li and Jianfeng Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/2024nerm.pdf:pdf},
  note = {DBLP last modified: 2024-10-11},
  pdf = {https://openreview.net/pdf?id=sOJriBlOFd},
  publisher = {OpenReview.net},
  title = {{NeRM}: {Learning} {Neural} {Representations} for {High-Framerate} {Human} {Motion} {Synthesis}},
  url = {https://openreview.net/forum?id=sOJriBlOFd},
  year = {2024}
}

@inproceedings{wei2024consistent,
  abstract = {Weakly supervised learning aims to construct effective predictive models from imperfectly labeled data. The recent trend of weakly supervised learning has focused on how to learn an accurate classifier from completely unlabeled data, given little supervised information such as class priors. In this paper, we consider a newly proposed weakly supervised learning problem called multi-class classification from multiple unlabeled datasets, where only multiple sets of unlabeled data and their class priors (i.e., the proportions of each class) are provided for training the classifier. To solve this problem, we first propose a classifier-consistent method (CCM) based on a probability transition matrix. However, CCM cannot guarantee risk consistency and lacks of purified supervision information during training. Therefore, we further propose a risk-consistent method (RCM) that progressively purifies supervision information during training by importance weighting. We provide comprehensive theoretical analyses for our methods to demonstrate the statistical consistency. Experimental results on multiple benchmark datasets and various prior matrices demonstrate the superiority of our proposed methods.},
  author = {Zixi Wei and Senlin Shu and Yuzhou Cao and Hongxin Wei and Bo An and Lei Feng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wei2024consistent.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=fW7DOHDQvF},
  publisher = {OpenReview.net},
  title = {Consistent Multi-Class Classification from Multiple Unlabeled Datasets},
  url = {https://openreview.net/forum?id=fW7DOHDQvF},
  year = {2024}
}

@inproceedings{wen2024class,
  abstract = {The paper addresses the domain adaptation problem in the context of label shift, where the label distributions between source and target domain differ, but the conditional distributions of features given the label are the same. To solve the label shift adaption problem, we develop a novel matching framework named class probability matching (CPM). It is inspired by a new understanding of the source domain's class probability, as well as a specific relationship between class probability ratios and feature probability ratios between the source and target domains. CPM is able to maintain the same theoretical guarantee with the existing feature probability matching framework, while significantly improving the computational efficiency due to directly matching the probabilities of the label variable. Within the CPM framework, we propose an algorithm named class probability matching with calibrated networks (CPMCN) for target domain classification.},
  author = {Hongwei Wen and Annika Betken and Hanyuan Hang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wen2024class.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mliQ2huFrZ},
  publisher = {OpenReview.net},
  title = {Class Probability Matching with Calibrated Networks for Label Shift Adaption},
  url = {https://openreview.net/forum?id=mliQ2huFrZ},
  year = {2024}
}

@inproceedings{wen2024batched,
  abstract = {Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. The paper introduces Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching for diverse LLM queries.},
  author = {Yeming Wen and Swarat Chaudhuri},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wen2024batched.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=w4abltTZ2f},
  publisher = {OpenReview.net},
  title = {Batched Low-Rank Adaptation of Foundation Models},
  url = {https://openreview.net/forum?id=w4abltTZ2f},
  year = {2024}
}

@inproceedings{wen2024dilu,
  abstract = {Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems.},
  author = {Licheng Wen and Daocheng Fu and Xin Li and Xinyu Cai and Tao Ma and Pinlong Cai and Min Dou and Botian Shi and Liang He and Yu Qiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wen2024dilu.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OqTMUPuLuC},
  publisher = {OpenReview.net},
  title = {{DiLu}: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models},
  url = {https://openreview.net/forum?id=OqTMUPuLuC},
  year = {2024}
}

@inproceedings{wen2024can,
  abstract = {Spatial relationships between objects represent key scene information for humans to understand and interact with the world. To study the capability of current computer vision systems to recognize physically grounded spatial relations, we start by proposing precise relation definitions that permit consistently annotating a benchmark dataset. Despite the apparent simplicity of this task relative to others in the recognition literature, we observe that existing approaches perform poorly on this benchmark. We propose new approaches exploiting the long-range attention capabilities of transformers for this task, and evaluating key design principles. We identify a simple "RelatiViT" architecture and demonstrate that it outperforms all current approaches. To our knowledge, this is the first method to convincingly outperform naive baselines on spatial relation prediction in in-the-wild settings.},
  author = {Chuan Wen and Dinesh Jayaraman and Yang Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wen2024can.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HgZUcwFhjr},
  publisher = {OpenReview.net},
  title = {Can Transformers Capture Spatial Relations between Objects?},
  url = {https://openreview.net/forum?id=HgZUcwFhjr},
  year = {2024}
}

@inproceedings{wen2024detecting,
  abstract = {Recent breakthroughs in diffusion models have exhibited exceptional image-generation capabilities. However, studies show that some outputs are merely replications of training data. Such replications present potential legal challenges for model owners, especially when the generated content contains proprietary information. In this work, we introduce a straightforward yet effective method for detecting memorized prompts by inspecting the magnitude of text-conditional predictions. Our proposed method seamlessly integrates without disrupting sampling algorithms, and delivers high accuracy even at the first generation step, with a single generation per prompt. Building on our detection strategy, we unveil an explainable approach that shows the contribution of individual words or tokens to memorization. This offers an interactive medium for users to adjust their prompts. Moreover, we propose two strategies to mitigate memorization by leveraging the magnitude of text-conditional predictions, either through minimization during inference or filtering during training. These proposed strategies effectively counteract memorization while maintaining high-generation quality.},
  author = {Yuxin Wen and Yuchen Liu and Chen Chen and Lingjuan Lyu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wen2024detecting.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=84n3UwkH7b},
  publisher = {OpenReview.net},
  title = {Detecting, Explaining, and Mitigating Memorization in Diffusion Models},
  url = {https://openreview.net/forum?id=84n3UwkH7b},
  year = {2024}
}

@inproceedings{wen2024cellplm,
  abstract = {The current state-of-the-art single-cell pre-trained models are greatly inspired by the success of large language models. They trained transformers by treating genes as tokens and cells as sentences. However, three fundamental differences between single-cell data and natural language data are overlooked: (1) scRNA-seq data are presented as bag-of-genes instead of sequences of RNAs; (2) Cell-cell relations are more intricate and important than inter-sentence relations; and (3) The quantity of single-cell data is considerably inferior to text data, and they are very noisy. In light of these characteristics, we propose a new pre-trained model, CellPLM, which takes cells as tokens and tissues as sentences. In addition, we leverage spatially-resolved transcriptomic data in pre-training to facilitate learning cell-cell relationships and introduce a Gaussian prior distribution as an additional inductive bias to overcome data limitations. CellPLM is the first single-cell pre-trained transformer that encodes cell-cell relations and it consistently outperforms existing pre-trained and non-pre-trained models in diverse downstream tasks, with 100 times higher inference speed on generating cell embeddings than previous pre-trained models.},
  author = {Hongzhi Wen and Wenzhuo Tang and Xinnan Dai and Jiayuan Ding and Wei Jin and Yuying Xie and Jiliang Tang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wen2024cellplm.pdf:pdf},
  note = {DBLP last modified: 2024-11-26},
  pdf = {https://openreview.net/pdf?id=BKXvPDekud},
  publisher = {OpenReview.net},
  title = {{CellPLM}: Pre-training of Cell Language Model Beyond Single Cells},
  url = {https://openreview.net/forum?id=BKXvPDekud},
  year = {2024}
}

@inproceedings{weng2024modulate,
  abstract = {Whitening loss offers a theoretical guarantee against feature collapse in self-supervised learning (SSL) with joint embedding architectures. Typically, it involves a hard whitening approach, transforming the embedding and applying loss to the whitened output. In this work, we introduce Spectral Transformation (ST), a framework to modulate the spectrum of embedding and to seek for functions beyond whitening that can avoid dimensional collapse. We show that whitening is a special instance of ST by definition, and our empirical investigations unveil other ST instances capable of preventing collapse. The paper also introduces a novel ST instance named IterNorm with trace loss (INTL). Theoretical analysis confirms INTL's efficacy in preventing collapse and modulating the spectrum of embedding toward equal-eigenvalues during optimization.},
  author = {Xi Weng and Yunhao Ni and Tengwei Song and Jie Luo and Rao Muhammad Anwer and Salman Khan and Fahad Khan and Lei Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/weng2024modulate.pdf:pdf},
  note = {DBLP last modified: 2025-01-17},
  pdf = {https://openreview.net/pdf?id=TKqMmKlmA7},
  publisher = {OpenReview.net},
  title = {Modulate Your Spectrum in Self-Supervised Learning},
  url = {https://openreview.net/forum?id=TKqMmKlmA7},
  year = {2024}
}

@inproceedings{weng2024mastering,
  abstract = {Language models' (LMs) proficiency in handling deterministic symbolic reasoning and rule-based tasks remains limited due to their dependency implicit learning on textual data. To endow LMs with genuine rule comprehension abilities, we propose "Neural Comprehension" - a framework that synergistically integrates compiled neural networks (CoNNs) into the standard transformer architecture. CoNNs are neural modules designed to explicitly encode rules through artificially generated attention weights. By incorporating CoNN modules, the Neural Comprehension framework enables LMs to accurately and robustly execute rule-intensive symbolic tasks. Extensive experiments demonstrate the superiority of our approach over existing techniques in terms of length generalization, efficiency, and interpretability for symbolic operations. Furthermore, it can be applied to LMs across different model scales, outperforming tool-calling methods in arithmetic reasoning tasks while maintaining superior inference efficiency.},
  author = {Yixuan Weng and Minjun Zhu and Fei Xia and Bin Li and Shizhu He and Kang Liu and Jun Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/weng2024mastering.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=9nsNyN0vox},
  publisher = {OpenReview.net},
  title = {Mastering Symbolic Operations: Augmenting Language Models with Compiled Neural Networks},
  url = {https://openreview.net/forum?id=9nsNyN0vox},
  year = {2024}
}

@inproceedings{west2024generative,
  abstract = {The recent wave of generative AI has sparked unprecedented global attention, with both excitement and concern over potentially superhuman levels of artificial intelligence: models now take only seconds to produce outputs that would challenge or exceed the capabilities even of expert humans. At the same time, models still show basic errors in understanding that would not be expected even in non-expert humans. This presents us with an apparent paradox: how do we reconcile seemingly superhuman capabilities with the persistence of errors that few humans would make? In this work, we posit that this tension reflects a divergence in the configuration of intelligence in today's generative models relative to intelligence in humans. Specifically, we propose and test the Generative AI Paradox hypothesis: generative models, having been trained directly to reproduce expert-like outputs, acquire generative capabilities that are not contingent upon‚Äîand can therefore exceed‚Äîtheir ability to understand those same types of outputs. This contrasts with humans, for whom basic understanding almost always precedes the ability to generate expert-level outputs. We test this hypothesis through controlled experiments analyzing generation vs. understanding in generative models, across both language and image modalities.},
  author = {Peter West and Ximing Lu and Nouha Dziri and Faeze Brahman and Linjie Li and Jena D. Hwang and Liwei Jiang and Jillian Fisher and Abhilasha Ravichander and Khyathi Raghavi Chandu and Benjamin Newman and Pang Wei Koh and Allyson Ettinger and Yejin Choi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/west2024generative.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CF8H8MS5P8},
  publisher = {OpenReview.net},
  title = {The Generative {AI} Paradox: ``What It Can Create, It May Not Understand''},
  url = {https://openreview.net/forum?id=CF8H8MS5P8},
  year = {2024}
}

@inproceedings{whitney2024learning,
  abstract = {Realistic simulation is critical for applications ranging from robotics to animation, but traditional analytic simulators sometimes struggle to capture sufficiently realistic simulation, leading to problems including the well-known sim-to-real gap in robotics. Learned simulators have emerged as an alternative for better capturing real-world physical dynamics, but typically require access to privileged ground truth physics information such as precise object geometry or particle tracks. We propose a method for learning simulators directly from observations. We introduce Visual Particle Dynamics ({VPD}), which jointly learns a latent particle-based representation of {3D} scenes, a neural simulator of the latent particle dynamics, and a renderer that can produce images of the scene. {VPD} learns end-to-end from posed {RGB-D} videos and does not require access to privileged information. Unlike existing {2D} video prediction models, we show that {3D} inductive biases enable {VPD} to compositionally generalize and make long-term predictions.},
  author = {William F. Whitney and Tatiana Lopez-Guevara and Tobias Pfaff and Yulia Rubanova and Thomas Kipf and Kim Stachenfeld and Kelsey R. Allen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/whitney2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4rBEgZCubP},
  publisher = {OpenReview.net},
  title = {Learning {3D} Particle-based Simulators from {RGB-D} Videos},
  url = {https://openreview.net/forum?id=4rBEgZCubP},
  year = {2024}
}

@inproceedings{wiedemer2024provable,
  abstract = {Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. Object-centric representations are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We formalize compositional generalization as an identifiability problem for a latent variable model where objects are represented by latent slots. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.},
  author = {Thadd√§us Wiedemer and Jack Brady and Alexander Panfilov and Attila Juhos and Matthias Bethge and Wieland Brendel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wiedemer2024provable.pdf:pdf},
  note = {DBLP last modified: 2024-07-29. Oral presentation},
  pdf = {https://openreview.net/pdf?id=7VPTUWkiDQ},
  publisher = {OpenReview.net},
  title = {Provable Compositional Generalization for Object-Centric Learning},
  url = {https://openreview.net/forum?id=7VPTUWkiDQ},
  year = {2024}
}

@inproceedings{winchenbach2024symmetric,
  abstract = {Learning physical simulations has emerged as a powerful tool to address the computational challenges of classical numerical solvers, especially in applications involving Navier-Stokes-based fluid mechanics. While classical numerical solvers have traditionally been computationally expensive and challenging to use in inverse problems, neural solvers aim to address both concerns through machine learning. In this work, we propose a general formulation for continuous convolutions using separable basis functions as a superset of existing methods. We evaluate a large set of basis functions in the context of (a) a compressible 1D SPH simulation, (b) a weakly compressible 2D SPH simulation, and (c) an incompressible 2D SPH simulation. We demonstrate that Fourier-based continuous convolutions outperform all other architectures regarding accuracy and generalization, and show that even and odd symmetries included in the basis functions are key aspects of stability and accuracy. Using these Fourier-based networks, we show that prior inductive biases, such as window functions, are no longer necessary.},
  author = {Ren√© Winchenbach and Nils Thuerey},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/winchenbach2024symmetric.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HKgRwNhI9R},
  publisher = {OpenReview.net},
  title = {Symmetric Basis Convolutions for Learning {L}agrangian Fluid Mechanics},
  url = {https://openreview.net/forum?id=HKgRwNhI9R},
  year = {2024}
}

@inproceedings{wizadwongsa2024diffusion,
  abstract = {Despite the remarkable success of diffusion models in image generation, slow sampling remains a persistent issue. To accelerate the sampling process, prior studies have reformulated diffusion sampling as an ODE/SDE and introduced higher-order numerical methods. However, these methods often produce divergence artifacts, especially with a low number of sampling steps, which limits the achievable acceleration. In this paper, we investigate the potential causes of these artifacts and suggest that the small stability regions of these methods could be the principal cause. To address this issue, we propose two novel techniques. The first technique involves the incorporation of Heavy Ball momentum, a well-known technique for improving optimization, into existing diffusion numerical methods to expand their stability regions. We also prove that the resulting methods have first-order convergence. The second technique, called Generalized Heavy Ball, is a momentum-based sampling algorithm that achieves superior quality compared to previous momentum-based diffusion samplers.},
  author = {Suttisak Wizadwongsa and Worameth Chinchuthakun and Pramook Khungurn and Amit Raj and Supasorn Suwajanakorn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wizadwongsa2024diffusion.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HXc5aXeoc8},
  publisher = {OpenReview.net},
  title = {Diffusion Sampling with Momentum for Mitigating Divergence Artifacts},
  url = {https://openreview.net/forum?id=HXc5aXeoc8},
  year = {2024}
}

@inproceedings{wong2024learning,
  abstract = {Effective planning in the real world requires not only world knowledge, but the ability to leverage that knowledge to build the right representation of the task at hand. Decades of hierarchical planning techniques have used domain-specific temporal action abstractions to support efficient and accurate planning, almost always relying on human priors and domain knowledge to decompose hard tasks into smaller subproblems appropriate for a goal or set of goals. In this work, we describe Ada (Action Domain Acquisition), a framework for automatically constructing task-specific planning representations using task-general background knowledge from language models. Starting with a general-purpose hierarchical planner and a low-level goal-conditioned policy, Ada interactively learns a library of planner-compatible high-level action abstractions and low-level controllers adapted to a particular domain of planning tasks. On two language-guided interactive planning benchmarks (Mini Minecraft and ALFRED Household Tasks), Ada strongly outperforms other approaches that use language models for sequential decision-making, offering more accurate plans and better generalization to complex tasks.},
  author = {Lionel Wong and Jiayuan Mao and Pratyusha Sharma and Zachary S. Siegel and Jiahai Feng and Noa Korneev and Joshua B. Tenenbaum and Jacob Andreas},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wong2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qJ0Cfj4Ex9},
  publisher = {OpenReview.net},
  title = {Learning Grounded Action Abstractions from Language},
  url = {https://openreview.net/forum?id=qJ0Cfj4Ex9},
  year = {2024}
}

@inproceedings{woo2024alam,
  abstract = {One of the key challenges in deep neural network training is the substantial amount of GPU memory required to store activations obtained in the forward pass. Various Activation-Compressed Training (ACT) schemes have been proposed to mitigate this issue; however, it is challenging to adopt those approaches in recent transformer-based large language models (LLMs), which experience significant performance drops when the activations are deeply compressed during training. In this work, we introduce ALAM, a novel ACT framework that utilizes average quantization and a lightweight sensitivity calculation scheme, enabling large memory saving in LLMs while maintaining training performance. We first demonstrate that compressing activations into their group average values minimizes the gradient variance. Employing this property, we propose Average Quantization which provides high-quality deeply compressed activations with an effective precision of less than 1 bit and improved flexibility of precision allocation. We also present a cost-effective yet accurate sensitivity calculation algorithm that solely relies on the L2 norm of parameter gradients, substantially reducing memory overhead due to sensitivity calculation. In experiments, the ALAM framework significantly reduces activation memory without compromising accuracy, achieving up to a 10√ó compression rate in LLMs.},
  author = {Sunghyeon Woo and Sunwoo Lee and Dongsuk Jeon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/woo2024alam.pdf:pdf},
  note = {DBLP last modified: 2025-01-31},
  pdf = {https://openreview.net/pdf?id=OfXqQ5TRwp},
  publisher = {OpenReview.net},
  title = {{ALAM}: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models},
  url = {https://openreview.net/forum?id=OfXqQ5TRwp},
  year = {2024}
}

@inproceedings{wortsman2024smallscale,
  abstract = {Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training instability at smaller scales. We focus on two sources of training instability described in previous work: the growth of logits in attention layers and divergence of the output logits from the log probabilities. By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. We introduce a novel metric termed Learning Rate Sensitivity, which quantifies the expected deviation from an optimal loss when a model is trained with varied learning rates, and show that interventions like QK layer normalization and z-loss regularization mitigate these instabilities.},
  author = {Mitchell Wortsman and Peter J. Liu and Lechao Xiao and Katie E. Everett and Alexander A. Alemi and Ben Adlam and John D. Co-Reyes and Izzeddin Gur and Abhishek Kumar and Roman Novak and Jeffrey Pennington and Jascha Sohl-Dickstein and Kelvin Xu and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wortsman2024smallscale.pdf:pdf},
  note = {DBLP last modified: 2024-07-29. Oral presentation},
  pdf = {https://openreview.net/pdf?id=d8w0pmvXbZ},
  publisher = {OpenReview.net},
  title = {Small-scale proxies for large-scale {T}ransformer training instabilities},
  url = {https://openreview.net/forum?id=d8w0pmvXbZ},
  year = {2024}
}

@inproceedings{wu2024mapeppi,
  abstract = {Protein-Protein Interactions (PPIs) are fundamental in various biological processes and play a key role in life activities. The growing demand and cost of experimental PPI assays require computational methods for efficient PPI prediction. In this work, we propose Microenvironment-Aware Protein Embedding for PPI prediction (MAPE-PPI), which encodes microenvironments into chemically meaningful discrete codes via a sufficiently large microenvironment vocabulary (i.e., codebook). We define the microenvironment of an amino acid residue by its sequence and structural contexts, which describe the surrounding chemical properties and geometric features. We propose a novel pre-training strategy, namely Masked Codebook Modeling (MCM), to capture the dependencies between different microenvironments by randomly masking the codebook and reconstructing the input. With the learned microenvironment codebook, it can be reused as an off-the-shelf tool to efficiently and effectively encode proteins of different sizes and functions for large-scale PPI prediction. Extensive experiments show that MAPE-PPI can scale to PPI prediction with millions of PPIs with superior trade-offs between effectiveness and computational efficiency than the state-of-the-art competitors.},
  author = {Lirong Wu and Yijun Tian and Yufei Huang and Siyuan Li and Haitao Lin and Nitesh V. Chawla and Stan Z. Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024mapeppi.pdf:pdf},
  note = {DBLP last modified: 2024-11-22. Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=itGkF993gz},
  publisher = {OpenReview.net},
  title = {{MAPE-PPI}: Towards Effective and Efficient Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding},
  url = {https://openreview.net/forum?id=itGkF993gz},
  year = {2024}
}

@inproceedings{wu2024making,
  abstract = {Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). The algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by the idea of Thompson sampling. Our algorithm minimizes Bayesian regret bound and query complexity, again achieving a near-optimal tradeoff between these two quantities. Computation-wise, similar to the prior Thompson sampling algorithms under the regular RL setting, the main computation primitives of our algorithm are Bayesian supervised learning oracles.},
  author = {Runzhe Wu and Wen Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024making.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Pe2lo3QOvo},
  publisher = {OpenReview.net},
  title = {Making {RL} with Preference-based Feedback Efficient via Randomization},
  url = {https://openreview.net/forum?id=Pe2lo3QOvo},
  year = {2024}
}

@inproceedings{wu2024ebmdock,
  abstract = {Protein complex formation is a pivotal challenge in contemporary biology, with recent interest from the machine learning community, particularly concerning protein-ligand docking tasks. In this work, we focus on the equally crucial but comparatively under-investigated domain of protein-protein docking. We present EBMDock, a geometric deep learning framework that employs statistical potential as its energy function, producing a probability distribution over docking poses, such that the identified docking pose aligns with a minimum point in the energy landscape. EBMDock employs a differential algorithm grounded in Langevin dynamics to efficiently sample from the docking pose distribution and incorporates energy-based training using contrastive divergence, enhancing both performance and stability. Empirical results demonstrate that our approach achieves superior performance on two benchmark datasets DIPS and DB5.5, and the results suggest EBMDock can serve as an orthogonal enhancement to existing methods.},
  author = {Huaijin Wu and Wei Liu and Yatao Bian and Jiaxiang Wu and Nianzu Yang and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024ebmdock.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qg2boc2AwU},
  publisher = {OpenReview.net},
  title = {{EBMDock}: Neural Probabilistic Protein-Protein Docking via a Differentiable Energy Model},
  url = {https://openreview.net/forum?id=qg2boc2AwU},
  year = {2024}
}

@inproceedings{wu2024robust,
  abstract = {Recent developments in neural architecture search ({NAS}) emphasize the significance of considering robust architectures against malicious data. However, there is a notable absence of benchmark evaluations and theoretical guarantees for searching these robust architectures, especially when adversarial training is considered. In this work, we aim to address these two challenges, making twofold contributions. First, we release a comprehensive data set that encompasses both clean accuracy and robust accuracy for a vast array of adversarially trained networks from the {NAS-Bench-201} search space on image datasets. Then, leveraging the neural tangent kernel ({NTK}) tool from deep learning theory, we establish a generalization theory for searching architecture in terms of clean accuracy and robust accuracy under multi-objective adversarial training.},
  author = {Yongtao Wu and Fanghui Liu and Carl-Johann Simon-Gabriel and Grigorios Chrysos and Volkan Cevher},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024robust.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cdUpf6t6LZ},
  publisher = {OpenReview.net},
  title = {Robust {NAS} under Adversarial Training: Benchmark, Theory, and Beyond},
  url = {https://openreview.net/forum?id=cdUpf6t6LZ},
  year = {2024}
}

@inproceedings{wu2024doubly,
  abstract = {Proximal causal learning is a powerful framework for identifying the causal effect under the existence of unmeasured confounders. Within this framework, the doubly robust ({DR}) estimator was derived and has shown its effectiveness in estimation, especially when the model assumption is violated. However, the current form of the {DR} estimator is restricted to binary treatments, while the treatments can be continuous in many real-world applications. The primary obstacle to continuous treatments resides in the delta function present in the original {DR} estimator, making it infeasible in causal effect estimation and introducing a heavy computational burden in nuisance function estimation. To address these challenges, we propose a kernel-based {DR} estimator that can well handle continuous treatments for proximal causal learning. Equipped with its smoothness, we show that its oracle form is a consistent approximation of the influence function. Further, we propose a new approach to efficiently solve the nuisance functions. We then provide a comprehensive convergence analysis in terms of the mean square error. We demonstrate the utility of our estimator on synthetic datasets and real-world applications.},
  author = {Yong Wu and Yanwei Fu and Shouyan Wang and Xinwei Sun},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024doubly.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TjGJFkU3xL},
  publisher = {OpenReview.net},
  title = {Doubly Robust Proximal Causal Learning for Continuous Treatments},
  url = {https://openreview.net/forum?id=TjGJFkU3xL},
  year = {2024}
}

@inproceedings{wu2024incentiveaware,
  abstract = {In federated learning ({FL}), incentivizing contributions of training resources (e.g., data, compute) from potentially competitive clients is crucial. Existing incentive mechanisms often distribute post-training monetary rewards, which suffer from practical challenges of timeliness and feasibility of the rewards. Rewarding the clients after the completion of training may incentivize them to abort the collaboration, and monetizing the contribution is challenging in practice. To address these problems, we propose an incentive-aware algorithm that offers differentiated training-time model rewards for each client at each {FL} iteration. We theoretically prove that such a local design ensures the global objective of client incentivization. Through theoretical analyses, we further identify the issue of error propagation in model rewards and thus propose a stochastic reference-model recovery strategy to ensure theoretically that all the clients eventually obtain the optimal model performance.},
  author = {Zhaoxuan Wu and Mohammad Mohammadi Amiri and Ramesh Raskar and Bryan Kian Hsiang Low},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024incentiveaware.pdf:pdf},
  note = {{DBLP} last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=FlY7WQ2hWS},
  publisher = {OpenReview.net},
  title = {Incentive-Aware Federated Learning with Training-Time Model Rewards},
  url = {https://openreview.net/forum?id=FlY7WQ2hWS},
  year = {2024}
}

@inproceedings{wu2024topomlp,
  abstract = {Topology reasoning aims to comprehensively understand road scenes and present drivable routes in autonomous driving. It requires detecting road centerlines (lane) and traffic elements, further reasoning their topology relationship, i.e., lane-lane topology, and lane-traffic topology. In this work, we first present that the topology score relies heavily on detection performance on lane and traffic elements. Therefore, we introduce a powerful {3D} lane detector and an improved {2D} traffic element detector to extend the upper limit of topology performance. Further, we propose {TopoMLP}, a simple yet high-performance pipeline for driving topology reasoning. Based on the impressive detection performance, we develop two simple {MLP}-based heads for topology generation. {TopoMLP} achieves state-of-the-art performance on {OpenLane-V2} dataset, i.e., 41.2\% {OLS} with {ResNet-50} backbone. It is also the 1st solution for 1st {OpenLane} Topology in Autonomous Driving Challenge. We hope such simple and strong pipeline can provide some new insights to the community.},
  author = {Dongming Wu and Jiahao Chang and Fan Jia and Yingfei Liu and Tiancai Wang and Jianbing Shen},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024topomlp.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0gTW5JUFTW},
  publisher = {OpenReview.net},
  title = {{TopoMLP}: A Simple yet Strong Pipeline for Driving Topology Reasoning},
  url = {https://openreview.net/forum?id=0gTW5JUFTW},
  year = {2024}
}

@inproceedings{wu2024modulated,
  abstract = {To promote the safe deployment of object detectors, a task of unsupervised out-of-distribution object detection ({OOD-OD}) is recently proposed, aiming to detect unknown objects during training without reliance on any auxiliary {OOD} data. To alleviate the impact of lacking {OOD} data, for this task, one feasible solution is to exploit the known in-distribution ({ID}) data to synthesize proper {OOD} information for supervision, which strengthens detectors' discrimination. From the frequency perspective, since the phase generally reflects the content of the input, in this paper, we explore leveraging the phase of {ID} features to generate expected {OOD} features involving different content. And a method of Modulated Phase Diffusion ({MPD}) is proposed, containing a shared forward and two different reverse processes. Specifically, after calculating the phase of the extracted features, to prevent the rapid loss of content in the phase, the forward process gradually performs Gaussian Average on the phase instead of adding noise. The averaged phase and original amplitude are combined to obtain the features taken as the input of the reverse process. Next, one {OOD} branch is defined to synthesize virtual {OOD} features by continually enlarging the content discrepancy between the {OOD} features and original ones. Meanwhile, another modulated branch is designed to generate augmented features owning a similar phase as the original features by scaling and shifting the {OOD} branch. Both original and augmented features are used for training, enhancing the discrimination.},
  author = {Aming Wu and Cheng Deng},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024modulated.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gHAr7ZA1OL},
  publisher = {OpenReview.net},
  title = {Modulated Phase Diffusor: Content-Oriented Feature Synthesis for Detecting Unknown Objects},
  url = {https://openreview.net/forum?id=gHAr7ZA1OL},
  year = {2024}
}

@inproceedings{wu2024multimodal,
  abstract = {Multimodal patient representation learning aims to integrate information from multiple modalities and generate comprehensive patient representations for subsequent clinical predictive tasks. However, many existing approaches either presuppose the availability of all modalities and labels for each patient or only deal with missing modalities. In reality, patient data often comes with both missing modalities and labels for various reasons (i.e., the missing modality and label issue). Moreover, multimodal models might over-rely on certain modalities, causing sub-optimal performance when these modalities are absent (i.e., the modality collapse issue). To address these challenges, we propose {MUSE}, a mutual-consistent graph contrastive learning method that uses a flexible bipartite graph to represent the patient-modality relationship, which can adapt to various missing modality patterns. To tackle the modality collapse issue, {MUSE} learns to focus on modality-general and label-decisive features via a mutual-consistent contrastive learning loss. Notably, the unsupervised component of the contrastive objective only requires self-supervision signals, thereby broadening the training scope to incorporate patients with missing labels. We evaluate {MUSE} on three publicly available datasets: {MIMIC-IV}, {eICU}, and {ADNI}. Results show that {MUSE} outperforms all baselines, and {MUSE}+ further elevates the absolute improvement to $\sim$4\% by extending the training scope to patients with absent labels.},
  author = {Zhenbang Wu and Anant Dadu and Nicholas J. Tustison and Brian B. Avants and Mike A. Nalls and Jimeng Sun and Faraz Faghri},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024multimodal.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Je5SHCKpPa},
  publisher = {OpenReview.net},
  title = {Multimodal Patient Representation Learning with Missing Modalities and Labels},
  url = {https://openreview.net/forum?id=Je5SHCKpPa},
  year = {2024}
}

@inproceedings{wu2024vertibench,
  abstract = {Vertical Federated Learning ({VFL}) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world {VFL} datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting {VFL} performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real {VFL} dataset to address the deficit in image-image {VFL} scenarios. Our comprehensive evaluation of cutting-edge {VFL} algorithms provides valuable insights for future research in the field.},
  author = {Zhaomin Wu and Junyi Hou and Bingsheng He},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024vertibench.pdf:pdf},
  note = {{DBLP} last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=glwwbaeKm2},
  publisher = {OpenReview.net},
  title = {{VertiBench}: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks},
  url = {https://openreview.net/forum?id=glwwbaeKm2},
  year = {2024}
}

@inproceedings{wu2024stanhop,
  abstract = {We present {STanHop-Net} (Sparse Tandem Hopfield Network) for multivariate time series prediction with memory-enhanced capabilities. At the heart of our approach is {STanHop}, a novel Hopfield-based neural network block, which sparsely learns and stores both temporal and cross-series representations in a data-dependent fashion. In essence, {STanHop} sequentially learns temporal representation and cross-series representation using two tandem sparse Hopfield layers. Additionally, {STanHop} incorporates two external memory modules: Plug-and-Play and Tune-and-Play for train-less and task-aware memory enhancements, respectively. They allow {STanHop-Net} to swiftly respond to sudden events. From the theoretical perspective, we introduce a unified construction (Generalized Sparse Modern Hopfield Model) for both dense and sparse modern Hopfield models and show that it endows a tighter memory retrieval error compared to the dense counterpart without sacrificing memory capacity. We validate the efficacy of {STanHop-Net} on many settings: time series prediction, fast test-time adaptation, and strongly correlated time series prediction.},
  author = {Dennis Wu and Jerry Yao-Chieh Hu and Weijian Li and Bo-Yu Chen and Han Liu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024stanhop.pdf:pdf},
  note = {{DBLP} last modified: 2025-06-02},
  pdf = {https://openreview.net/pdf?id=6iwg437CZs},
  publisher = {OpenReview.net},
  title = {{STanHop}: Sparse Tandem Hopfield Model for Memory-Enhanced Time Series Prediction},
  url = {https://openreview.net/forum?id=6iwg437CZs},
  year = {2024}
}

@inproceedings{wu2024mixture,
  abstract = {Low-Rank Adaptation ({LoRA}) has emerged as a pivotal technique for fine-tuning large pre-trained models, renowned for its efficacy across a wide array of tasks. The modular architecture of {LoRA} has catalyzed further research into the synergistic composition of multiple trained {LoRAs}, aiming to amplify performance across various tasks. However, the effective composition of these trained {LoRAs} presents a formidable challenge: Linear arithmetic composition can lead to the diminution of the generative capabilities inherent in the original pre-trained models or the distinctive attributes of the individually trained {LoRAs}, potentially resulting in suboptimal outcomes. To address this challenge, we introduce Mixture of {LoRA} Experts ({MoLE}), a method that achieves a more efficient and flexible composition of multiple trained {LoRAs} by employing hierarchical weight control through learnable gating functions within each layer of trained {LoRAs}. Extensive experiments on both {V\&L} and {NLP} domain demonstrate that {MoLE} can enhance {LoRA} composition performance and mitigates issues associated with existing composition methods.},
  author = {Xun Wu and Shaohan Huang and Furu Wei},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024mixture.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=uWvKBCYh4S},
  publisher = {OpenReview.net},
  title = {Mixture of {LoRA} Experts},
  url = {https://openreview.net/forum?id=uWvKBCYh4S},
  year = {2024}
}

@inproceedings{wu2024meta,
  abstract = {Regularization-based methods have so far been among the \textit{de facto} choices for continual learning. Recent theoretical studies have revealed that these methods all boil down to relying on the Hessian matrix approximation of model weights. However, these methods suffer from suboptimal trade-offs between knowledge transfer and forgetting due to fixed and unchanging Hessian estimations during training. Another seemingly parallel strand of Meta-Continual Learning (Meta-{CL}) algorithms enforces alignment between gradients of previous tasks and that of the current task. In this work we revisit Meta-{CL} and for the first time bridge it with regularization-based methods. Concretely, Meta-{CL} implicitly approximates Hessian in an online manner, which enjoys the benefits of timely adaptation but meantime suffers from high variance induced by random memory buffer sampling. We are thus highly motivated to combine the best of both worlds, through the proposal of Variance Reduced Meta-{CL} ({VR-MCL}) to achieve both timely and accurate Hessian approximation. Through comprehensive experiments across three datasets and various settings, we consistently observe that {VR-MCL} outperforms other {SOTA} methods, which further validates the effectiveness of {VR-MCL}.},
  author = {Yichen Wu and Long-Kai Huang and Renzhen Wang and Deyu Meng and Ying Wei},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024},
  file = {:/home/b/documents/inproceedings/wu2024meta.pdf:pdf},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=TpD2aG1h0D},
  publisher = {OpenReview.net},
  title = {Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction},
  url = {https://openreview.net/forum?id=TpD2aG1h0D},
  year = {2024}
}

@inproceedings{wu2024privately,
  abstract = {Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We provide a differentially private framework for aligning LLMs with RL that mathematically guarantees that the final model satisfies DP over the entire course of the alignment process, consisting of multiple stages of model training and weight sharing. We show how to adapt the PPO algorithm to the DP setting and demonstrate the effectiveness of our approach on standard benchmarks.},
  archiveprefix = {arXiv},
  author = {Fan Wu and Huseyin A. Inan and Arturs Backurs and Varun Chandrasekaran and Janardhan Kulkarni and Robert Sim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.16960},
  file = {:/home/b/documents/inproceedings/wu2024privately.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3d0OmYTNui},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Privately Aligning Language Models with Reinforcement Learning},
  url = {https://openreview.net/forum?id=3d0OmYTNui},
  year = {2024}
}

@inproceedings{wu2024unleashing,
  abstract = {Generative pre-trained models have demonstrated remarkable effectiveness in language and vision domains by learning useful representations. In this paper, we extend the scope of this effectiveness by showing that visual robot manipulation can significantly benefit from large-scale video generative pre-training. We introduce GR-1, a GPT-style model designed for multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs a language instruction, a sequence of observation images, and a sequence of robot states. It predicts robot actions as well as future images in an end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly finetuned on robot data after pre-trained on a large-scale video dataset. On CALVIN benchmark, our method outperforms state-of-the-art baseline methods and improves the success rate from 88.9% to 94.9%. In the setting of zero-shot unseen scene generalization, GR-1 improves the success rate from 53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline methods and shows strong potential in generalization to unseen scenes and objects.},
  archiveprefix = {arXiv},
  author = {Hongtao Wu and Ya Jing and Chilam Cheang and Guangzeng Chen and Jiafeng Xu and Xinghang Li and Minghuan Liu and Hang Li and Tao Kong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2312.13139},
  file = {:/home/b/documents/inproceedings/wu2024unleashing.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NxoFmGgWC9},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation},
  url = {https://openreview.net/forum?id=NxoFmGgWC9},
  year = {2024}
}

@inproceedings{wu2024neural,
  abstract = {The Language of Thought Hypothesis suggests that human cognition operates on a structured, language-like system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such representations from non-linguistic general observations, like images, remains a challenge. In this work, we introduce the Neural Language of Thought Model (NLoTM), a novel approach for unsupervised learning of LoTH-inspired representation and generation. NLoTM comprises two key components: (1) the Semantic Vector-Quantized Variational Autoencoder (SVQ), which learns hierarchical, composable discrete representations aligned with objects and their properties, and (2) the Autoregressive LoT Prior (ALP), an autoregressive transformer that learns to generate semantic concept tokens compositionally, capturing the underlying data distribution. We evaluated our model on several 2D and 3D datasets including the challenging CLEVRTex dataset, showing superior downstream task performance and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations.},
  archiveprefix = {arXiv},
  author = {Yi-Fu Wu and Minseung Lee and Sungjin Ahn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2402.01203},
  file = {:/home/b/documents/inproceedings/wu2024neural.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HYyRwm367m},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Neural Language of Thought Models},
  url = {https://openreview.net/forum?id=HYyRwm367m},
  year = {2024}
}

@inproceedings{wu2024sin3dm,
  abstract = {In this paper, we present Sin3DM, a diffusion model that learns the internal patch distribution from a single 3D textured shape and generates high-quality variations with fine geometry and texture details. Training a diffusion model directly in 3D would induce large memory and computational cost. Therefore, we first compress the input into a lower-dimensional latent space and then train a diffusion model on it. Specifically, we encode the input 3D textured shape into triplane feature maps that represent the signed distance and texture fields of the input. We first train an autoencoder to compress the input 3D textured shape into triplane feature maps, which are three axis-aligned 2D feature maps that, together with the decoder, implicitly represent the signed distance and texture fields of the input. Then, we train a diffusion model on the triplane feature maps to learn the distribution of the latent features. Aside from randomly generating new samples, our model also facilitates applications such as retargeting, outpainting and local editing. Through extensive qualitative and quantitative evaluation, we show that our model can generate 3D shapes of various types with better quality than prior methods.},
  archiveprefix = {arXiv},
  author = {Rundi Wu and Ruoshi Liu and Carl Vondrick and Changxi Zheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2305.15399},
  file = {:/home/b/documents/inproceedings/wu2024sin3dm.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=U0IOMStUQ8},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape},
  url = {https://openreview.net/forum?id=U0IOMStUQ8},
  year = {2024}
}

@inproceedings{wu2024pae,
  abstract = {Human intelligence is adept at absorbing valuable insights from external knowledge. This capability is equally crucial for artificial intelligence. In contrast, classical reinforcement learning agents lack such capabilities and often resort to extensive trial and error to explore the environment. This paper introduces PAE: Planner-Actor-Evaluator, a novel framework for teaching agents to learn to absorb external knowledge. PAE integrates the Planner's knowledge-state alignment mechanism, the Actor's mutual information skill control, and the Evaluator's adaptive intrinsic exploration reward to achieve 1) effective cross-modal information fusion, 2) enhanced linkage between knowledge and state, and 3) hierarchical mastery of complex tasks. Comprehensive experiments across 11 challenging tasks from the BabyAI and MiniHack environment suites demonstrate PAE's superior exploration efficiency with good interpretability.},
  author = {Zhe Wu and Haofei Lu and Junliang Xing and You Wu and Renye Yan and Yaozhong Gan and Yuanchun Shi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024pae.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=R7rZUSGOPD},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {PAE: Reinforcement Learning from External Knowledge for Efficient Exploration},
  url = {https://openreview.net/forum?id=R7rZUSGOPD},
  year = {2024}
}

@inproceedings{wu2024bongardopenworld,
  abstract = {We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We further investigate to which extent the recently introduced Large Language Models (LLMs) and Vision-Language Models (VLMs) can solve our task, by directly probing VLMs, and combining VLMs and LLMs in an interactive reasoning scheme. We even conceived a neuro-symbolic reasoning approach that reconciles LLMs and VLMs with logical reasoning to emulate the human problem-solving process for Bongard Problems. However, none of these approaches manage to close the human-machine gap, as the best learner achieves 64% accuracy while human participants easily reach 91%. We hope Bongard-OpenWorld can help us better understand the limitations of current visual intelligence and facilitate future research on visual agents with stronger few-shot visual reasoning capabilities.},
  archiveprefix = {arXiv},
  author = {Rujie Wu and Xiaojian Ma and Zhenliang Zhang and Wei Wang and Qing Li and Song-Chun Zhu and Yizhou Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2310.10207},
  file = {:/home/b/documents/inproceedings/wu2024bongardopenworld.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-02-05},
  pdf = {https://openreview.net/pdf?id=hWS4MueyzC},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World},
  url = {https://openreview.net/forum?id=hWS4MueyzC},
  year = {2024}
}

@inproceedings{wu2024compositional,
  abstract = {Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We propose a novel formulation for inverse design as an energy optimization problem and introduce Compositional Inverse Design with Diffusion Models (CinDM) to enable generalization to out-of-distribution and more complex design inputs than seen in training, which outperforms existing works in n-body and 2D airfoil design. Through extensive experiments, we demonstrate that CinDM achieves superior performance compared to existing optimization-based methods and enables more robust and effective inverse design across different domains.},
  archiveprefix = {arXiv},
  author = {Tailin Wu and Takashi Maruyama and Long Wei and Tao Zhang and Yilun Du and Gianluca Iaccarino and Jure Leskovec},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2401.13171},
  file = {:/home/b/documents/inproceedings/wu2024compositional.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2025-06-02},
  pdf = {https://openreview.net/pdf?id=wmX0CqFSd7},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Compositional Generative Inverse Design},
  url = {https://openreview.net/forum?id=wmX0CqFSd7},
  year = {2024}
}

@inproceedings{wu2024privacypreserving,
  abstract = {In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We evaluate DP-ICL on four text classification benchmarks and two language generation tasks, and our empirical results show that DP-ICL achieves a strong utility-privacy tradeoff.},
  archiveprefix = {arXiv},
  author = {Tong Wu and Ashwinee Panda and Jiachen T. Wang and Prateek Mittal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2305.01639},
  file = {:/home/b/documents/inproceedings/wu2024privacypreserving.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=x4OPJ7lHVU},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Privacy-Preserving In-Context Learning for Large Language Models},
  url = {https://openreview.net/forum?id=x4OPJ7lHVU},
  year = {2024}
}

@inproceedings{wu2024extending,
  abstract = {Nature performs complex computations constantly at clearly lower cost and higher performance than digital computers. It is crucial to understand how to harness the unique computational power of nature in Machine Learning (ML). In the past decade, besides the development of Neural Networks (NNs), the community has also relentlessly explored nature-powered ML paradigms. Although most of them are still predominantly theoretical, a new practical paradigm enabled by the recent advent of CMOS-compatible room-temperature nature-based computers has emerged. By harnessing a dynamical system's intrinsic behavior of chasing the lowest energy state, this paradigm can solve some simple binary problems delivering considerable speedup and energy savings compared with NNs, while maintaining comparable accuracy. Regrettably, its values to the real world are highly constrained by its binary nature. A clear pathway to its extension to real-valued problems remains elusive. We demonstrate orders of magnitude of speedup and energy efficiency in Graph Learning compared to baseline GNNs by extending binary Ising machines to real-valued dynamical systems through hardware architecture and Hamiltonian co-design for graph learning problems.},
  author = {Chunshu Wu and Ruibing Song and Chuan Liu and Yunan Yang and Ang Li and Michael C. Huang and Tong Geng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024extending.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=qT7DXUmX7j},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {Extending Power of Nature from Binary to Real-Valued Graph Learning in Real World},
  url = {https://openreview.net/forum?id=qT7DXUmX7j},
  year = {2024}
}

@inproceedings{wu2024mofi,
  abstract = {We present MOFI, Manifold OF Images, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: 1. pre-training data, and 2. training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. It's a simple, cost-effective method that can scale to handle billions of web-mined image-text pairs. Through this method, we have created Image-to-Entities (I2E), a new dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes like supervised pre-training, contrastive pre-training, and multi-task learning. For contrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model.},
  archiveprefix = {arXiv},
  author = {Wentao Wu and Aleksei Timofeev and Chen Chen and Bowen Zhang and Kun Duan and Shuangning Liu and Yantao Zheng and Jonathon Shlens and Xianzhi Du and Yinfei Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2306.07952},
  file = {:/home/b/documents/inproceedings/wu2024mofi.pdf:pdf},
  month = {5},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QQYpgReSRk},
  publisher = {OpenReview.net},
  series = {ICLR 2024},
  title = {MOFI: Learning Image Representations from Noisy Entity Annotated Images},
  url = {https://openreview.net/forum?id=QQYpgReSRk},
  year = {2024}
}

@inproceedings{wu2024offpolicy,
  abstract = {Primal-dual safe RL methods commonly perform iterations between the primal update of the policy and the dual update of the Lagrange Multiplier. Such a training paradigm is highly susceptible to the error in cumulative cost estimation since this estimation serves as the key bond connecting the primal and dual update processes. We show that this problem causes significant underestimation of cost when using off-policy methods, leading to the failure to satisfy the safety constraint. To address this issue, we propose conservative policy optimization, which learns a policy in a constraint-satisfying area by considering the uncertainty in cost estimation. This improves constraint satisfaction but also potentially hinders reward maximization. We then introduce local policy convexification to help eliminate such suboptimality by gradually reducing the estimation uncertainty. We provide theoretical interpretations of the joint coupling effect of these two ingredients and further verify them by extensive experiments. Results on benchmark tasks show that our method not only achieves an asymptotic performance comparable to state-of-the-art on-policy methods while using much fewer samples, but also significantly reduces constraint violation during training.},
  author = {Zifan Wu and Bo Tang and Qian Lin and Chao Yu and Shangqin Mao and Qianlong Xie and Xingxing Wang and Dong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024offpolicy.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=vy42bYs1Wo},
  publisher = {OpenReview.net},
  title = {Off-Policy Primal-Dual Safe Reinforcement Learning},
  url = {https://openreview.net/forum?id=vy42bYs1Wo},
  year = {2024}
}

@inproceedings{wu2024smartplay,
  abstract = {Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies.},
  author = {Yue Wu and Xuan Tang and Tom M. Mitchell and Yuanzhi Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024smartplay.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=S2oTVrlcp3},
  publisher = {OpenReview.net},
  title = {SmartPlay: A Benchmark for {LLM}s as Intelligent Agents},
  url = {https://openreview.net/forum?id=S2oTVrlcp3},
  year = {2024}
}

@inproceedings{wu2024annealing,
  abstract = {In standard adversarial training, models are optimized to fit invariant one-hot labels for adversarial data when the perturbations are within allowable budgets. However, the overconfident target harms generalization and causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that reflects the underlying distribution of data. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.},
  author = {Yu-Yu Wu and Hung-Jui Wang and Shang-Tse Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024annealing.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=eT6oLkm1cm},
  publisher = {OpenReview.net},
  title = {Annealing Self-Distillation Rectification Improves Adversarial Training},
  url = {https://openreview.net/forum?id=eT6oLkm1cm},
  year = {2024}
}

@inproceedings{wu2024zeroshot,
  abstract = {Prompt tuning in natural language processing (NLP) has become an increasingly popular method for adapting large language models to specific tasks. However, the transferability of these prompts, especially continuous prompts, between different models remains a challenge. In this work, we propose a zero-shot continuous prompt transfer method, where source prompts are encoded into relative space and the corresponding target prompts are searched for transferring to target models. Experimental results confirm the effectiveness of our method, showing that 'task semantics' in continuous prompts can be generalized across various language models. Moreover, we find that combining 'task semantics' from multiple source models can further enhance the performance of transfer.},
  author = {Zijun Wu and Yongkang Wu and Lili Mou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024zeroshot.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=26XphugOcS},
  publisher = {OpenReview.net},
  title = {Zero-Shot Continuous Prompt Transfer: Generalizing Task Semantics Across Language Models},
  url = {https://openreview.net/forum?id=26XphugOcS},
  year = {2024}
}

@inproceedings{wu2024refusion,
  abstract = {Retrieval-based augmentations (RA) incorporating knowledge from an external database into language models have greatly succeeded in various knowledge-intensive (KI) tasks. However, integrating retrievals in non-knowledge-intensive (NKI) tasks is still challenging. Existing works focus on concatenating retrievals with inputs to improve model performance. Unfortunately, the use of retrieval concatenation-based augmentations causes an increase in the input length, substantially raising the computational demands of attention mechanisms. This paper proposes a new paradigm of RA named ReFusion, a computation-efficient Retrieval representation Fusion with bi-level optimization. Unlike previous works, ReFusion directly fuses the retrieval representations into the hidden states of models. Specifically, ReFusion leverages an adaptive retrieval integrator to seek the optimal combination of the proposed ranking schemes across different model layers. Experimental results demonstrate that the proposed ReFusion can achieve superior and robust performance in various NKI tasks.},
  author = {Shangyu Wu and Ying Xiong and Yufei Cui and Xue Liu and Buzhou Tang and Tei-Wei Kuo and Chun Jason Xue},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024refusion.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=JtKGkz9fAe},
  publisher = {OpenReview.net},
  title = {ReFusion: Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion},
  url = {https://openreview.net/forum?id=JtKGkz9fAe},
  year = {2024}
}

@inproceedings{wu2024embodied,
  abstract = {The vulnerability of deep neural networks to adversarial patches has motivated numerous defense strategies for boosting model robustness. However, the prevailing defenses depend on single observation or pre-established adversary information to counter adversarial patches, often failing to be confronted with unseen or adaptive adversarial attacks and easily exhibiting unsatisfying performance in dynamic 3D environments. Inspired by active human perception and recurrent feedback mechanisms, we develop Embodied Active Defense (EAD), a proactive defensive strategy that actively contextualizes environmental information to address misaligned adversarial patches in 3D real-world settings. These models recurrently process a series of beliefs and observations, facilitating progressive refinement of their comprehension of the target object and enabling the development of strategic actions to counter adversarial patches in 3D environments. To optimize learning efficiency, we incorporate a differentiable approximation of environmental dynamics and deploy patches that are agnostic to the adversary's strategies. Extensive experiments demonstrate that EAD substantially enhances robustness against a variety of patches within just a few steps through its action policy in safety-critical tasks (e.g., face recognition and object detection), without compromising standard accuracy. Furthermore, due to the attack-agnostic characteristic, EAD facilitates excellent generalization to unseen attacks, diminishing the averaged attack success rate by 95\% across a range of unseen adversarial attacks.},
  author = {Lingxuan Wu and Xiao Yang and Yinpeng Dong and Liuwei Xie and Hang Su and Jun Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024embodied.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=uXjfOmTiDt},
  publisher = {OpenReview.net},
  title = {Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches},
  url = {https://openreview.net/forum?id=uXjfOmTiDt},
  year = {2024}
}

@inproceedings{wu2024panodiffusion,
  abstract = {Generating complete 360-degree panoramas from narrow field of view images is ongoing research as omnidirectional RGB data is not readily available. Existing GAN-based approaches face some barriers to achieving higher quality output, and have poor generalization performance over different mask types. In this paper, we present our 360-degree indoor RGB-D panorama outpainting model using latent diffusion models (LDM), called PanoDiffusion. We introduce a new bi-modal latent diffusion structure that utilizes both RGB and depth panoramic data during training, which works surprisingly well to outpaint depth-free RGB images during inference. We further propose a novel technique of introducing progressive camera rotations during each diffusion denoising step, which leads to substantial improvement in achieving panorama wraparound consistency. Results show that our PanoDiffusion not only significantly outperforms state-of-the-art methods on RGB-D panorama outpainting by producing diverse well-structured results for different types of masks, but can also synthesize high-quality depth panoramas to provide realistic 3D indoor models.},
  author = {Tianhao Wu and Chuanxia Zheng and Tat-Jen Cham},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024panodiffusion.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=ZNzDXDFZ0B},
  publisher = {OpenReview.net},
  title = {PanoDiffusion: 360-degree Panorama Outpainting via Diffusion},
  url = {https://openreview.net/forum?id=ZNzDXDFZ0B},
  year = {2024}
}

@inproceedings{wu2024how,
  abstract = {Transformers pretrained on diverse tasks exhibit remarkable in-context learning (ICL) capabilities, enabling them to solve unseen tasks solely based on input contexts without adjusting model parameters. In this paper, we study ICL in one of its simplest setups: pretraining a single-layer linear attention model for linear regression with a Gaussian prior. We establish a statistical learning theory for this setting, providing finite-sample convergence guarantees and sample complexity bounds. Our theoretical analysis reveals how the number of pretraining tasks, the number of examples per task, task diversity, and model capacity jointly determine the ICL performance. Specifically, we prove that when the total number of pretraining samples is fixed, the optimal number of pretraining tasks grows as a power law of the total sample size, which matches empirical observations. We also identify the phase transition phenomenon: a large number of pretraining tasks is necessary for ICL to perform better than zero-shot learning.},
  author = {Jingfeng Wu and Difan Zou and Zixiang Chen and Vladimir Braverman and Quanquan Gu and Peter L. Bartlett},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024how.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=vSh5ePa0ph},
  publisher = {OpenReview.net},
  title = {How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  url = {https://openreview.net/forum?id=vSh5ePa0ph},
  year = {2024}
}

@inproceedings{wu2024clipself,
  abstract = {Open-vocabulary dense prediction tasks including object detection and image segmentation have been advanced by the success of Contrastive Language-Image Pre-training (CLIP). However, recent works have shown that simply transferring the image-level CLIP representations to dense prediction tasks leads to suboptimal performance due to the domain shift from global to local image patches. To address this issue, we propose CLIPSelf, a self-distillation approach that distills the knowledge from the global CLIP vision transformer to local patch representations. CLIPSelf leverages the rich semantic information in CLIP's global representation to guide the learning of local patch embeddings through a teacher-student framework. The teacher network processes the full image to produce global semantics, while the student network learns to predict local patch representations that are aligned with both the global context and local visual features. Through comprehensive experiments on multiple dense prediction benchmarks, we demonstrate that CLIPSelf significantly improves the transfer of CLIP representations to local tasks, achieving substantial performance gains over existing methods.},
  author = {Size Wu and Wenwei Zhang and Lumin Xu and Sheng Jin and Xiangtai Li and Wentao Liu and Chen Change Loy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wu2024clipself.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=DjzvJCRsVf},
  publisher = {OpenReview.net},
  title = {{CLIPSelf}: Vision Transformer Distills Itself for Open-Vocabulary Dense Prediction},
  url = {https://openreview.net/forum?id=DjzvJCRsVf},
  year = {2024}
}

@inproceedings{wuerkaixi2024accurate,
  abstract = {Federated continual learning combines the benefits of federated learning and continual learning but faces unique challenges due to heterogeneity across clients and the need to learn sequential tasks without forgetting. In this work, we propose a novel approach to address catastrophic forgetting in heterogeneous federated continual learning settings. Unlike traditional approaches that view forgetting as purely detrimental, we argue that selective forgetting can be beneficial for federated continual learning. We introduce the concept of "accurate forgetting" - a mechanism that allows clients to selectively retain useful knowledge while discarding task-irrelevant or potentially harmful information. Our method leverages local client knowledge and global server coordination to achieve better knowledge transfer and prevent catastrophic forgetting. Through comprehensive experiments on multiple benchmark datasets, we demonstrate that our approach significantly outperforms existing federated continual learning methods, achieving better knowledge retention and improved performance on both old and new tasks in heterogeneous federated settings.},
  author = {Abudukelimu Wuerkaixi and Sen Cui and Jingfeng Zhang and Kunda Yan and Bo Han and Gang Niu and Lei Fang and Changshui Zhang and Masashi Sugiyama},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/wuerkaixi2024accurate.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=ShQrnAsbPI},
  publisher = {OpenReview.net},
  title = {Accurate Forgetting for Heterogeneous Federated Continual Learning},
  url = {https://openreview.net/forum?id=ShQrnAsbPI},
  year = {2024}
}

@inproceedings{xia2024sheared,
  abstract = {The popularity of {LLaMA} and other recently emerged moderate-sized large language models ({LLMs}) highlights the potential of building smaller yet powerful {LLMs}. Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller {LLMs} from pre-trained, larger models. Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. The efficacy of the approach is demonstrated by presenting the {Sheared-LLaMA} series, pruning the {LLaMA2-7B} model down to 1.3B and 2.7B parameters. {Sheared-LLaMA} models outperform state-of-the-art open-source models of equivalent sizes, such as {Pythia}, {INCITE}, {OpenLLaMA} and the concurrent {TinyLlama} models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3\% of compute compared to training such models from scratch. This work provides compelling evidence that leveraging existing {LLMs} with structured pruning is a far more cost-effective approach for building competitive small-scale {LLMs}.},
  author = {Mengzhou Xia and Tianyu Gao and Zhiyuan Zeng and Danqi Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xia2024sheared.pdf:pdf},
  note = {DBLP last modified: 2024-10-17},
  pdf = {https://openreview.net/pdf?id=09iOdaeOzp},
  publisher = {OpenReview.net},
  title = {{Sheared LLaMA}: Accelerating Language Model Pre-training via Structured Pruning},
  url = {https://openreview.net/forum?id=09iOdaeOzp},
  year = {2024}
}

@inproceedings{xia2024mitigating,
  abstract = {Randomized Smoothing ({RS}) has emerged as a leading approach for obtaining certified robustness guarantees for deep neural networks. However, it faces the curse of dimensionality: as the input dimension increases, the upper bound of $\ell_2$ certified robustness radius exhibits a diminishing trend, proportionally decreasing at a rate of approximately $1/\sqrt{d}$, where $d$ represents the input dimension. To address this challenge, we propose Dual Randomized Smoothing ({DRS}), which enables high-dimensional input certification by down-sampling the input image and applying smoothing in the lower-dimensional space. {DRS} consists of two smoothing operations applied sequentially: the first smoothing operates on the down-sampled image, and the second smoothing is applied to the original high-dimensional input. We provide theoretical analysis showing that {DRS} can achieve a better certified robustness radius than traditional {RS}, especially in high-dimensional cases. Our extensive experiments on {CIFAR}-10 and {ImageNet} demonstrate that {DRS} significantly improves the certified robustness compared to existing methods. Particularly, on {ImageNet}, {DRS} achieves up to $5\times$ improvement in terms of certified robustness radius over the baseline randomized smoothing approach.},
  author = {Song Xia and Yi Yu and Xudong Jiang and Henghui Ding},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xia2024mitigating.pdf:pdf},
  note = {DBLP last modified: 2024-11-12},
  pdf = {https://openreview.net/pdf?id=C1sQBG6Sqp},
  publisher = {OpenReview.net},
  title = {Mitigating the Curse of Dimensionality for Certified Robustness via Dual Randomized Smoothing},
  url = {https://openreview.net/forum?id=C1sQBG6Sqp},
  year = {2024}
}

@inproceedings{xia2024gnncert,
  abstract = {Graph classification, which aims to predict a label for a graph, has many real-world applications such as malware detection, fraud detection, and healthcare. However, many studies show an attacker could carefully perturb the structure and/or node features in a graph such that a graph classifier misclassifies the perturbed graph. Such vulnerability impedes the deployment of graph classification in security/safety-critical applications. Existing empirical defenses lack formal robustness guarantees and could be broken by adaptive or unknown attacks. Existing provable defenses have the following limitations: 1) they achieve sub-optimal robustness guarantees for graph structure perturbation, 2) they cannot provide robustness guarantees for arbitrarily node feature perturbations, 3) their robustness guarantees are probabilistic, meaning they could be incorrect with a non-zero probability, and 4) they incur large computation costs. We aim to address those limitations in this work. We propose {GNNCert}, a certified defense against both graph structure and node feature perturbations for graph classification. Our {GNNCert} provably predicts the same label for a graph when the number of perturbed edges and the number of nodes with perturbed features are bounded. Our results on 8 benchmark datasets show that {GNNCert} outperforms three state-of-the-art methods.},
  author = {Zaishuo Xia and Han Yang and Binghui Wang and Jinyuan Jia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xia2024gnncert.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IGzaH538fz},
  publisher = {OpenReview.net},
  title = {{GNNCert}: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations},
  url = {https://openreview.net/forum?id=IGzaH538fz},
  year = {2024}
}

@inproceedings{xiang2024versvideo,
  abstract = {Creating stable, controllable videos is a complex task due to the need for significant variation in temporal dynamics and cross-frame temporal consistency. To address this, we enhance the spatial-temporal capability and introduce a versatile video generation model, {VersVideo}, which leverages textual, visual, and stylistic conditions. Current video diffusion models typically extend image diffusion architectures by supplementing 2D operations (such as convolutions and attentions) with temporal operations. While this approach is efficient, it often restricts spatial-temporal performance. To counter this, we incorporate two key elements: (1) multi-excitation paths for spatial-temporal convolutions with dimension pooling across different axes, and (2) multi-expert spatial-temporal attention blocks. These enhancements boost the model's spatial-temporal performance without significantly escalating training and inference costs. We also tackle the issue of information loss that arises when a variational autoencoder is used to transform pixel space into latent features and then back into pixel frames. To mitigate this, we incorporate temporal modules into the decoder to maintain inter-frame consistency. Lastly, by utilizing the innovative denoising {UNet} and decoder, we develop a unified {ControlNet} model suitable for various conditions, including image, {Canny}, {HED}, depth, and style.},
  author = {Jinxi Xiang and Ricong Huang and Jun Zhang and Guanbin Li and Xiao Han and Yang Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiang2024versvideo.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=K9sVJ17zvB},
  publisher = {OpenReview.net},
  title = {{VersVideo}: Leveraging Enhanced Temporal Diffusion Models for Versatile Video Generation},
  url = {https://openreview.net/forum?id=K9sVJ17zvB},
  year = {2024}
}

@inproceedings{xiang2024badchain,
  abstract = {Large language models ({LLMs}) are shown to benefit from chain-of-thought ({COT}) prompting, particularly when tackling tasks that require systematic reasoning processes. On the other hand, {COT} prompting also poses new vulnerabilities in the form of backdoor attacks, wherein the model will output unintended malicious content under specific backdoor-triggered conditions during inference. Traditional methods for launching backdoor attacks involve either contaminating the training dataset with backdoored instances or directly manipulating the model parameters during deployment. However, these approaches are not practical for commercial {LLMs} that typically operate via {API} access. In this paper, we propose {BadChain}, the first backdoor attack against {LLMs} employing {COT} prompting, which does not require access to the training dataset or model parameters and imposes low computational overhead. {BadChain} leverages the inherent reasoning capabilities of {LLMs} by inserting a backdoor reasoning step into the sequence of reasoning steps of the model output, thereby altering the final response when a backdoor trigger exists in the query prompt. In particular, a subset of demonstrations will be manipulated to incorporate a backdoor reasoning step in {COT} prompting. Consequently, given any query prompt containing the backdoor trigger, the {LLM} will be misled to output unintended content. Empirically, we show the effectiveness of {BadChain} for two {COT} strategies across four {LLMs} ({Llama2}, {GPT-3.5}, {PaLM2}, and {GPT-4}) and six complex benchmark tasks encompassing arithmetic, commonsense, and symbolic reasoning. We show that the baseline backdoor attacks designed for simpler tasks such as semantic classification will fail on these complicated tasks. Moreover, we show that {LLMs} endowed with stronger reasoning capabilities exhibit higher susceptibility to {BadChain}, exemplified by a high average attack success rate of 97.0\% across the six benchmark tasks on {GPT-4}. We also demonstrate the interpretability of {BadChain} by showing that the relationship between the trigger and the backdoor reasoning step can be well-explained based on the output of the backdoored model. Finally, we propose two defenses based on shuffling and demonstrate their overall ineffectiveness against {BadChain}. Therefore, {BadChain} remains a severe threat to {LLMs}, underscoring the urgency for the development of robust and effective future defenses.},
  author = {Zhen Xiang and Fengqing Jiang and Zidi Xiong and Bhaskar Ramasubramanian and Radha Poovendran and Bo Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiang2024badchain.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=c93SBwz1Ma},
  publisher = {OpenReview.net},
  title = {{BadChain}: Backdoor Chain-of-Thought Prompting for Large Language Models},
  url = {https://openreview.net/forum?id=c93SBwz1Ma},
  year = {2024}
}

@inproceedings{xiao2024fedloge,
  abstract = {Federated Long-Tailed Learning ({Fed-LT}), a paradigm wherein data collected from decentralized local clients manifests a globally prevalent long-tailed distribution, has garnered considerable attention in recent times. In the context of {Fed-LT}, existing works have predominantly centered on addressing the data imbalance issue to enhance the efficacy of the generic global model while neglecting the performance at the local level. In contrast, conventional Personalized Federated Learning ({pFL}) techniques are primarily devised to optimize personalized local models under the presumption of a balanced global data distribution. This paper introduces an approach termed Federated Local and Generic Model Training in {Fed-LT} ({FedLoGe}), which enhances both local and generic model performance through the integration of representation learning and classifier alignment within a neural collapse framework. Furthermore, leveraging insights from imbalance neural collapse's classifier norm patterns, we develop Global and Local Adaptive Feature Realignment ({GLA-FR}) via an auxiliary global classifier and personalized {Euclidean} norm transfer to align global features with client preferences. Extensive experimental results on {CIFAR}-10/100-{LT}, {ImageNet-LT}, and {iNaturalist} demonstrate the advantage of our method over state-of-the-art {pFL} and {Fed-LT} approaches.},
  author = {Zikai Xiao and Zihan Chen and Liyinglan Liu and Yang Feng and Joey Tianyi Zhou and Jian Wu and Wanlu Liu and Howard Hao Yang and Zuozhu Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024fedloge.pdf:pdf},
  note = {DBLP last modified: 2025-02-12},
  pdf = {https://openreview.net/pdf?id=V3j5d0GQgH},
  publisher = {OpenReview.net},
  title = {{FedLoGe}: Joint Local and Generic Federated Learning under Long-tailed Data},
  url = {https://openreview.net/forum?id=V3j5d0GQgH},
  year = {2024}
}

@inproceedings{xiao2024rb,
  abstract = {Recent text-to-image ({T2I}) diffusion models have achieved remarkable progress in generating high-quality images given text-prompts as input. However, these models fail to convey appropriate spatial composition specified by a layout instruction. In this work, we probe into zero-shot grounded {T2I} generation with diffusion models, that is, generating images corresponding to the input layout information without training auxiliary modules or finetuning diffusion models. We propose a Region and Boundary ({R\&B}) aware cross-attention guidance approach that gradually modulates the attention maps of diffusion model during generative process, and assists the model to synthesize images (1) with high fidelity, (2) highly compatible with textual input, and (3) interpreting layout instructions accurately. Specifically, we leverage the discrete sampling to bridge the gap between consecutive attention maps and discrete layout constraints, and design a region-aware loss to refine the generative layout during diffusion process. We further propose a boundary-aware loss to strengthen object discriminability within the corresponding regions. Experimental results show that our method outperforms existing state-of-the-art zero-shot grounded {T2I} generation methods by a large margin both qualitatively and quantitatively on several benchmarks.},
  author = {Jiayu Xiao and Henglei Lv and Liang Li and Shuhui Wang and Qingming Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024rb.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8Q4uVOJ5bX},
  publisher = {OpenReview.net},
  title = {{R\&B}: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation},
  url = {https://openreview.net/forum?id=8Q4uVOJ5bX},
  year = {2024}
}

@inproceedings{xiao2024gaformer,
  abstract = {Analyzing multivariate time series is important in many domains. However, it has been difficult to learn robust and generalizable representations within multivariate datasets due to complex inter-channel relationships and dynamic shifts. In this paper, we introduce a novel approach for learning spatiotemporal structure and using it to improve the application of transformers to timeseries datasets. Our framework learns a set of group tokens, and builds an instance-specific group embedding ({GE}) layer that assigns input tokens to a small number of group tokens to incorporate structure into learning. We introduce Group-Aware transFormer ({GAFormer}), which incorporates both spatial and temporal group embeddings to achieve state-of-the-art performance on various time series classification and regression tasks. Our approach represents a novel method for learning data-adaptive position embeddings to incorporate learned spatial and temporal structure into transformer architectures. Experimental results demonstrate that {GAFormer} outperforms existing methods on a diverse set of time series datasets, showing improvements in both accuracy and computational efficiency.},
  author = {Jingyun Xiao and Ran Liu and Eva L. Dyer},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024gaformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=c56TWtYp0W},
  publisher = {OpenReview.net},
  title = {{GAFormer}: Enhancing Timeseries Transformers Through Group-Aware Embeddings},
  url = {https://openreview.net/forum?id=c56TWtYp0W},
  year = {2024}
}

@inproceedings{xiao2024neuroinspired,
  abstract = {Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world in autonomous systems and cyber-physical systems. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception ({ITHP}) model, which utilizes the concept of information bottleneck. Different from most traditional fusion models that incorporate all modalities identically in neural networks, our model designates a prime modality and regards the remaining modalities as detectors in the information pathway, serving to distill the flow of information. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby substantially enhancing the performance of multimodal representation learning. Experimental evaluations on the {MUStARD}, {CMU-MOSI}, and {CMU-MOSEI} datasets demonstrate that our model consistently distills crucial information in multimodal learning scenarios, outperforming state-of-the-art benchmarks. Remarkably, on the {CMU-MOSI} dataset, {ITHP} surpasses human-level performance in the multimodal sentiment binary classification task across all evaluation metrics (i.e., Binary Accuracy, {F1} Score, Mean Absolute Error, and Pearson Correlation).},
  author = {Xiongye Xiao and Gengshuo Liu and Gaurav Gupta and Defu Cao and Shixuan Li and Yaxing Li and Tianqing Fang and Mingxi Cheng and Paul Bogdan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024neuroinspired.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Z9AZsU1Tju},
  publisher = {OpenReview.net},
  title = {Neuro-Inspired Information-Theoretic Hierarchical Perception for Multimodal Learning},
  url = {https://openreview.net/forum?id=Z9AZsU1Tju},
  year = {2024}
}

@inproceedings{xiao2024bert,
  abstract = {Language modeling at scale has proven very effective and brought unprecedented success to natural language models. Many typical representatives, especially decoder-only models, e.g., {BLOOM} and {LLaMA}, and encoder-decoder models, e.g., {Flan-T5} and {AlexaTM}, have exhibited incredible instruction-following capabilities while keeping strong task completion ability. These large language models can achieve superior performance in various tasks and even yield emergent capabilities, e.g., reasoning and universal generalization. In this work, we adopt {XML-R} to explore the effectiveness of the {BERT} family for instruction following and zero-shot learning. We first design a simple yet effective strategy to utilize the encoder-only models for generation tasks and then conduct multi-task instruction tuning. Experimental results demonstrate that our fine-tuned model, {Instruct-XMLR}, outperforms {Bloomz} on all evaluation tasks and achieves comparable performance with {mT0} on most tasks. Surprisingly, {Instruct-XMLR} also possesses strong task and language generalization abilities, indicating that {Instruct-XMLR} can also serve as a good instruction follower and zero-shot learner. Besides, {Instruct-XMLR} can accelerate decoding due to its non-autoregressive generation manner, achieving around 3 times speedup compared with current autoregressive large language models.},
  author = {Yisheng Xiao and Juntao Li and Zechen Sun and Zechang Li and Qingrong Xia and Xinyu Duan and Zhefeng Wang and Min Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024bert.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=x8VNtpCu1I},
  publisher = {OpenReview.net},
  title = {Are {BERT} Family Good Instruction Followers? {A} Study on Their Potential And Limitations},
  url = {https://openreview.net/forum?id=x8VNtpCu1I},
  year = {2024}
}

@inproceedings{xiao2024efficient,
  abstract = {Deploying Large Language Models ({LLMs}) in streaming applications such as multi-round dialogue, where long interactions are expected, is challenging due to extensive memory consumption from caching previous tokens' Key and Value states ({KV}) and the inability of popular {LLMs} to generalize to longer texts than the training sequence length. We observe the attention sink phenomenon: keeping the {KV} of initial tokens will largely recover the performance of window attention. Based on this analysis, we introduce {StreamingLLM}, an efficient framework that enables {LLMs} trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that {StreamingLLM} can enable Llama-2, {MPT}, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more, with up to 22.2√ó speedup over sliding window recomputation baseline.},
  author = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024efficient.pdf:pdf},
  note = {{DBLP} last modified: 2025-02-03},
  pdf = {https://openreview.net/pdf?id=NG7sS51zVF},
  publisher = {OpenReview.net},
  title = {Efficient Streaming Language Models with Attention Sinks},
  url = {https://openreview.net/forum?id=NG7sS51zVF},
  year = {2024}
}

@inproceedings{xiao2024unified,
  abstract = {Human-Scene Interaction ({HSI}) is a vital component of fields like embodied {AI} and virtual reality. Despite advancements in motion quality and physical plausibility, two pivotal factors, versatile interaction control and the development of a user-friendly interface, require further exploration before the practical application of {HSI}. This paper presents a unified {HSI} framework, {UniHSI}, which supports unified control of diverse interactions through language commands. {UniHSI} defines interaction as "Chain of Contacts ({CoC}): steps of human joint-object part pairs, which is inspired by the strong correlation between interaction types and human-object contact regions. Based on the definition, {UniHSI} constitutes a Large Language Model ({LLM}) Planner to translate language prompts into task plans in the form of {CoC}, and a Unified Controller that turns {CoC} into uniform task execution. To facilitate training and evaluation, we collect a new dataset named {ScenePlan} that encompasses thousands of task plans generated by {LLMs} based on diverse scenarios. Comprehensive experiments demonstrate the effectiveness of our framework in versatile task execution and generalizability to real scanned scenes.},
  author = {Zeqi Xiao and Tai Wang and Jingbo Wang and Jinkun Cao and Wenwei Zhang and Bo Dai and Dahua Lin and Jiangmiao Pang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024unified.pdf:pdf},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=1vCnDyQkjg},
  publisher = {OpenReview.net},
  title = {Unified Human-Scene Interaction via Prompted Chain-of-Contacts},
  url = {https://openreview.net/forum?id=1vCnDyQkjg},
  year = {2024}
}

@inproceedings{xiao2024chainofexperts,
  abstract = {We study automatic modeling and programming for complex operation research ({OR}) problems, so as to alleviate the heavy dependence on domain experts and benefit a spectrum of industry sectors. We present Chain-of-Experts ({CoE}), a novel multi-agent cooperative framework to enhance reasoning capabilities for addressing complex {OR} problems using large language models ({LLMs}). Each agent is assigned a specific role and endowed with domain knowledge related to {OR}. The framework includes a conductor to orchestrate these agents via forward thought construction and backward reflection mechanism. To facilitate {OR} research and community development, we release a benchmark dataset ({ComplexOR}) of complex {OR} problems. Experimental results show that {CoE} significantly outperforms the state-of-the-art {LLM}-based approaches both on {LPWP} and {ComplexOR}.},
  author = {Ziyang Xiao and Dongxiang Zhang and Yangjun Wu and Lilin Xu and Yuan Jessica Wang and Xiongwei Han and Xiaojin Fu and Tao Zhong and Jia Zeng and Mingli Song and Gang Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiao2024chainofexperts.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HobyL1B9CZ},
  publisher = {OpenReview.net},
  title = {Chain-of-Experts: When {LLMs} Meet Complex Operations Research Problems},
  url = {https://openreview.net/forum?id=HobyL1B9CZ},
  year = {2024}
}

@inproceedings{xie2024selfguided,
  abstract = {Extending self-supervised learning to new modalities is non-trivial because the specifics of existing methods are tailored to each domain, such as domain-specific augmentations which reflect the invariances in the target task. While masked modeling is a promising domain-agnostic framework for self-supervised learning since it does not rely on input augmentations, its mask sampling procedure is nevertheless domain-specific. We present Self-guided Masked Autoencoders ({SMA}), a fully domain-agnostic masked modeling method. {SMA} trains an attention-based model using a masked modeling objective, by learning masks to sample without any domain-specific assumptions. The main idea behind {SMA} is that masking inputs based on a model's attention map can create a strong, domain-agnostic masking strategy for masked modeling. We evaluate {SMA} on three self-supervised learning benchmarks in protein biology, chemical property prediction, and particle physics, and show that {SMA} achieves state-of-the-art performance while remaining domain-agnostic.},
  author = {Johnathan Xie and Yoonho Lee and Annie S. Chen and Chelsea Finn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xie2024selfguided.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=HiYMiZYwkw},
  publisher = {OpenReview.net},
  title = {Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning},
  url = {https://openreview.net/forum?id=HiYMiZYwkw},
  year = {2024}
}

@inproceedings{xie2024adaptive,
  abstract = {Large language models are shown to store extensive world knowledge in their parameters. However, how they utilize this parametric memory in the face of external evidence that conflicts with it remains understudied. This paper presents the first comprehensive and controlled investigation into the behavior of {LLMs} when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from {LLMs} and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals that {LLMs} can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. We show that {LLMs} demonstrate adaptive behavior in handling knowledge conflicts: they are more likely to rely on external evidence when their parametric memory is uncertain or when the external evidence is of higher quality. However, we also identify seemingly contradictory behaviors of {LLMs}, suggesting room for improvement in their knowledge integration mechanisms.},
  author = {Jian Xie and Kai Zhang and Jiangjie Chen and Renze Lou and Yu Su},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xie2024adaptive.pdf:pdf},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=auKAUJZMO6},
  publisher = {OpenReview.net},
  title = {Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts},
  url = {https://openreview.net/forum?id=auKAUJZMO6},
  year = {2024}
}

@inproceedings{xie2024enhancing,
  abstract = {Learning neural subset selection tasks, such as compound selection in {AI}-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. We term this process background information integration. We validate our theoretical insights through comprehensive experiments on both synthetic and real-world datasets, including compound selection for drug discovery, demonstrating the effectiveness of our approach in enhancing neural subset selection performance.},
  author = {Binghui Xie and Yatao Bian and Kaiwen Zhou and Yongqiang Chen and Peilin Zhao and Bo Han and Wei Meng and James Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xie2024enhancing.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=eepoE7iLpL},
  publisher = {OpenReview.net},
  title = {Enhancing Neural Subset Selection: Integrating Background Information into Set Representations},
  url = {https://openreview.net/forum?id=eepoE7iLpL},
  year = {2024}
}

@inproceedings{xie2024omnicontrol,
  abstract = {We consider the problem of incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, our approach can incorporate flexible spatial control signals over different joints at different times with only one model. We propose {OmniControl}, which consists of two key technical contributions. First, we introduce analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. Second, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. Experiments on {HumanML3D} and {KIT-ML} datasets show that {OmniControl} not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints.},
  author = {Yiming Xie and Varun Jampani and Lei Zhong and Deqing Sun and Huaizu Jiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xie2024omnicontrol.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gd0lAEtWso},
  publisher = {OpenReview.net},
  title = {{OmniControl}: Control Any Joint at Any Time for Human Motion Generation},
  url = {https://openreview.net/forum?id=gd0lAEtWso},
  year = {2024}
}

@inproceedings{xie2024information,
  abstract = {The information bottleneck principle provides an information-theoretic method for learning a good representation as a trade-off between conciseness and predictive ability, which can reduce information redundancy, eliminate irrelevant and superfluous features, and thus enhance the in-domain generalizability. However, in low-resource or out-of-domain scenarios where the assumption of i.i.d does not necessarily hold true, superfluous (or redundant) relevant features may be supplemental to the mainline features of the model, and be beneficial in making prediction for test dataset with distribution shift. Therefore, instead of squeezing the input information by information bottleneck, we propose to keep as much relevant information as possible in use for making predictions. A three-stage supervised learning framework is designed and implemented to jointly learn the mainline and supplemental features, relieving supplemental features from the suppression of mainline features. Extensive experiments have shown that the learned representations of our method have good in-domain and out-of-domain generalization abilities, especially in low-resource cases.},
  author = {Zhipeng Xie and Yahe Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xie2024information.pdf:pdf},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=o83eu4H9Mb},
  publisher = {OpenReview.net},
  title = {Information Retention via Learning Supplemental Features},
  url = {https://openreview.net/forum?id=o83eu4H9Mb},
  year = {2024}
}

@inproceedings{xie2024badexpert,
  abstract = {We present a novel defense, against backdoor attacks on Deep Neural Networks ({DNNs}), wherein adversaries covertly implant malicious behaviors (backdoors) into {DNNs}. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by an ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert (Backdoor Input Detection with Backdoor Expert), effectively mitigates 17 {SOTA} backdoor attacks while minimally impacting clean utility. The effectiveness of BaDExpert has been verified on multiple datasets ({CIFAR10}, {GTSRB} and {ImageNet}) across various model architectures ({ResNet}, {VGG}, {MobileNetV2} and Vision Transformer).},
  author = {Tinghao Xie and Xiangyu Qi and Ping He and Yiming Li and Jiachen T. Wang and Prateek Mittal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xie2024badexpert.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=s56xikpD92},
  publisher = {OpenReview.net},
  title = {BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection},
  url = {https://openreview.net/forum?id=s56xikpD92},
  year = {2024}
}

@inproceedings{xie2024text2reward,
  abstract = {Designing reward functions is a longstanding challenge in reinforcement learning ({RL}); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation and shaping of dense reward functions based on large language models ({LLMs}). Given a goal described in natural language, Text2Reward generates shaped dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse {RL} and recent work that uses {LLMs} to write sparse reward codes or unshaped dense rewards with a constant function across timesteps, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks ({ManiSkill2}, {MetaWorld}) and two locomotion environments of {MuJoCo}. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better task success rates and convergence speed than expert-written reward codes. For locomotion tasks, our method learns six novel locomotion behaviors with a success rate exceeding 94\%. Furthermore, we show that the policies trained in the simulator with our method can be deployed in the real world. Finally, Text2Reward further improves the policies by refining their reward functions with human feedback. Video results are available at https://text-to-reward.github.io/.},
  author = {Tianbao Xie and Siheng Zhao and Chen Henry Wu and Yitao Liu and Qian Luo and Victor Zhong and Yanchao Yang 0001 and Tao Yu 0009},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xie2024text2reward.pdf:pdf},
  note = {{DBLP} last modified: 2025-01-17},
  pdf = {https://openreview.net/pdf?id=tUM39YTRxH},
  publisher = {OpenReview.net},
  title = {Text2Reward: Reward Shaping with Language Models for Reinforcement Learning},
  url = {https://openreview.net/forum?id=tUM39YTRxH},
  year = {2024}
}

@inproceedings{xiong2024structured,
  abstract = {Existing video-language pre-training methods primarily focus on instance-level alignment between video clips and captions via global contrastive learning but neglect rich fine-grained local information in both videos and text, which is of importance to downstream tasks requiring temporal localization and semantic reasoning. A powerful model is expected to be capable of capturing region-object correspondences and recognizing scene changes in a video clip, reflecting spatial and temporal granularity, respectively. To strengthen model's understanding into such fine-grained details, we propose a simple yet effective video-language modeling framework, S-ViLM, by exploiting the intrinsic structures of these two modalities. It includes two novel designs, inter-clip spatial grounding and intra-clip temporal grouping, to promote learning region-object alignment and temporal-aware features, simultaneously. Comprehensive evaluations demonstrate that S-ViLM performs favorably against existing approaches in learning more expressive representations. Specifically, S-ViLM surpasses the state-of-the-art methods substantially on four representative downstream tasks, covering text-video retrieval, video question answering, video action recognition, and temporal action localization.},
  author = {Yuanhao Xiong and Long Zhao and Boqing Gong and Ming-Hsuan Yang and Florian Schroff and Ting Liu and Cho-Jui Hsieh and Liangzhe Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiong2024structured.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5dlfiJIXoh},
  publisher = {OpenReview.net},
  title = {Structured Video-Language Modeling with Temporal Grouping and Spatial Grounding},
  url = {https://openreview.net/forum?id=5dlfiJIXoh},
  year = {2024}
}

@inproceedings{xiong2024how,
  abstract = {This paper rigorously shows how over-parameterization changes the convergence behaviors of gradient descent (GD) for the matrix sensing problem, where the goal is to recover an unknown low-rank ground-truth matrix from near-isotropic linear measurements. First, we consider the symmetric setting with the symmetric parameterization where $M^* \in \mathbb{R}^{n \times n}$ is a positive semi-definite unknown matrix of rank $r \ll n$, and one uses a symmetric parameterization $XX^\top$ to learn $M^*$. Here $X \in \mathbb{R}^{n \times k}$ with $k > r$ is the factor matrix. We give a novel $\Omega(1/T^2)$ lower bound of randomly initialized GD for the over-parameterized case ($k >r$) where $T$ is the number of iterations. This is in stark contrast to the exact-parameterization scenario ($k=r$) where the convergence rate is $\exp(-\Omega(T))$. Next, we study asymmetric setting where $M^* \in \mathbb{R}^{n_1 \times n_2}$ is the unknown matrix of rank $r \ll \min\{n_1,n_2\}$, and one uses an asymmetric parameterization $FG^\top$ to learn $M^*$ where $F \in \mathbb{R}^{n_1 \times k}$ and $G \in \mathbb{R}^{n_2 \times k}$. Building on prior work, we give a global exact convergence result of randomly initialized GD for the exact-parameterization case ($k=r$) with an $\exp(-\Omega(T))$ rate. Furthermore, we give the first global exact convergence result for the over-parameterization case ($k>r$) with an $\exp(-\Omega(\alpha^2 T))$ rate where $\alpha$ is the initialization scale. This linear convergence result in the over-parameterization case is especially significant because one can apply the asymmetric parameterization to the symmetric setting to speed up from $\Omega(1/T^2)$ to linear convergence.},
  author = {Nuoya Xiong and Lijun Ding and Simon Shaolei Du},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiong2024how.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=xGvPKAiOhq},
  publisher = {OpenReview.net},
  title = {How Over-Parameterization Slows Down Gradient Descent in Matrix Sensing: The Curses of Symmetry and Initialization},
  url = {https://openreview.net/forum?id=xGvPKAiOhq},
  year = {2024}
}

@inproceedings{xiong2024can,
  abstract = {The task of empowering large language models (LLMs) to accurately express their confidence, referred to as confidence elicitation, is essential in ensuring reliable and trustworthy decision-making processes. Previous methods, which primarily rely on model logits, have become less suitable for LLMs and even infeasible with the rise of closed-source LLMs (e.g., commercialized LLM APIs). This leads to a growing need to explore the untapped area of non-logit-based approaches to estimate the uncertainty of LLMs. Hence, in this study, we investigate approaches for confidence elicitation that do not require model fine-tuning or access to proprietary information. We introduce three categories of methods: verbalize-based, consistency-based, and their hybrid methods for benchmarking, and evaluate their performance across five types of datasets and four widely-used LLMs. Our analysis of these methods uncovers several key insights: 1) LLMs often exhibit a high degree of overconfidence when verbalizing their confidence; 2) Prompting strategies such as CoT, Top-K and Multi-step confidences improve calibration of verbalized confidence; 3) Consistency-based methods outperform the verbalized confidences in most cases, with particularly notable improvements on the arithmetic reasoning task; 4) Hybrid methods consistently deliver the best performance over their baselines, thereby emerging as a promising state-of-the-art approach; 5) Despite these advancements, all investigated methods continue to struggle with challenging tasks, such as those requiring professional knowledge, leaving significant scope for improvement of confidence elicitation.},
  author = {Miao Xiong and Zhiyuan Hu and Xinyang Lu and Yifei Li and Jie Fu and Junxian He and Bryan Hooi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiong2024can.pdf:pdf},
  note = {DBLP last modified: 2024-09-19},
  pdf = {https://openreview.net/pdf?id=gjeQKFxFpZ},
  publisher = {OpenReview.net},
  title = {Can {LLM}s Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in {LLM}s},
  url = {https://openreview.net/forum?id=gjeQKFxFpZ},
  year = {2024}
}

@inproceedings{xiong2024sampleefficient,
  abstract = {We study multi-agent reinforcement learning (MARL) for the general-sum Markov Games (MGs) under the general function approximation. In order to find the minimum assumption for sample-efficient learning, we introduce a novel complexity measure called the Multi-Agent Decoupling Coefficient (MADC) for general-sum MGs. Using this measure, we propose the first unified algorithmic framework that ensures sample efficiency in learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium for both model-based and model-free MARL problems with low MADC. We also show that our algorithm provides comparable sublinear regret to the existing works. Moreover, our algorithm combines an equilibrium-solving oracle with a single objective optimization subprocedure that solves for the regularized payoff of each deterministic joint policy, thus being more amenable to empirical implementation.},
  author = {Nuoya Xiong and Zhihan Liu and Zhaoran Wang and Zhuoran Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiong2024sampleefficient.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=o7qhUMylLU},
  publisher = {OpenReview.net},
  title = {Sample-Efficient Multi-Agent {RL}: An Optimization Perspective},
  url = {https://openreview.net/forum?id=o7qhUMylLU},
  year = {2024}
}

@inproceedings{xiong2024dqlore,
  abstract = {Recent advances in natural language processing, primarily propelled by Large Language Models (LLMs), have showcased their remarkable capabilities grounded in in-context learning. A promising avenue for guiding LLMs in intricate reasoning tasks involves the utilization of intermediate reasoning steps within the Chain-of-Thought (CoT) paradigm. Nevertheless, the central challenge lies in the effective selection of exemplars for facilitating in-context learning. In this study, we introduce a framework that leverages Dual Queries and Low-rank approximation Re-ranking (DQ-LoRe) to automatically select exemplars for in-context learning. Dual Queries first query LLM to obtain LLM-generated knowledge such as CoT, then query the retriever to obtain the final exemplars via both question and the knowledge. Moreover, for the second query, LoRe employs dimensionality reduction techniques to refine exemplar selection, ensuring close alignment with the input question's knowledge. Through extensive experiments, we demonstrate that DQ-LoRe significantly outperforms prior state-of-the-art methods in the automatic selection of exemplars for GPT-4, enhancing performance from 92.5% to 94.2%. Our comprehensive analysis further reveals that DQ-LoRe consistently outperforms retrieval-based approaches in terms of both performance and adaptability, especially in scenarios characterized by distribution shifts.},
  author = {Jing Xiong and Zixuan Li and Chuanyang Zheng and Zhijiang Guo and Yichun Yin and Enze Xie and Zhicheng Yang and Qingxing Cao and Haiming Wang and Xiongwei Han and Jing Tang and Chengming Li and Xiaodan Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xiong2024dqlore.pdf:pdf},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=qAoxvePSlq},
  publisher = {OpenReview.net},
  title = {{DQ-LoRe}: Dual Queries with Low Rank Approximation Re-ranking for In-Context Learning},
  url = {https://openreview.net/forum?id=qAoxvePSlq},
  year = {2024}
}

@inproceedings{xu2024dmv3d,
  abstract = {We propose DMV3D, a novel 3D generation approach that uses a transformer-based 3D large reconstruction model to denoise multi-view diffusion. Our reconstruction model incorporates a triplane NeRF representation and, functioning as a denoiser, can denoise noisy multi-view images via 3D NeRF reconstruction and rendering, achieving single-stage 3D generation in the 2D diffusion denoising process. We train DMV3D on large-scale multi-view image datasets of extremely diverse objects using only image reconstruction losses, without accessing 3D assets. We demonstrate state-of-the-art results for the single-image reconstruction problem where probabilistic modeling of unseen object parts is required for generating diverse reconstructions with sharp textures. We also show high-quality text-to-3D generation results outperforming previous 3D diffusion models. DMV3D can generate diverse high-fidelity 3D objects in ~30 seconds per asset on single A100 GPU, being much more efficient than previous methods.},
  author = {Yinghao Xu and Hao Tan and Fujun Luan and Sai Bi and Peng Wang and Jiahao Li and Zifan Shi and Kalyan Sunkavalli and Gordon Wetzstein and Zexiang Xu and Kai Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024dmv3d.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=H4yQefeXhp},
  publisher = {OpenReview.net},
  title = {{DMV3D}: Denoising Multi-view Diffusion Using {3D} Large Reconstruction Model},
  url = {https://openreview.net/forum?id=H4yQefeXhp},
  year = {2024}
}

@inproceedings{xu2024paradigm,
  abstract = {Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our results show that the model can achieve an average improvement of more than 12 BLEU and 12 COMET over its zero-shot performance across 10 translation directions from the WMT'21 (2 directions) and WMT'22 (8 directions) test datasets. The performance is significantly better than all prior work and even superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or 13B parameters. This method establishes the foundation for a novel training paradigm in machine translation.},
  author = {Haoran Xu and Young Jin Kim and Amr Sharaf and Hany Hassan Awadalla},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024paradigm.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=farT6XXntP},
  publisher = {OpenReview.net},
  title = {A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models},
  url = {https://openreview.net/forum?id=farT6XXntP},
  year = {2024}
}

@inproceedings{xu2024scaling,
  abstract = {The capacity of a modern deep learning system to determine if a sample falls within its realm of knowledge is fundamental and important. In this paper, we offer insights and analyses of recent state-of-the-art out-of-distribution (OOD) detection methods - extremely simple activation shaping (ASH). We demonstrate that activation pruning has a detrimental effect on OOD detection, while activation scaling enhances it. Moreover, we propose SCALE, a simple yet effective post-hoc network enhancement method for OOD detection, which attains state-of-the-art OOD detection performance without compromising in-distribution (ID) accuracy. By integrating scaling concepts into the training process to capture a sample's ID characteristics, we propose Intermediate Tensor SHaping (ISH), a lightweight method for training time OOD detection enhancement. We achieve AUROC scores of +1.85% for near-OOD and +0.74% for far-OOD datasets on the OpenOOD v1.5 ImageNet-1K benchmark.},
  author = {Kai Xu and Rongyu Chen and Gianni Franchi and Angela Yao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024scaling.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RDSTjtnqCg},
  publisher = {OpenReview.net},
  title = {Scaling for Training Time and Post-hoc Out-of-distribution Detection Enhancement},
  url = {https://openreview.net/forum?id=RDSTjtnqCg},
  year = {2024}
}

@inproceedings{xu2024causal,
  abstract = {An essential and challenging problem in causal inference is causal effect estimation from observational data. The problem becomes more difficult with the presence of unobserved confounding variables. The front-door adjustment is an approach for dealing with unobserved confounding variables. However, the restriction for the standard front-door adjustment is difficult to satisfy in practice. In this paper, we relax some of the restrictions by proposing the concept of conditional front-door (CFD) adjustment and develop the theorem that guarantees the causal effect identifiability of CFD adjustment. By leveraging the ability of deep generative models, we propose CFDiVAE to learn the representation of the CFD adjustment variable directly from data with the identifiable Variational AutoEncoder and formally prove the model identifiability. Extensive experiments on synthetic datasets validate the effectiveness of CFDiVAE and its superiority over existing methods. The experiments also show that the performance of CFDiVAE is less sensitive to the causal strength of unobserved confounding variables. We further apply CFDiVAE to a real-world dataset to demonstrate its potential application.},
  author = {Ziqi Xu and Debo Cheng and Jiuyong Li and Jixue Liu and Lin Liu and Kui Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024causal.pdf:pdf},
  note = {DBLP last modified: 2024-12-30},
  pdf = {https://openreview.net/pdf?id=wFf9m4v7oC},
  publisher = {OpenReview.net},
  title = {Causal Inference with Conditional Front-Door Adjustment and Identifiable Variational Autoencoder},
  url = {https://openreview.net/forum?id=wFf9m4v7oC},
  year = {2024}
}

@inproceedings{xu2024initializing,
  abstract = {Weight initialization plays an important role in neural network training. Widely used initialization methods are proposed and evaluated for networks that are trained from scratch. However, the growing number of pretrained models now offers new opportunities for tackling this classical problem of weight initialization. In this work, we introduce weight selection, a method for initializing smaller models by selecting a subset of weights from a pretrained larger model. This enables the transfer of knowledge from pretrained weights to smaller models. Our experiments demonstrate that weight selection can significantly enhance the performance of small models and reduce their training time. Notably, it can also be used together with knowledge distillation. Weight selection offers a new approach to leverage the power of pretrained models in resource-constrained settings, and we hope it can be a useful tool for training small models in the large-model era.},
  author = {Zhiqiu Xu and Yanjie Chen and Kirill Vishniakov and Yida Yin and Zhiqiang Shen and Trevor Darrell and Lingjie Liu and Zhuang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024initializing.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=dyrGMhicMw},
  publisher = {OpenReview.net},
  title = {Initializing Models with Larger Ones},
  url = {https://openreview.net/forum?id=dyrGMhicMw},
  year = {2024}
}

@inproceedings{xu2024towards,
  abstract = {Recent studies revealed that using third-party models may lead to backdoor threats, where adversaries can maliciously manipulate model predictions based on backdoors implanted during model training. Arguably, backdoor trigger inversion (BTI), which generates trigger patterns of given benign samples for a backdoored model, is the most critical module for backdoor defenses used in these scenarios. With BTI, defenders can remove backdoors by fine-tuning based on generated poisoned samples with ground-truth labels or deactivate backdoors by removing trigger patterns during the inference process. However, we find that existing BTI methods suffer from relatively poor performance, i.e., their generated triggers are significantly different from the ones used by the adversaries even in the feature space. We argue that it is mostly because existing methods require to 'extract' backdoor features at first, while this task is very difficult since defenders have no information (e.g., trigger pattern or target label) about poisoned samples. In this paper, we explore BTI from another perspective where we decouple benign features instead of decoupling backdoor features directly. Specifically, our method consists of two main steps, including (1) decoupling benign features and (2) trigger inversion by minimizing the differences between benign samples and their generated poisoned version in decoupled benign features while maximizing the differences in remaining backdoor features. In particular, our method is more efficient since it doesn't need to 'scan' all classes to speculate the target label, as required by existing BTI. We also exploit our BTI module to further design backdoor-removal and pre-processing-based defenses. Extensive experiments on benchmark datasets demonstrate that our defenses can reach state-of-the-art performances.},
  author = {Xiong Xu and Kunzhe Huang and Yiming Li and Zhan Qin and Kui Ren},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024towards.pdf:pdf},
  keywords = {Backdoor trigger inversion, Backdoor defense, Backdoor learning, Trustworthy ML, AI Security},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Tw9wemV6cb},
  publisher = {OpenReview.net},
  title = {Towards Reliable and Efficient Backdoor Trigger Inversion via Decoupling Benign Features},
  url = {https://openreview.net/forum?id=Tw9wemV6cb},
  year = {2024}
}

@inproceedings{xu2024mathematical,
  abstract = {In deep metric learning, the triplet loss has emerged as a popular method to learn many computer vision and natural language processing tasks such as facial recognition, object detection, and visual-semantic embeddings. One issue that plagues the triplet loss is network collapse, an undesirable phenomenon where the network projects the embeddings of all data onto a single point. Researchers predominately solve this problem by using triplet mining strategies. While hard negative mining is the most effective of these strategies, existing formulations lack strong theoretical justification for their empirical success. In this paper, we utilize the mathematical theory of isometric approximation to show an equivalence between the triplet loss sampled by hard negative mining and an optimization problem that minimizes a Hausdorff-like distance between the neural network and its ideal counterpart function. This provides the theoretical justifications for hard negative mining's empirical efficacy. Experiments performed on the Market-1501 and Stanford Online Products datasets with various network architectures corroborate our theoretical findings, indicating that network collapse tends to happen when batch size is too large or embedding dimension is too small. In addition, our novel application of the isometric approximation theorem provides the groundwork for future forms of hard negative mining that avoid network collapse.},
  author = {Albert Xu and Jhih-Yi Hsieh and Bhaskar Vundurthy and Nithya Kemp and Eliana Cohen and Lu Li and Howie Choset},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024mathematical.pdf:pdf},
  keywords = {Metric Learning, Triplet Loss, Hard Negative Mining},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=udO3k28bEw},
  publisher = {OpenReview.net},
  title = {Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem},
  url = {https://openreview.net/forum?id=udO3k28bEw},
  year = {2024}
}

@inproceedings{xu2024gtmgc,
  abstract = {The ground-state conformation of a molecule is often decisive for its properties. However, experimental or computational methods, such as density functional theory (DFT), are time-consuming and labor-intensive for obtaining this conformation. We propose GTMGC, a novel network based on Graph-Transformer (GT) that seamlessly predicts the spatial configuration of molecules in a 3D space from their 2D topological architecture in an end-to-end manner. GTMGC incorporates bonds and pairwise distances within the self-attention mechanism to capture both local and global molecular interactions, achieving state-of-the-art performance on the benchmarks with ground-state conformations. We introduce a novel self-attention mechanism called Molecule Structural Residual Self-Attention (MSRSA) for molecular structure modeling.},
  author = {Guikun Xu and Yongquan Jiang and PengChuan Lei and Yan Yang and Jim Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024gtmgc.pdf:pdf},
  keywords = {Graph Transformer, Molecular conformation, Computational chemistry, Graph neural networks, Self-attention mechanism},
  note = {ICLR 2024 Spotlight presentation; DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=F7QnIKlC1N},
  publisher = {OpenReview.net},
  title = {GTMGC: Using Graph Transformer to Predict Molecule's Ground-State Conformation},
  url = {https://openreview.net/forum?id=F7QnIKlC1N},
  year = {2024}
}

@inproceedings{xu2024llm,
  abstract = {The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness. This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.},
  arxiv = {2310.13345},
  author = {Xilie Xu and Keyi Kong and Ning Liu and Lizhen Cui and Di Wang and Jingfeng Zhang and Mohan S. Kankanhalli},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024llm.pdf:pdf},
  keywords = {Large language models, Adversarial robustness, Prompt-based attacks, LLM security, Natural language processing},
  note = {DBLP last modified: 2024-08-13},
  pdf = {https://openreview.net/pdf?id=VVgGbB9TNV},
  publisher = {OpenReview.net},
  title = {An LLM can Fool Itself: A Prompt-Based Adversarial Attack},
  url = {https://openreview.net/forum?id=VVgGbB9TNV},
  year = {2024}
}

@inproceedings{xu2024uncertaintyaware,
  abstract = {Inverse Constrained Reinforcement Learning (ICRL) considers inferring the constraints respected by expert agents from their demonstrations and learning imitation policies that adhere to these constraints. While previous ICRL works often neglected underlying uncertainties during training, we contend that modeling these uncertainties is crucial for facilitating robust constraint inference. We propose Uncertainty-aware Inverse Constrained Reinforcement Learning (UAICRL), a novel ICRL framework that models both the aleatoric and epistemic uncertainties towards uncertainty-aware constraint inference. The proposed framework enables robust constraint inference and policy learning under complex and uncertain environments.},
  author = {Sheng Xu and Guiliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024uncertaintyaware.pdf:pdf},
  keywords = {Reinforcement learning, Constraint inference, Uncertainty quantification, Inverse reinforcement learning, Safe reinforcement learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ILYjDvUM6U},
  publisher = {OpenReview.net},
  title = {Uncertainty-aware Constraint Inference in Inverse Constrained Reinforcement Learning},
  url = {https://openreview.net/forum?id=ILYjDvUM6U},
  year = {2024}
}

@inproceedings{xu2024dynamic,
  abstract = {Counterfactual regret minimization (CFR) is a family of iterative algorithms showing promising results in solving imperfect-information games. Recent novel CFR variants have significantly improved the convergence rate of the vanilla CFR. The key to these CFR variants' performance is weighting each iteration non-uniformly, i.e., discounting earlier iterations. However, these algorithms use a fixed, manually-specified scheme to weight each iteration, which enormously limits their potential. In this work, we propose Dynamic Discounted CFR (DDCFR), the first equilibrium-finding algorithm that dynamically learns the discounting scheme. We propose to use online convex optimization to learn the dynamic discounting scheme and theoretically prove that DDCFR converges to Nash equilibrium. Experiments on multiple games demonstrate that DDCFR achieves state-of-the-art performance.},
  author = {Hang Xu and Kai Li and Haobo Fu and Qiang Fu and Junliang Xing and Jian Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024dynamic.pdf:pdf},
  keywords = {Game theory, Counterfactual regret minimization, Nash equilibrium, Imperfect information games, Online convex optimization},
  note = {ICLR 2024 Spotlight presentation; DBLP last modified: 2024-12-18},
  pdf = {https://openreview.net/pdf?id=6PbvbLyqT6},
  publisher = {OpenReview.net},
  title = {Dynamic Discounted Counterfactual Regret Minimization},
  url = {https://openreview.net/forum?id=6PbvbLyqT6},
  year = {2024}
}

@inproceedings{xu2024reinforcement,
  abstract = {In nature, the behavior of many complex systems can be described by parsimonious mathematical equations, and Symbolic Regression (SR) is defined as the task of automatically distilling equations from limited data. However, current methods struggle to break bottlenecks when expressions tend toward infinity and especially when underlying mathematical formulas are intricate. We propose the Reinforcement Symbolic Regression Machine (RSRM) that masters the capability of uncovering complex math equations from only scarce data. RSRM leverages reinforcement learning to navigate the vast space of possible symbolic expressions and discover the most parsimonious equations that accurately describe the underlying patterns in the data.},
  author = {Yilong Xu and Yang Liu and Hao Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024reinforcement.pdf:pdf},
  keywords = {Symbolic regression, Reinforcement learning, Mathematical equation discovery, Automated discovery, Scientific machine learning},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=PJVUWpPnZC},
  publisher = {OpenReview.net},
  title = {Reinforcement Symbolic Regression Machine},
  url = {https://openreview.net/forum?id=PJVUWpPnZC},
  year = {2024}
}

@inproceedings{xu2024rebar,
  abstract = {The success of self-supervised contrastive learning depends on identifying positive data pairs that, when pushed together in embedding space, encode useful information for downstream tasks. Constructing positive pairs is challenging, as they must be similar enough to reflect shared semantic meaning but different enough to capture within-class variation. In the time-series domain, positive pair creation is especially difficult due to less obvious invariances compared to computer vision. We propose REBAR (Retrieval-Based Reconstruction), which measures the similarity between two sequences as the reconstruction error that results from reconstructing one sequence with retrieved information from the other. If the two sequences have high REBAR similarity, they are labeled as a positive pair. The REBAR error serves as a predictor of mutual class membership. Once integrated into a contrastive learning framework, REBAR learns an embedding that achieves state-of-the-art performance on downstream tasks across various modalities.},
  arxiv = {2311.00519},
  author = {Maxwell A. Xu and Alexander Moreno and Hui Wei and Benjamin M. Marlin and James Matthew Rehg},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024rebar.pdf:pdf},
  keywords = {Time-series analysis, Contrastive learning, Self-supervised learning, Reconstruction learning, Representation learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3zQo5oUvia},
  publisher = {OpenReview.net},
  title = {REBAR: Retrieval-Based Reconstruction for Time-series Contrastive Learning},
  url = {https://openreview.net/forum?id=3zQo5oUvia},
  year = {2024}
}

@inproceedings{xu2024energybased,
  abstract = {Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural networks to define the joint energy of candidate (input, concept, class) tuples. With such a unified interface, prediction, concept correction, and conditional dependency quantification are then represented as conditional probabilities, which are generated by composing different energy functions. Our ECBMs address both limitations of existing CBMs, providing higher accuracy and richer concept interpretations. Empirical results show that our approach outperforms the state-of-the-art on real-world datasets.},
  arxiv = {2401.14142},
  author = {Xinyue Xu and Yi Qin and Lu Mi and Hao Wang and Xiaomeng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024energybased.pdf:pdf},
  keywords = {Concept bottleneck models, Energy-based models, Interpretable machine learning, Concept intervention, Probabilistic interpretations},
  note = {DBLP last modified: 2024-09-30},
  pdf = {https://openreview.net/pdf?id=I1quoTXZzc},
  publisher = {OpenReview.net},
  title = {Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Probabilistic Interpretations},
  url = {https://openreview.net/forum?id=I1quoTXZzc},
  year = {2024}
}

@inproceedings{xu2024recomp,
  abstract = {Retrieving documents and prepending them in-context at inference time improves performance of language models (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise. If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.},
  arxiv = {2310.04408},
  author = {Fangyuan Xu and Weijia Shi and Eunsol Choi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024recomp.pdf:pdf},
  keywords = {Retrieval-augmented language models, Context compression, Selective augmentation, Natural language processing, Question answering},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mlJLVigNHp},
  publisher = {OpenReview.net},
  title = {RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation},
  url = {https://openreview.net/forum?id=mlJLVigNHp},
  year = {2024}
}

@inproceedings{xu2024besa,
  abstract = {Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to individual transformer blocks, and ii) it allocates layer-specific sparsity in a differentiable manner, both of which ensure reduced performance degradation after pruning. Our experiments show that BESA achieves state-of-the-art performance, efficiently pruning LLMs like LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five hours. Code is available at https://github.com/OpenGVLab/LLMPrune-BESA.},
  author = {Peng Xu and Wenqi Shao and Mengzhao Chen and Shitao Tang and Kaipeng Zhang and Peng Gao and Fengwei An and Yu Qiao and Ping Luo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024besa.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gC6JTEU3jl},
  publisher = {OpenReview.net},
  title = {{BESA}: {P}runing {L}arge {L}anguage {M}odels with {B}lockwise {P}arameter-{E}fficient {S}parsity {A}llocation},
  url = {https://openreview.net/forum?id=gC6JTEU3jl},
  year = {2024}
}

@inproceedings{xu2024boundary,
  abstract = {Video activity localization aims at understanding the semantic content in long untrimmed videos and retrieving actions of interest. The retrieved action with its start and end locations can be used for highlight generation, temporal action detection, etc. Unfortunately, learning the exact boundary location of activities is highly challenging because temporal activities are continuous in time, and there are often no clear-cut transitions between actions. Moreover, the definition of the start and end of events is subjective, which may confuse the model. To alleviate the boundary ambiguity, we propose to study the video activity localization problem from a denoising perspective. Specifically, we propose an encoder-decoder model named DenoiseLoc. During training, a set of action spans is randomly generated from the ground truth with a controlled noise scale. Then we attempt to reverse this process by boundary denoising, allowing the localizer to predict activities with precise boundaries and resulting in faster convergence speed. Experiments show that DenoiseLoc advances in several video activity understanding tasks. For example, we observe a gain of +12.36% average mAP on QV-Highlights dataset and +1.64% mAP@0.5 on THUMOS'14 dataset over the baseline. Moreover, DenoiseLoc achieves state-of-the-art performance on TACoS and MAD datasets, but with much fewer predictions compared to other current methods.},
  author = {Mengmeng Xu and Mattia Soldan and Jialin Gao and Shuming Liu and Juan-Manuel P\'{e}rez-R\'{u}a and Bernard Ghanem},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024boundary.pdf:pdf},
  note = {DBLP last modified: 2024-11-14},
  pdf = {https://openreview.net/pdf?id=bLpUtGyf9g},
  publisher = {OpenReview.net},
  title = {Boundary Denoising for Video Activity Localization},
  url = {https://openreview.net/forum?id=bLpUtGyf9g},
  year = {2024}
}

@inproceedings{xu2024towards,
  abstract = {Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our theoretical claims with extensive empirical evidence. Further, we present results affirming our task selection algorithm adeptly chooses related finetuning tasks, providing advantages to the model performance on target tasks. We believe our study shed new light on the effective adaptation of foundation models to new tasks that lack abundant labels.},
  author = {Zhuoyan Xu and Zhenmei Shi and Junyi Wei and Fangzhou Mu and Yin Li and Yingyu Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1jbh2e0b2K},
  publisher = {OpenReview.net},
  title = {Towards Few-Shot Adaptation of Foundation Models via Multitask Finetuning},
  url = {https://openreview.net/forum?id=1jbh2e0b2K},
  year = {2024}
}

@inproceedings{xu2024lemur,
  abstract = {We introduce Lemur and Lemur-Chat, openly accessible language models optimized for both natural language and coding capabilities to serve as the backbone of versatile language agents. The evolution from language chat models to functional language agents demands that models not only master human interaction, reasoning, and planning but also ensure grounding in the relevant environments. This calls for a harmonious blend of language and coding capabilities in the models. Lemur and Lemur-Chat are proposed to address this necessity, demonstrating balanced proficiencies in both domains, unlike existing open-source models that tend to specialize in either. Through meticulous pretraining using a code-intensive corpus and instruction fine-tuning on text and code data, our models achieve state-of-the-art averaged performance across diverse text and coding benchmarks. Comprehensive experiments demonstrate Lemur's superiority over existing open-source models and its proficiency across various agent tasks involving human communication, tool usage, and interaction under fully- and partially- observable environments. The harmonization between natural and programming languages enables Lemur-Chat to significantly narrow the gap with proprietary models on agent abilities, providing key insights into developing advanced open-source agents adept at reasoning, planning, and operating seamlessly across environments. Our model and code have been open-sourced at https://github.com/OpenLemur/Lemur.},
  author = {Yiheng Xu and Hongjin Su and Chen Xing and Boyu Mi and Qian Liu and Weijia Shi and Binyuan Hui and Fan Zhou and Yitao Liu and Tianbao Xie and Zhoujun Cheng and Siheng Zhao and Lingpeng Kong and Bailin Wang and Caiming Xiong and Tao Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024lemur.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=hNhwSmtXRh},
  publisher = {OpenReview.net},
  title = {Lemur: Harmonizing Natural Language and Code for Language Agents},
  url = {https://openreview.net/forum?id=hNhwSmtXRh},
  year = {2024}
}

@inproceedings{xu2024wizardlm,
  abstract = {Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation, WizardLM achieves more than 90\% capacity of ChatGPT on 17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some aspects, our findings suggest that fine-tuning with AI-evolved instructions is a promising direction for enhancing LLMs.},
  author = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Qingwei Lin and Daxin Jiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024wizardlm.pdf:pdf},
  note = {DBLP last modified: 2025-03-27},
  pdf = {https://openreview.net/pdf?id=CfXh93NDgH},
  publisher = {OpenReview.net},
  title = {{WizardLM}: {E}mpowering {L}arge {P}re-{T}rained {L}anguage {M}odels to Follow Complex Instructions},
  url = {https://openreview.net/forum?id=CfXh93NDgH},
  year = {2024}
}

@inproceedings{xu2024benign,
  abstract = {Neural networks trained by gradient descent (GD) have exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of classical, harmful overfitting -- achieving a perfect fit to training data with near-random performance on test data -- before transitioning (``grokking'') to near-optimal generalization later in training. In this work, we show that both of these phenomena provably occur in two-layer ReLU networks trained by GD on XOR cluster data where a constant fraction of the training labels are flipped. In this setting, we show that after the first step of GD, the network achieves 100\% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy. At a later training step, the network achieves near-optimal test accuracy while still fitting the random labels in the training data, exhibiting a ``grokking'' phenomenon. This provides the first theoretical result of benign overfitting in neural network classification when the data distribution is not linearly separable. Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.},
  author = {Zhiwei Xu and Yutong Wang and Spencer Frei and Gal Vardi and Wei Hu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024benign.pdf:pdf},
  note = {DBLP last modified: 2024-10-01},
  pdf = {https://openreview.net/pdf?id=BxHgpC6FNv},
  publisher = {OpenReview.net},
  title = {Benign Overfitting and Grokking in {ReLU} Networks for {XOR} Cluster Data},
  url = {https://openreview.net/forum?id=BxHgpC6FNv},
  year = {2024}
}

@inproceedings{xu2024logicmp,
  abstract = {Integrating first-order logic constraints (FOLCs) with neural networks is a crucial but challenging problem since it involves modeling intricate correlations to satisfy the constraints. This paper proposes a novel neural layer, LogicMP, whose layers perform mean-field variational inference over an MLN. It can be plugged into any off-the-shelf neural network to encode FOLCs while retaining modularity and efficiency. By exploiting the structure and symmetries in MLNs, we theoretically demonstrate that our well-designed, efficient mean-field iterations effectively mitigate the difficulty of MLN inference, reducing the inference from sequential calculation to a series of parallel tensor operations. Empirical results in three kinds of tasks over images, graphs, and text show that LogicMP outperforms advanced competitors in both performance and efficiency.},
  author = {Weidi Xu and Jingwei Wang and Lele Xie and Jianshan He and Hongting Zhou and Taifeng Wang and Xiaopei Wan and Jingdong Chen and Chao Qu and Wei Chu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024logicmp.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BLGQ3oqldb},
  publisher = {OpenReview.net},
  title = {{LogicMP}: A Neuro-symbolic Approach for Encoding First-order Logic Constraints},
  url = {https://openreview.net/forum?id=BLGQ3oqldb},
  year = {2024}
}

@inproceedings{xu2024qalora,
  abstract = {Recently years have witnessed a rapid development of large language models (LLMs). Despite the strong ability in many language-understanding tasks, the heavy computational burden largely restricts the application of LLMs especially when one needs to deploy them onto edge devices. In this paper, we propose a quantization-aware low-rank adaptation (QA-LoRA) algorithm. The motivation lies in the imbalanced degrees of freedom of quantization and adaptation, and the solution is to use group-wise operators which increase the degree of freedom of quantization meanwhile decreasing that of adaptation. QA-LoRA is easily implemented with a few lines of code, and it equips the original LoRA with two-fold abilities: (i) during fine-tuning, the LLM's weights are quantized (e.g., into INT4) to reduce time and memory usage; (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy.},
  author = {Yuhui Xu and Lingxi Xie and Xiaotao Gu and Xin Chen and Heng Chang and Hengheng Zhang and Zhengsu Chen and Xiaopeng Zhang and Qi Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024qalora.pdf:pdf},
  note = {DBLP last modified: 2024-08-23},
  pdf = {https://openreview.net/pdf?id=WvFoJccpo8},
  publisher = {OpenReview.net},
  title = {{QA-LoRA}: Quantization-Aware Low-Rank Adaptation of Large Language Models},
  url = {https://openreview.net/forum?id=WvFoJccpo8},
  year = {2024}
}

@inproceedings{xu2024sample,
  abstract = {Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like Œµ-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control environments, where the diverse task set aligns with the task selection by automatic curriculum learning, which is empirically shown to improve sample-efficiency.},
  author = {Ziping Xu and Zifan Xu and Runxuan Jiang and Peter Stone 0001 and Ambuj Tewari},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024sample.pdf:pdf},
  note = {DBLP last modified: 2024-11-19},
  pdf = {https://openreview.net/pdf?id=YZrg56G0JV},
  publisher = {OpenReview.net},
  title = {Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks},
  url = {https://openreview.net/forum?id=YZrg56G0JV},
  year = {2024}
}

@inproceedings{xu2024fits,
  abstract = {In this paper, we introduce FITS, a lightweight yet powerful model for time series analysis. Unlike existing models that directly process raw time-domain data, FITS operates on the principle that time series can be manipulated through interpolation in the complex frequency domain. By discarding high-frequency components with negligible impact on time series data, FITS achieves performance comparable to state-of-the-art models for time series forecasting and anomaly detection tasks, while having a remarkably compact size of only approximately $10k$ parameters. Such a lightweight model can be easily trained and deployed in edge devices, creating opportunities for various applications.},
  author = {Zhijian Xu and Ailing Zeng and Qiang Xu 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024fits.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=bWcnvZ3qMb},
  publisher = {OpenReview.net},
  title = {FITS: Modeling Time Series with 10k Parameters},
  url = {https://openreview.net/forum?id=bWcnvZ3qMb},
  year = {2024}
}

@inproceedings{xu2024rethinking,
  abstract = {Designing expressive Graph Neural Networks (GNNs) is an important topic in graph machine learning fields. Despite the existence of numerous approaches proposed to enhance GNNs based on Weisfeiler-Lehman (WL) tests, what GNNs can and cannot learn still lacks a deeper understanding. This paper adopts a fundamentally different approach to examine the expressive power of GNNs from a probabilistic perspective. By establishing connections between GNNs' predictions and the central inference problems of probabilistic graphical models (PGMs), we can analyze previous GNN variants with a novel hierarchical framework and gain new insights into their node-level and link-level behaviors. Additionally, we introduce novel methods that can provably enhance GNNs' ability to capture complex dependencies and make complex predictions. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approaches.},
  author = {Tuo Xu and Lei Zou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024rethinking.pdf:pdf},
  keywords = {graph neural networks, expressiveness, approximate inference},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf/20a211772e90fa923b10b62f56acff93f0b25cef.pdf},
  publisher = {OpenReview.net},
  title = {Rethinking and Extending the Probabilistic Inference Capacity of {GNN}s},
  url = {https://openreview.net/forum?id=7vVWiCrFnd},
  year = {2024}
}

@inproceedings{xu2024idempotence,
  abstract = {Idempotence is the stability of image codec to re-compression. At the first glance, it is unrelated to perceptual image compression. However, we find that theoretically: 1) Conditional generative model-based perceptual codec satisfies idempotence; 2) Unconditional generative model with idempotence constraint is equivalent to conditional generative codec. Based on this newfound equivalence, we propose a new paradigm of perceptual image codec by inverting unconditional generative model with idempotence constraints. Our codec is theoretically equivalent to conditional generative codec, and it does not require training new models. Instead, it only requires a pre-trained mean-square-error codec and unconditional generative model. Empirically, we show that our proposed approach outperforms state-of-the-art methods such as HiFiC and ILLM, in terms of Fr√©chet Inception Distance (FID). The source code is provided in https://github.com/tongdaxu/Idempotence-and-Perceptual-Image-Compression.},
  author = {Tongda Xu and Ziran Zhu and Dailan He and Yanghao Li and Lina Guo and Yuanyuan Wang and Zhe Wang and Hongwei Qin and Yan Wang and Jingjing Liu and Ya-Qin Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024idempotence.pdf:pdf},
  keywords = {perceptual image compression, neural image compression},
  note = {Spotlight. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Cy5v64DqEF},
  publisher = {OpenReview.net},
  title = {Idempotence and Perceptual Image Compression},
  url = {https://openreview.net/forum?id=Cy5v64DqEF},
  year = {2024}
}

@inproceedings{xu2024autolora,
  abstract = {Robust Fine-Tuning (RFT) is a low-cost strategy to obtain adversarial robustness in downstream applications, without requiring a lot of computational resources and collecting significant amounts of data. This paper uncovers an issue with the existing RFT, where optimizing both adversarial and natural objectives through the feature extractor (FE) yields significantly divergent gradient directions. This divergence introduces instability in the optimization process, thereby hindering the attainment of adversarial robustness and rendering RFT highly sensitive to hyperparameters. To address this challenge, we propose AutoLoRa, which introduces a low-rank branch that disentangles the gradients, thus enhancing the stability of the training process. Additionally, we present heuristic strategies for automated hyperparameter scheduling, specifically for the learning rate and loss term weights. Through extensive experiments across multiple datasets and downstream tasks, we demonstrate that AutoLoRa achieves state-of-the-art results in robust fine-tuning while maintaining competitive performance in natural accuracy.},
  author = {Xilie Xu and Jingfeng Zhang and Mohan S. Kankanhalli},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024autolora.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf/0782024f39ecc4e566699081da1eebc5e47ea399.pdf},
  publisher = {OpenReview.net},
  title = {{AutoLoRa}: An Automated Robust Fine-Tuning Framework},
  url = {https://openreview.net/forum?id=09xFexjhqE},
  year = {2024}
}

@inproceedings{xu2024drm,
  abstract = {Visual reinforcement learning ({RL}) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual {RL} methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the {RL} agent's network. Based on this understanding, we propose {DrM} (Dormant ratio Minimized), a method that uses an auxiliary loss to encourage neurons to remain active, which directly addresses the problem of inactive exploration in visual {RL}. Experiments show that {DrM} significantly improves the sample efficiency and asymptotic performance of existing methods across a wide range of visual control tasks.},
  author = {Guowei Xu and Ruijie Zheng and Yongyuan Liang and Xiyao Wang and Zhecheng Yuan and Tianying Ji and Yu Luo and Xiaoyu Liu and Jiaxin Yuan and Pu Hua and Shuzhen Li and Yanjie Ze and Hal Daum{\'e} {III} and Furong Huang and Huazhe Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xu2024drm.pdf:pdf},
  keywords = {visual reinforcement learning, dormant ratio, exploration},
  note = {Spotlight. DBLP last modified: 2025-04-15},
  pdf = {https://openreview.net/pdf?id=MSe8YFbhUE},
  publisher = {OpenReview.net},
  title = {{DrM}: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization},
  url = {https://openreview.net/forum?id=MSe8YFbhUE},
  year = {2024}
}

@inproceedings{xue2024interpretable,
  abstract = {Transformer-based machine translation models currently dominate the field of model-based program translation. However, these models fail to provide interpretative support for the generated program translations. Moreover, researchers frequently invest substantial time and computational resources in retraining models, yet the improvement in translation accuracy is quite limited. To address these issues, we introduce a novel approach, $k$NN-ECD, which combines $k$-nearest-neighbor search with a key-value error correction datastore to overwrite the wrong translations of TransCoder-ST. This provides a decision-making basis for interpreting the corrected translations. Building upon this, we further propose $k$NN-ECS$_m$, a methodology that employs a distributed structure with $m$ sub-datastores connected in series to enhance the error correction capability while reducing the required storage space. We evaluate our methods on three popular code-to-code translation datasets and demonstrate that our approaches achieve significant improvements in translation accuracy while providing interpretable corrections.},
  author = {Min Xue and Artur Andrzejak and Marla Leuther},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xue2024interpretable.pdf:pdf},
  keywords = {code translation, interpretability, k-nearest neighbors},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fVxIEHGnVT},
  publisher = {OpenReview.net},
  title = {An interpretable error correction method for enhancing code-to-code translation},
  url = {https://openreview.net/forum?id=fVxIEHGnVT},
  year = {2024}
}

@inproceedings{xue2024investigating,
  abstract = {An effective technique for obtaining high-quality representations is adding a projection head on top of the encoder during training, then discarding it and using the pre-projection representations. Despite its proven practical effectiveness, the reason behind the success of this technique is poorly understood. The pre-projection representations are not directly optimized by the loss function, raising the question: what makes them better? In this work, we provide a rigorous theoretical answer to this question. We start by examining linear models trained with self-supervised contrastive loss. We reveal that the implicit bias of training algorithms leads to layer-wise progressive feature weighting, where features become increasingly unequal as we go deeper into the layers. Based on this analysis, we demonstrate both theoretically and empirically that using a projection head can mitigate this bias, leading to more balanced feature learning and improved generalization. Our findings provide new theoretical insights into why projection heads are beneficial and when they should be used.},
  author = {Yihao Xue and Eric Gan and Jiayi Ni and Siddharth Joshi and Baharan Mirzasoleiman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xue2024investigating.pdf:pdf},
  keywords = {representation learning, projection head, contrastive learning},
  note = {DBLP last modified: 2024-10-18},
  pdf = {https://openreview.net/pdf?id=GgEAdqYPNA},
  publisher = {OpenReview.net},
  title = {Investigating the Benefits of Projection Head for Representation Learning},
  url = {https://openreview.net/forum?id=GgEAdqYPNA},
  year = {2024}
}

@inproceedings{xue2024understanding,
  abstract = {Recently, multimodal contrastive learning (MMCL) approaches, such as {CLIP}, have achieved a remarkable success in learning representations that are robust against distribution shift and generalize to new domains. Despite the empirical success, the mechanism behind learning such generalizable representations is not understood. In this work, we rigorously analyze this problem and uncover two mechanisms behind {MMCL}'s robustness: intra-class contrasting, which allows the model to learn features with a high variance, and inter-class feature sharing, where annotated details in one class help learning other classes better. Both mechanisms prevent spurious features that are over-represented in the training data to overshadow the generalizable core features. This yields superior zero-shot classification accuracy under distribution shift. Furthermore, we theoretically demonstrate the benefits of using rich captions on robustness and explore the effect of annotating different types of details in the captions. We validate our theoretical findings through experiments, including a well-designed synthetic experiment and an experiment involving training {CLIP} models on {MSCOCO}/Conceptual Captions and evaluating them on shifted {ImageNet}s.},
  author = {Yihao Xue and Siddharth Joshi and Dang Nguyen and Baharan Mirzasoleiman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xue2024understanding.pdf:pdf},
  keywords = {multimodal learning, contrastive learning, distribution shift, robustness},
  note = {DBLP last modified: 2024-10-18},
  pdf = {https://openreview.net/pdf?id=rtl4XnJYBh},
  publisher = {OpenReview.net},
  title = {Understanding the Robustness of Multi-modal Contrastive Learning to Distribution Shift},
  url = {https://openreview.net/forum?id=rtl4XnJYBh},
  year = {2024}
}

@inproceedings{xue2024toward,
  abstract = {While generative diffusion models excel in producing high-quality images, they can also be misused to mimic authorized images, posing a significant threat to {AI} systems. Efforts have been made to add calibrated perturbations to protect images from diffusion-based mimicry pipelines. However, most of the existing methods are too ineffective and even impractical to be used by individual users due to their high computation and memory requirements. In this work, we present novel findings on attacking latent diffusion models ({LDM}) and propose new plug-and-play strategies for more effective protection. In particular, we explore the bottleneck in attacking an {LDM}, discovering that the encoder module rather than the denoiser module is the vulnerable point. Based on this insight, we present our strategy using Score Distillation Sampling ({SDS}) to double the speed of protection and reduce memory occupation by half without compromising its strength. Additionally, we provide a robust protection strategy by counterintuitively minimizing the semantic loss, which can assist in generating more natural perturbations. Finally, we conduct extensive experiments to substantiate our findings and comprehensively evaluate our newly proposed strategies. We hope our insights and protective measures can contribute to better defense against malicious diffusion-based mimicry, advancing the development of secure {AI} systems.},
  author = {Haotian Xue and Chumeng Liang and Xiaoyu Wu and Yongxin Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xue2024toward.pdf:pdf},
  keywords = {diffusion models, adversarial protection, score distillation},
  note = {DBLP last modified: 2025-01-30},
  pdf = {https://openreview.net/pdf?id=NzxCMe88HX},
  publisher = {OpenReview.net},
  title = {Toward effective protection against diffusion-based mimicry through score distillation},
  url = {https://openreview.net/forum?id=NzxCMe88HX},
  year = {2024}
}

@inproceedings{xue2024encoding,
  abstract = {Metagenomics studies genomic material derived from mixed microbial communities in diverse environments, holding considerable significance for both human health and environmental sustainability. Metagenomic binning refers to the clustering of genomic subsequences obtained from high-throughput {DNA} sequencing into distinct bins, each representing a constituent organism within the community. Mainstream binning methods primarily rely on sequence features such as composition and abundance, making them unable to effectively handle sequences shorter than 1,000 bp and inherent noise within sequences. Several binning tools have emerged, aiming to enhance binning outcomes by using the assembly graph generated by assemblers, which encodes valuable overlapping information among genomic sequences. However, existing assembly graph-based binners mainly focus on simplified contig-level assembly graphs that are recreated from assembler's original graphs, unitig-level assembly graphs. The simplification reduces the resolution of the connectivity information in original graphs. In this paper, we design a novel binning tool named {UnitigBin}, which leverages representation learning on unitig-level assembly graphs while adhering to heterophilous constraints imposed by single-copy marker genes, ensuring that constrained contigs cannot be grouped together. Our approach significantly outperforms existing methods across multiple datasets and metrics.},
  author = {Hansheng Xue and Vijini Mallawaarachchi and Lexing Xie and Vaibhav Rajan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xue2024encoding.pdf:pdf},
  keywords = {metagenomics, graph neural networks, bioinformatics, assembly graphs},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vBw8JGBJWj},
  publisher = {OpenReview.net},
  title = {Encoding Unitig-level Assembly Graphs with Heterophilous Constraints for Metagenomic Contigs Binning},
  url = {https://openreview.net/forum?id=vBw8JGBJWj},
  year = {2024}
}

@inproceedings{xue2024easytpp,
  abstract = {Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes ({TPPs}) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn't been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present {EasyTPP}, the first central repository of research assets (e.g., data, models, evaluation programs, documentations) in the area of event sequence modeling. Our {EasyTPP} makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are easy to use and extend as well as facilitate reproducible research; implementations of popular neural {TPPs}, together with a rich library of modules by composing which one could quickly build complex models. Our benchmark will help promote reproducible research in this field, thus accelerating research progress as well as making more significant real-world impacts.},
  author = {Siqiao Xue and Xiaoming Shi and Zhixuan Chu and Yan Wang and Hongyan Hao and Fan Zhou and Caigao Jiang and Chen Pan and James Y. Zhang and Qingsong Wen and Jun Zhou and Hongyuan Mei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/xue2024easytpp.pdf:pdf},
  keywords = {temporal point processes, benchmarking, event sequence modeling},
  note = {DBLP last modified: 2025-05-20},
  pdf = {https://openreview.net/pdf?id=PJwAkg0z7h},
  publisher = {OpenReview.net},
  title = {{EasyTPP}: Towards Open Benchmarking Temporal Point Processes},
  url = {https://openreview.net/forum?id=PJwAkg0z7h},
  year = {2024}
}

@inproceedings{ya2024towards,
  abstract = {Saliency-based representation visualization (SRV) (e.g., Grad-CAM) is one of the most classical and widely adopted explainable artificial intelligence (XAI) methods for its simplicity and efficiency. It can be used to interpret deep neural networks by locating saliency areas contributing the most to their predictions. However, it is difficult to automatically measure and evaluate the performance of SRV methods due to the lack of ground-truth salience areas of samples. To tackle this problem, researchers have proposed backdoor-based approaches by injecting backdoors into DNNs and using triggered samples as ground-truth. However, in this paper, we first reveal the unreliable nature and implementation limitations of existing backdoor-based SRV evaluation methods, based on which we propose a generalization-limited backdoor watermark (GLBW) and design a more faithful XAI evaluation.},
  author = {Mengxi Ya and Yiming Li and Tao Dai and Bin Wang and Yong Jiang and Shu-Tao Xia},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ya2024towards.pdf:pdf},
  keywords = {XAI, XAI Evaluation, Backdoor Watermark, Backdoor Attack},
  note = {DBLP last modified: 2024-08-14},
  pdf = {https://openreview.net/pdf?id=cObFETcoeW},
  publisher = {OpenReview.net},
  title = {Towards Faithful {XAI} Evaluation via Generalization-Limited Backdoor Watermark},
  url = {https://openreview.net/forum?id=cObFETcoeW},
  year = {2024}
}

@inproceedings{yadav2024adaptive,
  abstract = {Cross-encoder (CE) models which compute similarity by jointly encoding a query-item pair perform better than using dot-product with embedding-based models (dual-encoders) at estimating query-item relevance. However, the deployment of CE-based nearest neighbor search faces two challenges: (a) it is computationally expensive to run CE inference over all items in the retrieval corpus and (b) surprisingly, dual-encoder-based retrieve-and-rerank approaches suffer from poor recall on new domains and the retrieval with DE is decoupled from the CE. While CUR-based approaches can be more accurate than the DE-based approach, they require a prohibitively large number of CE calls to compute item embeddings, thus making it impractical for deployment at scale. We address these challenges via a sparse-matrix factorization based method that efficiently computes latent query and item representations to approximate CE scores and performs k-NN search with the approximate CE similarity.},
  author = {Nishant Yadav and Nicholas Monath and Manzil Zaheer and Rob Fergus and Andrew McCallum},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yadav2024adaptive.pdf:pdf},
  keywords = {cross-encoder, k-NN, retrieval, nearest-neighbor search, metric learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1CPta0bfN2},
  publisher = {OpenReview.net},
  title = {Adaptive Retrieval and Scalable Indexing for k-{NN} Search with Cross-Encoders},
  url = {https://openreview.net/forum?id=1CPta0bfN2},
  year = {2024}
}

@inproceedings{yadav2024masked,
  abstract = {In this work, we propose a Multi-Window Masked Autoencoder (MW-MAE) fitted with a novel Multi-Window Multi-Head Attention (MW-MHA) module that facilitates the modelling of local-global interactions in every decoder transformer block through attention heads of several distinct local and global windows. Each attention head in the MW-MHA module can compute local self-attention over non-overlapping windows of different sizes, enabling it to capture time-frequency information at several local granularities and learn local-global representations while modeling interactions among different acoustic elements in each decoder transformer block. Empirical results on ten downstream audio tasks show that MW-MAEs consistently outperform standard MAEs in overall performance and learn better general-purpose audio representations. Furthermore, MW-MAEs also demonstrate considerably better scaling characteristics.},
  author = {Sarthak Yadav and Sergios Theodoridis and Lars Kai Hansen and Zheng-Hua Tan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yadav2024masked.pdf:pdf},
  keywords = {self-supervised learning, masked autoencoder, audio representation learning, multi-window attention},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Q53QLftNkA},
  publisher = {OpenReview.net},
  title = {Masked Autoencoders with Multi-Window Local-Global Attention Are Better Audio Learners},
  url = {https://openreview.net/forum?id=Q53QLftNkA},
  year = {2024}
}

@inproceedings{yan2024complete,
  abstract = {Crystal structures are characterized by atomic bases within a primitive unit cell that repeats along a regular lattice throughout 3D space. The periodic and infinite nature of crystals poses unique challenges for geometric graph representation learning. Specifically, constructing graphs that effectively capture the complete geometric information of crystals and handle chiral crystals remains an unsolved and challenging problem. In this paper, we introduce ComFormer, a SE(3) transformer designed specifically for crystalline materials. ComFormer includes two variants; namely, iComFormer that employs invariant geometric descriptors of Euclidean distances and angles, and eComFormer that utilizes equivariant vector representations. We utilize the periodic patterns of unit cells to establish the lattice-based representation for each atom, enabling efficient and expressive graph representations of crystals.},
  author = {Keqiang Yan and Cong Fu and Xiaofeng Qian and Xiaoning Qian and Shuiwang Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024complete.pdf:pdf},
  keywords = {crystal material property prediction, geometric graph representation learning, SE(3) transformer, crystalline materials},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BnQY9XiRAS},
  publisher = {OpenReview.net},
  title = {Complete and Efficient Graph Transformers for Crystal Material Property Prediction},
  url = {https://openreview.net/forum?id=BnQY9XiRAS},
  year = {2024}
}

@inproceedings{yan2024multiscale,
  abstract = {Multi-scale learning is central to semantic segmentation. However, two issues, scale inadequacy and field inactivation, may arise when learning multi-scale representations, which limit the performance of the semantic segmentation model. In this paper, we present varying window attention (VWA) to address them. VWA is a novel multi-scale learner that leverages local window attention (LWA) and disentangles it into query window and context window, making the scale of context vary for learning representations at multiple scales. Considering that varying context to large-scale windows will increase memory and computation costs (R¬≤ times larger than LWA), we propose a re-scaling strategy to eliminate the extra cost without compromising the multi-scale performance. Finally, we introduce VWFormer, a multi-scale decoder that is based on VWA and improves multi-scale representations for semantic segmentation.},
  author = {Haotian Yan and Ming Wu and Chuang Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024multiscale.pdf:pdf},
  keywords = {semantic segmentation, multi-scale learning, window attention, transformer, computer vision},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lAhWGOkpSR},
  publisher = {OpenReview.net},
  title = {Multi-Scale Representations by Varying Window Attention for Semantic Segmentation},
  url = {https://openreview.net/forum?id=lAhWGOkpSR},
  year = {2024}
}

@inproceedings{yan2024masked,
  abstract = {Transformer architecture search (TAS) has achieved remarkable progress in automating the neural architecture design process of vision transformers. Recent TAS advancements have discovered outstanding transformer architectures while saving tremendous labor from human experts. However, it is still cumbersome to deploy these methods in real-world applications due to the expensive costs of data labeling under the supervised learning paradigm. To this end, this paper proposes a masked image modelling (MIM) based self-supervised neural architecture search method specifically designed for vision transformers, termed as MaskTAS, which completely avoids the expensive costs of data labeling. Specifically, we first reveal the effectiveness of masked image modelling for architecture performance assessment.},
  author = {Caixia Yan and Xiaojun Chang and Zhihui Li and Lina Yao and Minnan Luo and Qinghua Zheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024masked.pdf:pdf},
  keywords = {knowledge distillation, self-supervised learning, neural architecture search, vision transformers, masked image modelling},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LUpC8KTvdV},
  publisher = {OpenReview.net},
  title = {Masked Distillation Advances Self-Supervised Transformer Architecture Search},
  url = {https://openreview.net/forum?id=LUpC8KTvdV},
  year = {2024}
}

@inproceedings{yan2024hypergraph,
  abstract = {Hypergraph neural networks (HGNNs) exhibit the potential to tackle tasks with high-order correlations and have achieved success in many tasks. However, existing evolution on the hypergraph has poor controllability and lacks sufficient theoretical support (like dynamic systems), thus yielding sub-optimal performance. One typical scenario is that only one or two layers of HGNNs can achieve good results and more layers lead to degeneration of performance. Under such circumstances, it is important to increase the controllability of HGNNs. To tackle the issue, we introduce hypergraph dynamic systems (HDS), which bridge hypergraphs and dynamic systems and characterize the continuous dynamics of representations. We propose a control-diffusion hypergraph dynamic system by an ordinary differential equation (ODE) and design a multi-layer HDS as a neural implementation, which contains control steps and diffusion steps.},
  author = {Jielong Yan and Yifan Feng and Shihui Ying and Yue Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024hypergraph.pdf:pdf},
  keywords = {hypergraph neural networks, dynamic systems, ordinary differential equations, graph neural networks, high-order correlations},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=NLbRvr840Q},
  publisher = {OpenReview.net},
  title = {Hypergraph Dynamic System},
  url = {https://openreview.net/forum?id=NLbRvr840Q},
  year = {2024}
}

@inproceedings{yan2024provably,
  abstract = {Conformal prediction is a powerful tool to generate uncertainty sets with guaranteed coverage using any predictive model, under the assumption that the training and test data are i.i.d. However, adversarial examples are able to manipulate conformal methods to construct prediction sets with invalid coverage rates, as the i.i.d. assumption is violated. To address this issue, previous work proposed Randomized Smoothed Conformal Prediction (RSCP) to certify the robustness of conformal prediction methods to adversarial noise. However, RSCP has two major limitations: (i) its robustness guarantee is flawed when used in practice and (ii) it tends to produce large uncertainty sets. In this work, we first propose a novel framework called RSCP+ to provide provable robustness guarantee in evaluation, which fixes the issues in the original RSCP method. We then propose two novel methods, Post-Training Transformation (PTT) and Robust Conformal Training (RCT), to effectively reduce prediction set size with little computation overhead.},
  author = {Ge Yan and Yaniv Romano and Tsui-Wei Weng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024provably.pdf:pdf},
  keywords = {conformal prediction, randomized smoothing, adversarial robustness, uncertainty quantification, machine learning},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BWAhEjXjeG},
  publisher = {OpenReview.net},
  title = {Provably Robust Conformal Prediction with Improved Efficiency},
  url = {https://openreview.net/forum?id=BWAhEjXjeG},
  year = {2024}
}

@inproceedings{yan2024autocast,
  abstract = {Machine-based prediction of real-world events is garnering attention due to its potential for informed decision-making. Whereas traditional forecasting predominantly hinges on structured data like time-series, recent breakthroughs in language models enable predictions using unstructured text. Building upon this, the AutoCast benchmark utilizes news articles for forecasting queries, yet methods still lag behind human performance. In this work, we introduce AutoCast++, a zero-shot ranking-based context retrieval system tailored to sift through expansive news document collections for event forecasting. Our approach first re-ranks articles based on zero-shot question-passage relevance, emphasizing semantically pertinent news. Following re-ranking, chosen articles undergo zero-shot summarization to attain succinct context. The system leverages a pre-trained language model to conduct both relevance evaluation and article summarization without needing domain-specific training.},
  author = {Qi Yan and Raihan Seraj and Jiawei He and Lili Meng and Tristan Sylvain},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024autocast.pdf:pdf},
  keywords = {world event prediction, large language models, zero-shot learning, context retrieval, news forecasting},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=COYDmKkQH4},
  publisher = {OpenReview.net},
  title = {AutoCast++: Enhancing World Event Prediction with Zero-shot Ranking-based Context Retrieval},
  url = {https://openreview.net/forum?id=COYDmKkQH4},
  year = {2024}
}

@inproceedings{yan2024multiview,
  abstract = {A promising direction for pre-training 3D point clouds is to leverage the massive amount of data in 2D, whereas the domain gap between 2D and 3D creates a fundamental challenge. Different from the popular practice of predicting 2D features first and then obtaining 3D features through dimensionality lifting, our approach directly uses a 3D network for feature extraction. We train the 3D feature extraction network with the help of the novel 2D knowledge transfer loss, which enforces the 2D projections of the 3D feature to be consistent with the output of pre-trained 2D networks. To prevent the feature from discarding 3D signals, we introduce the multi-view consistency loss that additionally encourages the projected 2D feature representations to capture pixel-wise correspondences across different views. Such correspondences induce 3D geometry and effectively retain 3D features in the projected 2D features.},
  author = {Siming Yan and Chen Song and Youkang Kong and Qixing Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024multiview.pdf:pdf},
  keywords = {point cloud pre-training, 3D representation learning, multi-view learning, knowledge transfer, computer vision},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=imZcqOrbig},
  publisher = {OpenReview.net},
  title = {Multi-View Representation is What You Need for Point-Cloud Pre-Training},
  url = {https://openreview.net/forum?id=imZcqOrbig},
  year = {2024}
}

@inproceedings{yan2024understanding,
  abstract = {This paper explores the elusive mechanism underpinning in-context learning in Large Language Models (LLMs). Our work provides a novel perspective by examining in-context learning via the lens of surface repetitions. We quantitatively investigate the role of surface features in text generation, and empirically establish the existence of token co-occurrence reinforcement, a principle that strengthens the relationship between two tokens based on their contextual co-occurrences. Furthermore, we find similar reinforcements lie behind the pretraining corpus, revealing the existence is due to LLMs' efforts to maximize the likelihood. By investigating the dual impacts of these features, our research illuminates the internal workings of in-context learning and expounds on the reasons for its failures.},
  author = {Jianhao Yan and Jin Xu and Chiyu Song and Chenming Wu and Yafu Li and Yue Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024understanding.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=bGGYcvw8mp},
  publisher = {OpenReview.net},
  title = {Understanding In-Context Learning from Repetitions},
  url = {https://openreview.net/forum?id=bGGYcvw8mp},
  year = {2024}
}

@inproceedings{yan20243d,
  abstract = {Masked autoencoders (MAE) have recently been introduced to 3D self-supervised pretraining for point clouds due to their great success in NLP and computer vision. Unlike MAEs used in the image domain, where the pretext task is to restore features at the masked pixels, such as colors, the existing 3D MAE works reconstruct the missing geometry only, i.e, the location of the masked points. In contrast to previous studies, we advocate that point location recovery is inessential and restoring intrinsic point features is much superior. To this end, we propose to ignore point position reconstruction and recover high-order features at masked points including surface normals and surface variations, through a novel attention-based decoder which is independent of the encoder design. We validate the effectiveness of our pretext task and decoder design using different encoder structures for 3D training and demonstrate the advantages of our pretrained networks on various point cloud analysis tasks.},
  author = {Siming Yan and Yuqi Yang and Yuxiao Guo and Hao Pan and Peng-shuai Wang and Xin Tong and Yang Liu and Qixing Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan20243d.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LokR2TTFMs},
  publisher = {OpenReview.net},
  title = {{3D} Feature Prediction for Masked-{AutoEncoder}-Based Point Cloud Pretraining},
  url = {https://openreview.net/forum?id=LokR2TTFMs},
  year = {2024}
}

@inproceedings{yan2024humanai,
  abstract = {The standard active learning setting assumes a willing labeler, who provides labels on informative examples to speed up learning. However, if the labeler wishes to be compensated for as many labels as possible before learning finishes, the labeler may benefit from actually slowing down learning. This incentive arises for instance if the labeler is to be replaced by the ML model once it is trained. In this paper, we initiate the study of learning from a strategic labeler, who may abstain from labeling to slow down learning. We develop a near-optimal deterministic algorithm, prove its robustness to strategic labeling, and contrast it with other active learning algorithms. We also analyze extensions that encompass more general learning goals and labeler assumptions, and characterize the query cost of multi-task active learning, with and without abstention.},
  author = {Tom Yan and Chicheng Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024humanai.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=s5hSp7EdL3},
  publisher = {OpenReview.net},
  title = {The Human-{AI} Substitution game: active learning from a strategic labeler},
  url = {https://openreview.net/forum?id=s5hSp7EdL3},
  year = {2024}
}

@inproceedings{yan2024making,
  abstract = {The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Language models possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this work, we present TP-BERTa, a specifically pre-trained LM for tabular data prediction. We identify three key challenges: (1) the numerical tokenization for capturing the magnitude and precision of numerical values; (2) the intra-feature attention for information fusion within individual features; and (3) the inter-feature attention for information fusion across distinct features. To address these challenges, we propose a relative magnitude tokenization method to enhance the model's perception of numerical values and an intra-feature attention mechanism to exploit feature context. Comprehensive evaluation on extensive downstream datasets shows that TP-BERTa consistently outperforms existing tabular deep learning models and achieves competitive performance with gradient boosting decision trees.},
  author = {Jiahuan Yan and Bo Zheng and Hongxia Xu and Yiheng Zhu and Danny Z. Chen and Jimeng Sun and Jian Wu and Jintai Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yan2024making.pdf:pdf},
  note = {DBLP last modified: 2025-01-21; ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=anzIzGZuLi},
  publisher = {OpenReview.net},
  title = {Making Pre-trained Language Models Great on Tabular Prediction},
  url = {https://openreview.net/forum?id=anzIzGZuLi},
  year = {2024}
}

@inproceedings{yang2024task,
  abstract = {Unsupervised reinforcement learning (URL) aims to learn general skills for unseen downstream tasks. Mutual Information Skill Learning (MISL) addresses URL by maximizing the mutual information between states and skills but lacks sufficient theoretical analysis, such as how well its learned skills can initialize a downstream task's policy. We provide theoretical analysis showing that the diversity and separatability of learned skills are fundamentally critical to downstream task adaptation but MISL does not necessarily guarantee them. To improve MISL, we propose a novel disentanglement metric called LSEPIN and build an information-geometric connection between LSEPIN and downstream task adaptation cost. For better geometric properties, we investigate a new strategy that replaces the KL divergence in information geometry with Wasserstein distance. This leads to a novel skill-learning objective called WSEP. The WSEP objective is theoretically justified to be helpful to downstream task adaptation and is capable of discovering more initial policies for downstream tasks than MISL. We also propose another Wasserstein distance-based algorithm PWSEP that can theoretically discover all optimal initial policies.},
  author = {Yucheng Yang and Tianyi Zhou and Qiang He and Lei Han and Mykola Pechenizkiy and Meng Fang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024task.pdf:pdf},
  note = {DBLP last modified: 2024-08-08; ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=zSxpnKh1yS},
  publisher = {OpenReview.net},
  title = {Task Adaptation from Skills: Information Geometry, Disentanglement, and New Objectives for Unsupervised Reinforcement Learning},
  url = {https://openreview.net/forum?id=zSxpnKh1yS},
  year = {2024}
}

@inproceedings{yang2024large,
  abstract = {Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.},
  author = {Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024large.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Bb4VGOWELI},
  publisher = {OpenReview.net},
  title = {Large Language Models as Optimizers},
  url = {https://openreview.net/forum?id=Bb4VGOWELI},
  year = {2024}
}

@inproceedings{yang2024looped,
  abstract = {Transformers have demonstrated effectiveness in in-context solving data-fitting problems from various (latent) models. However, the absence of an inherent iterative structure in the transformer architecture presents a challenge in emulating the iterative algorithms, which are commonly employed in traditional machine learning methods. To address this, we propose the utilization of looped transformer architecture and its associated training methodology, with the aim of incorporating iterative characteristics into the transformer architectures. Experimental results suggest that the looped transformer achieves performance comparable to the standard transformer in solving various data-fitting problems, while utilizing less than 10% of the parameter count.},
  author = {Liu Yang and Kangwook Lee and Robert D. Nowak and Dimitris Papailiopoulos},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024looped.pdf:pdf},
  note = {DBLP last modified: 2025-01-07},
  pdf = {https://openreview.net/pdf?id=HHbRxoDTxE},
  publisher = {OpenReview.net},
  title = {Looped Transformers are Better at Learning Learning Algorithms},
  url = {https://openreview.net/forum?id=HHbRxoDTxE},
  year = {2024}
}

@inproceedings{yang2024dnagpt,
  abstract = {Large language models (LLMs) have shown remarkable performance in various tasks. However, there are growing concerns about their potential misuse for generating harmful content. In this work, we propose DNA-GPT, a novel training-free detection strategy based on divergent N-gram analysis. Our method first truncates a text in the middle and then uses only the preceding portion as input to the LLMs to regenerate the new remaining parts. By analyzing the differences between the original and new remaining parts through N-gram analysis in black-box or probability divergence in white-box, we can illustrate significant discrepancies between machine-generated and human-written text. We conducted extensive experiments on advanced LLMs from OpenAI, including text-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such as GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach exhibits state-of-the-art performance in distinguishing between human and GPT-generated text on four English and one German dataset, outperforming OpenAI's own classifier.},
  author = {Xianjun Yang and Wei Cheng and Yue Wu and Linda Ruth Petzold and William Yang Wang and Haifeng Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024dnagpt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Xlayxj2fWp},
  publisher = {OpenReview.net},
  title = {{DNA-GPT}: Divergent {N}-Gram Analysis for Training-Free Detection of {GPT}-Generated Text},
  url = {https://openreview.net/forum?id=Xlayxj2fWp},
  year = {2024}
}

@inproceedings{yang2024testtime,
  abstract = {Existing test-time adaptation methods mainly concentrate on uni-modal tasks, overlooking the complexity of multi-modal scenarios. In this paper, we introduce a new challenge called reliability bias, which refers to information discrepancies across different modalities derived from intra-modal distribution shifts. To address this challenge, we propose a novel method called READ (REliable fusion and robust ADaptation). READ employs a new paradigm that modulates the attention between modalities in a self-adaptive way, supporting reliable fusion against reliability bias. It also adopts a novel objective function for robust multi-modal adaptation, where confident predictions are amplified and negative impacts of noisy predictions are mitigated. We introduce two new benchmarks to facilitate comprehensive evaluations of multi-modal TTA under reliability bias.},
  author = {Mouxing Yang and Yunfan Li and Changqing Zhang and Peng Hu and Xi Peng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024testtime.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=TPZRq4FALB},
  publisher = {OpenReview.net},
  title = {Test-time Adaptation against Multi-modal Reliability Bias},
  url = {https://openreview.net/forum?id=TPZRq4FALB},
  year = {2024}
}

@inproceedings{yang2024scalable,
  abstract = {Generative models trained on internet-scale data are capable of generating novel and realistic texts, images, and videos. A natural next question is whether these models can advance science, for example by generating novel stable materials. However, generative models for materials have faced significant challenges due to the complex multi-constraint nature of the problem, where generated materials must satisfy strict constraints like thermodynamic stability. In this work, we show how to scale up diffusion models on a novel unified representation of crystal structures that can generate orders of magnitude more novel stable materials compared to previous generative modeling approaches. Our model is trained on a comprehensive dataset and evaluated using metrics that directly assess the ability to generate novel and stable materials verified by Density Functional Theory calculations.},
  author = {Sherry Yang and KwangHwan Cho and Amil Merchant and Pieter Abbeel and Dale Schuurmans and Igor Mordatch and Ekin Dogus Cubuk},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024scalable.pdf:pdf},
  note = {DBLP last modified: 2024-09-06},
  pdf = {https://openreview.net/pdf?id=wm4WlHoXpC},
  publisher = {OpenReview.net},
  title = {Scalable Diffusion for Materials Generation},
  url = {https://openreview.net/forum?id=wm4WlHoXpC},
  year = {2024}
}

@inproceedings{yang2024denoising,
  abstract = {Denoising Diffusion Probabilistic Models (DDPMs) have garnered popularity for data generation across various domains. However, a significant bottleneck is the necessity for whole-network computation during every step of the generative process, leading to high computational overheads. This paper presents a novel framework, Denoising Diffusion Step-aware Models (DDSM), to address this challenge. Unlike conventional approaches, DDSM employs a spectrum of neural networks whose sizes are adapted according to the importance of each generative step, as determined through evolutionary search. This step-wise network variation effectively circumvents redundant computational efforts, particularly in less critical steps, thereby enhancing the efficiency of the diffusion model. Furthermore, the step-aware design can be seamlessly integrated with other efficiency-geared diffusion models such as DDIMs and latent diffusion, thus broadening the scope of computational savings. Empirical evaluations demonstrate that DDSM achieves computational savings of 49\% for CIFAR-10, 61\% for CelebA-HQ, 59\% for LSUN-bedroom, 71\% for AFHQ, and 76\% for ImageNet, all without compromising the generation quality.},
  author = {Shuai Yang and Yukang Chen and Luozhou Wang and Shu Liu and Ying-Cong Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024denoising.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=c43FGk8Pcg},
  publisher = {OpenReview.net},
  title = {Denoising Diffusion Step-aware Models},
  url = {https://openreview.net/forum?id=c43FGk8Pcg},
  year = {2024}
}

@inproceedings{yang2024probabilistic,
  abstract = {Large text-to-video models trained on internet-scale data have demonstrated exceptional capabilities in generating high-fidelity videos from arbitrary textual descriptions. However, similar to proprietary language models, large text-to-video models are often black boxes whose weight parameters are not publicly available, posing a significant challenge to adapting these models to specific domains such as robotics, animation, and personalized stylization. We propose Video Adapter, which leverages the score function of a large pretrained video diffusion model as a probabilistic prior to guide the generation of a task-specific small video model. Our experiments show that by incorporating broad knowledge and fidelity of the pretrained model probabilistically, a small model with as few as 1.25\% parameters of the pretrained model can generate high-quality yet domain-specific videos for a variety of downstream domains.},
  author = {Sherry Yang and Yilun Du and Bo Dai and Dale Schuurmans and Joshua B. Tenenbaum and Pieter Abbeel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024probabilistic.pdf:pdf},
  note = {DBLP last modified: 2024-09-06},
  pdf = {https://openreview.net/pdf?id=pjtIEgscE3},
  publisher = {OpenReview.net},
  title = {Probabilistic Adaptation of Black-Box Text-to-Video Models},
  url = {https://openreview.net/forum?id=pjtIEgscE3},
  year = {2024}
}

@inproceedings{yang2024learning,
  abstract = {Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as ``open the drawer'' and low-level controls such as ``move by x,y'' from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.},
  author = {Sherry Yang and Yilun Du and Seyed Kamyar Seyed Ghasemipour and Jonathan Tompson and Leslie Pack Kaelbling and Dale Schuurmans and Pieter Abbeel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-09-06. Outstanding Paper Award at ICLR 2024},
  pdf = {https://openreview.net/pdf?id=sFyTZEqmUY},
  publisher = {OpenReview.net},
  title = {Learning Interactive Real-World Simulators},
  url = {https://openreview.net/forum?id=sFyTZEqmUY},
  year = {2024}
}

@inproceedings{yang2024mixsup,
  abstract = {Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods. Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision. We start by observing that point clouds are usually textureless, making it hard to learn semantics. However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes. Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes. To reduce the annotation burden, we also propose PointSAM based on Segment Anything Model for automated coarse labeling. Extensive experiments on nuScenes, Waymo Open Dataset, and KITTI with various detectors demonstrate that MixSup achieves up to 97.31\% of fully supervised performance using cheap cluster annotations and only 10\% box annotations.},
  author = {Yuxue Yang and Lue Fan and Zhaoxiang Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024mixsup.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Q1vkAhdI6j},
  publisher = {OpenReview.net},
  title = {{MixSup}: Mixed-grained Supervision for Label-efficient {LiDAR}-based {3D} Object Detection},
  url = {https://openreview.net/forum?id=Q1vkAhdI6j},
  year = {2024}
}

@inproceedings{yang2024lipschitz,
  abstract = {Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed and well-conditioned. In this work, we explore a perplexing tendency of diffusion models: they often display infinite Lipschitz constants of the network with respect to the time variable near the zero point. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and present empirical results confirming this observation. To address this challenge, we propose a novel approach, dubbed E-TSDM (Eliminating Time Singularities in Diffusion Models), which alleviates the Lipschitz singularities of the diffusion model near the zero point of timesteps. As a byproduct of our method, we achieve a dramatic reduction in the Fr√©chet Inception Distance of acceleration methods relying on network Lipschitz, including DDIM and DPM-Solver, by over 33\%.},
  author = {Zhantao Yang and Ruili Feng and Han Zhang and Yujun Shen and Kai Zhu and Lianghua Huang and Yifei Zhang and Yu Liu and Deli Zhao and Jingren Zhou and Fan Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024lipschitz.pdf:pdf},
  note = {DBLP last modified: 2025-03-19. Oral presentation},
  pdf = {https://openreview.net/pdf?id=WNkW0cOwiz},
  publisher = {OpenReview.net},
  title = {Lipschitz Singularities in Diffusion Models},
  url = {https://openreview.net/forum?id=WNkW0cOwiz},
  year = {2024}
}

@inproceedings{yang2024emernerf,
  abstract = {We present EmerNeRF, a simple yet powerful approach for learning spatial-temporal representations of dynamic driving scenes. Grounded in neural fields, EmerNeRF simultaneously captures scene geometry, appearance, motion, and semantics via self-bootstrapping. Our approach stratifies scenes into static and dynamic fields. This decomposition emerges purely from self-supervision, enabling our model to learn from general, in-the-wild data sources. EmerNeRF parameterizes an induced flow field from the dynamic field and uses this flow field to further aggregate multi-frame features, amplifying the rendering precision of dynamic objects. Coupling these components, EmerNeRF achieves state-of-the-art performance in sensor simulation, significantly outperforming previous methods when reconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In addition, to bolster EmerNeRF's semantic generalization, we lift 2D visual foundation model features into 4D space-time and demonstrate remarkable 3D semantic emergence, with 37.50\% relative improvement in occupancy prediction accuracy on average.},
  author = {Jiawei Yang and Boris Ivanovic and Or Litany and Xinshuo Weng and Seung Wook Kim and Boyi Li and Tong Che and Danfei Xu and Sanja Fidler and Marco Pavone and Yue Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024emernerf.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=ycv2z8TYur},
  publisher = {OpenReview.net},
  title = {{EmerNeRF}: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision},
  url = {https://openreview.net/forum?id=ycv2z8TYur},
  year = {2024}
}

@inproceedings{yang2024rlcd,
  abstract = {We propose Reinforcement Learning from Contrastive Distillation (RLCD), a method for aligning language models to follow principles expressed in natural language (e.g., to be more harmless) without using human feedback. RLCD creates preference pairs from two contrasting model outputs, one using a positive prompt designed to encourage following the given principles, and one using a negative prompt designed to encourage violating them. Using two different prompts causes model outputs to be more differentiated on average, resulting in cleaner preference labels in the absence of human annotations. We then train a preference model on the resulting pairs, which is used to guide the LLM alignment via PPO. Empirically, RLCD outperforms RLAIF and context distillation methods for harmlessness and helpfulness alignment on LLaMA-7B and LLaMA-30B models. RLCD is also more efficient, requiring only 2,940 queries to the LLaMA-30B teacher model compared to the 62,000 needed for RLAIF.},
  author = {Kevin Yang and Dan Klein and Asli Celikyilmaz and Nanyun Peng and Yuandong Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024rlcd.pdf:pdf},
  note = {DBLP last modified: 2025-05-19},
  pdf = {https://openreview.net/pdf?id=v3XXtxWKi6},
  publisher = {OpenReview.net},
  title = {{RLCD}: Reinforcement Learning from Contrastive Distillation for {LM} Alignment},
  url = {https://openreview.net/forum?id=v3XXtxWKi6},
  year = {2024}
}

@inproceedings{yang2024navigating,
  abstract = {Advances in machine learning are closely tied to dataset creation. Despite the critical importance of data documentation, there is a lack of systematic empirical understanding of current dataset documentation practices. This paper presents the first large-scale analysis of dataset documentations in AI, focusing on HuggingFace as a prominent platform. We systematically analyze all 7,433 dataset cards on HuggingFace, examining their structure, content, and adherence to documentation guidelines. Our findings reveal significant gaps in current documentation practices: most dataset cards lack essential information about data provenance, ethical considerations, and potential limitations. We identify key patterns in documentation quality and provide recommendations for improving dataset documentation standards. This work contributes to the broader effort of making AI systems more transparent, reproducible, and responsible through better data practices.},
  author = {Xinyu Yang and Weixin Liang and James Zou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024navigating.pdf:pdf},
  note = {DBLP last modified: 2025-01-15},
  pdf = {https://openreview.net/pdf?id=xC8xh2RSs2},
  publisher = {OpenReview.net},
  title = {Navigating Dataset Documentations in {AI}: A Large-Scale Analysis of Dataset Cards on {HuggingFace}},
  url = {https://openreview.net/forum?id=xC8xh2RSs2},
  year = {2024}
}

@inproceedings{yang2024languageinterfaced,
  abstract = {Tabular data is frequently afflicted with class-imbalance, biasing machine learning model predictions towards major classes. A data-centric solution is oversampling where classes are balanced by adding synthetic minority samples via generative methods. Although tabular generative models can generate synthetic samples under a balanced distribution, their integrity suffers when the number of minority samples is low. Pre-trained generative language models with rich prior knowledge are fitting candidates for this task. We leverage their conditional sampling capabilities to synthesize minority samples by progressively masking important features of majority class samples and imputing them towards the minority distribution. We utilize the power of the language model to self-authenticate the labels of generated samples, filtering out ill-converted samples. Extensive experiments on various datasets and imbalance ratios reveal that our proposed method successfully generates reliable minority samples to boost classifier performance, even under heavy imbalance ratios.},
  author = {June Yong Yang and Geondo Park and Joowon Kim and Hyeongwon Jang 0001 and Eunho Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024languageinterfaced.pdf:pdf},
  note = {DBLP last modified: 2025-05-21},
  pdf = {https://openreview.net/pdf?id=8F6bws5JBy},
  publisher = {OpenReview.net},
  title = {Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication},
  url = {https://openreview.net/forum?id=8F6bws5JBy},
  year = {2024}
}

@inproceedings{yang2024can,
  abstract = {Unsupervised domain adaptation (UDA) involves adapting a model trained on a label-rich source domain to an unlabeled target domain. However, in real-world scenarios, the absence of target-domain labels makes it challenging to evaluate the performance of UDA models. Furthermore, prevailing UDA methods relying on adversarial training and self-training could lead to model degeneration and negative transfer, further exacerbating the evaluation problem. In this paper, we propose a novel metric called the Transfer Score to address these issues. The proposed metric enables the unsupervised evaluation of UDA models by assessing the spatial uniformity of the classifier via model parameters, as well as the transferability and discriminability of deep representations. Based on the metric, we achieve three novel objectives without target-domain labels: (1) selecting the best UDA method from a range of available options, (2) optimizing hyperparameters of UDA models to prevent model degeneration, and (3) avoiding negative transfer by providing an early stopping criterion during adaptation.},
  author = {Jianfei Yang and Hanjie Qian and Yuecong Xu and Kai Wang 0036 and Lihua Xie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024can.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fszrlQ2DuP},
  publisher = {OpenReview.net},
  title = {Can We Evaluate Domain Adaptation Models Without Target-Domain Labels?},
  url = {https://openreview.net/forum?id=fszrlQ2DuP},
  year = {2024}
}

@inproceedings{yang2024bayesian,
  abstract = {Low-rank adaptation ({LoRA}) has emerged as a new paradigm for cost-efficient fine-tuning of large language models ({LLMs}). However, fine-tuned {LLMs} often become overconfident especially when fine-tuned on small datasets. {Bayesian} methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce {Laplace-LoRA}, which applies a {Bayesian} approach to the {LoRA} parameters. Specifically, {Laplace-LoRA} applies a {Laplace} approximation to the posterior over the {LoRA} parameters, considerably improving the calibration of fine-tuned {LLMs}.},
  author = {Adam X. Yang and Maxime Robeyns and Xi Wang and Laurence Aitchison},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024bayesian.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FJiUyzOF1m},
  publisher = {OpenReview.net},
  title = {Bayesian Low-rank Adaptation for Large Language Models},
  url = {https://openreview.net/forum?id=FJiUyzOF1m},
  year = {2024}
}

@inproceedings{yang2024effect,
  abstract = {{Byzantine}-robust distributed learning ({BRDL}), in which computing devices are likely to behave abnormally due to accidental failures or malicious attacks, has recently become a hot research topic. However, even in the independent and identically distributed (i.i.d.) case, existing {BRDL} methods will suffer a significant drop on model accuracy due to the large variance of stochastic gradients. Increasing batch sizes is a simple yet effective way to reduce the variance. However, when the total number of gradient computation is fixed, a too-large batch size will lead to a too-small iteration number (update number), which may also degrade the model accuracy. In view of this challenge, we mainly study the effect of batch size when the total number of gradient computation is fixed in this work. In particular, we theoretically and empirically show that when the total number of gradient computation is fixed, the optimal batch size in {BRDL} increases with the fraction of {Byzantine} workers. Therefore, compared to the case without attacks, the batch size should be set larger when under {Byzantine} attacks.},
  author = {Yi-Rui Yang and Chang-Wei Shi and Wu-Jun Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024effect.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=wriKDQqiOQ},
  publisher = {OpenReview.net},
  title = {On the Effect of Batch Size in Byzantine-Robust Distributed Learning},
  url = {https://openreview.net/forum?id=wriKDQqiOQ},
  year = {2024}
}

@inproceedings{yang2024adamerging,
  abstract = {Multi-task learning ({MTL}) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute {MTL} without necessitating a retraining process using the initial training data. Nevertheless, this direct addition of models often leads to a significant deterioration in the overall performance of the merged model. This decline occurs due to potential conflicts and intricate correlations among the multiple tasks. This paper introduces an innovative technique called {Adaptive Model Merging} ({AdaMerging}). This approach aims to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data. Specifically, our {AdaMerging} method operates as an automatic, unsupervised task arithmetic scheme. It leverages entropy minimization on unlabeled test samples from the multi-task setup as a surrogate objective function to iteratively refine the merging coefficients of the multiple models.},
  author = {Enneng Yang and Zhenyi Wang and Li Shen and Shiwei Liu and Guibing Guo and Xingwei Wang and Dacheng Tao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024adamerging.pdf:pdf},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=nZP6NgD3QY},
  publisher = {OpenReview.net},
  title = {{AdaMerging}: Adaptive Model Merging for Multi-Task Learning},
  url = {https://openreview.net/forum?id=nZP6NgD3QY},
  year = {2024}
}

@inproceedings{yang2024maskbased,
  abstract = {Most Neural Radiance Fields ({NeRFs}) exhibit limited generalization capabilities, which restrict their applicability in representing multiple scenes using a single model. To address this problem, existing generalizable {NeRF} methods simply condition the model on image features. These methods still struggle to learn precise global representations over diverse scenes since they lack an effective mechanism for interacting among different points and views. We unveil that 3D implicit representation learning can be significantly improved by mask-based modeling. Specifically, we propose masked ray and view modeling for generalizable {NeRF} ({MRVM-NeRF}), which is a self-supervised pretraining target to predict complete scene representations from partially masked features along each ray. The high-level global information learned through mask-based pretraining, which we call the 3D scene prior knowledge, is extremely useful for generalizable Neural Radiance Field.},
  author = {Ganlin Yang and Guoqiang Wei and Zhizheng Zhang and Yan Lu and Dong Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024maskbased.pdf:pdf},
  note = {DBLP last modified: 2024-08-16},
  pdf = {https://openreview.net/pdf?id=SEiuSzlD1d},
  publisher = {OpenReview.net},
  title = {Mask-Based Modeling for Neural Radiance Fields},
  url = {https://openreview.net/forum?id=SEiuSzlD1d},
  year = {2024}
}

@inproceedings{yang2024bounding,
  abstract = {Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest. However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability. In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes. We compute the box stability score ({BoS} score) to measure this stability. We find that {BoS} score has a strong, positive correlation with standard metrics such as {AP} across a wide variety of detectors, training strategies, and datasets, enabling {BoS} score to serve as an effective proxy for assessing detector performance.},
  author = {Yang Yang and Wenhai Wang and Zhe Chen and Jifeng Dai and Liang Zheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024bounding.pdf:pdf},
  note = {DBLP last modified: 2024-11-18},
  pdf = {https://openreview.net/pdf?id=lmM4Ecm4HJ},
  publisher = {OpenReview.net},
  title = {Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments},
  url = {https://openreview.net/forum?id=lmM4Ecm4HJ},
  year = {2024}
}

@inproceedings{yang2024quantifying,
  abstract = {Multi-modal models show promising capability to integrate information from various sources, yet they are vulnerable to perturbations like uni-modal attacks and missing conditions, requiring robust multi-modal representations positioned well away from the discriminative decision boundary. However, multi-modal robust training faces the challenge of integrating gradients with different magnitudes from multiple modalities, causing issues like modality preference where the model overly relies on one modality while ignoring others. In this paper, we theoretically discover that larger uni-modal representation margins and more reliable integration for modalities are essential components for achieving higher robustness. We introduce a training procedure called {Certifiable Robust Multi-modal Training} ({CRMT}), which can alleviate influence from modality preference and explicitly regulate essential components to significantly improve robustness in a certifiable manner.},
  author = {Zequn Yang and Yake Wei and Ce Liang and Di Hu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024quantifying.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=XyrB1Ay44j},
  publisher = {OpenReview.net},
  title = {Quantifying and Enhancing Multi-modal Robustness with Modality Preference},
  url = {https://openreview.net/forum?id=XyrB1Ay44j},
  year = {2024}
}

@inproceedings{yang2024gnerp,
  abstract = {Existing {NeRF} methods for reconstructing specular objects produce unsatisfactory results due to the entanglement of specular radiance and complicated geometry. Recent methods have tried to tackle this issue by introducing polarization priors, but these methods still struggle with highly reflective surfaces as they assume that objects are exclusively reflective. In this paper, we propose {GNeRP}, a {Gaussian}-based representation of normals in {SDF} fields. Supervised by polarization priors, this representation guides the learning of geometry behind the specular reflection and captures more details than existing methods. We also propose a reweighting strategy in the optimization process to alleviate the noise issue of polarization priors. Comparisons prove that our method outperforms existing neural 3D reconstruction methods in reflective scenes by a large margin.},
  author = {Li Yang and Ruizheng Wu and Jiyong Li and Ying-Cong Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024gnerp.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=pTN8dV2pL8},
  publisher = {OpenReview.net},
  title = {{GNeRP}: {Gaussian}-guided Neural Reconstruction of Reflective Objects with Noisy Polarization Priors},
  url = {https://openreview.net/forum?id=pTN8dV2pL8},
  year = {2024}
}

@inproceedings{yang2024unitabe,
  abstract = {The advancements in large language models ({LLMs}) have ignited interest in domain-specific large models for tabular data in data science applications, yet existing methods have notable limitations. In this paper, we aim to design a tabular language model for universal tabular understanding. We begin by analyzing the inherent properties of tabular data, identifying three key challenges: {1}) table is a composite of multiple information including meta information and cell entities, {2}) tabular cells are highly heterogeneous and lack standardized structure, and {3}) cell entities are closely related through implicit table structures. To tackle these challenges, we design a straightforward yet effective method to process tables in a uniform manner, devoid of constraints imposed by specific table structures. Our proposed method, termed as {UniTabE}, is a universal pretraining protocol for tabular foundation model in data science. The core idea of {UniTabE} is to represent each basic table element with a module, termed {TabUnit}, followed by a {Transformer} encoder to refine the representation. We pretrain {UniTabE} on an expansive tabular dataset comprising approximately 13 billion samples, meticulously gathered from the {Kaggle} platform.},
  author = {Yazheng Yang and Yuqi Wang and Guang Liu and Ledell Wu and Qi Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024unitabe.pdf:pdf},
  note = {DBLP last modified: 2024-11-15},
  pdf = {https://openreview.net/pdf?id=6LLho5X6xV},
  publisher = {OpenReview.net},
  title = {{UniTabE}: A Universal Pretraining Protocol for Tabular Foundation Model in Data Science},
  url = {https://openreview.net/forum?id=6LLho5X6xV},
  year = {2024}
}

@inproceedings{yang2024dmbp,
  abstract = {Offline reinforcement learning ({RL}) aims to derive effective policies from historical datasets without further interaction with the environment. Recent progress has shown that the incorporation of diffusion models can significantly boost the performance of offline {RL} algorithms. However, offline methods can be overly conservative or fail to generate optimal actions in perturbed states unobserved during training. This becomes particularly severe for state-based {RL} when the testing environment has perturbed state observations. In this work, we propose {DMBP}, a novel framework that recovers the actual states with conditional diffusion models for state-based {RL} tasks, and derive robust policies to handle the perturbations. To mitigate the error accumulation issue in model-based estimation, we propose a non-{Markovian} training objective to minimize the sum entropy of denoised states in {RL} trajectory. Experiments on standard benchmark problems demonstrate that {DMBP} can significantly enhance the robustness of existing offline {RL} algorithms against different scales of random noises and adversarial attacks on state observations.},
  author = {Zhihe Yang and Yunjian Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024dmbp.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZULjcYLWKe},
  publisher = {OpenReview.net},
  title = {{DMBP}: Diffusion model-based predictor for robust offline reinforcement learning against state observation perturbations},
  url = {https://openreview.net/forum?id=ZULjcYLWKe},
  year = {2024}
}

@inproceedings{yang2024neural,
  abstract = {Neural field methods have seen great progress in computer vision and computer graphics tasks including novel view synthesis and geometry reconstruction. The common pipeline is to predict coordinate-based continuous target values and optimize via regression loss. In this work, we investigate the fundamental question of whether regression models are really better than classification models for neural field methods. We make the surprising finding that neural field classifiers can significantly outperform the standard regression-based neural field counterparts. We further analyze the advantages of neural field classifiers: uniform sampling over the target space, training efficiency, and better coverage of the sampling space. To demonstrate the effectiveness of neural field classifiers, we conduct comprehensive experiments across multiple application domains including novel view synthesis, signed distance field reconstruction, and image representation, achieving remarkable improvements over existing methods. We believe neural field classifiers will become a powerful new paradigm for neural field methods.},
  author = {Xindi Yang and Zeke Xie and Xiong Zhou and Boyu Liu and Buhua Liu and Yi Liu and Haoran Wang and Yunfeng Cai and Mingming Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024neural.pdf:pdf},
  note = {DBLP last modified: 2025-04-14},
  pdf = {https://openreview.net/pdf?id=9NqC72m31m},
  publisher = {OpenReview.net},
  title = {Neural Field Classifiers via Target Encoding and Classification Loss},
  url = {https://openreview.net/forum?id=9NqC72m31m},
  year = {2024}
}

@inproceedings{yang2024realtime,
  abstract = {Reconstructing dynamic 3D scenes from 2D images and generating diverse views over time is challenging due to scene complexity and temporal dynamics. Despite advancements in neural implicit models, limitations persist: (i) Inadequate Scene Structure: Existing methods struggle to reveal the spatial and temporal structure of dynamic scenes from directly learning the complex 6D plenoptic function. (ii) Scaling Deformation Modeling: Explicitly modeling scene element deformation becomes impractical for complex dynamics. To address these issues, we consider the spacetime as an entirety and propose to approximate the underlying spatio-temporal 4D volume of a dynamic scene by optimizing a collection of 4D primitives, with explicit geometry and appearance modeling. Our model is conceptually simple, consisting of a 4D Gaussian parameterized by anisotropic ellipses that can rotate arbitrarily in space and time, as well as view-dependent and time-evolved appearance represented by the coefficient of 4D spherindrical harmonics. This approach offers simplicity, flexibility for variable-length video and end-to-end training, and efficient real-time rendering, making it suitable for capturing complex dynamic scene motions.},
  author = {Zeyu Yang and Hongye Yang and Zijie Pan and Li Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024realtime.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=WhgB5sispV},
  publisher = {OpenReview.net},
  title = {Real-time Photorealistic Dynamic Scene Representation and Rendering with {4D} {G}aussian {S}platting},
  url = {https://openreview.net/forum?id=WhgB5sispV},
  year = {2024}
}

@inproceedings{yang2024impus,
  abstract = {We present a diffusion-based image morphing approach with perceptually-uniform sampling (IMPUS) that produces smooth, direct and realistic interpolations given an image pair. The embeddings of two images may lie on distinct conditioned distributions of a latent diffusion model, especially when they have significant semantic difference. To bridge this gap, we interpolate in the locally linear and continuous text embedding space and Gaussian latent space. We first optimize the endpoint text embeddings and then map the images to the latent space using a probability flow ODE. Unlike existing work that takes an indirect morphing path, we show that the model adaptation yields a direct path and suppresses ghosting artifacts in the interpolated images. To achieve this, we propose a heuristic bottleneck constraint based on a novel relative perceptual path diversity score that automatically controls the bottleneck size and balances the diversity along the path with its directness. We also propose a perceptually-uniform sampling technique that enables visually smooth changes between the interpolated images. Extensive experiments validate that our IMPUS can achieve smooth, direct, and realistic image morphing and is adaptable to several other generative tasks.},
  author = {Zhaoyuan Yang and Zhengyang Yu and Zhiwei Xu and Jaskirat Singh and Jing Zhang and Dylan Campbell and Peter H. Tu and Richard Hartley},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024impus.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=gG38EBe2S8},
  publisher = {OpenReview.net},
  title = {{IMPUS}: Image Morphing with Perceptually-Uniform Sampling Using Diffusion Models},
  url = {https://openreview.net/forum?id=gG38EBe2S8},
  year = {2024}
}

@inproceedings{yang2024tensor,
  abstract = {By classifying infinite-width neural networks and identifying the optimal limit, Tensor Programs IV and V demonstrated a universal way, called ŒºP, for widthwise hyperparameter transfer, i.e., predicting optimal hyperparameters of wide neural networks from narrow ones. Here we investigate the analogous classification for depthwise parametrizations of deep residual networks (ResNets). We classify depthwise parametrizations of block multiplier and learning rate by their infinite-width-then-depth limits. In ResNets where each block has only one layer, we identify a unique optimal parametrization, called Depth-ŒºP that extends ŒºP and show empirically it admits depthwise hyperparameter transfer. However, if each block is deeper (such as modern transformers), then we find fundamental limitations in all possible infinite-depth limits of such parametrizations. We demonstrate this both theoretically and empirically on networks including the Megatron Transformer. This work introduces Depth-ŒºP, a principled approach for depth scaling that allows for training arbitrarily deep networks while maximizing feature learning and feature diversity.},
  author = {Greg Yang and Dingli Yu and Chen Zhu and Soufiane Hayou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024tensor.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=17pVDnpwwl},
  publisher = {OpenReview.net},
  title = {Tensor Programs {VI}: Feature Learning in Infinite Depth Neural Networks},
  url = {https://openreview.net/forum?id=17pVDnpwwl},
  year = {2024}
}

@inproceedings{yang2024fedtrans,
  abstract = {Federated Learning (FL) is an important privacy-preserving learning paradigm that plays an important role in the Intelligent Internet of Things. Training a global model in FL, however, is vulnerable to the noise in the heterogeneous data across the clients. In this paper, we introduce FedTrans, a novel client-transparent client utility estimation method designed to guide client selection for noisy scenarios, mitigating performance degradation problems. To estimate the client utility, we propose a Bayesian framework that models client utility and its relationships with the weight parameters and the performance of local models. We then introduce a variational inference algorithm to effectively infer client utility, given only a small amount of auxiliary data. Our evaluation demonstrates that leveraging FedTrans as a guide for client selection can lead to a better accuracy performance (up to 7.8\%), ensuring robustness in noisy scenarios.},
  author = {Mingkun Yang and Ran Zhu and Qing Wang and Jie Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024fedtrans.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=DRu8PMHgCh},
  publisher = {OpenReview.net},
  title = {{FedTrans}: Client-Transparent Utility Estimation for Robust Federated Learning},
  url = {https://openreview.net/forum?id=DRu8PMHgCh},
  year = {2024}
}

@inproceedings{yang2024movingparts,
  abstract = {We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions.},
  author = {Kaizhi Yang and Xiaoshuai Zhang and Zhiao Huang and Xuejin Chen and Zexiang Xu and Hao Su},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024movingparts.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=QQ6RgKYiQq},
  publisher = {OpenReview.net},
  title = {{MovingParts}: Motion-based {3D} Part Discovery in Dynamic Radiance Field},
  url = {https://openreview.net/forum?id=QQ6RgKYiQq},
  year = {2024}
}

@inproceedings{yang2024imagenetood,
  abstract = {The task of out-of-distribution (OOD) detection is notoriously ill-defined. Earlier works focused on new-class detection, aiming to identify label-altering data distribution shifts, also known as 'semantic shift.' However, recent works argue for a focus on failure detection, expanding the OOD evaluation framework to account for label-preserving data distribution shifts, also known as 'covariate shift.' Intriguingly, under this new framework, complex OOD detectors that were previously considered state-of-the-art now perform similarly to, or even worse than the simple maximum softmax probability baseline. This raises the question: what are the latest OOD detectors actually detecting? Deciphering the behavior of OOD detection algorithms requires evaluation datasets that decouples semantic shift and covariate shift. To aid our investigations, we present ImageNet-OOD, a clean semantic shift dataset that minimizes the interference of covariate shift. Through comprehensive experiments, we show that OOD detectors are more sensitive to covariate shift than to semantic shift, and the benefits of recent OOD detection algorithms on semantic shift detection is minimal. Our dataset and analyses provide important insights for guiding the design of future OOD detectors.},
  author = {William Yang and Byron Zhang and Olga Russakovsky},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024imagenetood.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=VTYg5ykEGS},
  publisher = {OpenReview.net},
  title = {ImageNet-OOD: Deciphering Modern Out-of-Distribution Detection Algorithms},
  url = {https://openreview.net/forum?id=VTYg5ykEGS},
  year = {2024}
}

@inproceedings{yang2024supervised,
  abstract = {Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks.},
  author = {Linyi Yang and Shuibai Zhang and Zhuohao Yu and Guangsheng Bao and Yidong Wang and Jindong Wang and Ruochen Xu and Wei Ye and Xing Xie and Weizhu Chen and Yue Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yang2024supervised.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=bAMPOUF227},
  publisher = {OpenReview.net},
  title = {Supervised Knowledge Makes Large Language Models Better In-context Learners},
  url = {https://openreview.net/forum?id=bAMPOUF227},
  year = {2024}
}

@inproceedings{yao2024collie,
  abstract = {Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g. generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g. language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 1,132 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.},
  author = {Shunyu Yao and Howard Chen and Austin W. Hanjie and Runzhe Yang and Karthik R. Narasimhan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024collie.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=kxgSlyirUZ},
  publisher = {OpenReview.net},
  title = {COLLIE: Systematic Construction of Constrained Text Generation Tasks},
  url = {https://openreview.net/forum?id=kxgSlyirUZ},
  year = {2024}
}

@inproceedings{yao2024zipformer,
  abstract = {The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explicitly learns the parameter scale. It achieves faster converge and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models.},
  author = {Zengwei Yao and Liyong Guo and Xiaoyu Yang and Wei Kang and Fangjun Kuang and Yifan Yang and Zengrui Jin and Long Lin and Daniel Povey},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024zipformer.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=9WD9KwssyT},
  publisher = {OpenReview.net},
  title = {Zipformer: A faster and better encoder for automatic speech recognition},
  url = {https://openreview.net/forum?id=9WD9KwssyT},
  year = {2024}
}

@inproceedings{yao2024spikedriven,
  abstract = {Neuromorphic computing, which exploits Spiking Neural Networks (SNNs) on neuromorphic chips, is a promising energy-efficient alternative to traditional AI. CNN-based SNNs are the current mainstream of neuromorphic computing. By contrast, no neuromorphic chips are designed especially for Transformer-based SNNs, which have just emerged, and their performance is only on par with CNN-based SNNs, offering no distinct advantage. In this work, we propose a general Transformer-based SNN architecture, termed as "Meta-SpikeFormer", whose goals are: 1) Lower-power, supports the spike-driven paradigm that there is only sparse addition in the network; 2) Versatility, handles various vision tasks; 3) High-performance, shows overwhelming performance advantages over CNN-based SNNs; 4) Meta-architecture, provides inspiration for future next-generation Transformer-based neuromorphic chip designs. Specifically, we extend the Spike-driven Transformer and explore the impact of structure, spike-driven self-attention, and skip connection on its performance. On ImageNet-1K, Meta-SpikeFormer achieves 80.0% top-1 accuracy (55M), surpassing the current state-of-the-art (SOTA) SNN baselines (66M) by 3.7%. This is the first direct training SNN backbone that can simultaneously supports classification, detection, and segmentation, obtaining SOTA results in SNNs. Finally, we discuss the inspiration of the meta SNN architecture for neuromorphic chip design.},
  author = {Man Yao and Jiakui Hu and Tianxiang Hu and Yifan Xu and Zhaokun Zhou and Yonghong Tian and Bo Xu and Guoqi Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024spikedriven.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=1SIBN5Xyw7},
  publisher = {OpenReview.net},
  title = {Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips},
  url = {https://openreview.net/forum?id=1SIBN5Xyw7},
  year = {2024}
}

@inproceedings{yao2024retroformer,
  abstract = {Recent months have witnessed the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient optimization. Specifically, our proposed method, Retroformer, comprises two language model components: an actor LLM, which generates reasoning thoughts and actions, and a retrospective LLM, which generates verbal reinforcement cues. The Retroformer method focuses on fine-tuning the retrospective model in the language agent system architecture, without accessing the Actor LLM parameters or needing to propagate gradients through it. We demonstrate the effectiveness of our approach on real-world tasks including HotPotQA, AlfWorld, and WebShop.},
  author = {Weiran Yao and Shelby Heinecke and Juan Carlos Niebles and Zhiwei Liu and Yihao Feng and Le Xue and Rithesh R. N. and Zeyuan Chen and Jianguo Zhang and Devansh Arpit and Ran Xu and Phil Mui and Huan Wang and Caiming Xiong and Silvio Savarese},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024retroformer.pdf:pdf},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=KOZu91CzbK},
  publisher = {OpenReview.net},
  title = {Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization},
  url = {https://openreview.net/forum?id=KOZu91CzbK},
  year = {2024}
}

@inproceedings{yao2024constructing,
  abstract = {Vertical federated learning (VFL) is a promising approach that enables collaborative learning among parties with overlapping samples but disjoint features, which has found applications in finance, healthcare, and IoT systems. However, adversarial attacks, particularly through the injection of adversarial examples (AEs), pose serious challenges to the security of VFL models. In this paper, we investigate the vulnerabilities of VFL models by developing a novel adversarial attack methodology. Specifically, we propose a novel adversarial attack on inference of vertical federated learning, which selects the optimal client corruption pattern through solving a multi-armed bandit problem. Under a practical scenario where the adversary can adaptively corrupt a subset of clients, we formulate the problem as an online optimization problem with an inner problem of adversarial example generation (AEG) and an outer problem of corruption pattern selection. We provide theoretical analysis on the convergence of the proposed method and conduct extensive experiments to demonstrate its effectiveness in disrupting VFL model performance.},
  author = {Duanyi Yao and Songze Li and Ye Xue and Jin Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024constructing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=m52uU0dVbH},
  publisher = {OpenReview.net},
  title = {Constructing Adversarial Examples for Vertical Federated Learning: Optimal Client Corruption through Multi-Armed Bandit},
  url = {https://openreview.net/forum?id=m52uU0dVbH},
  year = {2024}
}

@inproceedings{yao2024multiview,
  abstract = {We present a unified framework for studying the identifiability of representations learned from simultaneously observed views, such as different data modalities, in a partially observed setting where each view constitutes a nonlinear mixture of a subset of underlying latent variables, which can be causally related. We prove that the information shared across all subsets of any number of views can be learned up to a smooth bijection using contrastive learning and a single encoder per view. We also provide graphical criteria indicating which latent variables can be identified through a simple set of rules, which we refer to as identifiability algebra. Our general framework and theoretical results unify and extend several previous works on multi-view nonlinear ICA, disentanglement, and causal representation learning. We experimentally validate our claims on numerical, image, and multi-modal data sets.},
  author = {Dingling Yao and Danru Xu and S√©bastien Lachapelle and Sara Magliacane and Perouz Taslakian and Georg Martius and Julius von K√ºgelgen and Francesco Locatello},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024multiview.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=OGtnhKQJms},
  publisher = {OpenReview.net},
  title = {Multi-View Causal Representation Learning with Partial Observability},
  url = {https://openreview.net/forum?id=OGtnhKQJms},
  year = {2024}
}

@inproceedings{yao2024improving,
  abstract = {Domain generalization is challenging when training domains have limited coverage of the target domain. In this work, we propose D¬≥G, a novel approach that addresses this problem by incorporating external domain information. Instead of learning a single domain-invariant model, D¬≥G leverages domain similarities based on domain metadata to learn domain-specific models. These domain relations can be directly obtained and learned from domain metadata. The approach learns functions that are specific to the domain it is trained on, then reweights during the test stage based on how similar a new domain is to the training domain. We show that D¬≥G consistently outperforms strong baselines on both synthetic and real-world benchmarks, demonstrating the effectiveness of using domain relations for domain generalization.},
  author = {Huaxiu Yao and Xinyu Yang and Xinyi Pan and Shengchao Liu and Pang Wei Koh and Chelsea Finn},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024improving.pdf:pdf},
  note = {DBLP last modified: 2025-01-14},
  pdf = {https://openreview.net/pdf?id=Dc4rXq3HIA},
  publisher = {OpenReview.net},
  title = {Improving Domain Generalization with Domain Relations},
  url = {https://openreview.net/forum?id=Dc4rXq3HIA},
  year = {2024}
}

@inproceedings{yao2024constrained,
  abstract = {This paper presents a new approach and algorithm for solving a class of constrained Bi-Level Optimization (BLO) problems in which the lower-level problem involves constraints coupling both upper-level and lower-level variables. Such problems have recently gained significant attention due to their broad applicability in machine learning. We devise a smooth proximal Lagrangian value function to handle the constrained lower-level problem. We develop a Hessian-free gradient-based algorithm‚Äîtermed proximal Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA)‚Äîthat is straightforward to implement in a single loop manner. The paper offers non-asymptotic convergence analysis for LV-HBA, eliminating the need for traditional strong convexity assumptions for the lower-level problem while also being capable of accommodating non-singleton scenarios. Empirical results substantiate the algorithm's superior practical performance.},
  author = {Wei Yao and Chengming Yu and Shangzhi Zeng and Jin Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024constrained.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=xJ5N8qrEPl},
  publisher = {OpenReview.net},
  title = {Constrained Bi-Level Optimization: Proximal Lagrangian Value Function Approach and Hessian-free Algorithm},
  url = {https://openreview.net/forum?id=xJ5N8qrEPl},
  year = {2024}
}

@inproceedings{yao2024masked,
  abstract = {We propose Masked Structural Growth (MSG), a novel framework for progressive pre-training of language models to speed up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems associated with progressive growth: determining the optimal growth schedule and designing efficient growth operators. MSG addresses these challenges by introducing (i) growth schedules involving all possible dimensions and (ii) strictly function-preserving growth operators that are independent of the initialization of new weights. Experiments show that MSG can achieve up to 2.2x speedup in pre-training different types of LLMs while maintaining comparable or better downstream performance. MSG proposes a novel framework for progressive pre-training of language models, achieving up to 2.2x speedup while maintaining comparable or better downstream performances.},
  author = {Yiqun Yao and Zheng Zhang and Jing Li and Yequan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yao2024masked.pdf:pdf},
  note = {DBLP last modified: 2025-02-24},
  pdf = {https://openreview.net/pdf?id=rL7xsg1aRn},
  publisher = {OpenReview.net},
  title = {Masked Structural Growth for 2x Faster Language Model Pre-training},
  url = {https://openreview.net/forum?id=rL7xsg1aRn},
  year = {2024}
}

@inproceedings{yasunaga2024large,
  abstract = {We introduce analogical prompting, a new approach designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning‚Äîa cognitive process where humans draw from relevant past experiences to tackle new problems‚Äîour approach prompts language models to self-generate relevant exemplars or knowledge in the context before proceeding to solve the given problem. This method presents several advantages: it eliminates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench, with an average accuracy gain of +5%.},
  author = {Michihiro Yasunaga and Xinyun Chen and Yujia Li and Panupong Pasupat and Jure Leskovec and Percy Liang and Ed H. Chi and Denny Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yasunaga2024large.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AgDICX1h50},
  publisher = {OpenReview.net},
  title = {Large Language Models as Analogical Reasoners},
  url = {https://openreview.net/forum?id=AgDICX1h50},
  year = {2024}
}

@inproceedings{ye2024ptarl,
  abstract = {PTaRL proposes a prototype-based tabular representation learning framework for tabular prediction tasks. The motivation stems from existing deep tabular ML methods suffering from representation entanglement and localization, which hinders prediction performance and leads to inconsistency. The core idea of PTaRL is to construct prototype-based projection space (P-Space) and learn disentangled representation around global data prototypes. PTaRL involves two main stages: (i) Prototype Generation, which constructs global prototypes as basis vectors of P-Space, and (ii) Prototype Projection, which projects data samples into P-Space while preserving core global data information via Optimal Transport. The method uses two constraint strategies: (i) a diversification constraint for representation calibration to diversify coordinates towards global prototypes, and (ii) a matrix orthogonalization constraint to ensure independence of global prototypes and avoid entanglement.},
  author = {Hangting Ye and Wei Fan and Xiaozhuang Song and Shun Zheng and He Zhao and Dandan Guo and Yi Chang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024ptarl.pdf:pdf},
  note = {DBLP last modified: 2025-06-16},
  pdf = {https://openreview.net/pdf?id=G32oY4Vnm8},
  publisher = {OpenReview.net},
  title = {{PTaRL}: Prototype-based Tabular Representation Learning via Space Calibration},
  url = {https://openreview.net/forum?id=G32oY4Vnm8},
  year = {2024}
}

@inproceedings{ye2024fake,
  abstract = {Domain generalization is challenging when training domains have limited coverage of the target domain. Data heterogeneity in federated learning (FL) is a key bottleneck that causes model divergence and limits performance. Instead of treating data heterogeneity as an inherent property and correcting models, we propose generating data to complement the original dataset to fundamentally mitigate heterogeneity level. We introduce federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process.},
  author = {Rui Ye and Yaxin Du and Zhenyang Ni and Yanfeng Wang and Siheng Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024fake.pdf:pdf},
  note = {DBLP last modified: 2024-11-14},
  pdf = {https://openreview.net/pdf?id=NY3wMJuaLf},
  publisher = {OpenReview.net},
  title = {Fake It Till Make It: Federated Learning with Consensus-Oriented Generation},
  url = {https://openreview.net/forum?id=NY3wMJuaLf},
  year = {2024}
}

@inproceedings{ye2024bayes,
  abstract = {The paper introduces the concept of conditional mutual information (CMI) into the estimation of BCPD and proposes a novel estimator called the maximum CMI (MCMI) method. In MCMI estimation, both the log-likelihood and CMI of the teacher are simultaneously maximized when the teacher is trained. By employing a teacher trained via MCMI estimation rather than one trained via MLL estimation in various state-of-the-art KD frameworks, the student's classification accuracy consistently increases, with the gain of up to 3.32%. The student's accuracy increases with the gain of up to 5.72% when 5% of the training samples are available to the student (few-shot), and increases from 0% to as high as 84% for an omitted class (zero-shot). The paper addresses the fundamental question of how teachers in knowledge distillation can provide better estimates of the Bayes conditional probability distribution, proposing an improvement over the conventional maximum log-likelihood method.},
  author = {Linfeng Ye and Shayan Mohajer Hamidi and Renhao Tan and En-Hui Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024bayes.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=yV6wwEbtkR},
  publisher = {OpenReview.net},
  title = {Bayes Conditional Distribution Estimation for Knowledge Distillation Based on Conditional Mutual Information},
  url = {https://openreview.net/forum?id=yV6wwEbtkR},
  year = {2024}
}

@inproceedings{ye2024flask,
  abstract = {Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlation between model-based and human-based evaluations.},
  author = {Seonghyeon Ye and Doyoung Kim and Sungdong Kim and Hyeonbin Hwang and Seungone Kim and Yongrae Jo and James Thorne and Juho Kim and Minjoon Seo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024flask.pdf:pdf},
  keywords = {large language models, language model evaluation, natural language processing},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=CYmF38ysDa},
  publisher = {OpenReview.net},
  title = {{FLASK}: Fine-grained Language Model Evaluation based on Alignment Skill Sets},
  url = {https://openreview.net/forum?id=CYmF38ysDa},
  year = {2024}
}

@inproceedings{ye2024adaptive,
  abstract = {Multi-objective optimization (MOO) has become an influential framework for various machine learning problems, including reinforcement learning and multi-task learning. In this paper, we study the black-box multi-objective optimization problem, where we aim to optimize multiple potentially conflicting objectives with function queries only. The paper proposes a novel algorithm called ASMG (Adaptive Stochastic Gradient) to address this optimization challenge, using stochastic gradient approximation to obtain gradients and employing an adaptive weight to aggregate stochastic gradients. We provide theoretical convergence proofs for convex and non-convex scenarios and demonstrate competitive performance on numerical benchmarks and multi-task learning problems.},
  author = {Feiyang Ye and Yueming Lyu and Xuehao Wang and Yu Zhang and Ivor W. Tsang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024adaptive.pdf:pdf},
  keywords = {multi-objective optimization, black-box optimization, black-box multi-objective optimization},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=bm1JVsVZVu},
  publisher = {OpenReview.net},
  title = {Adaptive Stochastic Gradient Algorithm for Black-box Multi-Objective Learning},
  url = {https://openreview.net/forum?id=bm1JVsVZVu},
  year = {2024}
}

@inproceedings{ye2024lightmilpopt,
  abstract = {Machine Learning (ML)-based optimization approaches emerge as a promising technique for solving large-scale Mixed Integer Linear Programs (MILPs). However, existing ML-based frameworks suffer from high model computation complexity, weak problem reduction, and reliance on large-scale optimizers and large training datasets, resulting in performance bottlenecks for large-scale MILPs. This paper proposes Light-MILPopt, a lightweight large-scale optimization framework that only uses a lightweight optimizer and small training dataset to solve large-scale MILPs. Specifically, Light-MILPopt can be divided into four stages: Problem Formulation for problem division to reduce model computational costs, Model-based Initial Solution Prediction for predicting and constructing the initial solution using a small-scale training dataset, Problem Reduction for both variable and constraint reduction, and Data-driven Optimization for current solution improvement employing a lightweight optimizer.},
  author = {Huigen Ye and Hua Xu and Hongyan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024lightmilpopt.pdf:pdf},
  keywords = {large-scale MILP, learning for optimization, lightweight optimization framework, mixed integer linear programming},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=2oWRumm67L},
  publisher = {OpenReview.net},
  title = {Light-{MILPopt}: Solving Large-scale Mixed Integer Linear Programs with Lightweight Optimizer and Small-scale Training Dataset},
  url = {https://openreview.net/forum?id=2oWRumm67L},
  year = {2024}
}

@inproceedings{ye2024real3dportrait,
  abstract = {One-shot 3D talking portrait generation aims to reconstruct a 3D avatar from an unseen image, and then animate it with a reference video or audio to generate a talking portrait video. The existing methods fail to simultaneously achieve the goals of accurate 3D avatar reconstruction and stable talking face animation. Besides, while the existing works mainly focus on synthesizing the head part, it is also vital to generate natural torso and background segments to obtain a realistic talking portrait video. To summarize, in this paper, we propose Real3D-Portrait, a one-shot and Realistic 3D talking Portrait generation method that: (1) improves the 3D reconstruction and animation power with I2P model and motion adapter; (2) achieves natural torso movement and switchable background rendering with HTB-SR model; and (3) proposes a generic A2M model, hence becomes the first one-shot 3D face system that both supports audio and video-driven scenarios. Extensive experiments show that Real3D-Portrait generalizes well to unseen identities and generates more realistic talking portrait videos compared to previous methods.},
  author = {Zhenhui Ye and Tianyun Zhong and Yi Ren and Jiaqi Yang and Weichuang Li and Jiawei Huang and Ziyue Jiang and Jinzheng He and Rongjie Huang and Jinglin Liu and Chen Zhang and Xiang Yin and Zejun Ma and Zhou Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ye2024real3dportrait.pdf:pdf},
  keywords = {3D face synthesis, talking portrait, neural rendering, computer vision},
  note = {Spotlight presentation},
  pdf = {https://openreview.net/pdf?id=7ERQPyR2eb},
  publisher = {OpenReview.net},
  title = {Real3D-Portrait: One-shot Realistic {3D} Talking Portrait Synthesis},
  url = {https://openreview.net/forum?id=7ERQPyR2eb},
  year = {2024}
}

@inproceedings{yeh2024navigating,
  abstract = {Text-to-image generative models have garnered immense attention for their ability to produce high-fidelity images from text prompts. Among these, Stable Diffusion distinguishes itself as a leading open-source model in this fast-growing field. However, the intricacies of fine-tuning these models pose multiple challenges from new methodology integration to systematic evaluation. Addressing these issues, this paper introduces LyCORIS (Lora beYond Conventional methods, Other Rank adaptation Implementations for Stable diffusion), an open-source library that offers a wide selection of fine-tuning methodologies for Stable Diffusion. Furthermore, we present a thorough framework for the systematic assessment of varied fine-tuning techniques. This framework employs a diverse suite of metrics and delves into multiple facets of fine-tuning, including hyperparameter adjustments and the evaluation with different prompt types across various concept categories.},
  author = {Shih-Ying Yeh and Yu-Guan Hsieh and Zhidong Gao and Bernard B. W. Yang and Giyeong Oh and Yanmin Gong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yeh2024navigating.pdf:pdf},
  keywords = {text-to-image generation, stable diffusion, fine-tuning, computer vision},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=wfzXa8e783},
  publisher = {OpenReview.net},
  title = {Navigating Text-To-Image Customization: From {LyCORIS} Fine-Tuning to Model Evaluation},
  url = {https://openreview.net/forum?id=wfzXa8e783},
  year = {2024}
}

@inproceedings{yehudai2024achieving,
  abstract = {The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data.},
  author = {Asaf Yehudai and Boaz Carmeli and Yosi Mass and Ofir Arviv and Nathaniel Mills and Eyal Shnarch and Leshem Choshen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yehudai2024achieving.pdf:pdf},
  keywords = {data generation, content-grounded long-form question-answering, summarization, generative models},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=RjYKTQ0L0W},
  publisher = {OpenReview.net},
  title = {Achieving Human Parity in Content-Grounded Datasets Generation},
  url = {https://openreview.net/forum?id=RjYKTQ0L0W},
  year = {2024}
}

@inproceedings{yemini2024lipvoicer,
  abstract = {Lip-to-speech involves generating natural-sounding speech synchronized with a soundless video. The paper presents LipVoicer, a novel method that generates high-quality speech by incorporating text modality. The approach involves predicting spoken text using a pre-trained lip-reading network, conditioning a diffusion model on video, and using extracted text through a classifier-guidance mechanism with a pre-trained ASR. The authors demonstrate LipVoicer's effectiveness through performance on LRS2 and LRS3 datasets, showing improved speech intelligibility and synchronization compared to existing methods.},
  author = {Yochai Yemini and Aviv Shamsian and Lior Bracha and Sharon Gannot and Ethan Fetaya},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yemini2024lipvoicer.pdf:pdf},
  keywords = {lip-to-speech, lip-reading, diffusion models, speech synthesis, computer vision},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=ZZCPSC5OgD},
  publisher = {OpenReview.net},
  title = {LipVoicer: Generating Speech from Silent Videos Guided by Lip Reading},
  url = {https://openreview.net/forum?id=ZZCPSC5OgD},
  year = {2024}
}

@inproceedings{yi2024fedp3,
  abstract = {The interest in federated learning has surged in recent research due to its unique ability to train a global model using privacy-secured information held locally on each client. This paper pays particular attention to the issue of client-side model heterogeneity, a pervasive challenge in the practical implementation of FL that escalates its complexity. We introduce FedP3, a flexible and privacy-aware federated network pruning framework, addressing model heterogeneity and allowing client-specific personalization.},
  author = {Kai Yi and Nidham Gazagnadou and Peter Richt√°rik and Lingjuan Lyu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yi2024fedp3.pdf:pdf},
  keywords = {network pruning, federated learning, model heterogeneity, privacy-preserving learning, personalized federated learning},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=hbHwZYqk9T},
  publisher = {OpenReview.net},
  title = {FedP3: Federated Personalized and Privacy-friendly Network Pruning under Model Heterogeneity},
  url = {https://openreview.net/forum?id=hbHwZYqk9T},
  year = {2024}
}

@inproceedings{yin2024rethinking,
  abstract = {Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Particularly, answering complex queries based on first-order logic is one of the crucial tasks to verify learning to reason abilities for generalization and composition. This paper develops a new dataset with ten novel query types, proposes a neural-symbolic method called Fuzzy Inference with Truth value (FIT), and analyzes complexity and limitations of previous query embedding approaches.},
  author = {Hang Yin and Zihao Wang and Yangqiu Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yin2024rethinking.pdf:pdf},
  keywords = {knowledge graphs, neural link prediction, complex queries, first-order logic, neural-symbolic reasoning},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=1BmveEMNbG},
  publisher = {OpenReview.net},
  title = {Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors},
  url = {https://openreview.net/forum?id=1BmveEMNbG},
  year = {2024}
}

@inproceedings{yin2024hybrid,
  abstract = {Existing multi-label classification methods have long suffered from label heterogeneity, where learning a label obscures another. By modeling multi-label classification as a multi-task problem, this issue can be regarded as a negative transfer, which indicates challenges to achieve simultaneously satisfied performance across multiple tasks. This paper proposes the Hybrid Sharing Query (HSQ), a transformer-based model that introduces the mixture-of-experts architecture to image multi-label classification. HSQ is designed to leverage label correlations while mitigating heterogeneity effectively. Extensive experiments are conducted on two benchmark datasets, with the results demonstrating that the proposed method achieves state-of-the-art performance and yields simultaneous improvements across most labels.},
  author = {Zihao Yin and Chen Gan and Kelei He and Yang Gao and Junfeng Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yin2024hybrid.pdf:pdf},
  keywords = {multi-task learning, multi-label learning, mixture-of-experts, image classification, transformer},
  note = {Poster presentation},
  pdf = {https://openreview.net/pdf?id=yVJd8lKyVX},
  publisher = {OpenReview.net},
  title = {Hybrid Sharing for Multi-Label Image Classification},
  url = {https://openreview.net/forum?id=yVJd8lKyVX},
  year = {2024}
}

@inproceedings{yin2024mcm,
  abstract = {This paper addresses the problem of anomaly detection in tabular data, which is usually implemented in an one-class classification setting where the training set only contains normal samples. Inspired by the success of masked image/language modeling in vision and natural language domains, we extend masked modeling methods to address this problem by capturing intrinsic correlations between features in training set. Thus, a sample deviate from such correlations is related to a high possibility of anomaly. To obtain multiple and diverse correlations, we propose a novel masking strategy which generates multiple masks by learning, and design a diversity loss to reduce the similarity of different masks.},
  author = {Jiaxin Yin and Yuanyuan Qiao and Zitang Zhou and Xiangchao Wang and Jie Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yin2024mcm.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=lNZJyEDxy4},
  publisher = {OpenReview.net},
  title = {{MCM}: Masked Cell Modeling for Anomaly Detection in Tabular Data},
  url = {https://openreview.net/forum?id=lNZJyEDxy4},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{yin2024fair,
  abstract = {In critical applications, it is vital for classifiers to defer decision-making to humans. We propose a post-hoc method that makes existing classifiers selectively abstain from predicting certain samples. Our abstaining classifier is incentivized to maintain the original accuracy for each sub-population (i.e. no harm) while achieving a set of group fairness definitions to a user specified degree. To this end, we design an Integer Programming (IP) procedure that assigns abstention decisions for each training sample to satisfy a set of constraints.},
  author = {Tongxin Yin and Jean-Francois Ton and Ruocheng Guo and Yuanshun Yao and Mingyan Liu and Yang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yin2024fair.pdf:pdf},
  note = {DBLP last modified: 2024-08-14},
  pdf = {https://openreview.net/pdf?id=jvveGAbkVx},
  publisher = {OpenReview.net},
  title = {Fair Classifiers that Abstain without Harm},
  url = {https://openreview.net/forum?id=jvveGAbkVx},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{yin2024dream,
  abstract = {Graph neural network methods have been developed to tackle domain shifts in graph data, but these methods presuppose that unlabeled target graphs belong to categories previously seen in the source domain, which may not hold true for in-the-wild target graphs. This paper explores open-set graph domain adaptation with the objective to not only identify target graphs from new categories but also accurately classify remaining target graphs into their respective categories under domain shift and label scarcity. To solve this challenging problem, we introduce a new method named Dual Structured Exploration with Mixup (DREAM).},
  author = {Nan Yin and Mengzhu Wang and Zhenghan Chen and Li Shen and Huan Xiong and Bin Gu and Xiao Luo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yin2024dream.pdf:pdf},
  note = {DBLP last modified: 2024-08-07; spotlight paper},
  pdf = {https://openreview.net/pdf?id=4olqbTBt1Y},
  publisher = {OpenReview.net},
  title = {{DREAM}: Dual Structured Exploration with Mixup for Open-set Graph Domain Adaption},
  url = {https://openreview.net/forum?id=4olqbTBt1Y},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{yin2024dformer,
  abstract = {This paper presents DFormer, a novel RGB-D pretraining framework to learn transferable representations for RGB-D segmentation tasks. DFormer has two new key innovations: 1) Unlike previous works that encode RGB-D information with RGB pretrained backbone, we pretrain the backbone using image-depth pairs from ImageNet-1K, and hence the DFormer is endowed with the capacity to encode RGB-D representations; 2) DFormer comprises a sequence of RGB-D blocks, which are tailored for encoding both RGB and depth information through a novel building block design. DFormer avoids the mismatched encoding of the 3D geometry relationships in depth maps by RGB pretrained backbones, which widely lies in existing methods but has not been resolved. Experimental results show that DFormer achieves new state-of-the-art performance on these two tasks with less than half of the computational cost of the current best methods on two RGB-D semantic segmentation datasets and five RGB-D salient object detection datasets.},
  author = {Bowen Yin and Xuying Zhang and Zhong-Yu Li and Li Liu and Ming-Ming Cheng and Qibin Hou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yin2024dformer.pdf:pdf},
  note = {DBLP last modified: 2025-06-23},
  pdf = {https://openreview.net/pdf?id=h1sFUGlI09},
  publisher = {OpenReview.net},
  title = {{DFormer}: Rethinking {RGBD} Representation Learning for Semantic Segmentation},
  url = {https://openreview.net/forum?id=h1sFUGlI09},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{yong2024continuous,
  abstract = {Invariance learning methods aim to learn invariant features for generalization under distributional shifts, but current techniques generally assume categorically indexed domains while many tasks are naturally characterized by continuous domains. For example, auto-scaling in cloud computing needs CPU utilization prediction models that generalize across different times, where 'time' is a continuous domain index. We theoretically show that existing invariance learning methods can fail for continuous domain problems, as splitting continuous domains into discrete ones ignores underlying relationships among domains and potentially leads to suboptimal performance. To address this, we propose Continuous Invariance Learning (CIL), which extracts invariant features across continuously indexed domains using a novel adversarial procedure that measures and controls conditional independence between labels and continuous domain indices.},
  author = {Lin Yong and Fan Zhou and Lu Tan and Lintao Ma and Jianmeng Liu and Yansu He and Yuan Yuan and Yu Liu and James Y. Zhang and Yujiu Yang and Hao Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yong2024continuous.pdf:pdf},
  note = {DBLP last modified: 2024-09-30},
  pdf = {https://openreview.net/pdf?id=70IgE3tRbu},
  publisher = {OpenReview.net},
  title = {Continuous Invariance Learning},
  url = {https://openreview.net/forum?id=70IgE3tRbu},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{yoon2024ctpt,
  abstract = {In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration, which is a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT), for optimizing prompts during test-time with enhanced calibration.},
  author = {Hee Suk Yoon and Eunseop Yoon and Joshua Tian Jin Tee and Mark A. Hasegawa-Johnson and Yingzhen Li and Chang D. Yoo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yoon2024ctpt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jzzEHTBFOT},
  publisher = {OpenReview.net},
  title = {{C-TPT}: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion},
  url = {https://openreview.net/forum?id=jzzEHTBFOT},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{yoran2024making,
  abstract = {Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.},
  author = {Ori Yoran and Tomer Wolfson and Ori Ram and Jonathan Berant},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yoran2024making.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ZS4m74kZpH},
  publisher = {OpenReview.net},
  title = {Making Retrieval-Augmented Language Models Robust to Irrelevant Context},
  url = {https://openreview.net/forum?id=ZS4m74kZpH},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{you2024swap,
  abstract = {This study addresses the challenge of inaccurate gradients in computing the empirical Fisher Information Matrix during network pruning. We introduce SWAP, a formulation of Entropic Wasserstein regression (EWR) for network pruning, capitalizing on the geometric properties of the optimal transport problem. The "swap" of the commonly used linear regression with the EWR in optimization is analytically demonstrated to offer noise mitigation effects by incorporating neighborhood interpolation across data points with only marginal additional computational cost. The unique strength of SWAP is its intrinsic ability to balance noise reduction and covariance information preservation effectively. Extensive experiments performed on various networks and datasets show comparable performance of SWAP with state-of-the-art (SoTA) network pruning algorithms. Our proposed method outperforms the SoTA when the network size or the target sparsity is large, the gain is even larger with the existence of noisy gradients, possibly from noisy data, analog memory, or adversarial attacks. Notably, our proposed method achieves a gain of 6% improvement in accuracy and 8% improvement in testing loss for MobileNetV1 with less than one-fourth of the network parameters remaining.},
  author = {Lei You and Hei Victor Cheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/you2024swap.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LJWizuuBUy},
  publisher = {OpenReview.net},
  title = {{SWAP}: Sparse Entropic Wasserstein Regression for Robust Network Pruning},
  url = {https://openreview.net/forum?id=LJWizuuBUy},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{you2024cobit,
  abstract = {The field of Vision-and-Language (VL) has witnessed a proliferation of pretrained foundation models. Current techniques typically employ only one type of training objective, whether it's (1) contrastive objectives (like CLIP), (2) image-to-text generative objectives (like PaLI), or (3) text-to-image generative objectives (like Parti). However, all these three objectives are mutually relevant and are all based on image-text pairs. Intuitively, the first two objectives can be considered as complementary projections between two modalities, and contrastive learning can preserve global alignment and generations facilitate fine-grained understanding. Inspired by this, we present a Contrastive Bi-directional Image-Text generation model (CoBIT) to first time unify the three pre-training objectives in one framework. Specifically, CoBIT employs a novel unicoder-decoder structure consisting of an image unicoder, a text unicoder, and a cross-modal decoder. The image/text unicoders can switch between encoding and decoding in different tasks, enabling flexibility and shared knowledge that benefits both image-to-text and text-to-image generations. CoBIT achieves superior performance in image understanding, image-text understanding (Retrieval, Captioning, VQA, SNLI-VE), and text-based content creation, particularly in zero-shot scenarios.},
  author = {Haoxuan You and Mandy Guo and Zhecan Wang and Kai-Wei Chang and Jason M. Baldridge and Jiahui Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/you2024cobit.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8ISRqgtjPc},
  publisher = {OpenReview.net},
  title = {{CoBIT}: A Contrastive Bi-directional Image-Text Generation Model},
  url = {https://openreview.net/forum?id=8ISRqgtjPc},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{you2024efficient,
  abstract = {Convolution-BatchNorm (ConvBN) blocks are integral components in various computer vision tasks and other domains. A ConvBN block can operate in three modes: Train, Eval, and Deploy. While the Train mode is indispensable for training models from scratch, the Eval mode is suitable for transfer learning and beyond, and the Deploy mode is designed for the deployment of models. This paper focuses on the trade-off between stability and efficiency in ConvBN blocks: Deploy mode is efficient but suffers from training instability; Eval mode is widely used in transfer learning but lacks efficiency. To solve the dilemma, we theoretically reveal the reason behind the diminished training stability observed in the Deploy mode. Subsequently, we propose a novel Tune mode to bridge the gap between Eval mode and Deploy mode. The proposed Tune mode is as stable as Eval mode for transfer learning, and its computational efficiency closely matches that of the Deploy mode. Through extensive experiments in object detection, classification, and adversarial example generation across 5 datasets and 12 model architectures, we demonstrate that the proposed Tune mode retains the performance while significantly reducing GPU memory footprint and training time, thereby contributing efficient ConvBN blocks for transfer learning and beyond.},
  author = {Kaichao You and Guo Qin and Anchang Bao and Meng Cao and Ping Huang and Jiulong Shan and Mingsheng Long},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/you2024efficient.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=lHZm9vNm5H},
  publisher = {OpenReview.net},
  title = {Efficient {ConvBN} Blocks for Transfer Learning and Beyond},
  url = {https://openreview.net/forum?id=lHZm9vNm5H},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{you2024ferret,
  abstract = {We introduce {FERRET}, a new {Multimodal Large Language Model (MLLM)} capable of understanding spatial referring of any shape or granularity within an image and accurately grounding open-vocabulary descriptions. {FERRET} employs a novel and powerful hybrid region representation that integrates discrete coordinates and continuous features jointly to represent a region in the image. To extract the continuous features of versatile regions, we propose a spatial-aware visual sampler, adept at handling varying sparsity across different shapes. {FERRET} can accept diverse region inputs, such as points, bounding boxes, and free-form shapes. To bolster the desired capability of {FERRET}, we curated {GRIT}, a comprehensive refer-and-ground instruction tuning dataset including 1.1M samples that contain rich hierarchical spatial knowledge, with 95K hard negative data to promote model robustness. The resulting model not only achieves superior performance in classical referring and grounding tasks, but also greatly outperforms existing {MLLMs} in region-based and localization-demanded multimodal chatting. Our evaluations also reveal a significantly improved capability of describing image details and a remarkable alleviation in object hallucination.},
  author = {Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/you2024ferret.pdf:pdf},
  note = {DBLP last modified: 2024-07-29. Also available at arXiv:2310.07704},
  pdf = {https://openreview.net/pdf?id=2msbbX3ydD},
  publisher = {OpenReview.net},
  title = {Ferret: Refer and Ground Anything Anywhere at Any Granularity},
  url = {https://openreview.net/forum?id=2msbbX3ydD},
  year = {2024}
}

@inproceedings{you2024latent,
  abstract = {This paper addresses the generation of 3D graphs with symmetry-group equivariance, which has applications in machine vision and molecular discovery. While emerging approaches use diffusion generative models ({DGMs}) to capture 3D graph distributions, this paper asks the fundamental question of in what latent space we should diffuse 3D graphs. We provide theoretical analysis showing that performance bounds of 3D graph diffusion can be improved in a latent space versus the original space, provided that the latent space has (i) low dimensionality yet (ii) high quality (i.e., low reconstruction error) and {DGMs} have (iii) symmetry preservation as an inductive bias. We propose a pipeline to compress 3D graphs into the latent space, where a (vectorial) diffusion model is implemented to capture the distribution. Empirical results on multiple synthetic and real-world 3D graph generation tasks show that our approach significantly outperforms existing methods, achieving better generation quality, better mode coverage, and better sampling efficiency.},
  author = {Yuning You and Ruida Zhou and Jiwoong Park and Haotian Xu and Chao Tian and Zhangyang Wang and Yang Shen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/you2024latent.pdf:pdf},
  note = {DBLP last modified: 2024-08-15. Code available: https://github.com/Shen-Lab/LDM-3DG},
  pdf = {https://openreview.net/pdf?id=cXbnGtO0NZ},
  publisher = {OpenReview.net},
  title = {Latent 3D Graph Diffusion},
  url = {https://openreview.net/forum?id=cXbnGtO0NZ},
  year = {2024}
}

@inproceedings{yu2024rigid,
  abstract = {The study of rigid protein-protein docking plays an essential role in a variety of tasks such as drug design and protein engineering. Recently, several learning-based methods have been proposed for the task, exhibiting much faster docking speed than those computational methods. In this paper, we propose a novel learning-based method called {ElliDock}, which predicts an elliptic paraboloid to represent the protein-protein docking interface. To be specific, our model estimates elliptic paraboloid interfaces for the two input proteins respectively, and obtains the roto-translation transformation for docking by making two interfaces coincide. By its design, {ElliDock} is independently equivariant with respect to arbitrary rotations/translations of the proteins, which is an indispensable property to ensure the generalization of the docking process. Experimental evaluations show that {ElliDock} achieves the fastest inference time among all compared methods and is strongly competitive with current state-of-the-art learning-based models such as {DiffDock-PP} and {Multimer} particularly for antibody-antigen docking.},
  author = {Ziyang Yu and Wenbing Huang and Yang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024rigid.pdf:pdf},
  note = {DBLP last modified: 2025-06-04. Also available at arXiv:2401.08986. Code: https://github.com/yaledeus/ElliDock},
  pdf = {https://openreview.net/pdf?id=zgQ0PHeGnL},
  publisher = {OpenReview.net},
  title = {Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid Interface Prediction},
  url = {https://openreview.net/forum?id=zgQ0PHeGnL},
  year = {2024}
}

@inproceedings{yu2024vonet,
  abstract = {Unsupervised video object learning seeks to decompose video scenes into structural object representations without any supervision from depth, optical flow, or segmentation. We present {VONet}, an innovative approach that is inspired by {MONet}. While utilizing a U-Net architecture, {VONet} employs an efficient and effective parallel attention inference process, generating attention masks for all slots simultaneously. Additionally, to enhance the temporal consistency of each mask across consecutive video frames, {VONet} develops an object-wise sequential {VAE} framework. The integration of these innovative encoder-side techniques, in conjunction with an expressive transformer-based decoder, establishes {VONet} as the leading unsupervised method for object learning across five {MOVI} datasets, encompassing videos of diverse complexities.},
  author = {Haonan Yu and Wei Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024vonet.pdf:pdf},
  note = {DBLP last modified: 2024-08-07. Also available at arXiv:2401.11110. Code: https://github.com/hnyu/vonet},
  pdf = {https://openreview.net/pdf?id=qCyhvr0GG8},
  publisher = {OpenReview.net},
  title = {VONet: Unsupervised Video Object Learning With Parallel U-Net Attention and Object-wise Sequential VAE},
  url = {https://openreview.net/forum?id=qCyhvr0GG8},
  year = {2024}
}

@inproceedings{yu2024learning,
  abstract = {Recently, many mesh-based graph neural network ({GNN}) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur. The key challenge stems from the long-range interaction where the collision effect at the contact point should be immediately propagated to the entire object. To tackle this challenge, we present Hierarchical Contact Mesh Transformer ({HCMT}), which enables long-range interactions and the hierarchical mesh structure quickly propagates collision effects to faraway positions. To this end, HCMT consists of a contact mesh Transformer and a hierarchical mesh Transformer ({CMT} and {HMT}, respectively). We propose a flexible body dynamics dataset, consisting of trajectories that reflect experimental settings frequently used in the display industry for product designs. We also compare the performance of several baselines using well-known benchmark datasets. Our results show that {HCMT} provides significant performance improvements over existing methods.},
  author = {Youn-Yeol Yu and Jeongwhan Choi and Woojin Cho and Kookjin Lee and Nayong Kim and Kiseok Chang and ChangSeung Woo and Ilho Kim and SeokWoo Lee and Joon-Young Yang and Sooyoung Yoon and Noseong Park},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07. Also available at arXiv:2312.12467},
  pdf = {https://openreview.net/pdf?id=90yw2uM6J5},
  publisher = {OpenReview.net},
  title = {Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer},
  url = {https://openreview.net/forum?id=90yw2uM6J5},
  year = {2024}
}

@inproceedings{yu2024skillmix,
  abstract = {With {LLMs} shifting their role from statistical modeling of language to serving as general-purpose {AI} agents, how should {LLM} evaluations change? Arguably, a key ability of an {AI} agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora \& Goyal, 2023). This work introduces {Skill-Mix}, a new evaluation to measure ability to combine skills. Using a list of $N$ skills the evaluator repeatedly picks random subsets of $k$ skills and asks the {LLM} to produce text combining that subset of skills. Since the number of subsets grows like $N^k$, for even modest $k$ this evaluation will, with high probability, require the {LLM} to produce text significantly different from any text in the training set. The paper develops methodology for both designing and administering such evaluations, as well as automatic grading using {GPT-4}. Administering a version of {SKILL-MIX} to popular chatbots gave results that, while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular {LLM} leaderboards. Furthermore, simple probability calculations indicate that {GPT-4}'s reasonable performance on $k=5$ is suggestive of going beyond ``stochastic parrot'' behavior, i.e., it combines skills in ways that it had not seen during training.},
  author = {Dingli Yu and Simran Kaur and Arushi Gupta and Jonah Brown-Cohen and Anirudh Goyal and Sanjeev Arora},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024skillmix.pdf:pdf},
  note = {DBLP last modified: 2024-08-07. Also available at arXiv:2310.17567. Website: https://skill-mix.github.io},
  pdf = {https://openreview.net/pdf?id=Jf5gplvglq},
  publisher = {OpenReview.net},
  title = {{SKILL-MIX}: a Flexible and Expandable Family of Evaluations for {AI} Models},
  url = {https://openreview.net/forum?id=Jf5gplvglq},
  year = {2024}
}

@inproceedings{yu2024multimodal,
  abstract = {Self-supervised learning has recently gained growing interest in molecular modeling for scientific tasks such as {AI}-assisted drug discovery. Current studies consider leveraging both 2D and 3D molecular structures for representation learning. However, relying on straightforward alignment strategies that treat each modality separately, these methods fail to exploit the intrinsic correlation between 2D and 3D representations that reflect the underlying structural characteristics of molecules, and only perform coarse-grained molecule-level alignment. To derive fine-grained alignment and promote structural molecule understanding, the paper introduces an atomic-relation level ``blend-then-predict'' self-supervised learning approach, {MoleBLEND}, which first blends atom relations represented by different modalities into one unified relation matrix for joint encoding, then recovers modality-specific information. {MoleBLEND} explicitly leverages the alignment of atom relations between 2D and 3D structures and blends input signals from different modalities as one unified data structure to pre-train one single model. {MoleBLEND} consists of a two-stage blend-then-predict training procedure: modality-blended encoding and modality-targeted prediction. During encoding, different depictions of atom relations from 2D and 3D views are blended into one relation matrix. During prediction, the model recovers missing 2D and 3D information as supervision signals. With such a relation-level blending approach, multimodal molecular information is mingled within a unified model, and fine-grained atom-relation alignment in the multimodal input space leads to a deeper structural understanding of molecular makeup. Extensive evaluation demonstrates that {MoleBLEND} achieves state-of-the-art performance over diverse 2D and 3D tasks, verifying the effectiveness of relation-level alignment.},
  author = {Qiying Yu and Yudi Zhang and Yuyan Ni and Shikun Feng and Yanyan Lan and Hao Zhou and Jingjing Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024multimodal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07. Also available at arXiv:2307.06235},
  pdf = {https://openreview.net/pdf?id=oM7Jbxdk6Z},
  publisher = {OpenReview.net},
  title = {Multimodal Molecular Pretraining via Modality Blending},
  url = {https://openreview.net/forum?id=oM7Jbxdk6Z},
  year = {2024}
}

@inproceedings{yu2024thought,
  abstract = {Large Language Models ({LLMs}) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt {LLMs} to reason from scratch. To address these issues, we propose \textbf{Thought Propagation ({TP})}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of {LLMs}. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, {TP} first prompts {LLMs} to propose and solve a set of analogous problems that are related to the input one. Then, {TP} reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. {TP} is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate {TP} enjoys a substantial improvement over the baselines by an average of 12\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\% improvement of human preference in Creative Writing, and 15\% enhancement in the task completion rate of {LLM}-Agent Planning.},
  author = {Junchi Yu and Ran He and Zhitao Ying},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024thought.pdf:pdf},
  note = {DBLP last modified: 2024-08-07. Also available at arXiv:2310.03965. Code: https://github.com/Samyu0304/thought-propagation},
  pdf = {https://openreview.net/pdf?id=SBoRhRCzM3},
  publisher = {OpenReview.net},
  title = {Thought Propagation: an Analogical Approach to Complex Reasoning with Large Language Models},
  url = {https://openreview.net/forum?id=SBoRhRCzM3},
  year = {2024}
}

@inproceedings{yu2024optimal,
  abstract = {Memorization with neural networks is to study the expressive power of neural networks to interpolate a finite classification data set, which is closely related to the generalizability of deep learning. However, the important problem of robust memorization has not been thoroughly studied. In this paper, several basic problems about robust memorization are solved. First, we prove that it is {NP}-hard to compute neural networks with certain simple structures, which are robust memorization. A network hypothesis space is called optimal robust memorization for a data set if it can achieve robust memorization for any budget less than half the separation bound of the data set. We characterize when a {ReLU} network is optimal robust memorization, and provide an upper bound on the width of two-layer {ReLU} networks for optimal robust memorization. We also provide an efficient greedy algorithm to construct a near-optimal robust memorizing network.},
  author = {Lijia Yu and Xiao-Shan Gao and Lijun Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024optimal.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=47hDbAMLbc},
  publisher = {OpenReview.net},
  title = {Optimal robust Memorization with {ReLU} Neural Networks},
  url = {https://openreview.net/forum?id=47hDbAMLbc},
  year = {2024}
}

@inproceedings{yu2024safe,
  abstract = {Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property ({IP}) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution ({OoD}) image, which serves as a secret key for {IP} verification. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time and sample-efficient without training data, but also robust against the watermark removal attacks. The key innovation is using a new method for safely and robustly injecting watermark after training without training data. The independence of training data makes it agnostic to third-party promises of {IP} security.},
  author = {Shuyang Yu and Junyuan Hong and Haobo Zhang and Haotao Wang and Zhangyang Wang and Jiayu Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024safe.pdf:pdf},
  note = {DBLP last modified: 2025-01-28. Also available at arXiv:2309.01786. Code: https://github.com/illidanlab/Single\_oodwatermark},
  pdf = {https://openreview.net/pdf?id=PCm1oT8pZI},
  publisher = {OpenReview.net},
  title = {Safe and Robust Watermark Injection with a Single {OoD} Image},
  url = {https://openreview.net/forum?id=PCm1oT8pZI},
  year = {2024}
}

@inproceedings{yu2024metamath,
  abstract = {Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problems due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a finetuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives, which results in a new dataset called MetaMathQA. Then we finetune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.5% on GSM8K and 19.8% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.},
  author = {Longhui Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024metamath.pdf:pdf},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=N8N0hgNDRt},
  publisher = {OpenReview.net},
  title = {{MetaMath}: Bootstrap Your Own Mathematical Questions for Large Language Models},
  url = {https://openreview.net/forum?id=N8N0hgNDRt},
  year = {2024}
}

@inproceedings{yu2024langevin,
  abstract = {We revisit the problem of sampling from a target distribution that has a smooth strongly log-concave density everywhere in $\mathbb{R}^p$. In this context, if no additional density information is available, the randomized midpoint discretization for the kinetic Langevin diffusion is known to be the most scalable method in high dimensions with large condition numbers. Our main result is a nonasymptotic and easy to compute upper bound on the Wasserstein-2 error of this method. To provide a more thorough explanation of our method for establishing the computable upper bound, we conduct an analysis of the midpoint discretization for the vanilla Langevin process. This analysis helps to clarify the underlying principles and provides valuable insights that we use to establish an improved upper bound for the kinetic Langevin process with the midpoint discretization.},
  author = {Lu Yu and Avetik G. Karagulyan and Arnak S. Dalalyan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024langevin.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hOxgrGM63n},
  publisher = {OpenReview.net},
  title = {Langevin {Monte} {Carlo} for strongly log-concave distributions: Randomized midpoint revisited},
  url = {https://openreview.net/forum?id=hOxgrGM63n},
  year = {2024}
}

@inproceedings{yu2024uncertaintyaware,
  abstract = {Hyperspectral imaging (HSI) technology captures spectral information across a broad wavelength range and provides richer pixel features compared to traditional color images. While pixel classification in HSI has been extensively studied using graph convolution neural networks (GCNs), quantifying epistemic and aleatoric uncertainties associated with HSI classification results remains an unexplored area. These two uncertainties are effective for out-of-distribution (OOD) and misclassification detection, respectively. In this work, we adapt two advanced uncertainty quantification models, evidential GCNs (EGCN) and graph posterior networks (GPN), designed for node classifications in graphs, into the realm of hyperspectral image classification. We first reveal theoretically that a popular uncertainty cross-entropy (UCE) loss function is insufficient to produce good epistemic uncertainty when learning EGCNs. To mitigate the limitations, we propose two regularization terms. One leverages the inherent property of HSI data where each feature vector is a linear combination of the spectra signatures of confounding materials, while the other is the total variation (TV) regularization to enforce the spatial smoothness of the evidence with edge-preserving. We demonstrate the effectiveness of the proposed regularization terms on both EGCN and GPN on three real-world HSI classification datasets for OOD and misclassification detection tasks.},
  author = {Linlin Yu and Yifei Lou and Feng Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024uncertaintyaware.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=8dN7gApKm3},
  publisher = {OpenReview.net},
  title = {Uncertainty-aware Graph-based Hyperspectral Image Classification},
  url = {https://openreview.net/forum?id=8dN7gApKm3},
  year = {2024}
}

@inproceedings{yu2024language,
  abstract = {While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VVC) according to human evaluations, and (2) learning effective representations for action recognition tasks.},
  author = {Lijun Yu and Jos Lezama and Nitesh Bharadwaj Gundavarapu and Luca Versari and Kihyuk Sohn and David Minnen and Yong Cheng and Agrim Gupta and Xiuye Gu and Alexander G. Hauptmann and Boqing Gong and Ming-Hsuan Yang and Irfan Essa and David A. Ross and Lu Jiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024language.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gzqrANCF4g},
  publisher = {OpenReview.net},
  title = {Language Model Beats Diffusion - Tokenizer is key to visual generation},
  url = {https://openreview.net/forum?id=gzqrANCF4g},
  year = {2024}
}

@inproceedings{yu2024dropout,
  abstract = {Bilevel optimization problems appear in many widely used machine learning tasks. Bilevel optimization models are sensitive to small changes, and bilevel training tasks typically involve limited datasets. Therefore, overfitting is a common challenge in bilevel training tasks. This paper considers the use of dropout to address this problem. We propose a bilevel optimization model that depends on the distribution of dropout masks. We investigate how the dropout rate affects the hypergradient of this model. We propose a dropout bilevel method to solve the dropout bilevel optimization model. Subsequently, we analyze the resulting dropout bilevel method from an optimization perspective. Analyzing the optimization properties of methods with dropout is essential because it provides convergence guarantees for methods using dropout. However, there has been limited investigation in this research direction. We provide the complexity of the resulting dropout bilevel method in terms of reaching an $\epsilon$ stationary point of the proposed stochastic bilevel model. Empirically, we demonstrate that overfitting occurs in data cleaning problems, and the method proposed in this work mitigates this issue.},
  author = {Peiran Yu and Junyi Li and Heng Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024dropout.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=06lrITXVAx},
  publisher = {OpenReview.net},
  title = {Dropout Enhanced Bilevel Training},
  url = {https://openreview.net/forum?id=06lrITXVAx},
  year = {2024}
}

@inproceedings{yu2024shadow,
  abstract = {Hyperbolic space has proven to be well-suited for capturing hierarchical relations in data, such as trees and directed acyclic graphs. Prior work introduced the concept of entailment cones, which uses partial orders defined by nested cones in the Poincar{\'e} ball to model hierarchies. Here, we introduce the 'shadow cones' framework, a physics-inspired entailment cone construction. Specifically, we model partial orders as subset relations between shadows formed by a light source and opaque objects in hyperbolic space. The shadow cones framework generalizes entailment cones to a broad class of formulations and hyperbolic space models beyond the Poincar{\'e} ball. This results in clear advantages over existing constructions: for example, shadow cones possess better optimization properties over constructions limited to the Poincar{\'e} ball. Our experiments on datasets of various sizes and hierarchical structures show that shadow cones consistently and significantly outperform existing entailment cone constructions. These results indicate that shadow cones are an effective way to model partial orders in hyperbolic space, offering physically intuitive and novel insights about the nature of such structures.},
  author = {Tao Yu and Toni J. B. Liu and Albert Tseng and Christopher De Sa},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024shadow.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=zbKcFZ6Dbp},
  publisher = {OpenReview.net},
  title = {Shadow Cones: A Generalized Framework for Partial Order Embeddings},
  url = {https://openreview.net/forum?id=zbKcFZ6Dbp},
  year = {2024}
}

@inproceedings{yu2024efficient,
  abstract = {Video diffusion models have recently made great progress in generation quality, but are still limited by the high memory and computational requirements. This is because current video diffusion models often attempt to process high-dimensional videos directly. To tackle this issue, we propose content-motion latent diffusion model ({CMD}), a novel efficient extension of pretrained image diffusion models for video generation. Specifically, we propose an autoencoder that succinctly encodes a video as a combination of a content frame (like an image) and a low-dimensional motion latent representation. The former represents the common content, and the latter represents the underlying motion in the video, respectively. We generate the content frame by fine-tuning a pretrained image diffusion model, and we generate the motion latent representation by training a new lightweight diffusion model. A key innovation here is the design of a compact latent space that can directly utilizes a pretrained image diffusion model, which has not been done in previous latent video diffusion models. This leads to considerably better quality generation and reduced computational costs. For instance, {CMD} can sample a video 7.7$\times$ faster than prior approaches by generating a video of $512 \times 1024$ resolution and length 16 in 3.1 seconds. Moreover, {CMD} achieves an {FVD} score of 238.3 on WebVid-10M, 18.5\% better than the previous state-of-the-art of 292.4.},
  author = {Sihyun Yu and Weili Nie and De-An Huang and Boyi Li and Jinwoo Shin and Anima Anandkumar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024efficient.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=dQVtTdsvZH},
  publisher = {OpenReview.net},
  title = {Efficient Video Diffusion Models via Content-Frame Motion-Latent Decomposition},
  url = {https://openreview.net/forum?id=dQVtTdsvZH},
  year = {2024}
}

@inproceedings{yu2024robustifying,
  abstract = {State-space models ({SSMs}) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence ({S4}) layer, which uses the diagonal-plus-low-rank structure of the {HiPPO} initialization framework. However, the complicated structure of the {S4} layer poses challenges; and, in an effort to address these challenges, models such as {S4D} and {S5} have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the {HiPPO} framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable 'perturb-then-diagonalize' ({PTD}) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining {SSMs}. Based on this, we introduce the {S4-PTD} and {S5-PTD} models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the {S4-PTD}/{S5-PTD} initialization strongly converges to the {HiPPO} framework, while the {S4D}/{S5} initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the {S4D}/{S5} models. In addition to improved robustness, our {S5-PTD} model averages 87.6\% accuracy on the Long-Range Arena benchmark, demonstrating that the {PTD} methodology helps to improve the accuracy of deep learning models.},
  author = {Annan Yu and Arnur Nigmetov and Dmitriy Morozov and Michael W. Mahoney and N. Benjamin Erichson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024robustifying.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=DjeQ39QoLQ},
  publisher = {OpenReview.net},
  title = {Robustifying State-space Models for Long Sequences via Approximate Diagonalization},
  url = {https://openreview.net/forum?id=DjeQ39QoLQ},
  year = {2024}
}

@inproceedings{yu2024,
  abstract = {Program synthesis aims to create accurate, executable programs from problem specifications, specifically from natural language descriptions in our context. Recent studies have leveraged the power of reinforcement learning ({RL}) in conjunction with large language models ({LLMs}), significantly enhancing code generation capabilities. The application of {RL} focuses on directly optimizing for functional correctness, offering an advantage over conventional supervised methods. Despite policy-based {RL} methods dominating the literature on {RL} for program synthesis, the nature of program synthesis tasks hints at a natural alignment with value-based methods. This stems from the rich collection of off-policy programs, including those developed by human programmers and also historical samples, coupled with the straightforward verification of generated programs through automated unit testing, meaning rewards are easy to obtain. Diverging from the dominant use of policy-based algorithms, our work explores the feasibility of value-based approaches, leading to the development of our $\mathcal{B}$-Coder (pronounced Bellman coder). Yet, training value-based methods presents challenges due to the enormous search space inherent to program synthesis. To this end, we introduce an initialization protocol for {RL} agents utilizing pre-trained {LMs} and a conservative Bellman operator to reduce training complexities. Moreover, we demonstrate how to leverage the learned value functions as a dual strategy to post-process generated programs. Our empirical evaluations demonstrated $\mathcal{B}$-Coder's capability in achieving state-of-the-art performance when compared to policy-based methods. Remarkably, this achievement is reached with minimal reward engineering effort, highlighting the effectiveness of value-based {RL}, independent of reward designs.},
  author = {Zishun Yu and Yunzhe Tao and Liyu Chen and Tao Sun and Hongxia Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=fLf589bx1f},
  publisher = {OpenReview.net},
  title = {$\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program Synthesis},
  url = {https://openreview.net/forum?id=fLf589bx1f},
  year = {2024}
}

@inproceedings{yu2024treatment,
  abstract = {In observational studies, balancing covariates in different treatment groups is essential to estimate treatment effects. One of the most commonly used methods for such purposes is weighting. The performance of this class of methods usually depends on strong regularity conditions for the underlying model, which might not hold in practice. In this paper, we investigate weighting methods from a functional estimation perspective and argue that the weights needed for covariate balancing could differ from those needed for treatment effects estimation under low regularity conditions. Motivated by this observation, we introduce a new framework of weighting that directly targets the treatment effects estimation. Unlike existing methods, the resulting estimator for a treatment effect under this new framework is a simple kernel-based $U$-statistic after applying a data-driven transformation to the observed covariates. We characterize the theoretical properties of the new estimators of treatment effects under a nonparametric setting and show that they are able to work robustly under low regularity conditions. The new framework is also applied to several numerical examples to demonstrate its practical merits.},
  author = {Ruoqi Yu and Shulei Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024treatment.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oOGqJ6Z1sA},
  publisher = {OpenReview.net},
  title = {Treatment Effects Estimation By Uniform Transformer},
  url = {https://openreview.net/forum?id=oOGqJ6Z1sA},
  year = {2024}
}

@inproceedings{yu2024kola,
  abstract = {The unprecedented performance of large language models ({LLM}s) necessitates improvements in evaluations. Rather than merely exploring the breadth of {LLM} abilities, we believe meticulous and thoughtful designs are essential to thorough, unbiased, and applicable evaluations. Given the importance of world knowledge to {LLM}s, we construct a Knowledge-oriented {LLM} Assessment benchmark ({KoLA}), in which we carefully design three crucial factors: (1) For ability modeling, we mimic human cognition to form a four-level taxonomy of knowledge-related abilities, covering 19 tasks. (2) For data, to ensure fair comparisons, we use both Wikipedia, a corpus prevalently pre-trained by {LLM}s, along with continuously collected emerging corpora, aiming to evaluate the capacity to handle unseen data and evolving knowledge. (3) For evaluation criteria, we adopt a contrastive system, including overall standard scores for better numerical comparability across tasks and models and a unique self-contrast metric for automatically evaluating knowledge-creating ability. We evaluate 28 open-source and commercial {LLM}s and obtain some intriguing findings. The {KoLA} dataset and open-participation leaderboard are publicly released at https://kola.xlore.cn and will be continuously updated to provide references for developing {LLM}s and knowledge-related systems.},
  author = {Jifan Yu and Xiaozhi Wang and Shangqing Tu and Shulin Cao and Daniel Zhang-Li and Xin Lv and Hao Peng and Zijun Yao and Xiaohan Zhang and Hanming Li and Chunyang Li and Zheyuan Zhang and Yushi Bai and Yantao Liu and Amy Xin and Kaifeng Yun and Linlu Gong and Nianyi Lin and Jianhui Chen and Zhili Wu and Yunjia Qi and Weikai Li and Yong Guan and Kaisheng Zeng and Ji Qi and Hailong Jin and Jinxin Liu and Yu Gu and Yuan Yao and Ning Ding and Lei Hou and Zhiyuan Liu and Bin Xu and Jie Tang and Juanzi Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024kola.pdf:pdf},
  note = {{DBLP} last modified: 2025-06-17},
  pdf = {https://openreview.net/pdf?id=AqN23oqraW},
  publisher = {OpenReview.net},
  title = {{KoLA}: Carefully Benchmarking World Knowledge of Large Language Models},
  url = {https://openreview.net/forum?id=AqN23oqraW},
  year = {2024}
}

@inproceedings{yu2024cauchyschwarz,
  abstract = {The information bottleneck ({IB}) approach is popular to improve the generalization, robustness and explainability of deep neural networks. Essentially, it aims to find a minimum sufficient representation by striking a trade-off between a compression term and a prediction term, where mutual information is involved. Mutual information is for the {IB} for the most part expressed in terms of the Kullback-Leibler ({KL}) divergence, which in the regression case corresponds to prediction based on mean squared error ({MSE}) loss with {G}aussian assumption and compression approximated by variational inference. In this paper, we study the {IB} principle for the regression problem and develop a new way to parameterize the {IB} with deep neural networks by exploiting favorable properties of the Cauchy-Schwarz ({CS}) divergence. By doing so, we move away from {MSE}-based regression and ease estimation by avoiding variational approximations or distributional assumptions. We demonstrate that the {CS} divergence offers a new prediction term which does not take any distributional assumptions on the decoder, and that the compression term can be explicitly estimated from samples without variational or non-parametric upper bound approximations, by making use of the Cauchy-Schwarz quadratic mutual information ({CS}-{QMI}). We demonstrate strong adversarial robustness guarantees and superior performance on six real-world regression tasks over other popular deep {IB} approaches.},
  author = {Shujian Yu and Xi Yu and Sigurd L{\o}kse and Robert Jenssen and Jos{\'{e}} C. Pr{\'{i}}ncipe},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024cauchyschwarz.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7wY67ZDQTE},
  publisher = {OpenReview.net},
  title = {Cauchy-Schwarz Divergence Information Bottleneck for Regression},
  url = {https://openreview.net/forum?id=7wY67ZDQTE},
  year = {2024}
}

@inproceedings{yuan2024early,
  abstract = {Early stopping methods in deep learning face the challenge of balancing the volume of training and validation data, especially in the presence of label noise. Concretely, sparing more data for validation from training data would limit the performance of the learned model, yet insufficient validation data could result in a sub-optimal selection of the desired model. To address this issue, we propose a novel early stopping method called Label Wave, which does not require validation data for selecting the desired model in the presence of label noise. Label Wave works by tracking the changes in the model's predictions on the training set during the training process, aiming to halt training before the model unduly fits mislabeled data. This method is empirically supported by our observation that minimum fluctuations in predictions typically occur at the training epoch before the model excessively fits mislabeled data. We introduce two key metrics: a stability metric using the k-epoch learning metric to quantify the stability of the model's predictions for specific subsets of examples, and a variability metric using prediction changes to emphasize the model's inconsistency in classifying training set examples. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approach compared to existing early stopping methods.},
  author = {Suqin Yuan and Lei Feng and Tongliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024early.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=CMzF2aOfqp},
  publisher = {OpenReview.net},
  title = {Early Stopping Against Label Noise Without Validation Data},
  url = {https://openreview.net/forum?id=CMzF2aOfqp},
  year = {2024}
}

@inproceedings{yuan2024craft,
  abstract = {Large language models ({LLM}s) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces ({API}s), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment {LLM}s with tools are constrained by general-purpose {API}s and lack the flexibility for tailoring them to specific tasks. To address this limitation, we propose {CRAFT}, a general tool creation and retrieval framework for {LLM}s. {CRAFT} creates toolsets specifically curated for the tasks and equips {LLM}s with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting {GPT}-4 to solve the training examples, then abstract them into diverse, reusable, and executable tools. During inference, {CRAFT} employs a multi-view matching approach, incorporating information about the target problem, {API} names, and docstrings, to identify and utilize relevant tools. We evaluate {CRAFT} on vision-language, tabular processing, and mathematical reasoning tasks and achieve substantial improvements compared to strong baselines. Our analysis reveals that consistent performance improvement can be achieved by scaling up the number of tools and the capability of the backbone models.},
  author = {Lifan Yuan and Yangyi Chen and Xingyao Wang and Yi R. Fung and Hao Peng and Heng Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024craft.pdf:pdf},
  note = {{DBLP} last modified: 2025-05-25},
  pdf = {https://openreview.net/pdf?id=G0vdDSt9XM},
  publisher = {OpenReview.net},
  title = {{CRAFT}: Customizing {LLM}s by Creating and Retrieving from Specialized Toolsets},
  url = {https://openreview.net/forum?id=G0vdDSt9XM},
  year = {2024}
}

@inproceedings{yuan2024decodable,
  abstract = {We propose Hyper-Dimensional Function Encoding ({HDFE}), which produces an explicit vector representation of continuous objects that is invariant to the sample distribution and density. {HDFE} addresses four key properties: (1) sample distribution invariance: the framework is not affected by the distribution from which the samples are collected, (2) sample size invariance: the framework is not affected by the number of samples, (3) explicit representation: the framework generates outputs with fixed dimensions, such as fixed-length vectors, and (4) decodability: the continuous object can be reconstructed at arbitrary resolution from the representation. {HDFE} does not require any training and is proved to map the object into an organized embedding space. The encoding is decodable, which enables neural networks to regress continuous objects by regressing their encodings. We demonstrate {HDFE}'s effectiveness in two applications: function-to-function mapping where vanilla {HDFE} achieves competitive performance with the state-of-the-art algorithm, and point cloud surface normal estimation where a simple replacement from {PointNet} to {HDFE} leads to 12\% and 15\% error reductions in two benchmarks. By integrating {HDFE} into the {PointNet}-based state-of-the-art network, we improve the baseline by 2.5\% and 1.7\% on the same benchmarks.},
  author = {Dehao Yuan and Furong Huang and Cornelia Ferm{\"u}ller and Yiannis Aloimonos},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024decodable.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QLoepRnoue},
  publisher = {OpenReview.net},
  title = {Decodable and Sample Invariant Continuous Object Encoder},
  url = {https://openreview.net/forum?id=QLoepRnoue},
  year = {2024}
}

@inproceedings{yuan2024unirlhf,
  abstract = {Reinforcement Learning with Human Feedback ({RLHF}) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. However, quantifying progress in {RLHF} with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-{RLHF}, a comprehensive system implementation tailored for {RLHF}. Uni-{RLHF} contains three packages: (1) a universal multi-feedback annotation platform, (2) large-scale crowdsourced feedback datasets, and (3) modular offline {RLHF} baseline implementations. We develop a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of mainstream {RL} environments. We establish a systematic pipeline of crowdsourced annotations, resulting in large-scale annotated datasets comprising more than 15 million steps across 30+ popular tasks. Through extensive experiments, results in the collected datasets demonstrate competitive performance compared to those from well-designed manual rewards. We evaluate various design choices and offer insights into their strengths and potential areas of improvement. We hope to build valuable open-source platforms, datasets, and baselines to facilitate the development of more robust and reliable {RLHF} solutions based on realistic human feedback.},
  author = {Yifu Yuan and Jianye Hao and Yi Ma and Zibin Dong and Hebin Liang and Jinyi Liu and Zhixin Feng and Kai Zhao and Yan Zheng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024unirlhf.pdf:pdf},
  note = {{DBLP} last modified: 2024-11-12},
  pdf = {https://openreview.net/pdf?id=WesY0H9ghM},
  publisher = {OpenReview.net},
  title = {Uni-{RLHF}: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback},
  url = {https://openreview.net/forum?id=WesY0H9ghM},
  year = {2024}
}

@inproceedings{yuan2024gpt,
  abstract = {Safety alignment techniques are increasingly important for preventing Large Language Models ({LLM}s) from generating harmful content. However, these techniques are mainly conducted in natural languages, which we show can be bypassed by simply communicating with {LLM}s in cipher. We propose a novel framework called CipherChat to systematically examine the generalizability of safety alignment to non-natural languages -- ciphers. CipherChat enables humans to chat with {LLM}s through cipher prompts topped with system role descriptions and few-shot enciphered demonstrations. We use CipherChat to assess state-of-the-art {LLM}s, including {ChatGPT} and {GPT}-4 for different representative human ciphers across 11 safety domains in both English and Chinese. Experimental results show that certain ciphers succeed almost 100\% of the time in bypassing the safety alignment of {GPT}-4 in several safety domains, demonstrating the necessity of developing safety alignment for non-natural languages. Notably, we identify that {LLM}s seem to have a secret cipher, and propose a novel SelfCipher that uses only role play and several demonstrations in natural language to evoke this capability. SelfCipher surprisingly outperforms existing human ciphers in almost all cases.},
  author = {Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024gpt.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=MbfAK4s61A},
  publisher = {OpenReview.net},
  title = {{GPT}-4 Is Too Smart To Be Safe: Stealthy Chat with {LLM}s via Cipher},
  url = {https://openreview.net/forum?id=MbfAK4s61A},
  year = {2024}
}

@inproceedings{yuan2024pretraining,
  abstract = {Recent studies have shown the effectiveness of utilizing human demonstration data to improve the sample efficiency of reinforcement learning ({RL}). However, these approaches require a substantial amount of high-quality human data, which can be expensive and time-consuming to collect. In this work, we present {PTGM}, a novel approach that leverages goal-based models for pre-training to enhance sample efficiency in {RL}. {PTGM} involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent {RL} tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in {RL}, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate {PTGM}'s superiority in sample efficiency compared to baseline methods. We provide comprehensive ablation studies and analysis to validate our design choices and demonstrate the effectiveness of each component in our approach.},
  author = {Haoqi Yuan and Zhancun Mu and Feiyang Xie and Zongqing Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024pretraining.pdf:pdf},
  note = {{DBLP} last modified: 2025-04-09; Oral presentation at {ICLR} 2024},
  pdf = {https://openreview.net/pdf?id=o2IEmeLL9r},
  publisher = {OpenReview.net},
  title = {Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning},
  url = {https://openreview.net/forum?id=o2IEmeLL9r},
  year = {2024}
}

@inproceedings{yuan2024diffusionts,
  abstract = {Denoising diffusion probabilistic models ({DDPM}s) are becoming the leading paradigm for generative models. They have recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-{TS}, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-{TS} to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. The framework contains two parts: a sequence encoder and an interpretable decoder which decomposes the time series into seasonal part and trend part. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a {F}ourier-based loss term. Diffusion-{TS} is expected to generate time series satisfying both interpretability and realness. The proposed Diffusion-{TS} can be easily extended to conditional generation tasks, such as forecasting and imputation, without any model changes. Through qualitative and quantitative experiments, results show that Diffusion-{TS} achieves the state-of-the-art results on various realistic analyses of time series.},
  author = {Xinyu Yuan and Yan Qiao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024diffusionts.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4h1apFjO99},
  publisher = {OpenReview.net},
  title = {Diffusion-{TS}: Interpretable Diffusion for General Time Series Generation},
  url = {https://openreview.net/forum?id=4h1apFjO99},
  year = {2024}
}

@inproceedings{yuan2024pb,
  abstract = {Network binarization, a radical form of quantization, compressing model weights to a single bit, has shown tremendous promise for creating highly efficient neural networks. However, when applied to Large Language Models ({LLM}s), traditional binarization methods suffer significant performance degradation. This paper explores network binarization specifically for {LLM}s compression and proposes a novel approach, Partially-Binarized {LLM} ({PB}-{LLM}), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized {LLM}s. Our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, {PB}-{LLM} filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. {PB}-{LLM} is extended to recover the capacities of quantized {LLM}s by analyzing from the perspective of both post-training quantization ({PTQ}) and quantization-aware training ({QAT}). Under {PTQ}, we reconstruct the binarized weight matrix guided by the {H}essian matrix. Under {QAT}, we freeze the salient weights during training and explore the derivation of optimal scaling factors crucial for minimizing the quantization error. Extensive experiments demonstrate that {PB}-{LLM} achieves substantial performance improvements compared to traditional binarization approaches while significantly compressing the model size.},
  author = {Zhihang Yuan and Yuzhang Shang and Zhen Dong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024pb.pdf:pdf},
  note = {{DBLP} last modified: 2024-11-05},
  pdf = {https://openreview.net/pdf?id=BifeBRhikU},
  publisher = {OpenReview.net},
  title = {{PB}-{LLM}: Partially Binarized Large Language Models},
  url = {https://openreview.net/forum?id=BifeBRhikU},
  year = {2024}
}

@inproceedings{yuan2024new,
  abstract = {Hard-thresholding is an important type of algorithm in machine learning that is used to solve $\ell_0$ constrained optimization problems. However, the true gradient of the objective function can be difficult to access in certain scenarios, which normally can be approximated by zeroth-order (ZO) methods. This paper focuses on ZO hard-thresholding algorithms and proposes a generalized variance reduced ZO hard-thresholding algorithm that aims to eliminate restrictions on random directions and demonstrates improved convergence rates.},
  author = {Xinzhe Yuan and William de Vazelhes and Bin Gu and Huan Xiong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024new.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=fjf3YenThE},
  publisher = {OpenReview.net},
  title = {New Insight of Variance Reduce in Zero-Order Hard-Thresholding: Mitigating Gradient Error and Expansivity Contradictions},
  url = {https://openreview.net/forum?id=fjf3YenThE},
  year = {2024}
}

@inproceedings{yuan2024active,
  abstract = {Retrosynthetic planning is a sequential decision-making process of identifying synthetic routes from the available building block materials to reach a desired target molecule. Though existing planning approaches show promisingly high solving rates and low costs, the trivial route cost evaluation via pre-trained forward reaction prediction models certainly falls short of real-world chemical practice. This paper proposes an active retrosynthetic planning approach that is aware of route quality through reinforcement learning.},
  author = {Luotian Yuan and Yemin Yu and Ying Wei and Yongwei Wang and Zhihua Wang and Fei Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024active.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=h7DGnWGeos},
  publisher = {OpenReview.net},
  title = {Active Retrosynthetic Planning Aware of Route Quality},
  url = {https://openreview.net/forum?id=h7DGnWGeos},
  year = {2024}
}

@inproceedings{yuan2024mitigating,
  abstract = {The paper addresses the vulnerability of graph neural networks to adversarial attacks. The authors propose a novel graph defense method that creates a graph robust learning framework resistant to robustness degradation while avoiding computational complexity that prevents scaling to large graphs. The approach includes a denoising module to eliminate edges associated with attacked nodes and uses mixture-of-experts to select differentially private noises while avoiding heavy adjacency matrix computations like SVD.},
  author = {Xiangchi Yuan and Chunhui Zhang and Yijun Tian and Yanfang Ye and Chuxu Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024mitigating.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=Koh0i2u8qX},
  publisher = {OpenReview.net},
  title = {Mitigating Emergent Robustness Degradation while Scaling Graph Learning},
  url = {https://openreview.net/forum?id=Koh0i2u8qX},
  year = {2024}
}

@inproceedings{yuan2024realfake,
  abstract = {Synthetic training data has gained prominence in numerous learning tasks and scenarios, offering advantages such as dataset augmentation, generalization evaluation, and privacy preservation. Despite these benefits, the efficiency of synthetic data generated by current methodologies remains inferior when training advanced deep models exclusively, limiting its practical utility. To address this challenge, this paper analyzes the principles underlying training data synthesis for supervised learning and elucidates a principled theoretical framework from the distribution-matching perspective that explicates the mechanisms governing synthesis efficacy.},
  author = {Jianhao Yuan and Jie Zhang and Shuyang Sun and Philip H. S. Torr and Bo Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yuan2024realfake.pdf:pdf},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=svIdLLZpsA},
  publisher = {OpenReview.net},
  title = {Real-Fake: Effective Training Data Synthesis Through Distribution Matching},
  url = {https://openreview.net/forum?id=svIdLLZpsA},
  year = {2024}
}

@inproceedings{yue2024towards,
  abstract = {The paper addresses selective rationalization in neural networks, which explains prediction results by identifying a small subset of inputs sufficient to support them. The authors propose a Shortcuts-fused Selective Rationalization (SSR) method to boost rationalization by discovering and exploiting potential shortcuts. SSR first designs a shortcuts discovery approach to detect potential shortcuts, then introduces two strategies to mitigate the problem of utilizing shortcuts to compose rationales, along with two data augmentation methods to close the gap in the number of annotated rationales.},
  author = {Linan Yue and Qi Liu and Yichao Du and Li Wang and Weibo Gao and Yanqing An},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yue2024towards.pdf:pdf},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=uGtfk2OphU},
  publisher = {OpenReview.net},
  title = {Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery},
  url = {https://openreview.net/forum?id=uGtfk2OphU},
  year = {2024}
}

@inproceedings{yue2024exploring,
  abstract = {The paper explores the potential of Denoising Diffusion Probabilistic Models (DM) in unsupervised learning of modular attributes. The authors build a theoretical framework that connects the diffusion time-steps and the hidden attributes, which serves as an effective inductive bias for unsupervised learning. The key insight is that the forward diffusion process incrementally adds Gaussian noise to samples at each time-step, which essentially collapses different samples into similar ones by losing attributes, with fine-grained attributes like texture being lost at earlier time steps.},
  author = {Zhongqi Yue and Jiankun Wang and Qianru Sun and Lei Ji and Eric I-Chao Chang and Hanwang Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yue2024exploring.pdf:pdf},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=bWzxhtl1HP},
  publisher = {OpenReview.net},
  title = {Exploring Diffusion Time-steps for Unsupervised Representation Learning},
  url = {https://openreview.net/forum?id=bWzxhtl1HP},
  year = {2024}
}

@inproceedings{yue2024agile3d,
  abstract = {AGILE3D is a paper about interactive 3D segmentation where a model and user work together to delineate objects of interest in a 3D point cloud through an iterative process. AGILE3D introduces an efficient, attention-based model that supports simultaneous segmentation of multiple 3D objects, yields more accurate segmentation masks with fewer user clicks, and offers faster inference. The core idea is to encode user clicks as spatial-temporal queries and enable explicit interactions between click queries as well as between them and the 3D scene through a click attention module.},
  author = {Yuanwen Yue and Sabarinath Mahadevan and Jonas Schult and Francis Engelmann and Bastian Leibe and Konrad Schindler and Theodora Kontogianni},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yue2024agile3d.pdf:pdf},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=9cQtXpRshE},
  publisher = {OpenReview.net},
  title = {{AGILE3D}: Attention Guided Interactive Multi-object {3D} Segmentation},
  url = {https://openreview.net/forum?id=9cQtXpRshE},
  year = {2024}
}

@inproceedings{yue2024mammoth,
  abstract = {MAmmoTH introduces a series of open-source large language models (LLMs) specifically tailored for general math problem-solving, and was accepted to ICLR 2024 as spotlight. The MAmmoTH models are trained on MathInstruct, a meticulously curated instruction tuning dataset compiled from 13 math datasets with intermediate rationales. The dataset presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, ensuring extensive coverage of diverse fields in math. This hybrid approach not only unleashes the potential of tool use but also allows different thought processes for different math problems.},
  author = {Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yue2024mammoth.pdf:pdf},
  note = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=yLClGs770I},
  publisher = {OpenReview.net},
  title = {{MAmmoTH}: Building Math Generalist Models through Hybrid Instruction Tuning},
  url = {https://openreview.net/forum?id=yLClGs770I},
  year = {2024}
}

@inproceedings{yue2024large,
  abstract = {The paper investigates approaches for building LLM cascades to save costs when using few-shot LLMs for reasoning tasks. The cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the most challenging questions necessitate the stronger and more expensive LLM. The researchers consider the answer consistency of the weaker LLM as a signal of question difficulty and propose several methods for answering sampling and consistency checking, including one leveraging a mixture of two thought representations (Chain-of-Thought and Program-of-Thought).},
  author = {Murong Yue and Jie Zhao and Min Zhang and Liang Du and Ziyu Yao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yue2024large.pdf:pdf},
  note = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=6okaSfANzh},
  publisher = {OpenReview.net},
  title = {Large Language Model Cascades with Mixture of Thought Representations for Cost-Efficient Reasoning},
  url = {https://openreview.net/forum?id=6okaSfANzh},
  year = {2024}
}

@inproceedings{yu2024attention,
  abstract = {The paper investigates the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text, proposing to model factual queries as constraint satisfaction problems. The researchers found a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. They propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification.},
  author = {Mert Y{\"u}ksekg√∂n{\"u}l and Varun Chandrasekaran and Erik Jones and Suriya Gunasekar and Ranjita Naik and Hamid Palangi and Ece Kamar and Besmira Nushi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yu2024attention.pdf:pdf},
  note = {ICLR 2024},
  pdf = {https://openreview.net/pdf?id=gfFVATffPd},
  publisher = {OpenReview.net},
  title = {Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models},
  url = {https://openreview.net/forum?id=gfFVATffPd},
  year = {2024}
}

@inproceedings{oug2024firstorder,
  abstract = {Meta-learning has recently received significant interest due to its empirical success in few-shot classification and reinforcement learning. Meta-learning methods leverage data from previous tasks to learn a new task in a sample-efficient manner. Although it has been empirically suggested that such methods perform well by learning shared representations during pretraining, there is limited theoretical evidence of such behavior. More importantly, it has not been shown that these methods still learn a shared structure, despite architectural misspecifications. This work shows, in the limit of an infinite number of tasks, that first-order {ANIL} with a linear two-layer network architecture successfully learns linear shared representations. This result even holds with overparametrization; having a width larger than the dimension of the shared representations results in an asymptotically low-rank solution. The learned solution then yields a good adaptation performance on any new task after a single gradient step. Overall, this illustrates how well model-agnostic methods such as first-order {ANIL} can learn shared representations.},
  author = {O\u{g}uz Kaan Y\"{u}ksel and Etienne Boursier and Nicolas Flammarion},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/oug2024firstorder.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=if2vRbS8Ew},
  publisher = {OpenReview.net},
  title = {First-order {ANIL} provably learns representations despite overparametrisation},
  url = {https://openreview.net/forum?id=if2vRbS8Ew},
  year = {2024}
}

@inproceedings{yvinec2024network,
  abstract = {The massive interest in deep neural networks ({DNN}s) for both computer vision and natural language processing has been sparked by the growth in computational power. However, this led to an increase in the memory footprint, to a point where it can be challenging to simply load a model on commodity devices such as mobile phones. To address this limitation, quantization is a favored solution as it maps high precision tensors to a low precision, memory efficient format. In terms of memory footprint reduction, its most effective variants are based on codebooks. These methods, however, suffer from two limitations. First, they either define a single codebook for each tensor, or use a memory-expensive mapping to multiple codebooks. Second, gradient descent optimization of the mapping favors jumps toward extreme values, hence not defining a proximal search. In this work, we propose to address these two limitations. First, we initially group similarly distributed neurons and leverage the re-ordered structure to either apply different scale factors to the different groups, or map weights that fall in these groups to several codebooks, without any mapping overhead. Second, stemming from this initialization, we propose a joint learning of the codebook and weight mappings that bears similarities with recent gradient-based post-training quantization techniques. Third, drawing estimation from straight-through estimation techniques, we introduce a novel gradient update definition to enable a proximal search of the codebooks and their mappings. The proposed jointly learnable codebooks and mappings ({JLCM}) method allows a very efficient approximation of any {DNN}: as such, a {Llama} 7{B} can be compressed down to 2{Go} and loaded on 5-year-old smartphones.},
  author = {Edouard Yvinec and Arnaud Dapogny and Kevin Bailly},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/yvinec2024network.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1RrOtCmuKr},
  publisher = {OpenReview.net},
  title = {Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings},
  url = {https://openreview.net/forum?id=1RrOtCmuKr},
  year = {2024}
}

@inproceedings{zadem2024reconciling,
  abstract = {Goal representation affects the performance of Hierarchical Reinforcement Learning ({HRL}) algorithms by decomposing the complex learning problem into easier subtasks. Recent studies show that representations that preserve temporally abstract environment dynamics are successful in solving difficult problems and provide theoretical guarantees for optimality. These methods however cannot scale to tasks where environment dynamics increase in complexity i.e. the temporally abstract transition relations depend on larger number of variables. On the other hand, other efforts have tried to use spatial abstraction to mitigate the previous issues. Their limitations include scalability to high dimensional environments and dependency on prior knowledge. In this paper, we propose a novel three-layer {HRL} algorithm that introduces, at different levels of the hierarchy, both a spatial and a temporal goal abstraction. We provide theoretical analysis for the resulting algorithm and demonstrate the effectiveness of spatial and temporal abstractions on complex goal-reaching tasks. We show that our algorithm generalizes both \textit{classical temporal} and \textit{classical spatial} abstractions. Our algorithm generally outperforms baselines and existing {HRL} approaches in navigation and robotic manipulation tasks.},
  author = {Mehdi Zadem and Sergio Mover and Sao Mai Nguyen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zadem2024reconciling.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=odY3PkI5VB},
  publisher = {OpenReview.net},
  title = {Reconciling Spatial and Temporal Abstractions for Goal Representation},
  url = {https://openreview.net/forum?id=odY3PkI5VB},
  year = {2024}
}

@inproceedings{zadouri2024pushing,
  abstract = {The Mixture of Experts ({MoE}) is a widely known neural architecture where an ensemble of specialized sub-models optimizes overall performance with a constant computational cost. However, conventional {MoE}s pose challenges at scale due to the need to store all experts in memory. In this paper, we push {MoE} to the limit. We propose extremely parameter-efficient {MoE} by uniquely combining {MoE} architecture with lightweight experts. Our {MoE} architecture outperforms standard parameter-efficient fine-tuning ({PEFT}) methods and is on par with full fine-tuning by only updating the lightweight experts -- less than 1\% of an 11{B} parameters model. Furthermore, our method generalizes to unseen tasks as it does not depend on any prior task knowledge. Our research underscores the versatility of the mixture of experts architecture, showcasing its ability to deliver robust performance even when subjected to rigorous parameter constraints.},
  author = {Ted Zadouri and Ahmet \"Ust\"un and Arash Ahmadian and Beyza Ermi\c{s} and Acyr Locatelli and Sara Hooker},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zadouri2024pushing.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EvDeiLv7qc},
  publisher = {OpenReview.net},
  title = {Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning},
  url = {https://openreview.net/forum?id=EvDeiLv7qc},
  year = {2024}
}

@inproceedings{zang2024overcoming,
  abstract = {Existing vision-language models exhibit strong generalization on a variety of visual domains and tasks. However, such models mainly perform zero-shot recognition in a closed-set manner, and thus struggle to handle open-domain visual concepts by design. There are recent finetuning methods, such as prompt learning, that not only study the discrimination between in-distribution ({ID}) and out-of-distribution ({OOD}) samples, but also show some improvements in both {ID} and {OOD} accuracies. In this paper, we first demonstrate that vision-language models, after long enough finetuning but without proper regularization, tend to overfit the known classes in the given dataset, with degraded performance on unknown classes. Then we propose a novel approach {OGEN} to address this pitfall, with the main focus on improving the {OOD} {GEN}eralization of finetuned models. Specifically, a class-conditional feature generator is introduced to synthesize {OOD} features using just the class name of any unknown class. Such synthesized features will provide useful knowledge about unknowns and help regularize the decision boundary between {ID} and {OOD} data when optimized jointly. Equally important is our adaptive self-distillation mechanism to regularize our feature generation model during joint optimization, i.e., adaptively transferring knowledge between model states to further prevent overfitting. Experiments validate that our method yields convincing gains in {OOD} generalization performance in different settings.},
  author = {Yuhang Zang and Hanlin Goh and Joshua M. Susskind and Chen Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zang2024overcoming.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PKICZXVY9M},
  publisher = {OpenReview.net},
  title = {Overcoming the Pitfalls of Vision-Language Model Finetuning for {OOD} Generalization},
  url = {https://openreview.net/forum?id=PKICZXVY9M},
  year = {2024}
}

@inproceedings{zanger2024diverse,
  author = {Moritz Akiya Zanger and Wendelin Boehmer and Matthijs T. J. Spaan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zanger2024diverse.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qe49ybvvPs},
  publisher = {OpenReview.net},
  title = {Diverse Projection Ensembles for Distributional Reinforcement Learning},
  url = {https://openreview.net/forum?id=qe49ybvvPs},
  year = {2024}
}

@inproceedings{zargarbashi2024conformal,
  abstract = {Conformal prediction ({CP}) transforms any model's output into prediction sets guaranteed to include (cover) the true label. {CP} requires exchangeability, a relaxation of the i.i.d. assumption, to obtain a valid distribution-free coverage guarantee. This makes it directly applicable to transductive node-classification. However, conventional {CP} cannot be applied in inductive settings due to the implicit shift in the (calibration) scores caused by message passing with the new nodes. We fix this issue for both cases of node and edge-exchangeable graphs, recovering the standard coverage guarantee without sacrificing statistical efficiency. We further prove that the guarantee holds independently of the prediction time, e.g. upon arrival of a new node/edge or at any subsequent moment. Our approach enables principled uncertainty quantification for {GNN}s in inductive node classification, facilitating their deployment in safety-critical applications.},
  author = {Soroush H. Zargarbashi and Aleksandar Bojchevski},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zargarbashi2024conformal.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=homn1jOKI5},
  publisher = {OpenReview.net},
  title = {Conformal Inductive Graph Neural Networks},
  url = {https://openreview.net/forum?id=homn1jOKI5},
  year = {2024}
}

@inproceedings{zeighami2024towards,
  abstract = {Machine learning models have demonstrated substantial performance enhancements over non-learned alternatives in various fundamental data management operations, including indexing (locating items in an array), cardinality estimation (estimating the number of matching records in a database), and range-sum estimation (estimating aggregate attribute values for query-matched records). However, real-world systems frequently favor less efficient non-learned methods due to their ability to offer (worst-case) error guarantees - an aspect where learned approaches often fall short. The primary objective of these guarantees is to ensure system reliability, ensuring that the chosen approach consistently delivers the desired level of accuracy across all databases. This paper addresses a fundamental challenge in deploying learned database operations: the lack of theoretical guarantees on their performance poses a significant hurdle to their practical deployment, especially since the non-learned alternatives often provide the required theoretical guarantees. Such guarantees are needed to ensure the reliability of the learned operations across all databases at deployment time, that is, to ensure consistent performance of the learned model on databases where the learned model had not been apriori evaluated. This research represents the first theoretical study of such guarantees for learned methods, presenting the necessary conditions for such guarantees to hold when using machine learning models for database operations.},
  author = {Sepanta Zeighami and Cyrus Shahabi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zeighami2024towards.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6tqgL8VluV},
  publisher = {OpenReview.net},
  title = {Towards Establishing Guaranteed Error for Learned Database Operations},
  url = {https://openreview.net/forum?id=6tqgL8VluV},
  year = {2024}
}

@inproceedings{zeng2024expressive,
  abstract = {Low-Rank Adaptation ({LoRA}), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of {LoRA} have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of {LoRA}. We prove that {LoRA} can adapt any model to accurately represent any smaller target model if {LoRA}-rank is greater than or equal to (width of the original model) times (depth of target model) divided by (depth of the original model). For Transformer networks, we show any model can be adapted to a target model of the same size with rank-(embedding size/2) {LoRA} adapters. We also quantify the approximation error when the {LoRA}-rank is lower than the threshold. This represents the first known theoretical results on the expressive power of {LoRA}. The study reveals numerous theoretical insights on hyperparameter tuning and algorithm development for {LoRA}, all of which are empirically validated.},
  author = {Yuchen Zeng and Kangwook Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zeng2024expressive.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=likXVjmh3E},
  publisher = {OpenReview.net},
  title = {The Expressive Power of Low-Rank Adaptation},
  url = {https://openreview.net/forum?id=likXVjmh3E},
  year = {2024}
}

@inproceedings{zeng2024framework,
  abstract = {How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a {PMI} framework that consists of perception, memory and inference components. The memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain extensive and complex relational knowledge and experience. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, reducing information conflicts and averting memory overflow. Our framework is end-to-end differentiable, with iterative competitive memory update that accumulates relational knowledge and experience incrementally. We empirically demonstrate the effectiveness of the proposed framework on tasks requiring working memory such as question-answering, knowledge graph reasoning, and reading comprehension across multiple datasets.},
  author = {Xiangyu Zeng and Jie Lin and Piao Hu and Ruizheng Huang and Zhicheng Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zeng2024framework.pdf:pdf},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vBo7544jZx},
  publisher = {OpenReview.net},
  title = {A Framework for Inference Inspired by Human Memory Mechanisms},
  url = {https://openreview.net/forum?id=vBo7544jZx},
  year = {2024}
}

@inproceedings{zeng2024m,
  abstract = {Realistic graphs contain both (1) rich self-features of nodes and (2) informative structures of neighborhoods, jointly handled by a Graph Neural Network (GNN) in the typical setup. We propose to decouple the two modalities by Mixture of weak and strong experts (Mowst), where the weak expert is a light-weight Multi-layer Perceptron (MLP), and the strong expert is an off-the-shelf GNN. To adapt the experts' collaboration to different target nodes, we propose a "confidence" mechanism based on the dispersion of the weak expert's prediction logits. The strong expert is conditionally activated in the low-confidence region when either the node's classification relies on neighborhood information, or the weak expert has low model quality. We reveal interesting training dynamics by analyzing the influence of the confidence function on loss: our training algorithm encourages the specialization of each expert by effectively generating soft splitting of the graph. In addition, our "confidence" design imposes a desirable bias toward the strong expert to benefit from GNN's better generalization capability. Mowst is easy to optimize and achieves strong expressive power, with a computation cost comparable to a single GNN. Empirically, Mowst on 4 backbone GNN architectures show significant accuracy improvement on 6 standard node classification benchmarks, including both homophilous and heterophilous graphs.},
  author = {Hanqing Zeng and Hanjia Lyu and Diyi Hu and Yinglong Xia and Jiebo Luo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zeng2024m.pdf:pdf},
  note = {DBLP last modified: 2024-12-03},
  note_presentation = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=wYvuY60SdD},
  publisher = {OpenReview.net},
  title = {{M}ixture of {W}eak and {S}trong {E}xperts on {G}raphs},
  url = {https://openreview.net/forum?id=wYvuY60SdD},
  year = {2024}
}

@inproceedings{zeng2024l,
  abstract = {Distribution shifts over time are common in real-world machine-learning applications. This scenario is formulated as Evolving Domain Generalization (EDG), where models aim to generalize well to unseen target domains in a time-varying system by learning and leveraging the underlying evolving pattern of the distribution shifts across domains. However, existing methods encounter challenges due to the limited number of timestamps (every domain corresponds to a timestamp) in EDG datasets, leading to difficulties in capturing evolving dynamics and risking overfitting to the sparse timestamps, which hampers their generalization and adaptability to new tasks. To address this limitation, we propose a novel approach SDE-EDG that collects the Infinitely Fined-Grid Evolving Trajectory (IFGET) of the data distribution with continuous-interpolated samples to bridge temporal gaps (intervals between two successive timestamps). Furthermore, by leveraging the inherent capacity of Stochastic Differential Equations (SDEs) to capture continuous trajectories, we propose their use to align and enhance the learning process in evolving domain scenarios.},
  author = {Qiuhao Zeng and Changjian Shui and Long-Kai Huang and Peng Liu and Xi Chen and Charles Ling and Boyu Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zeng2024l.pdf:pdf},
  note = {DBLP last modified: 2024-08-05},
  note_presentation = {ICLR 2024 Oral},
  pdf = {https://openreview.net/pdf?id=bTMMNT7IdW},
  publisher = {OpenReview.net},
  title = {{L}atent {T}rajectory {L}earning for {L}imited {T}imestamps under {D}istribution {S}hift over {T}ime},
  url = {https://openreview.net/forum?id=bTMMNT7IdW},
  year = {2024}
}

@inproceedings{zeng2024e,
  abstract = {As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these \"LLM evaluators\", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. We manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. We discover that different LLM evaluators exhibit distinct performance and behavior patterns, with GPT-4 achieving the best performance with an accuracy of 80.3\% on our dataset. We also present novel prompting strategies to improve evaluation capability.},
  author = {Zhiyuan Zeng and Jiatong Yu and Tianyu Gao and Yu Meng and Tanya Goyal and Danqi Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zeng2024e.pdf:pdf},
  note = {DBLP last modified: 2024-10-17},
  note_presentation = {ICLR 2024 Poster},
  pdf = {https://openreview.net/pdf?id=tr0KidwPLc},
  publisher = {OpenReview.net},
  title = {{E}valuating {L}arge {L}anguage {M}odels at {E}valuating {I}nstruction {F}ollowing},
  url = {https://openreview.net/forum?id=tr0KidwPLc},
  year = {2024}
}

@inproceedings{zhai2024u,
  abstract = {Data augmentation is critical to the empirical success of modern self-supervised representation learning, such as contrastive learning and masked language modeling. However, a theoretical understanding of the exact role of the augmentation remains limited. Recent work has built the connection between self-supervised learning and the approximation of the top eigenspace of a graph Laplacian operator, suggesting that learning a linear probe atop such representation can be connected to RKHS regression. Building on this insight, this work delves into a statistical analysis of augmentation-based pretraining. Starting from the isometry property, a geometric characterization of the target function given by the augmentation, we disentangle the effects of the model and the augmentation, and prove two generalization bounds that are free of model complexity. Our first bound works for an arbitrary encoder, and it is the sum of an estimation error bound incurred by fitting a linear probe, and an approximation error bound by RKHS approximation. Our second bound specifically addresses the case where the encoder extracts the top-d eigenspace of a finite-sample-based approximation of the underlying RKHS. A key ingredient in our analysis is the augmentation complexity, which we use to quantitatively compare different augmentations and analyze their impact on downstream performance.},
  author = {Runtian Zhai and Bingbin Liu and Andrej Risteski and J. Zico Kolter and Pradeep Kumar Ravikumar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhai2024u.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  note_presentation = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=Ax2yRhCQr1},
  publisher = {OpenReview.net},
  title = {{U}nderstanding {A}ugmentation-based {S}elf-{S}upervised {R}epresentation {L}earning via {RKHS} {A}pproximation and {R}egression},
  url = {https://openreview.net/forum?id=Ax2yRhCQr1},
  year = {2024}
}

@inproceedings{zhai2024spectrally,
  abstract = {Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data. We provide new STKR estimators applicable to the inductive setting, together with statistical guarantees and complexity analysis.},
  author = {Runtian Zhai and Rattana Pukdee and Roger Jin and Maria-Florina Balcan and Pradeep Kumar Ravikumar},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhai2024spectrally.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  note_presentation = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=OeQE9zsztS},
  publisher = {OpenReview.net},
  title = {Spectrally Transformed Kernel Regression},
  url = {https://openreview.net/forum?id=OeQE9zsztS},
  year = {2024}
}

@inproceedings{zhan2024provable,
  abstract = {Preference-based Reinforcement Learning (PbRL) is a paradigm in which an RL agent learns to optimize a task using pair-wise preference-based feedback over trajectories, rather than explicit reward signals. While PbRL has demonstrated practical success in fine-tuning language models, existing theoretical work focuses on regret minimization and fails to capture most of the practical frameworks. In this study, we fill in such a gap between theoretical PbRL and practical algorithms by proposing a theoretical reward-agnostic PbRL framework where exploratory trajectories that enable accurate learning of hidden reward functions are acquired before collecting any human feedback. Theoretical analysis demonstrates that our algorithm requires less human feedback for learning the optimal policy under preference-based models with linear parameterization and unknown transitions, compared to the existing theoretical literature. Specifically, our framework can incorporate linear and low-rank MDPs with efficient sample complexity. Additionally, we investigate reward-agnostic RL with action-based comparison feedback and introduce an efficient querying algorithm tailored to this scenario.},
  author = {Wenhao Zhan and Masatoshi Uehara and Wen Sun and Jason D. Lee},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhan2024provable.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  note_presentation = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=yTBXeXdbMf},
  publisher = {OpenReview.net},
  title = {Provable Reward-Agnostic Preference-Based Reinforcement Learning},
  url = {https://openreview.net/forum?id=yTBXeXdbMf},
  year = {2024}
}

@inproceedings{zhan2024provable,
  abstract = {In this paper, we investigate the problem of offline Preference-based Reinforcement Learning (PbRL) with human feedback where feedback is available in the form of preference between trajectory pairs rather than explicit rewards. Our proposed algorithm consists of two main steps: (1) estimate the implicit reward using Maximum Likelihood Estimation (MLE) with general function approximation from offline data and (2) solve a distributionally robust planning problem over a confidence set around the MLE. This algorithm is sample-efficient and provably learns optimal policies from preference feedback in the offline setting.},
  author = {Wenhao Zhan and Masatoshi Uehara and Nathan Kallus and Jason D. Lee and Wen Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhan2024provable.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  note_presentation = {ICLR 2024 Spotlight},
  pdf = {https://openreview.net/pdf?id=tVMPfEGT2w},
  publisher = {OpenReview.net},
  title = {Provable Offline Preference-Based Reinforcement Learning},
  url = {https://openreview.net/forum?id=tVMPfEGT2w},
  year = {2024}
}

@inproceedings{zhang2024p,
  abstract = {Deep clustering, which learns representation and semantic clustering without labels information, poses a great challenge for deep learning-based approaches. Despite significant progress in recent years, most existing methods focus on uniformly distributed datasets, significantly limiting the practical applicability of their methods. In this paper, we first introduce a more practical problem setting named deep imbalanced clustering, where the underlying classes exhibit an imbalance distribution. To tackle this problem, we propose a novel pseudo-labeling-based learning framework. Our framework formulates pseudo-label generation as a progressive partial optimal transport problem, which progressively transports each sample to imbalanced clusters under prior distribution constraints, thus generating imbalance-aware pseudo-labels and learning from high-confident samples. In addition, we transform the initial formulation into an unbalanced optimal transport problem with augmented constraints, which can be solved efficiently by a fast matrix scaling algorithm.},
  author = {Chuyu Zhang and Hui Ren and Xuming He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024p.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hD3sGVqPsr},
  publisher = {OpenReview.net},
  title = {{P}$^2${OT}: Progressive Partial Optimal Transport for Deep Imbalanced Clustering},
  url = {https://openreview.net/forum?id=hD3sGVqPsr},
  year = {2024}
}

@inproceedings{zhang2024s,
  abstract = {Graph contrastive learning (GCL) has become a powerful tool for learning graph data, but its scalability remains a significant challenge. In this work, we propose a simple yet effective training framework called Structural Compression (StructComp) to address this issue. Inspired by a sparse low-rank approximation on the diffusion matrix, StructComp trains the encoder with the compressed nodes. This allows the encoder not to perform any message passing during the training stage, and significantly reduces the number of sample pairs in the contrastive loss. We theoretically prove that the original GCL loss can be approximated with the contrastive loss computed by StructComp. Moreover, StructComp can be regarded as an additional regularization term for GCL models, resulting in a more robust encoder. Empirical studies on various datasets show that StructComp greatly reduces the time and memory consumption while improving model performance compared to the vanilla GCL models and scalable training methods.},
  author = {Shengzhong Zhang and Wenjie Yang and Xinyuan Cao and Hongwei Zhang and Zengfeng Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024s.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=a4DBEeGfQq},
  publisher = {OpenReview.net},
  title = {{S}truct{C}omp: Substituting propagation with {S}tructural {C}ompression in {T}raining {G}raph {C}ontrastive {L}earning},
  url = {https://openreview.net/forum?id=a4DBEeGfQq},
  year = {2024}
}

@inproceedings{zhang2024personalize,
  abstract = {Driven by large-data pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful promptable framework, revolutionizing the segmentation field. Despite the generality, customizing SAM for specific visual concepts without man-powered prompting is under-explored, e.g., automatically segmenting your pet dog in numerous images. In this work, we propose a training-free personalization approach for SAM, termed PerSAM. Given only a single reference image with a reference mask, PerSAM first localizes the target concept by a novel localization mechanism, and then guides SAM to segment objects in other images via target-guided attention and target-semantic prompting. Without any training, PerSAM can effectively segment the target object in different images while maintaining SAM's generalization ability. We conduct extensive experiments on various datasets, demonstrating that PerSAM achieves superior performance compared to existing methods.},
  author = {Renrui Zhang and Zhengkai Jiang and Ziyu Guo and Shilin Yan and Junting Pan and Hao Dong and Yu Qiao and Peng Gao and Hongsheng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024personalize.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6Gzkhoc6YS},
  publisher = {OpenReview.net},
  title = {Personalize {S}egment {A}nything {M}odel with {O}ne {S}hot},
  url = {https://openreview.net/forum?id=6Gzkhoc6YS},
  year = {2024}
}

@inproceedings{zhang2024controlvideo,
  abstract = {Text-driven diffusion models have unlocked unprecedented abilities in image generation, whereas their video counterpart still lags behind due to the excessive training cost of temporal modeling. Besides the training burden, the generated videos also suffer from appearance inconsistency and structural flickers, especially in long video synthesis. To address these challenges, we designed a training-free framework called {ControlVideo} to enable natural and efficient text-to-video generation. {ControlVideo}, adapted from {ControlNet}, leverages coarsely structural consistency from input motion sequences, and introduces three modules to improve video generation: (1) Fully cross-frame interaction in self-attention modules to ensure appearance coherence; (2) An interleaved-frame smoother that employs frame interpolation on alternated frames to mitigate the flicker effect; (3) A hierarchical sampler that separately synthesizes each short clip with holistic coherency to produce long videos efficiently. Experimental results demonstrate that {ControlVideo} generates both short and long videos with favorable performance.},
  author = {Yabo Zhang and Yuxiang Wei and Dongsheng Jiang and Xiaopeng Zhang and Wangmeng Zuo and Qi Tian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024controlvideo.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5a79AqFr0c},
  publisher = {OpenReview.net},
  title = {{ControlVideo}: Training-free Controllable Text-to-video Generation},
  url = {https://openreview.net/forum?id=5a79AqFr0c},
  year = {2024}
}

@inproceedings{zhang2024finitetime,
  abstract = {Federated reinforcement learning ({FRL}) has emerged as a promising paradigm for reducing the sample complexity of reinforcement learning tasks by exploiting information from different agents. However, when each agent interacts with a potentially different environment, little to nothing is known theoretically about the non-asymptotic performance of {FRL} algorithms. The lack of such results can be attributed to various technical challenges and their intricate interplay: Markovian sampling, linear function approximation, multiple local updates to save communication, heterogeneity in the reward functions and transition kernels of the agents' {MDP}s, and continuous state-action spaces. Moreover, in the on-policy setting, the behavior policies vary with time, further complicating the analysis. In this work, we introduce {FedSARSA}, a novel federated on-policy reinforcement learning scheme, equipped with linear function approximation, to address these challenges and provide a comprehensive finite-time error analysis. Our theoretical results show that {FedSARSA} converges to a policy that is near-optimal for all agents, with the extent of near-optimality proportional to the level of heterogeneity. {FedSARSA} leverages agent collaboration to enable linear speedups as the number of agents increases, which holds for both fixed and adaptive step-size configurations.},
  author = {Chenyu Zhang and Han Wang and Aritra Mitra and James Anderson},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024finitetime.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=D2eOVqPX9g},
  publisher = {OpenReview.net},
  title = {Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement Learning},
  url = {https://openreview.net/forum?id=D2eOVqPX9g},
  year = {2024}
}

@inproceedings{zhang2024mixedtype,
  abstract = {Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces {TabSyn}, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder ({VAE}) crafted latent space. The key advantages of the proposed {TabSyn} include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that {TabSyn} outperforms existing methods. Specifically, it reduces the error rates by 86\% and 67\% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines.},
  author = {Hengrui Zhang and Jiani Zhang and Zhengyuan Shen and Balasubramaniam Srinivasan and Xiao Qin and Christos Faloutsos and Huzefa Rangwala and George Karypis},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024mixedtype.pdf:pdf},
  note = {DBLP last modified: 2025-05-12; Presented as oral presentation},
  pdf = {https://openreview.net/pdf?id=4Ay23yeuz0},
  publisher = {OpenReview.net},
  title = {Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space},
  url = {https://openreview.net/forum?id=4Ay23yeuz0},
  year = {2024}
}

@inproceedings{zhang2024epitopological,
  abstract = {Sparse training ({ST}) aims to improve deep learning by replacing fully connected artificial neural networks ({ANN}s) with sparse or ultra-sparse networks, similar to brain networks, potentially benefiting from brain-inspired learning paradigms from complex network intelligence theory. Here we launch the ultra-sparse advantage challenge to provide evidence on how ultra-sparse topologies (around 1\% connection retained) can achieve learning advantages against fully connected networks. We introduce two main methods: {ESML} (Epitopological Sparse Machine Learning) and {CHT} (Cannistraci-Hebb Training). Results show that with only 1\% of links retained during training, {CHT} surpasses fully connected networks on {VGG16}, {GoogLeNet}, {ResNet50}, and {ResNet152}, providing evidence for ultra-sparse advantage and marking a milestone in deep learning. {CHT} functions as a gradient-free oracle that uses {CH3-L3}-based epitopological learning to guide the placement of new links in ultra-sparse network topology, facilitating sparse-weight gradient learning and reducing convergence time. The term epitopological means new topology and refers to epitopological learning, which is a field of network science and complex network intelligence that studies how to implement learning on complex networks by changing the shape of their connectivity structure (epitopological plasticity).},
  author = {Yingtao Zhang and Jialin Zhao and Wenjing Wu and Alessandro Muscoloni and Carlo Vittorio Cannistraci},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024epitopological.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=iayEcORsGd},
  publisher = {OpenReview.net},
  title = {Epitopological learning and Cannistraci-Hebb network shape intelligence brain-inspired theory for ultra-sparse advantage in deep learning},
  url = {https://openreview.net/forum?id=iayEcORsGd},
  year = {2024}
}

@inproceedings{zhang2024codesage,
  abstract = {Recent studies have shown that code language models at scale demonstrate significant performance gains on downstream tasks, i.e., code generation. However, most of the existing works on code representation learning train models at a hundred million parameter scale using very limited pretraining corpora. In this work, we fuel code representation learning with a vast amount of code data via a two-stage pretraining scheme. We first train the encoders via a mix that leverages both randomness in masking language modeling and the structure aspect of programming language. We then enhance the representations via contrastive learning with hard negative and hard positive constructed in an unsupervised manner. We establish an off-the-shelf encoder model, {CodeSage}, that persistently outperforms the existing models on a wide variety of downstream tasks by large margin. On code search, {CodeSage} improves the success rate by up to 20\%. On eight different code classification tasks in the {CodeXGLUE} benchmark, {CodeSage} improves the average accuracy by up to 10.2\%. We further show the effectiveness of our method with ~2B parameters on code generation.},
  author = {Dejiao Zhang and Wasi Uddin Ahmad and Ming Tan and Hantian Ding and Ramesh Nallapati and Dan Roth and Xiaofei Ma and Bing Xiang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024codesage.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vfzRRjumpX},
  publisher = {OpenReview.net},
  title = {{CodeSage}: Code Representation Learning at Scale},
  url = {https://openreview.net/forum?id=vfzRRjumpX},
  year = {2024}
}

@inproceedings{zhang2024differentially,
  abstract = {Differentially Private Stochastic Gradient Descent with Gradient Clipping ({DPSGD-GC}) is a powerful tool for training deep learning models using sensitive data, providing both a solid theoretical privacy guarantee and high efficiency. However, using {DPSGD-GC} to ensure Differential Privacy ({DP}) comes at the cost of model performance degradation due to {DP} noise injection and gradient clipping. Existing research has shown that {DPSGD-GC} only converges when using large clipping thresholds that are dependent on problem-specific parameters that are often unknown in practice. Unfortunately, these parameters are often unknown in practice, making it hard to choose the optimal clipping threshold. Therefore, {DPSGD-GC} suffers from degraded performance due to the constant bias introduced by the clipping. In our work, we propose a new error-feedback ({EF}) {DP} algorithm as an alternative to {DPSGD-GC}, which offers a diminishing utility bound without inducing a constant clipping bias. More importantly, it allows for an arbitrary choice of clipping threshold that is independent of the problem. We establish an algorithm-specific {DP} analysis for our proposed algorithm, providing privacy guarantees based on {R\\'enyi} {DP}.},
  author = {Xinwei Zhang and Zhiqi Bu and Steven Wu and Mingyi Hong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024differentially.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=uFbWHyTlPn},
  publisher = {OpenReview.net},
  title = {Differentially Private SGD Without Clipping Bias: An Error-Feedback Approach},
  url = {https://openreview.net/forum?id=uFbWHyTlPn},
  year = {2024}
}

@inproceedings{zhang2024dont,
  abstract = {Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. However, we investigate the effect of hue variance in the context of video understanding and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video understanding, named Motion Coherent Augmentation ({MCA}), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. The method consists of two main components: (1) {SwapMix}: modifies the appearances of video samples efficiently; (2) Variation Alignment: resolves the distribution shift caused by {SwapMix}. We demonstrate obvious performance gain, generalization ability over different architectures and datasets, compatibility with other augmentation methods, and application of Variation Alignment in other augmentation methods for even higher performance.},
  author = {Yitian Zhang and Yue Bai and Huan Wang and Yizhou Wang and Yun Fu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024dont.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=RIcYTbpO38},
  publisher = {OpenReview.net},
  title = {Don't Judge by the Look: Towards Motion Coherent Video Representation},
  url = {https://openreview.net/forum?id=RIcYTbpO38},
  year = {2024}
}

@inproceedings{zhang2024hedgehog,
  abstract = {Linear attentions have shown potential for improving Transformer efficiency, reducing attention's quadratic complexity to linear in sequence length. This holds exciting promise for (1) training linear Transformers from scratch, (2) finetuned-conversion of task-specific Transformers into linear versions that recover task performance, and (3) pretrained-conversion of Transformers such as large language models into linear versions finetunable on downstream tasks. However, linear attentions often underperform standard softmax attention in quality. To close this performance gap, we study what properties of softmax attention contribute to its effectiveness. We find prior linear attentions lack key properties of softmax attention tied to good performance: low-entropy (or spiky) weights and dot-product monotonicity. We propose {Hedgehog}, a learnable linear attention that retains these spiky and monotonic properties while maintaining linear complexity. {Hedgehog} uses simple trainable {MLP}s to produce attention weights mimicking softmax attention. Experiments show {Hedgehog} recovers over 99\% of standard Transformer quality in train-from-scratch and finetuned-conversion settings, outperforming prior linear attentions up to 6 perplexity points on {WikiText-103} with causal {GPT}s, and up to 8.7 {GLUE} score points on finetuned bidirectional {BERT}s.},
  author = {Michael Zhang and Kush S. Bhatia and Hermann Kumbong and Christopher R{\'e}},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024hedgehog.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=4g02l2N2Nx},
  publisher = {OpenReview.net},
  title = {The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry},
  url = {https://openreview.net/forum?id=4g02l2N2Nx},
  year = {2024}
}

@inproceedings{zhang2024plugandplay,
  abstract = {With the rapid growth of large language models ({LLM}s), there is increasing demand for memory and computation in {LLM}s. Recent efforts on post-training pruning of {LLM}s aim to reduce the model size and computation requirements, yet the performance is still sub-optimal. In this paper, we present a plug-and-play solution for post-training pruning of {LLM}s. The proposed solution has two innovative components: (1) Relative Importance and Activations ({RIA}), a new pruning metric that jointly considers the weight and activations efficiently on {LLM}s, and (2) Channel Permutation, a new approach to maximally preserves important weights under {N:M} sparsity. The two proposed components can be readily combined to further enhance the {N:M} semi-structured pruning of {LLM}s. Our empirical experiments show that {RIA} alone can already surpass all existing post-training pruning methods on prevalent {LLM}s, e.g., {LLaMA} ranging from 7{B} to 65{B}. We also provide comprehensive ablations and analysis to validate our method.},
  author = {Yingtao Zhang and Haoli Bai and Haokun Lin and Jialin Zhao and Lu Hou and Carlo Vittorio Cannistraci},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024plugandplay.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Tr0lPx9woF},
  publisher = {OpenReview.net},
  title = {Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models},
  url = {https://openreview.net/forum?id=Tr0lPx9woF},
  year = {2024}
}

@inproceedings{zhang2024diffusion,
  abstract = {Sampling from intractable high-dimensional density functions is a fundamental task that often appears in machine learning and statistics. Recent sampling-based approaches that leverage controlled stochastic processes to model approximate samples from these target densities have shown considerable promise. However, the main drawback of these approaches is that the training objective requires full trajectories to compute, resulting in sluggish credit assignment issues due to use of entire trajectories and a learning signal present only at the terminal time. In this work, we present Diffusion Generative Flow Samplers ({DGFS}), a sampling-based framework where the learning process can be tractably broken down into short partial trajectory segments, via parameterizing an additional flow function. Our method takes inspiration from the theory developed for generative flow networks ({GFlowNets}), allowing us to make use of intermediate learning signals. Through experiments, we demonstrate that {DGFS} results in more accurate estimates of the normalization constant than closely-related prior methods.},
  author = {Dinghuai Zhang and Ricky T. Q. Chen and Cheng-Hao Liu and Aaron C. Courville and Yoshua Bengio},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024diffusion.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=OIsahq1UYC},
  publisher = {OpenReview.net},
  title = {Diffusion Generative Flow Samplers: Improving learning signals through partial trajectory optimization},
  url = {https://openreview.net/forum?id=OIsahq1UYC},
  year = {2024}
}

@inproceedings{zhang2024communicationefficient,
  abstract = {Distributed and federated learning algorithms and techniques associated primarily with minimization problems. However, with the increase of minimax optimization and variational inequality problems in machine learning, the necessity of designing efficient distributed/federated learning approaches for these problems is becoming more apparent. In this paper, we provide a unified convergence analysis of communication-efficient local training methods for distributed variational inequality problems ({VIPs}). Our approach is based on a general key assumption on the stochastic estimates that allows us to propose and analyze several novel local training algorithms under a single framework for solving a class of structured non-monotone {VIPs}. We present the first local gradient descent-accent algorithms with provable improved communication complexity for solving distributed variational inequalities on heterogeneous data. The general algorithmic framework recovers state-of-the-art algorithms and their sharp convergence guarantees when the setting is specialized to minimization or minimax optimization problems. Finally, we demonstrate the strong performance of the proposed algorithms compared to state-of-the-art methods when solving federated minimax optimization problems.},
  author = {Siqi Zhang and Sayantan Choudhury and Sebastian U. Stich and Nicolas Loizou},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024communicationefficient.pdf:pdf},
  pdf = {https://arxiv.org/pdf/2306.05100.pdf},
  publisher = {OpenReview.net},
  title = {{Communication-Efficient Gradient Descent-Accent Methods for Distributed Variational Inequalities: Unified Analysis and Local Updates}},
  url = {https://openreview.net/forum?id=hORCalGn3Z},
  year = {2024}
}

@inproceedings{zhang2024deep,
  abstract = {Recent studies in using deep reinforcement learning ({DRL}) to solve Job-shop scheduling problems ({JSSP}) focus on construction heuristics. However, their performance is still far from optimality, mainly because the underlying graph representation scheme is unsuitable for modelling partial solutions at each construction step. This paper proposes a novel {DRL}-guided improvement heuristic for solving {JSSP}, where graph representation is employed to encode complete solutions. We design a Graph-Neural-Network-based representation scheme, consisting of two modules to effectively capture the information of dynamic topology and different types of nodes in graphs encountered during the improvement process. To speed up solution evaluation during improvement, we present a novel message-passing mechanism that can evaluate multiple solutions simultaneously. We prove that the computational complexity of our method scales linearly with problem size. Experiments on classic benchmarks show that the improvement policy learned by our method outperforms state-of-the-art {DRL}-based methods by a large margin.},
  author = {Cong Zhang and Zhiguang Cao and Wen Song and Yaoxin Wu and Jie Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024deep.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=jsWCmrsHHs},
  publisher = {OpenReview.net},
  title = {{Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling}},
  url = {https://openreview.net/forum?id=jsWCmrsHHs},
  year = {2024}
}

@inproceedings{zhang2024adversarial,
  abstract = {Fairness-aware graph neural networks ({GNNs}) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. However, although these methods greatly improve the algorithmic fairness of {GNNs}, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate adversarial attacks on fairness of {GNNs} and propose {G-FairAttack}, a general framework for attacking various types of fairness-aware {GNNs} in terms of fairness with an unnoticeable effect on prediction utility. Additionally, we propose a fast computation technique to reduce the time complexity of {G-FairAttack}. The experimental study demonstrates that {G-FairAttack} successfully corrupts the fairness of different types of {GNNs} while keeping the attack unnoticeable through extensive experiments on three real-world datasets.},
  author = {Binchi Zhang and Yushun Dong and Chen Chen and Yada Zhu and Minnan Luo and Jundong Li},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024adversarial.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=q3KNrmW6Ql},
  publisher = {OpenReview.net},
  title = {{Adversarial Attacks on Fairness of Graph Neural Networks}},
  url = {https://openreview.net/forum?id=q3KNrmW6Ql},
  year = {2024}
}

@inproceedings{zhang2024building,
  abstract = {This work addresses challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of {LLMs} and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Our framework consists of five key modules: Perception, Memory, Communication, Planning, and Execution. The Communication Module and the Planning Module leverage {LLMs} to generate messages and make plans, while the Memory Module stores knowledge and experience. Experiments on {C-WAH} and {TDW-MAT} demonstrate that our approach driven by {GPT-4} can surpass strong planning-based methods by achieving more than 40\% efficiency improvements and exhibit emergent effective communication. We also discovered that our approach communicating in natural language can earn more trust and cooperate more effectively with humans.},
  author = {Hongxin Zhang and Weihua Du and Jiaming Shan and Qinhong Zhou and Yilun Du and Joshua B. Tenenbaum and Tianmin Shu and Chuang Gan},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024building.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=EnXJfQqy0K},
  publisher = {OpenReview.net},
  title = {{Building Cooperative Embodied Agents Modularly with Large Language Models}},
  url = {https://openreview.net/forum?id=EnXJfQqy0K},
  year = {2024}
}

@inproceedings{zhang2024mediator,
  abstract = {A recent paper by Farina \& Pipis (2023) established the existence of uncoupled no-linear-swap regret dynamics with polynomial-time iterations in extensive-form games. The equilibrium points reached by these dynamics, known as linear correlated equilibria, are currently the tightest known relaxation of correlated equilibrium that can be learned in polynomial time in any finite extensive-form game. However, their properties remain vastly unexplored, and their computation is onerous. In this paper, we provide several contributions shedding light on the fundamental nature of linear-swap regret. We show a connection between linear deviations and a generalization of communication deviations in which the player can make queries to a mediator who replies with action recommendations, and the player is not constrained to match the timing of the game. We coin this latter set the untimed communication ({UTC}) deviations. We prove that {UTC} deviations coincide precisely with linear deviations, so any player minimizing {UTC} regret also minimizes linear-swap regret. Leveraging this connection, we develop state-of-the-art no-regret algorithms for computing linear correlated equilibria with polynomially better per-iteration runtimes in theory and several orders of magnitude improvements in practice.},
  author = {Brian Hu Zhang and Gabriele Farina and Tuomas Sandholm},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024mediator.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=bsKMPAFHO7},
  publisher = {OpenReview.net},
  title = {{Mediator Interpretation and Faster Learning Algorithms for Linear Correlated Equilibria in General Sequential Games}},
  url = {https://openreview.net/forum?id=bsKMPAFHO7},
  year = {2024}
}

@inproceedings{zhang2024domaininspired,
  abstract = {Recent studies have shown that Sharpness-Aware Minimization ({SAM}) can improve the generalization of neural networks. However, {SAM}'s improvement becomes less significant or even worse when the training data has domain shifts. In this work, we present a Domain-Inspired Sharpness-Aware Minimization ({DISAM}) algorithm. It is motivated by the inconsistent convergence degree of {SAM} across different domains, which induces optimization bias towards certain domains and thus impairs the overall convergence. To address this issue, we consider the domain-level convergence consistency in the sharpness estimation to prevent the overwhelming perturbations for less optimized domains and deficient perturbations for well optimized domains. Specifically, {DISAM} introduces the constraint of minimizing variance in the domain loss, which allows the elastic gradient calibration in perturbation generation: when one domain is optimized above the averaging level with respect to loss, the gradient perturbation towards that domain will be weakened automatically, and vice versa. Extensive experiments demonstrate that our approach achieves consistent improvements over {SAM} on various domain shift benchmarks.},
  author = {Ruipeng Zhang and Ziqing Fan and Jiangchao Yao and Ya Zhang and Yanfeng Wang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024domaininspired.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=I4wB3HA3dJ},
  publisher = {OpenReview.net},
  title = {{Domain-Inspired Sharpness-Aware Minimization Under Domain Shifts}},
  url = {https://openreview.net/forum?id=I4wB3HA3dJ},
  year = {2024}
}

@inproceedings{zhang2024metalearning,
  abstract = {Prior knowledge accumulated from related tasks offers a powerful approach to learning a novel task from a limited number of training data. Recent approaches use a family of prior probability density functions or recurrent neural network models, whose parameters can be optimized by utilizing labeled data from observed tasks. While these approaches have appealing empirical performance, expressiveness of their prior is relatively low, which limits generalization and interpretation of meta-learning. This work puts forth a novel prior representation model that leverages the notion of algorithm unrolling, aiming at expressive yet meaningful priors. The key idea is to unroll the proximal gradient descent steps, where learnable piecewise linear functions are developed to approximate the desired proximal operators within tight theoretical error bounds established for both smooth and non-smooth proximal functions. The resultant unrolled proximal networks are trained meta-learners over related tasks. Comprehensive experiments demonstrate that the proposed approach achieves competitive performance against recent meta-learning methods.},
  author = {Yilang Zhang and Georgios B. Giannakis},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024metalearning.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=b3Cu426njo},
  publisher = {OpenReview.net},
  title = {{Meta-Learning Priors Using Unrolled Proximal Networks}},
  url = {https://openreview.net/forum?id=b3Cu426njo},
  year = {2024}
}

@inproceedings{zhang2024beyond,
  abstract = {Designing expressive Graph Neural Networks ({GNNs}) is a fundamental topic in graph learning. So far, {GNN} expressiveness has been primarily assessed via the Weisfeiler-Lehman ({WL}) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this work, we introduce a unified framework for quantitatively studying the expressiveness of {GNN} architectures, which addresses all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of {GNN} models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between {GNN} models, while the practicality allows for understanding concrete {GNN} abilities such as subgraph counting. By examining four classes of prominent {GNNs} as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Extensive experiments on both synthetic and real-world tasks verify our theory.},
  author = {Bohang Zhang and Jingchu Gai and Yiheng Du and Qiwei Ye and Di He and Liwei Wang},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024beyond.pdf:pdf},
  note = {Outstanding Paper Award},
  pdf = {https://openreview.net/pdf?id=HSKaGOi7Ar},
  publisher = {OpenReview.net},
  title = {{Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness}},
  url = {https://openreview.net/forum?id=HSKaGOi7Ar},
  year = {2024}
}

@inproceedings{zhang2024batteryml,
  abstract = {Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often grapple with the intricacies of battery science, while battery researchers face hurdles in adapting intricate models tailored to specific datasets. Beyond this, a cohesive standard for battery degradation modeling, inclusive of data formats and evaluative benchmarks, is conspicuously absent. Recognizing these impediments, we present {BatteryML} -- a one-step, all-encompass, and open-source platform designed to unify data preprocessing, feature extraction, and the implementation of both traditional and state-of-the-art models. This streamlined approach promises to enhance the practicality and efficiency of research applications. Our platform encompasses an extensive dataset collection, standardized data formats, and a comprehensive suite of models, paving the way for deeper insights into battery degradation and enhanced collaboration across interdisciplinary domains.},
  author = {Han Zhang and Xiaofan Gui and Shun Zheng and Ziheng Lu and Yuqi Li and Jiang Bian},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024batteryml.pdf:pdf},
  note = {Spotlight},
  pdf = {https://openreview.net/pdf?id=sxGugrYhP9},
  publisher = {OpenReview.net},
  title = {{BatteryML: An Open-source Platform for Machine Learning on Battery Degradation}},
  url = {https://openreview.net/forum?id=sxGugrYhP9},
  year = {2024}
}

@inproceedings{zhang2024tapmo,
  abstract = {Previous motion generation methods are limited to the pre-rigged 3D human model, hindering their applications in the animation of various non-rigged characters. In this work, we present {TapMo}, a Text-driven Animation Pipeline for synthesizing Motion in a broad spectrum of skeleton-free 3D characters. The pivotal innovation in {TapMo} is its use of shape deformation-aware features as a condition to guide the diffusion model, thereby enabling the generation of mesh-specific motions for various characters. {TapMo} comprises two main components -- Mesh Handle Predictor and Shape-aware Diffusion Module. The former predicts the skinning weights and clusters mesh vertices into adaptive handles for deformation control, which eliminates the need for traditional skeletal rigging. The latter employs text-guided motions and mesh features extracted during the first stage, preserving the geometric integrity of the animations by accounting for the character's shape and deformation. Trained in a weakly-supervised manner, {TapMo} can accommodate a multitude of non-human meshes, both with and without associated text motions. The effectiveness and generalizability of {TapMo} have been demonstrated through rigorous qualitative and quantitative experiments. The results reveal that {TapMo} consistently outperforms existing auto-animation methods, delivering superior-quality animations for both seen or unseen heterogeneous 3D characters.},
  author = {Jiaxu Zhang and Shaoli Huang and Zhigang Tu and Xin Chen and Xiaohang Zhan and Gang Yu and Ying Shan},
  booktitle = {The Twelfth International Conference on Learning Representations ({ICLR})},
  file = {:/home/b/documents/inproceedings/zhang2024tapmo.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=OeH6Fdhv7q},
  publisher = {OpenReview.net},
  title = {{TapMo: Shape-aware Motion Generation of Skeleton-free Characters}},
  url = {https://openreview.net/forum?id=OeH6Fdhv7q},
  year = {2024}
}

@inproceedings{zhang2024soft,
  abstract = {Robust Markov Decision Processes (MDPs) and risk-sensitive MDPs are both powerful tools for making decisions in the presence of uncertainties. Previous efforts have aimed to establish their connections, revealing equivalences in specific formulations. This paper introduces a new formulation for risk-sensitive MDPs, which assesses risk in a slightly different manner compared to the classical Markov risk measure, and establishes its equivalence with a class of soft robust MDP (RMDP) problems, including the standard RMDP as a special case.},
  author = {Runyu Zhang and Yang Hu and Na Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024soft.pdf:pdf},
  note = {DBLP last modified: 2024-10-09},
  pdf = {https://openreview.net/pdf?id=dEz3ge8QSo},
  publisher = {OpenReview.net},
  title = {Soft Robust {MDP}s and Risk-Sensitive {MDP}s: Equivalence, Policy Gradient, and Sample Complexity},
  url = {https://openreview.net/forum?id=dEz3ge8QSo},
  year = {2024}
}

@inproceedings{zhang2024sinenet,
  abstract = {We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model's performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage.},
  author = {Xuan Zhang and Jacob Helwig and Yuchao Lin and Yaochen Xie and Cong Fu and Stephan Wojtowytsch and Shuiwang Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024sinenet.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=LSYhE2hLWG},
  publisher = {OpenReview.net},
  title = {{SineNet}: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations},
  url = {https://openreview.net/forum?id=LSYhE2hLWG},
  year = {2024}
}

@inproceedings{zhang2024llama,
  abstract = {We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge.},
  author = {Renrui Zhang and Jiaming Han and Chris Liu and Aojun Zhou and Pan Lu and Yu Qiao and Hongsheng Li and Peng Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024llama.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=d4UiXAHN2W},
  publisher = {OpenReview.net},
  title = {{LLaMA}-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention},
  url = {https://openreview.net/forum?id=d4UiXAHN2W},
  year = {2024}
}

@inproceedings{zhang2024continuousmultiple,
  abstract = {Image outpainting aims to generate the content of an input sub-image beyond its original boundaries. It is an important task in content generation yet remains an open problem for generative models. This work identifies two key challenges of existing approaches: outpainting with arbitrary and continuous multiples and performing outpainting in a single step. We propose PQDiff (Positional Query Diffusion) to address both challenges. The arbitrary multiple outpainting is achieved by utilizing randomly cropped views from the same image during training to capture arbitrary relative positional information. Specifically, by feeding one view and positional embeddings as queries, we can reconstruct another view.},
  author = {Shaofeng Zhang and Jinfa Huang and Qiang Zhou and Zhibin Wang and Fan Wang and Jiebo Luo and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024continuousmultiple.pdf:pdf},
  note = {DBLP last modified: 2024-12-05},
  pdf = {https://openreview.net/pdf?id=7hxoYxKDTV},
  publisher = {OpenReview.net},
  title = {Continuous-Multiple Image Outpainting in One-Step via Positional Query and A Diffusion-based Approach},
  url = {https://openreview.net/forum?id=7hxoYxKDTV},
  year = {2024}
}

@inproceedings{zhang2024enhancing,
  abstract = {Instance-level image classification tasks have traditionally relied on single-instance labels to train models, e.g., few-shot learning and transfer learning. However, set-level coarse-grained labels that capture relationships among instances can provide richer information in real-world scenarios. In this paper, we present a novel approach to enhance instance-level image classification by leveraging set-level labels. We provide a theoretical analysis of the proposed method, including recognition conditions for fast excess risk rate, shedding light on the theoretical foundations of our approach.},
  author = {Renyu Zhang and Aly A. Khan and Yuxin Chen and Robert L. Grossman},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024enhancing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AZW3qlCGTe},
  publisher = {OpenReview.net},
  title = {Enhancing Instance-Level Image Classification with Set-Level Labels},
  url = {https://openreview.net/forum?id=AZW3qlCGTe},
  year = {2024}
}

@inproceedings{zhang2024foundation,
  abstract = {Machine learning has demonstrated remarkable performance over finite datasets, yet whether the scores over the fixed benchmarks can sufficiently indicate the model's performance in the real world is still in discussion. In reality, an ideal robust model will probably behave similarly to the oracle (e.g., the human users), thus a good evaluation protocol is probably to evaluate the models' behaviors in comparison to the oracle. In this paper, we introduce a new robustness measurement that directly measures the image classification model's performance compared with a surrogate oracle (i.e., a foundation model).},
  author = {Peiyan Zhang and Haoyang Liu and Chaozhuo Li and Xing Xie and Sunghun Kim and Haohan Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024foundation.pdf:pdf},
  note = {DBLP last modified: 2025-02-24},
  pdf = {https://openreview.net/pdf?id=jd5GokdySz},
  publisher = {OpenReview.net},
  title = {Foundation Model-oriented Robustness: Robust Image Model Evaluation with Pretrained Models},
  url = {https://openreview.net/forum?id=jd5GokdySz},
  year = {2024}
}

@inproceedings{zhang2024metacoco,
  abstract = {Out-of-distribution problems in few-shot classification occur when novel classes sampled from testing distributions differ from base classes drawn from training distributions, which considerably degrades the performance of deep learning models deployed in real-world applications. Recent studies suggest that the OOD problems in FSC mainly include cross-domain few-shot classification and spurious-correlation few-shot classification. Despite CD-FSC has been extensively studied, SC-FSC remains understudied due to lack of the corresponding evaluation benchmarks. In this paper, we present Meta Concept Context (MetaCoCo), a benchmark with spurious-correlation shifts collected from real-world scenarios.},
  author = {Min Zhang and Haoxuan Li and Fei Wu and Kun Kuang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024metacoco.pdf:pdf},
  note = {DBLP last modified: 2025-02-06},
  pdf = {https://openreview.net/pdf?id=DiWRG9JTWZ},
  publisher = {OpenReview.net},
  title = {{MetaCoCo}: A New Few-Shot Classification Benchmark with Spurious Correlation},
  url = {https://openreview.net/forum?id=DiWRG9JTWZ},
  year = {2024}
}

@inproceedings{zhang2024horizonfree,
  abstract = {A recent line of works showed regret bounds in reinforcement learning can be (nearly) independent of planning horizon, a.k.a. the horizon-free bounds. However, these regret bounds only apply to settings where a polynomial dependency on the size of transition model is allowed, such as tabular Markov Decision Process and linear mixture MDP. We give the first horizon-free bound for the popular linear MDP setting where the size of the transition model can be exponentially large or even uncountable. In contrast to prior works which explicitly estimate the transition model and compute the inhomogeneous value functions at different time steps, we directly estimate the value functions and confidence sets.},
  author = {Zihan Zhang and Jason D. Lee and Yuxin Chen and Simon Shaolei Du},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024horizonfree.pdf:pdf},
  note = {DBLP last modified: 2024-08-08},
  pdf = {https://openreview.net/pdf?id=SdBApv9iT4},
  publisher = {OpenReview.net},
  title = {Horizon-Free Regret for Linear {M}arkov Decision Processes},
  url = {https://openreview.net/forum?id=SdBApv9iT4},
  year = {2024}
}

@inproceedings{zhang2024neural,
  abstract = {The learning-based video compression method has made significant progress in recent years, exhibiting promising compression performance compared with traditional video codecs. However, prior works have primarily focused on advanced compression architectures while neglecting the rate control technique. Rate control can precisely control the coding bitrate with optimal compression performance, which is a critical technique in practical deployment. To address this issue, we present a fully neural network-based rate control system for learned video compression methods. Our system accurately encodes videos at a given bitrate while enhancing the rate-distortion performance.},
  author = {Yiwei Zhang and Guo Lu and Yunuo Chen and Shen Wang and Yibo Shi and Jing Wang and Li Song},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024neural.pdf:pdf},
  note = {DBLP last modified: 2025-05-27},
  pdf = {https://openreview.net/pdf?id=42lcaojZug},
  publisher = {OpenReview.net},
  title = {Neural Rate Control for Learned Video Compression},
  url = {https://openreview.net/forum?id=42lcaojZug},
  year = {2024}
}

@inproceedings{zhang2024seal,
  abstract = {Real-world Super-Resolution methods focus on dealing with diverse real-world images and have attracted increasing attention in recent years. The key idea is to use a complex and high-order degradation model to mimic real-world degradations. Although they have achieved impressive results in various scenarios, they are faced with the obstacle of evaluation. Currently, these methods are only assessed by their average performance on a small set of degradation cases randomly selected from a large space, which fails to provide a comprehensive understanding of their overall performance and often yields inconsistent and potentially misleading results. To overcome the limitation in evaluation, we propose SEAL, a framework for systematic evaluation of real-SR.},
  author = {Wenlong Zhang and Xiaohui Li and Xiangyu Chen and Xiaoyun Zhang and Yu Qiao and Xiao-Ming Wu and Chao Dong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024seal.pdf:pdf},
  note = {DBLP last modified: 2024-09-17},
  pdf = {https://openreview.net/pdf?id=CGlczSBBSj},
  publisher = {OpenReview.net},
  title = {{SEAL}: A Framework for Systematic Evaluation of Real-World Super-Resolution},
  url = {https://openreview.net/forum?id=CGlczSBBSj},
  year = {2024}
}

@inproceedings{zhang2024language,
  abstract = {Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and generalization to novel tasks. Recent advances with architectures have allowed for improved scaling along one or two of these axes, but are still computationally prohibitive to use. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions, as a step towards generalist agents. Comparing LCD with other state-of-the-art models on the CALVIN language benchmark finds that LCD outperforms other SOTA methods in multi-task success rates, whilst improving inference speed over other comparable diffusion models by 3.3x~15x. We show that LCD can successfully leverage the unique strength of diffusion models to produce coherent long range plans while addressing their weakness in generating low-level details and control.},
  author = {Edwin Zhang and Yujie Lu and Shinda Huang and William Yang Wang and Amy Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024language.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0H6DFoZZXZ},
  publisher = {OpenReview.net},
  title = {Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks},
  url = {https://openreview.net/forum?id=0H6DFoZZXZ},
  year = {2024}
}

@inproceedings{zhang2024jointnet,
  abstract = {We introduce JointNet, a novel neural network architecture for modeling the joint distribution of images and an additional dense modality (e.g., depth maps). JointNet is extended from a pre-trained text-to-image diffusion model, where a copy of the original network is created for the new dense modality branch and is densely connected with the RGB branch. The RGB branch is locked during network fine-tuning, which enables efficient learning of the new modality distribution while maintaining the strong generalization ability of the large-scale pre-trained diffusion model. We demonstrate the effectiveness of JointNet by using RGBD diffusion as an example and through extensive experiments, showcasing its applicability in a variety of applications, including joint RGBD generation, dense depth prediction, depth-conditioned image generation, and coherent tile-based 3D panorama generation.},
  author = {Jingyang Zhang and Shiwei Li and Yuanxun Lu and Tian Fang and David McKinnon and Yanghai Tsin and Long Quan and Yao Yao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024jointnet.pdf:pdf},
  note = {DBLP last modified: 2025-06-16},
  pdf = {https://openreview.net/pdf?id=kv5xE1p3jz},
  publisher = {OpenReview.net},
  title = {JointNet: Extending Text-to-Image Diffusion for Dense Distribution Modeling},
  url = {https://openreview.net/forum?id=kv5xE1p3jz},
  year = {2024}
}

@inproceedings{zhang2024stochastic,
  abstract = {Dropout is a widely utilized regularization technique in the training of neural networks, nevertheless, its underlying mechanism and impact on achieving good generalization abilities remain to be further understood. In this work, we start by undertaking a rigorous theoretical derivation of the stochastic modified equations, with the primary aim of providing an effective approximation for the discrete iterative process of dropout. Meanwhile, we experimentally verify SDE's ability to approximate dropout under a wider range of settings. Subsequently, we empirically delve into the intricate mechanisms by which dropout facilitates the identification of flatter minima. This exploration is conducted through intuitive approximations, exploiting the structural analogies inherent in the Hessian of loss landscape and the covariance of dropout. Our empirical findings substantiate the ubiquitous presence of the Hessian-variance alignment relation throughout the training process of dropout.},
  author = {Zhongwang Zhang and Yuqing Li and Tao Luo and Zhi-Qin John Xu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024stochastic.pdf:pdf},
  note = {DBLP last modified: 2024-08-13},
  pdf = {https://openreview.net/pdf?id=Bpkhu2ExxU},
  publisher = {OpenReview.net},
  title = {Stochastic Modified Equations and Dynamics of Dropout Algorithm},
  url = {https://openreview.net/forum?id=Bpkhu2ExxU},
  year = {2024}
}

@inproceedings{zhang2024learning,
  abstract = {We present a method for 3D reconstruction of moving articulated objects from monocular videos without additional information about object structure. Our approach treats an articulated object as an unknown, semi-rigid skeletal structure surrounded by nonrigid material. We simultaneously estimate the visible representation (3D shapes, colors, camera parameters) and the implicit skeletal representation from motion cues in the object video without 3D supervision. The implicit representation consists of four components: skeleton, skinning weights, rigidity coefficients, and time-varying transformations. We introduce an algorithm that uses physical constraints as regularization terms and iteratively estimates both implicit and explicit representations. Our approach is category-agnostic, thus eliminating the need for category-specific skeletons, and we demonstrate that it outperforms state-of-the-art across standard video datasets.},
  author = {Hao Zhang and Fang Li and Samyak Rawlekar and Narendra Ahuja},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024learning.pdf:pdf},
  note = {DBLP last modified: 2025-06-25},
  pdf = {https://openreview.net/pdf?id=KQ2i6jazVK},
  publisher = {OpenReview.net},
  title = {Learning Implicit Representation for Reconstructing Articulated Objects},
  url = {https://openreview.net/forum?id=KQ2i6jazVK},
  year = {2024}
}

@inproceedings{zhang2024omni,
  abstract = {Open-ended algorithms aim to learn new, interesting behaviors forever. That requires a vast environment search space, but there are thus infinitely many possible tasks. Even after filtering for tasks the current agent can learn (i.e., learning progress), countless learnable yet uninteresting tasks remain (e.g., minor variations of previously learned tasks). An Achilles Heel of open-endedness research is the inability to quantify (and thus prioritize) tasks that are not just learnable, but also interesting (e.g., worthwhile and novel). We propose solving this problem by Open-endedness via Models of human Notions of Interestingness (OMNI). The insight is that we can utilize foundation models (FMs) as a model of interestingness (MoI), because they already internalize human concepts of interestingness from training on vast amounts of human-generated data, where humans naturally write about what they find interesting or boring. We show that FM-based MoIs improve open-ended learning by focusing on tasks that are both learnable and interesting, outperforming baselines based on uniform task sampling or learning progress alone.},
  author = {Jenny Zhang and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024omni.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AgM3MzT99c},
  publisher = {OpenReview.net},
  title = {OMNI: Open-endedness via Models of human Notions of Interestingness},
  url = {https://openreview.net/forum?id=AgM3MzT99c},
  year = {2024}
}

@inproceedings{zhang2024enhancing,
  abstract = {Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities.},
  author = {Xinlu Zhang and Shiyang Li and Xianjun Yang and Chenxin Tian and Yao Qin and Linda Ruth Petzold},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024enhancing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ztpy1gsUpT},
  publisher = {OpenReview.net},
  title = {Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting},
  url = {https://openreview.net/forum?id=ztpy1gsUpT},
  year = {2024}
}

@inproceedings{zhang2024learning,
  abstract = {In this paper, we investigate a problem of actively learning threshold in latent space, where the unknown reward g(Œ≥, v) depends on the proposed threshold Œ≥ and latent value v and it can be only achieved if the threshold is lower than or equal to the unknown latent value. This problem has broad applications in practical scenarios, e.g., reserve price optimization in online auctions, online task assignments in crowdsourcing, setting recruiting bars in hiring, etc. We first characterize the query complexity of learning a threshold with the expected reward at most Œµ smaller than the optimum and prove that the number of queries needed can be infinitely large even when g(Œ≥, v) is monotone with respect to both Œ≥ and v. On the positive side, we provide a tight query complexity √ï(1/Œµ¬≥) when g is monotone and the CDF of value distribution is Lipschitz.},
  author = {Jiahao Zhang and Tao Lin and Weiqiang Zheng and Zhe Feng and Yifeng Teng and Xiaotie Deng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-13},
  pdf = {https://openreview.net/pdf?id=qaKRfobbTg},
  publisher = {OpenReview.net},
  title = {Learning Thresholds with Latent Values and Censored Feedback},
  url = {https://openreview.net/forum?id=qaKRfobbTg},
  year = {2024}
}

@inproceedings{zhang2024continual,
  abstract = {We propose and study a realistic continual learning (CL) setting where learning algorithms are granted a restricted computational budget per time step while training, applied to large-scale semi-supervised continual learning scenarios with sparse label rates. Previous proficient CL methods perform very poorly in this challenging setting, with overfitting to the sparse labeled data and insufficient computational budget being the two main culprits for such poor performance. The new setting encourages learning methods to effectively and efficiently utilize the unlabeled data during training. To address this, we propose a simple but highly effective baseline called DietCL, which utilizes both unlabeled and labeled data jointly and meticulously allocates computational budget for both types of data. We validate our baseline at scale on several datasets including CLOC, ImageNet10K, and CGLM under constrained budget setups. DietCL outperforms by a large margin all existing supervised CL algorithms as well as more recent continual semi-supervised methods.},
  author = {Wenxuan Zhang and Youssef Mohamed and Bernard Ghanem and Philip H. S. Torr and Adel Bibi and Mohamed Elhoseiny},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024continual.pdf:pdf},
  note = {DBLP last modified: 2025-02-24},
  pdf = {https://openreview.net/pdf?id=Xvfz8NHmCj},
  publisher = {OpenReview.net},
  title = {Continual Learning on a Diet: Learning from Sparsely Labeled Streams Under Constrained Computation},
  url = {https://openreview.net/forum?id=Xvfz8NHmCj},
  year = {2024}
}

@inproceedings{zhang2024towards,
  abstract = {Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task, but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. We provide conceptual arguments for why certain metrics or methods may be preferred, backed by empirical observations.},
  author = {Fred Zhang and Neel Nanda},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Hf17y6u9BC},
  publisher = {OpenReview.net},
  title = {Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  url = {https://openreview.net/forum?id=Hf17y6u9BC},
  year = {2024}
}

@inproceedings{zhang2024towards,
  abstract = {Branch-and-bound (B\&B) has long been favored for tackling complex Mixed Integer Programming (MIP) problems, where the choice of branching strategy plays a pivotal role. Recently, Imitation Learning (IL)-based policies have emerged as potent alternatives to traditional rule-based approaches. However, it is nontrivial to acquire high-quality training samples, and IL often converges to suboptimal variable choices for branching, restricting the overall performance. In response to these challenges, we propose a novel hybrid online and offline reinforcement learning (RL) approach to enhance the branching policy by cost-effective training sample augmentation. In the online phase, we train an online RL agent to dynamically decide the sample generation processes, drawing from either the learning-based policy or the expert policy. The objective is to strike a balance between exploration and exploitation of the sample generation process.},
  author = {Changwen Zhang and Wenli Ouyang and Hao Yuan and Liming Gong and Yong Sun and Ziao Guo and Zhichen Dong and Junchi Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024towards.pdf:pdf},
  note = {DBLP last modified: 2025-04-24},
  pdf = {https://openreview.net/pdf?id=NdcQQ82mfy},
  publisher = {OpenReview.net},
  title = {Towards Imitation Learning to Branch for MIP: A Hybrid Reinforcement Learning based Sample Augmentation Approach},
  url = {https://openreview.net/forum?id=NdcQQ82mfy},
  year = {2024}
}

@inproceedings{zhang2024deep,
  abstract = {Many well-known and effective anomaly detection methods assume that a reasonable decision boundary has a hypersphere shape, which however is difficult to obtain in practice and is not sufficiently compact, especially when the data are in high-dimensional spaces. In this paper, we first propose a novel deep anomaly detection model that improves the original hypersphere learning through an orthogonal projection layer, which ensures that the training data distribution is consistent with the hypersphere hypothesis, thereby increasing the true positive rate and decreasing the false negative rate. Moreover, we propose a bi-hypersphere compression method to obtain a hyperspherical shell that yields a more compact decision region than a hyperball, which is demonstrated theoretically and numerically. The proposed methods are not confined to common datasets such as image and tabular data, but are also extended to a more challenging but promising scenario, graph-level anomaly detection, which learns graph representation with maximum mutual information between the substructure and global structure features while exploring orthogonal single- or bi-hypersphere anomaly decision boundaries. The numerical and visualization results on benchmark datasets demonstrate the superiority of our methods in comparison to many baselines and state-of-the-art methods.},
  author = {Yunhe Zhang and Yan Sun and Jinyu Cai and Jicong Fan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024deep.pdf:pdf},
  note = {Spotlight presentation. DBLP last modified: 2025-02-11},
  pdf = {https://openreview.net/pdf/b8052c3c7a3cfeccc963ae2d0d24045831b4d84e.pdf},
  publisher = {OpenReview.net},
  title = {Deep Orthogonal Hypersphere Compression for Anomaly Detection},
  url = {https://openreview.net/forum?id=cJs4oE4m9Q},
  year = {2024}
}

@inproceedings{zhang2024tell,
  abstract = {In human-written articles, we often leverage the subtleties of text style, such as bold and italics, to guide the attention of readers. These textual emphases are vital for the readers to grasp the conveyed information. When interacting with large language models ({LLM}s), we have a similar need -- steering the model to pay closer attention to user-specified information, e.g., an instruction. Existing methods, however, are constrained to process plain text and do not support such a mechanism. This motivates us to introduce {PASTA} -- Post-hoc Attention {STeering} Approach, a method that allows {LLM}s to read text with user-specified emphasis marks. To this end, {PASTA} identifies a small subset of attention heads and applies precise attention reweighting on them, directing the model attention to user-specified parts. Like prompting, {PASTA} is applied at inference time and does not require changing any model parameters. Experiments demonstrate that {PASTA} can substantially enhance an {LLM}'s ability to follow user instructions or integrate new knowledge from user inputs, leading to a significant performance improvement on a variety of tasks, e.g., an average accuracy improvement of 22\% for {LLAMA}-7B. Our code is publicly available at https://github.com/QingruZhang/PASTA.},
  author = {Qingru Zhang and Chandan Singh and Liyuan Liu and Xiaodong Liu and Bin Yu and Jianfeng Gao and Tuo Zhao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024tell.pdf:pdf},
  note = {DBLP last modified: 2024-08-14},
  pdf = {https://openreview.net/pdf/04523a36c3572f2d286402a814eb2ec77d67cc25.pdf},
  publisher = {OpenReview.net},
  title = {Tell Your Model Where to Attend: Post-hoc Attention Steering for {LLM}s},
  url = {https://openreview.net/forum?id=xZDWO0oejD},
  year = {2024}
}

@inproceedings{zhang2024detecting,
  abstract = {Large language models ({LLM}s) such as {ChatGPT} have exhibited remarkable performance in generating human-like texts. However, machine-generated texts ({MGT}s) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect {MGT}s in many situations. Unfortunately, it is challenging to distinguish {MGT}s and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of {LLM}s. In this paper, we seek to exploit maximum mean discrepancy ({MMD}) to address this issue in the sense that {MMD} can well identify distributional discrepancies. However, directly training a detector with {MMD} using diverse {MGT}s will incur a significantly increased variance of {MMD} since {MGT}s may contain multiple text populations due to various {LLM}s. This will severely impair {MMD}'s ability to measure the difference between two samples. To tackle this, we propose a novel multi-population aware optimization method for {MMD} called {MMD-MP}, which can avoid variance increases and thus improve the stability to measure the distributional discrepancy. Relying on {MMD-MP}, we develop two methods for paragraph-based and sentence-based detection, respectively. Extensive experiments on various {LLM}s, e.g., {GPT2} and {ChatGPT}, show superior detection performance of our {MMD-MP}.},
  author = {Shuhai Zhang and Yiliao Song and Jiahao Yang and Yuanqing Li and Bo Han and Mingkui Tan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024detecting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3fEKavFsnv},
  publisher = {OpenReview.net},
  title = {Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy},
  url = {https://openreview.net/forum?id=3fEKavFsnv},
  year = {2024}
}

@inproceedings{zhang2024connect,
  abstract = {Recent works have shown that leveraging a pre-trained multi-modal contrastive representation space enables cross-modal tasks to be learned with uni-modal data, based on the assumption that contrastive optimization makes embeddings from different modalities interchangeable. However, this assumption is under-explored due to the poorly understood geometry of the multi-modal contrastive space, where a modality gap exists. In this work, we provide a theoretical explanation for the modality gap and analyze the geometry of multi-modal contrastive representation space. Based on our analysis, we then propose {C}$^3$ -- Connect, Collapse, Corrupt, a simple method to bridge the modality gap. {C}$^3$ enhances the interchangeability of embeddings from different modalities in the multi-modal contrastive space, and thus enables cross-modal tasks to be learned with uni-modal data. We validate our method on image captioning, text-to-image generation, audio captioning, and other cross-modal tasks. Our method achieves state-of-the-art performance in zero-shot settings and demonstrates consistent improvements across different modalities, datasets, and embedding spaces.},
  author = {Yuhui Zhang and Elaine Sui and Serena Yeung},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024connect.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ttXg3SKAg5},
  publisher = {OpenReview.net},
  title = {Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data},
  url = {https://openreview.net/forum?id=ttXg3SKAg5},
  year = {2024}
}

@inproceedings{zhang2024flow,
  abstract = {Offline preference-based reinforcement learning ({PbRL}) offers an effective solution to overcome the challenges associated with designing rewards and the high costs of online interactions. In offline {PbRL}, agents are provided with a fixed dataset containing human preferences between pairs of trajectories. Previous studies mainly focus on recovering the rewards from the preferences, followed by policy optimization with an off-the-shelf offline {RL} algorithm. However, given that preference label in {PbRL} is inherently trajectory-based, accurately learning transition-wise rewards from such label can be challenging, potentially leading to misguidance during subsequent offline {RL} training. To address this issue, we introduce our method named Flow-to-Better ({FTB}), which leverages the pairwise preference relationship to guide a generative model in producing preferred trajectories, avoiding Temporal Difference ({TD}) learning with inaccurate rewards. Conditioning on a low-preference trajectory, {FTB} uses a diffusion model to generate a better one with a higher preference, achieving high-fidelity full-horizon trajectory improvement. During diffusion training, we propose a technique called Preference Augmentation to alleviate the problem of insufficient preference data. As a result, we surprisingly find that the model-generated trajectories not only exhibit increased preference and consistency with the real transition but also introduce elements of novelty and diversity, from which we can derive a desirable policy through imitation learning. Experimental results on {D4RL} benchmarks demonstrate that {FTB} achieves a remarkable improvement compared to state-of-the-art offline {PbRL} methods. Furthermore, we show that {FTB} can also serve as an effective data augmentation method for offline {RL}.},
  author = {Zhilong Zhang and Yihao Sun and Junyin Ye and Tian-Shuo Liu and Jiaji Zhang and Yang Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024flow.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=EG68RSznLT},
  publisher = {OpenReview.net},
  title = {Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation},
  url = {https://openreview.net/forum?id=EG68RSznLT},
  year = {2024}
}

@inproceedings{zhang2024sampleefficient,
  abstract = {A powerful concept behind much of the recent progress in machine learning is the extraction of common features across data from heterogeneous sources or tasks. Intuitively, using all of one's data to learn a common representation function benefits both computational effort and statistical generalization by leaving a smaller number of parameters to fine-tune on a given task. Toward theoretically grounding these merits, we propose a general setting of recovering linear operators $M$ from noisy vector measurements $y = Mx + w$, where the covariates $x$ may be both non-i.i.d. and non-isotropic. We demonstrate that existing isotropy-agnostic meta-learning approaches incur biases on the representation update, which causes the scaling of the noise terms to lose favorable dependence on the number of source tasks. This in turn can cause the sample complexity of representation learning to be bottlenecked by the single-task data size. We introduce an adaptation, De-bias \& Feature-Whiten ({DFW}), of the popular alternating minimization-descent ({AMD}) scheme proposed in Collins et al., (2021), and establish linear convergence to the optimal representation with noise level scaling down with the total source data size. This leads to generalization bounds on the same order as an oracle empirical risk minimizer. We verify the vital importance of {DFW} on various numerical simulations. In particular, we show that vanilla alternating-minimization descent fails catastrophically even for iid, but mildly non-isotropic data.},
  author = {Thomas T. C. K. Zhang and Leonardo Felipe Toso and James Anderson and Nikolai Matni},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024sampleefficient.pdf:pdf},
  note = {Spotlight presentation. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Tr3fZocrI6},
  publisher = {OpenReview.net},
  title = {Sample-Efficient Linear Representation Learning from Non-{IID} Non-Isotropic Data},
  url = {https://openreview.net/forum?id=Tr3fZocrI6},
  year = {2024}
}

@inproceedings{zhang2024private,
  abstract = {Private stochastic optimization studies the fundamental trade-off between privacy and utility in optimization problems over sensitive data. A key quantity is the dataset size required to solve the optimization problem with a given approximation guarantee while satisfying differential privacy. Most prior works on this topic consider convex and smooth loss functions, and a handful of works have given suboptimal algorithms for convex but nonsmooth functions. In this work, we take a significant step beyond existing work by giving the first nontrivial algorithm for zeroth-order stochastic optimization on nonconvex and nonsmooth objectives. Our algorithm satisfies ($\epsilon$, $\delta$)-differential privacy and can find an $(\epsilon, \epsilon^2/2)$-second-order stationary point with probability at least $1-\beta$ using $\tilde{O}\left(\frac{d^2}{\epsilon^4} + \frac{d^2}{\beta \epsilon^{3/2}}\right)$ samples from the objective function, where $d$ is the problem dimension. Our algorithm builds on the Online-to-Nonconvex Conversion (O2NC) framework, which converts any online convex optimization algorithm with $O(\sqrt{T})$ regret into a nonsmooth nonconvex optimization algorithm.},
  author = {Qinzi Zhang and Hoang Tran and Ashok Cutkosky},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024private.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IzqZbNMZ0M},
  publisher = {OpenReview.net},
  title = {Private Zeroth-Order Nonsmooth Nonconvex Optimization},
  url = {https://openreview.net/forum?id=IzqZbNMZ0M},
  year = {2024}
}

@inproceedings{zhang2024retsim,
  abstract = {Comprehensive text similarity measures are crucial for many downstream tasks including clustering, retrieval, and data deduplication. Prior approaches to text similarity fall into two camps: costly but accurate neural approaches and efficient but inaccurate non-neural approaches (e.g. {MinHash}). We bridge the gap between accuracy and efficiency by introducing {RETSim} (Resilient and Efficient Text Similarity), a neural text similarity model that is both highly efficient and resilient to a wide range of lexical, syntactic and semantic variations. {RETSim} is trained using a multi-stage training pipeline that initially trains the model on large amounts of unlabeled text pairs, followed by refinement stages using hard negatives and adversarial examples to improve robustness. We demonstrate that {RETSim} achieves better performance than existing text similarity approaches while being significantly more efficient, achieving an order of magnitude speedup over traditional neural approaches. Furthermore, we show that {RETSim} generalizes well across different domains and languages. We also introduce the {W}iki-40{B} 4dversarial {N}ear-{T}3xt {D}ataset ({W4NT3D}), a large-scale adversarial benchmark for evaluating text similarity models. Our experiments demonstrate that {RETSim} achieves state-of-the-art performance on {W4NT3D} and other text similarity benchmarks while maintaining high efficiency.},
  author = {Marina Zhang and Owen S. Vallis and Aysegul Bumin and Tanay Vakharia and Elie Bursztein},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024retsim.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=23b9KSNQTX},
  publisher = {OpenReview.net},
  title = {{RETSim}: Resilient and Efficient Text Similarity},
  url = {https://openreview.net/forum?id=23b9KSNQTX},
  year = {2024}
}

@inproceedings{zhang2024selfsupervised,
  abstract = {Merging multi-exposure images is a common approach for obtaining high dynamic range ({HDR}) images, with the primary challenge being the avoidance of ghosting artifacts in dynamic scenes. However, the methods typically rely on sufficient data with {HDR} ground-truths, which are difficult and costly to collect. In this work, to eliminate the need for labeled data, we propose {SelfHDR}, a self-supervised {HDR} reconstruction method that only requires dynamic multi-exposure images during training. Specifically, {SelfHDR} learns a reconstruction network under the supervision of two complementary components, which can be constructed from multi-exposure images and focus on {HDR} color as well as structure, respectively. To this end, we introduce an {HDR} color loss that enforces the model to learn reasonable color mappings by exploring the relations among different exposures. Meanwhile, we develop a structure consistency loss that encourages structural consistencies by leveraging the image statistics. The two losses jointly facilitate the self-supervised learning of {HDR} reconstruction in dynamic scenes. Experiments on real-world images demonstrate that {SelfHDR} achieves superior results against the state-of-the-art self-supervised methods, and comparable performance to supervised ones.},
  author = {Zhilu Zhang and Haoyu Wang and Shuai Liu and Xiaotao Wang and Lei Lei and Wangmeng Zuo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024selfsupervised.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=jjiOHEcS2c},
  publisher = {OpenReview.net},
  title = {Self-Supervised High Dynamic Range Imaging with Multi-Exposure Images in Dynamic Scenes},
  url = {https://openreview.net/forum?id=jjiOHEcS2c},
  year = {2024}
}

@inproceedings{zhang2024graph,
  abstract = {Graph Neural Networks ({GNN}s) have emerged as the leading deep learning models for graph-based representation learning. However, the training and inference of {GNN}s on large graphs remain resource-intensive, impeding their utility in real-world scenarios and curtailing their applicability in deeper and more sophisticated {GNN} architectures. To address this issue, the Graph Lottery Ticket ({GLT}) hypothesis assumes that {GNN} with random initialization harbors a pair of core subgraph and sparse subnetwork, which can yield comparable performance and higher efficiency to that of the original dense network and complete graph. Despite the promise that {GLT} shows, current {GLT} identification methods suffer two major limitations: 1) they focus exclusively on the edge sparsity while overlooking subnetwork sparsity, resulting in the suboptimal performance for diverse {GNN} architectures; 2) worse still, current methods suffer scalability issues when applied to deep {GNN}s, as they maintain the same topology structure across all layers. These challenges hinder the integration of {GLT} into deeper and larger-scale {GNN} contexts. To bridge this critical gap, this paper introduces an Adaptive, Dynamic, and Automated framework for identifying Graph Lottery Tickets ({AdaGLT}). Our proposed method derives its key advantages and addresses the above limitations through the following three aspects: 1) tailoring layer-adaptive sparse structures for various datasets and {GNN}s, thus endowing it with the capability to facilitate deeper {GNN}s; 2) integrating the pruning and training processes, thereby achieving a dynamic workflow encompassing both pruning and restoration; 3) automatically capturing graph lottery tickets across diverse sparsity levels, obviating the necessity for extensive pruning parameter tuning. Extensive experiments demonstrate that our {AdaGLT} outperforms state-of-the-art baselines on diverse graph learning tasks while significantly reducing computational overhead.},
  author = {Guibin Zhang and Kun Wang and Wei Huang and Yanwei Yue and Yang Wang and Roger Zimmermann and Aojun Zhou and Dawei Cheng and Jin Zeng and Yuxuan Liang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024graph.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=nmBjBZoySX},
  publisher = {OpenReview.net},
  title = {Graph Lottery Ticket Automated},
  url = {https://openreview.net/forum?id=nmBjBZoySX},
  year = {2024}
}

@inproceedings{zhang2024whittle,
  abstract = {The paper extends the Whittle index to a multi-agent reinforcement learning ({MARL}) setting for inventory management, proposing {WIMS} ({Whittle} {Index} with {Multiple} actions and {State} constraint). It allows agents to choose replenishing quantity levels for stock-keeping units while maximizing total profit under inventory constraints.},
  author = {Chuheng Zhang and Xiangsen Wang and Wei Jiang and Xianliang Yang and Siwei Wang and Lei Song and Jiang Bian},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024whittle.pdf:pdf},
  note = {DBLP last modified: 2025-06-25},
  pdf = {https://openreview.net/pdf?id=5sixirvG0I},
  publisher = {OpenReview.net},
  title = {{Whittle} {Index} with {Multiple} {Actions} and {State} {Constraint} for {Inventory} {Management}},
  url = {https://openreview.net/forum?id=5sixirvG0I},
  year = {2024}
}

@inproceedings{zhang2024mintrec20,
  abstract = {The paper introduces {MIntRec2.0}, a large-scale dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples across text, video, and audio modalities, featuring 30 fine-grained intent classes and over 5,700 out-of-scope samples.},
  author = {Hanlei Zhang and Xin Wang and Hua Xu and Qianrui Zhou and Kai Gao and Jianhua Su and Jinyue Zhao and Wenrui Li and Yanting Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024mintrec20.pdf:pdf},
  note = {DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=nY9nITZQjc},
  publisher = {OpenReview.net},
  title = {{MIntRec2.0}: {A} {Large-scale} {Benchmark} {Dataset} for {Multimodal} {Intent} {Recognition} and {Out-of-scope} {Detection} in {Conversations}},
  url = {https://openreview.net/forum?id=nY9nITZQjc},
  year = {2024}
}

@inproceedings{zhang2024dissecting,
  abstract = {Finetuning language models on domain-specific corpus is a common approach to enhance their domain knowledge and capability. While improving performance on domain tasks, it often brings a side-effect of forgetting of the model's general abilities. In this study, we analyze the effects of finetuning on language models by dissecting its impacts on the modeling of topic, style, and factual knowledge in text. Our method uses instruction-following {LLMs} such as {ChatGPT} to auto-generate controlled-variable text examples which we use to probe the model. Our findings reveal that finetuning results in significant shifts in the language model's topic and style priors, while actual knowledge learning only contributes to a small fraction of the total probability change.},
  author = {Xiao Zhang and Ji Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024dissecting.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=tmsqb6WpLz},
  publisher = {OpenReview.net},
  title = {Dissecting learning and forgetting in language model finetuning},
  url = {https://openreview.net/forum?id=tmsqb6WpLz},
  year = {2024}
}

@inproceedings{zhang2024synergistic,
  abstract = {The {Vision} {Transformer} ({ViT}) has emerged as a powerful architecture for various computer vision tasks. Nonetheless, this comes with substantially heavier computational costs than {Convolutional} {Neural} {Networks} ({CNNs}). The attention mechanism in {ViTs}, which integrates information from different image patches to the class token ([{CLS}]), renders traditional structured pruning methods used in {CNNs} unsuitable. To overcome this issue, we propose {SynergisTic} p{A}tch p{R}uning ({STAR}) that unifies intra-layer and inter-layer patch importance scoring. Specifically, our approach combines a) online evaluation of intra-layer importance for the [{CLS}] and b) offline evaluation of the inter-layer importance of each patch. The two importance scores are fused by minimizing a weighted average of {Kullback-Leibler} ({KL}) {Divergences} and patches are successively pruned at each layer by maintaining only the top-k most important ones.},
  author = {Yuyao Zhang and Lan Wei and Nikolaos M. Freris},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024synergistic.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=COO51g41Q4},
  publisher = {OpenReview.net},
  title = {Synergistic {Patch} {Pruning} for {Vision} {Transformer}: {Unifying} {Intra}- \& {Inter}-{Layer} {Patch} {Importance}},
  url = {https://openreview.net/forum?id=COO51g41Q4},
  year = {2024}
}

@inproceedings{zhang2024copilot4d,
  abstract = {Learning world models can teach an agent how the world works in an unsupervised manner. Even though it can be viewed as a special case of sequence modeling, progress for scaling world models on robotic applications such as autonomous driving has been somewhat less rapid than scaling language models with {Generative} {Pre-trained} {Transformers} ({GPT}). We identify two reasons as major bottlenecks: dealing with complex and unstructured observation space, and having a scalable generative model. Consequently, we propose {Copilot4D}, a novel world modeling approach that first tokenizes sensor observations with {VQVAE}, then predicts the future via discrete diffusion. To efficiently decode and denoise tokens in parallel, we recast {Masked} {Generative} {Image} {Transformer} as discrete diffusion and enhance it with a few simple changes. When applied to learning world models on point cloud observations, {Copilot4D} reduces prior {SOTA} {Chamfer} distance by more than 65\% for 1s prediction, and more than 50\% for 3s prediction, across {NuScenes}, {KITTI} {Odometry}, and {Argoverse2} datasets.},
  author = {Lunjun Zhang and Yuwen Xiong and Ze Yang and Sergio Casas and Rui Hu and Raquel Urtasun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024copilot4d.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Psl75UCoZM},
  publisher = {OpenReview.net},
  title = {{Copilot4D}: {Learning} {Unsupervised} {World} {Models} for {Autonomous} {Driving} via {Discrete} {Diffusion}},
  url = {https://openreview.net/forum?id=Psl75UCoZM},
  year = {2024}
}

@inproceedings{zhang2024prototypical,
  abstract = {Multimodal learning significantly benefits cancer survival prediction, especially the integration of pathological images and genomic data. Despite advantages of multimodal learning for cancer survival prediction, massive redundancy in multimodal data prevents it from extracting discriminative and compact information: (1) An extensive amount of intra-modal task-unrelated information blurs discriminability, especially for gigapixel whole slide images ({WSIs}) with many patches in pathology and thousands of pathways in genomic data, leading to an ``intra-modal redundancy'' issue. (2) Duplicated information among modalities dominates the representation of multimodal data, which makes modality-specific information prone to being ignored, resulting in an ``inter-modal redundancy'' issue. To address these, we propose a new framework, {Prototypical} {Information} {Bottlenecking} and {Disentangling} ({PIBD}), consisting of {Prototypical} {Information} {Bottleneck} ({PIB}) module for intra-modal redundancy and {Prototypical} {Information} {Disentanglement} ({PID}) module for inter-modal redundancy.},
  author = {Yilan Zhang and Yingxue Xu and Jianqi Chen and Fengying Xie and Hao Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024prototypical.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=otHZ8JAIgh},
  publisher = {OpenReview.net},
  title = {Prototypical {Information} {Bottlenecking} and {Disentangling} for {Multimodal} {Cancer} {Survival} {Prediction}},
  url = {https://openreview.net/forum?id=otHZ8JAIgh},
  year = {2024}
}

@inproceedings{zhang2024thresholdconsistent,
  abstract = {Existing losses used in deep metric learning ({DML}) for image retrieval often lead to highly non-uniform intra-class and inter-class representation structures across test classes and data distributions. When combined with the common practice of using a fixed threshold to declare a match, this gives rise to significant performance variations in terms of false accept rate ({FAR}) and false reject rate ({FRR}) across test classes and data distributions. We define this issue in {DML} as threshold inconsistency. The paper quantifies the notion of threshold inconsistency in deep metric learning through a novel variance-based metric, and introduces a simple yet effective regularization loss to improve threshold consistency for {DML} models. To measure this inconsistency, the authors propose a novel variance-based metric called {Operating-Point-Inconsistency-Score} ({OPIS}) that quantifies the variance in the operating characteristics.},
  author = {Qin Zhang and Linghan Xu and Jun Fang and Qingming Tang and Ying Nian Wu and Joseph Tighe and Yifan Xing},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024thresholdconsistent.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vE5MyzpP92},
  publisher = {OpenReview.net},
  title = {Threshold-{Consistent} {Margin} {Loss} for {Open-World} {Deep} {Metric} {Learning}},
  url = {https://openreview.net/forum?id=vE5MyzpP92},
  year = {2024}
}

@inproceedings{zhang2024ideal,
  abstract = {In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced.},
  author = {Shaokun Zhang and Xiaobo Xia and Zhaoqing Wang and Ling-Hao Chen and Jiale Liu and Qingyun Wu and Tongliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024ideal.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Spp2i1hKwV},
  publisher = {OpenReview.net},
  title = {{IDEAL}: {Influence-Driven} {Selective} {Annotations} {Empower} {In-Context} {Learners} in {Large} {Language} {Models}},
  url = {https://openreview.net/forum?id=Spp2i1hKwV},
  year = {2024}
}

@inproceedings{zhang2024robust,
  abstract = {Federated semi-supervised learning ({FSSL}) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency. Advanced {FSSL} methods predominantly focus on training a single model on each client. However, this approach could lead to a discrepancy between the objective functions of labeled and unlabeled data, resulting in gradient conflicts. To alleviate gradient conflict, we propose a novel twin-model paradigm, called {Twinsight}, designed to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data. In particular, {Twinsight} concurrently trains a supervised model with a supervised objective function while training an unsupervised model using an unsupervised objective function. To enhance the synergy between these two models, {Twinsight} introduces a neighborhood-preserving constraint, which encourages the preservation of the neighborhood relationship among data features.},
  author = {Yonggang Zhang and Zhiqin Yang and Xinmei Tian and Nannan Wang and Tongliang Liu and Bo Han},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024robust.pdf:pdf},
  note = {DBLP last modified: 2025-01-29},
  pdf = {https://openreview.net/pdf?id=qxLVaYbsSI},
  publisher = {OpenReview.net},
  title = {Robust {Training} of {Federated} {Models} with {Extremely} {Label} {Deficiency}},
  url = {https://openreview.net/forum?id=qxLVaYbsSI},
  year = {2024}
}

@inproceedings{zhang2024finitestate,
  abstract = {Learned lossless data compression has garnered significant attention recently due to its superior compression ratios compared to traditional compressors. However, the computational efficiency of these models jeopardizes their practicality. This paper proposes a novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression. Our approach incorporates two essential innovations. First, we propose the {Finite-State} {AutoRegressive} ({FSAR}) entropy coder, an efficient autoregressive {Markov} model based entropy coder that utilizes a lookup table to expedite autoregressive entropy coding. Next, we present a {Straight-Through} {Hardmax} {Quantization} ({STHQ}) scheme to enhance the optimization of discrete latent space. Our experiments show that the proposed lossless compression method could improve the compression ratio by up to 6\% compared to the baseline, with negligible extra computational time.},
  author = {Yufeng Zhang and Hang Yu and Jianguo Li and Weiyao Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024finitestate.pdf:pdf},
  note = {DBLP last modified: 2024-08-06},
  pdf = {https://openreview.net/pdf?id=D5mJSNtUtv},
  publisher = {OpenReview.net},
  title = {Finite-{State} {Autoregressive} {Entropy} {Coding} for {Efficient} {Learned} {Lossless} {Compression}},
  url = {https://openreview.net/forum?id=D5mJSNtUtv},
  year = {2024}
}

@inproceedings{zhang2024path,
  abstract = {Rigorousness and clarity are both essential for interpretations of DNNs to engender human trust. Path methods are commonly employed to generate rigorous attributions that satisfy three axioms. However, the meaning of attributions remains ambiguous due to distinct path choices.},
  author = {Borui Zhang and Wenzhao Zheng and Jie Zhou and Jiwen Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024path.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=gzYgsZgwXa},
  publisher = {OpenReview.net},
  title = {Path Choice Matters for Clear Attributions in Path Methods},
  url = {https://openreview.net/forum?id=gzYgsZgwXa},
  year = {2024}
}

@inproceedings{zhang2024x,
  abstract = {In this paper, we present a hybrid X-shaped vision Transformer, named Xformer, which performs notably on image denoising tasks. We explore strengthening the global representation of tokens from different scopes. In detail, we adopt two types of Transformer blocks: The spatial-wise Transformer block performs fine-grained local patches interactions across tokens defined by spatial dimension. The channel-wise Transformer block performs direct global context interactions across tokens defined by channel dimension. Based on the concurrent network structure, we design two branches to conduct these two interaction fashions. Within each branch, we employ an encoder-decoder architecture to capture multi-scale features. Besides, we propose the Bidirectional Connection Unit (BCU) to couple the learned representations from these two branches while providing enhanced information fusion.},
  author = {Jiale Zhang and Yulun Zhang and Jinjin Gu and Jiahua Dong and Linghe Kong and Xiaokang Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024x.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=vXrIQLzIKY},
  publisher = {OpenReview.net},
  title = {{X}former: Hybrid {X}-Shaped Transformer for Image Denoising},
  url = {https://openreview.net/forum?id=vXrIQLzIKY},
  year = {2024}
}

@inproceedings{zhang2024s,
  abstract = {Current speech large language models build upon discrete speech representations, which can be categorized into semantic tokens and acoustic tokens. However, existing speech tokens are not specifically designed for speech language modeling. To assess the suitability of speech tokens for building speech language models, we established the first benchmark, SLMTokBench. Our results indicate that neither semantic nor acoustic tokens are ideal for this purpose. Therefore, we propose SpeechTokenizer, a unified speech tokenizer for speech large language models. SpeechTokenizer adopts the Encoder-Decoder architecture with residual vector quantization (RVQ). Unifying semantic and acoustic tokens, SpeechTokenizer disentangles different aspects of speech information hierarchically across different RVQ layers.},
  author = {Xin Zhang and Dong Zhang and Shimin Li and Yaqian Zhou and Xipeng Qiu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024s.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=AF9Q8Vip84},
  publisher = {OpenReview.net},
  title = {{S}peech{T}okenizer: Unified Speech Tokenizer for Speech Language Models},
  url = {https://openreview.net/forum?id=AF9Q8Vip84},
  year = {2024}
}

@inproceedings{zhang2024longtailed,
  abstract = {Diffusion models struggle with long-tailed data distributions. We propose a weighted denoising score-matching technique for knowledge transfer directly from head to tail classes using a Bayesian framework and a gating mechanism.},
  author = {Tianjiao Zhang and Huangjie Zheng and Jiangchao Yao and Xiangfeng Wang and Mingyuan Zhou and Ya Zhang and Yanfeng Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhang2024longtailed.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=NW2s5XXwXU},
  publisher = {OpenReview.net},
  title = {Long-tailed Diffusion Models with Oriented Calibration},
  url = {https://openreview.net/forum?id=NW2s5XXwXU},
  year = {2024}
}

@inproceedings{zhao2024w,
  abstract = {Chatbots such as GPT-4 and ChatGPT are now serving millions of users. Despite their widespread use, there remains a lack of public datasets showcasing how these tools are used by a population of users in practice. To bridge this gap, we present WildChat, a collection of 1 million user-ChatGPT conversations, which consists of over 2.5 million interaction turns. We compare WildChat with other popular user-chatbot interaction datasets, and highlight unique characteristics of WildChat. We show that WildChat can enable new insights into user behaviors and preferences in conversational AI. We also demonstrate the potential of WildChat to improve instruction-following capabilities of language models.},
  author = {Wenting Zhao and Xiang Ren and Jack Hessel and Claire Cardie and Yejin Choi and Yuntian Deng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024w.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=Bl8u7ZRlbM},
  publisher = {OpenReview.net},
  title = {{W}ild{C}hat: 1{M} {C}hat{GPT} Interaction Logs in the Wild},
  url = {https://openreview.net/forum?id=Bl8u7ZRlbM},
  year = {2024}
}

@inproceedings{zhao20243d,
  abstract = {Prior methods that tackle the problem of generalizable object pose estimation highly rely on having dense views of the unseen object. By contrast, we address the scenario where only a single reference view of the object is available. Our goal then is to estimate the relative object pose between this reference view and a query image that depicts the object in a different pose. In this scenario, robust generalization is imperative due to the presence of unseen objects during testing and the large-scale object pose variation between the reference and the query. To this end, we present a new hypothesis-and-verification framework, in which we generate and evaluate multiple pose hypotheses, ultimately selecting the most reliable one as the relative object pose.},
  author = {Chen Zhao and Tong Zhang and Mathieu Salzmann},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao20243d.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=U6hEOZlDf5},
  publisher = {OpenReview.net},
  title = {{3D}-Aware Hypothesis \& Verification for Generalizable Relative Object Pose Estimation},
  url = {https://openreview.net/forum?id=U6hEOZlDf5},
  year = {2024}
}

@inproceedings{zhao2024provable,
  abstract = {We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs.},
  author = {Xuandong Zhao and Prabhanjan Vijendra Ananth and Lei Li and Yu-Xiang Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024provable.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=SsmT8aO45L},
  publisher = {OpenReview.net},
  title = {Provable Robust Watermarking for {AI}-Generated Text},
  url = {https://openreview.net/forum?id=SsmT8aO45L},
  year = {2024}
}

@inproceedings{zhao2024sparse,
  abstract = {Sparse Mixture-of-Experts (SMoE) has gained increasing popularity as a promising framework for scaling up multilingual machine translation (MMT) models with negligible extra computational overheads. However, current SMoE solutions neglect the intrinsic structures of the MMT problem: (a) Linguistics Hierarchy. Languages are naturally grouped according to their lingual properties like genetic families, phonological characteristics, etc; (b) Language Complexity. The learning difficulties are varied for diverse languages due to their grammar complexity, available resources, etc. Therefore, routing a fixed number of experts (e.g., 1 or 2 experts in usual) only at the word level leads to inferior performance. To fill in the missing puzzle, we propose Lingual-SMoE by equipping the SMoE with adaptive and linguistic-guided routing policies.},
  author = {Xinyu Zhao and Xuxi Chen and Yu Cheng and Tianlong Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024sparse.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=ySS7hH1smL},
  publisher = {OpenReview.net},
  title = {Sparse {M}o{E} with Language Guided Routing for Multilingual Machine Translation},
  url = {https://openreview.net/forum?id=ySS7hH1smL},
  year = {2024}
}

@inproceedings{zhao2024breaking,
  abstract = {Pre-trained large language models (LLMs) have become a cornerstone of modern natural language processing, with their capabilities extending across a wide range of applications and languages. However, the fine-tuning of multilingual LLMs, especially for low-resource languages, faces significant challenges arising from data-sharing restrictions (the physical border) and inherent linguistic differences (the linguistic border). These barriers hinder users of various languages, particularly those in low-resource regions, from fully benefiting from the advantages of LLMs. To address these challenges, we propose a Federated Prompt Tuning paradigm for multilingual LLMs, preserving user privacy and improving performance for low-resource languages.},
  author = {Wanru Zhao and Yihong Chen and Royson Lee and Xinchi Qiu and Yan Gao and Hongxiang Fan and Nicholas Donald Lane},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024breaking.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=zzqn5G9fjn},
  publisher = {OpenReview.net},
  title = {Breaking Physical and Linguistic Borders: Multilingual Federated Prompt Tuning for Low-Resource Languages},
  url = {https://openreview.net/forum?id=zzqn5G9fjn},
  year = {2024}
}

@inproceedings{zhao2024a,
  abstract = {Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The experiments demonstrate that AutoChunk can reduce over 80% of activation memory while maintaining speed loss within 10%, extend max sequence length by 3.2x to 11.7x, and outperform state-of-the-art methods by a large margin.},
  author = {Xuanlei Zhao and Shenggan Cheng and Guangyang Lu and Haotian Zhou and Bin Jia and Yang You},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024a.pdf:pdf},
  pdf = {https://openreview.net/pdf?id=GQGNLEHmdl},
  publisher = {OpenReview.net},
  title = {{A}uto{C}hunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference},
  url = {https://openreview.net/forum?id=GQGNLEHmdl},
  year = {2024}
}

@inproceedings{zhao2024mmicl,
  abstract = {Vision-language models (VLMs) enhanced by large language models (LLMs) have grown exponentially in popularity. However, most VLMs struggle with understanding complex multi-modal prompts with multiple images, making them less effective in downstream tasks. This paper introduces MMICL, a new approach for VLMs to handle multi-modal inputs with a novel context scheme to improve in-context learning. We construct the Multi-modal In-Context Learning (MIC) dataset and achieve state-of-the-art zero-shot performance on vision-language tasks while effectively tackling complex multi-modal prompt understanding and alleviating language bias in VLMs.},
  author = {Haozhe Zhao and Zefan Cai and Shuzheng Si and Xiaojian Ma and Kaikai An and Liang Chen and Zixuan Liu and Sheng Wang and Wenjuan Han and Baobao Chang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024mmicl.pdf:pdf},
  note = {DBLP last modified: 2025-05-14},
  pdf = {https://openreview.net/pdf?id=5KojubHBr8},
  publisher = {OpenReview.net},
  title = {{MMICL}: Empowering Vision-language Model with Multi-Modal In-Context Learning},
  url = {https://openreview.net/forum?id=5KojubHBr8},
  year = {2024}
}

@inproceedings{zhao2024group,
  abstract = {Many LLM applications require nuanced subjective judgments that differ across groups, while existing alignment algorithms are expensive, requiring prohibitive amounts of group-specific preference data and computation. We introduce Group Preference Optimization (GPO), an alignment framework that steers language models to preferences of individual groups in a few-shot manner. In GPO, we augment the base LLM with an independent transformer module trained to predict group preferences, parameterized as an in-context autoregressive transformer and trained via meta-learning on several groups. We validate GPO through evaluations on three human opinion adaptation tasks involving US demographic groups, global countries, and individual users, demonstrating that GPO aligns models more accurately while requiring fewer group-specific preferences and less computational resources compared to existing methods.},
  author = {Siyan Zhao and John Dang and Aditya Grover},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024group.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DpFeMH4l8Q},
  publisher = {OpenReview.net},
  title = {Group Preference Optimization: Few-Shot Alignment of Large Language Models},
  url = {https://openreview.net/forum?id=DpFeMH4l8Q},
  year = {2024}
}

@inproceedings{zhao2024pinnsformer,
  abstract = {Physics-Informed Neural Networks (PINNs) have emerged as a promising deep learning framework for approximating numerical solutions to partial differential equations (PDEs). However, conventional PINNs, relying on multilayer perceptrons (MLP), neglect the crucial temporal dependencies inherent in practical physics systems and thus fail to propagate the initial condition constraints globally and accurately capture the true solutions under various scenarios. We introduce PINNsFormer, a novel Transformer-based framework designed to address these limitations. PINNsFormer can accurately approximate PDE solutions by utilizing multi-head attention mechanisms to capture temporal dependencies, transforms point-wise inputs into pseudo sequences, and replaces point-wise PINNs loss with a sequential loss. It incorporates a novel activation function, Wavelet, which anticipates Fourier decomposition through deep neural networks. Empirical results demonstrate that PINNsFormer achieves superior generalization ability and accuracy across various scenarios, including PINNs failure modes and high-dimensional PDEs.},
  author = {Leo Zhiyuan Zhao and Xueying Ding and B. Aditya Prakash},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024pinnsformer.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=DO2WFXU1Be},
  publisher = {OpenReview.net},
  title = {{PINNsFormer}: A Transformer-Based Framework For Physics-Informed Neural Networks},
  url = {https://openreview.net/forum?id=DO2WFXU1Be},
  year = {2024}
}

@inproceedings{zhao2024improving,
  abstract = {We explore parameter space symmetries which are loss-invariant transformations that change model parameters, and introduce teleportation which applies such transformations to accelerate optimization. We show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence, and that teleporting to minima with different curvatures improves generalization. We demonstrate that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence, showcasing the versatility of teleportation and demonstrating the potential of incorporating symmetry in optimization.},
  author = {Bo Zhao and Robert M. Gower and Robin Walters and Rose Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024improving.pdf:pdf},
  note = {Oral presentation. DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=L0r0GphlIL},
  publisher = {OpenReview.net},
  title = {Improving Convergence and Generalization Using Parameter Symmetries},
  url = {https://openreview.net/forum?id=L0r0GphlIL},
  year = {2024}
}

@inproceedings{zhao2024dynavol,
  abstract = {We address the challenge of unsupervised learning of object-centric representations in dynamic visual scenes. Unlike most previous approaches that learn to decompose 2D images, DynaVol presents a 3D scene generative model that unifies geometric structures and object-centric learning in a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers the probability distribution over objects at individual spatial locations. These voxel features evolve over time through a canonical-space deformation function, forming the basis for global representation learning via slot attention. We demonstrate the effectiveness of our approach on both synthetic and real-world datasets, showing superior object decomposition and reconstruction quality compared to existing methods.},
  author = {Yanpeng Zhao and Siyu Gao and Yunbo Wang and Xiaokang Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024dynavol.pdf:pdf},
  note = {DBLP last modified: 2025-03-07},
  pdf = {https://openreview.net/pdf?id=koYsgfEwCQ},
  publisher = {OpenReview.net},
  title = {{DynaVol}: Unsupervised Learning for Dynamic Scenes through Object-Centric Voxelization},
  url = {https://openreview.net/forum?id=koYsgfEwCQ},
  year = {2024}
}

@inproceedings{zhao2024rethinking,
  abstract = {While channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting and reduce overfitting risks, they miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, where some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. We propose LIFT, a novel method that efficiently estimates the relationships and dynamically incorporates leading indicators in the frequency domain for MTS forecasting. LIFT can work as a plug-and-play module and is generally applicable to arbitrary forecasting models. We also introduce LightMTS as a lightweight yet strong baseline for MTS forecasting, which keeps similar parameter efficiency with linear models and shows considerable performance compared with state-of-the-art methods.},
  author = {Lifan Zhao and Yanyan Shen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024rethinking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=JiTVtCUOpS},
  publisher = {OpenReview.net},
  title = {Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators},
  url = {https://openreview.net/forum?id=JiTVtCUOpS},
  year = {2024}
}

@inproceedings{zhao2024tuning,
  abstract = {We introduce an efficient strategy to transform Large Language Models (LLMs) into Multi-Modal Large Language Models by conceptualizing this transformation as a domain adaptation process. We find that within each attention block, tuning LayerNorm suffices to yield strong performance. When compared to LoRA on a 13B model scale, our approach can enhance performance by an average of over 20% across five multi-modal tasks, while resulting in a significant reduction of trainable parameters by 41.9% and a decrease in GPU memory usage by 17.6%. Our research demonstrates that tuning only the LayerNorm parameters provides a comprehensive analysis of their role in adapting LLMs to the multi-modal domain and improving the expressive power of the model, making it a highly efficient alternative to full parameter fine-tuning or other parameter-efficient methods.},
  author = {Bingchen Zhao and Haoqin Tu and Chen Wei and Jieru Mei and Cihang Xie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024tuning.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=YR3ETaElNK},
  publisher = {OpenReview.net},
  title = {Tuning {LayerNorm} in Attention: Towards Efficient Multi-Modal {LLM} Finetuning},
  url = {https://openreview.net/forum?id=YR3ETaElNK},
  year = {2024}
}

@inproceedings{zhao2024antgpt,
  abstract = {The long-term action anticipation (LTA) task aims to predict an actor's future behavior from video observations in the form of verb and noun sequences, and it is crucial for human-machine interaction. We propose to formulate the LTA task from two perspectives: a bottom-up approach that predicts the next actions autoregressively by modeling temporal dynamics; and a top-down approach that infers the goal of the actor and plans the needed procedure to accomplish the goal. We propose AntGPT, which represents video observations as sequences of human actions, and uses the action representation for an LLM to infer the goals and model temporal dynamics. We hypothesize that large language models (LLMs), which have been pretrained on procedure text data, have the potential to help LTA from both perspectives. AntGPT achieves state-of-the-art performance on Ego4D LTA v1 and v2, EPIC-Kitchens-55, as well as EGTEA GAZE+, thanks to LLMs' goal inference and temporal dynamics modeling capabilities.},
  author = {Qi Zhao and Shijie Wang and Ce Zhang and Changcheng Fu and Minh Quan Do and Nakul Agarwal and Kwonjoon Lee and Chen Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024antgpt.pdf:pdf},
  note = {DBLP last modified: 2024-08-09},
  pdf = {https://openreview.net/pdf?id=Bb21JPnhhr},
  publisher = {OpenReview.net},
  title = {{AntGPT}: Can Large Language Models Help Long-term Action Anticipation from Videos?},
  url = {https://openreview.net/forum?id=Bb21JPnhhr},
  year = {2024}
}

@inproceedings{zhao2024towards,
  abstract = {Feature shaping refers to methods that achieve state-of-the-art performance for out-of-distribution (OOD) detection by manipulating feature representations from pre-trained deep learning models to better differentiate between in-distribution and OOD samples. However, existing methods typically use manually designed rules for specific architectures and datasets, limiting generalization. We propose an abstract optimization framework for feature-shaping methods, develop a concrete reduction with a piecewise constant shaping function, and provide a closed-form solution using only in-distribution data. Our experiments demonstrate improved generalization across various datasets and model architectures, providing a principled approach to optimal feature shaping for OOD detection.},
  author = {Qinyu Zhao and Ming Xu and Kartik Gupta and Akshay Asthana and Liang Zheng and Stephen Gould},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-12-11},
  pdf = {https://openreview.net/pdf?id=dm8e7gsH0d},
  publisher = {OpenReview.net},
  title = {Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection},
  url = {https://openreview.net/forum?id=dm8e7gsH0d},
  year = {2024}
}

@inproceedings{zhao2024causalityinspired,
  abstract = {Dynamic Graph Neural Networks (DyGNNs) have gained significant popularity but are limited by low transparency, making it challenging to draw human-understandable insights from their predictions due to complex spatial-temporal correlations in dynamic graphs. This is the first work to explain dynamic graph neural networks. We present a novel causality-inspired generative model for enhancing the interpretability of DyGNNs by identifying complex causal relationships in dynamic graphs, introducing a structural causal model (SCM) to disentangle trivial, static, and dynamic relationships, employing contrastive learning and a dynamic VGAE-based framework. Experimental results demonstrate significant improvements in interpretability and predictive performance on both synthetic and real-world datasets.},
  author = {Kesen Zhao and Liang Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024causalityinspired.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AJBkfwXh3u},
  publisher = {OpenReview.net},
  title = {Causality-Inspired Spatial-Temporal Explanations for Dynamic Graph Neural Networks},
  url = {https://openreview.net/forum?id=AJBkfwXh3u},
  year = {2024}
}

@inproceedings{zhao2024towards,
  abstract = {We characterize the statistical efficiency of knowledge transfer through n samples from a teacher to a probabilistic student classifier with input space S over labels A. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate ‚àö(|S||A|/n). The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to |S||A|/n. However, under this data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss.},
  author = {Qingyue Zhao and Banghua Zhu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhao2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Zh2iqiOtMt},
  publisher = {OpenReview.net},
  title = {{Towards the Fundamental Limits of Knowledge Transfer over Finite Domains}},
  url = {https://openreview.net/forum?id=Zh2iqiOtMt},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024large,
  abstract = {Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias", namely, they prefer to select specific option IDs as answers (like "Option A"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.},
  author = {Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024large.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=shr9PXz7T0},
  publisher = {OpenReview.net},
  title = {{Large Language Models Are Not Robust Multiple Choice Selectors}},
  type = {Spotlight},
  url = {https://openreview.net/forum?id=shr9PXz7T0},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024towards,
  abstract = {Graph Neural Networks (GNNs) are neural models that leverage the dependency structure in graphical data via message passing among the graph nodes. GNNs have emerged as pivotal architectures in analyzing graph-structured data, and their expansive application in sensitive domains requires a comprehensive understanding of their decision-making processes --- necessitating a framework for GNN explainability. An explanation function for GNNs takes a pre-trained GNN along with a graph as input, to produce a 'sufficient statistic' subgraph with respect to the graph label. A main challenge in studying GNN explainability is to provide fidelity measures that evaluate the performance of these explanation functions. This paper studies this foundational challenge, spotlighting the inherent limitations of prevailing fidelity metrics, including Fid‚Çä, Fid‚Çã, and FidŒî. Specifically, a formal, information-theoretic definition of explainability is introduced and it is shown that existing metrics often fail to align with this definition across various statistical scenarios. The reason is due to potential distribution shifts when subgraphs are removed in computing these fidelity measures. Subsequently, a robust class of fidelity measures are introduced, and it is shown analytically that they are resilient to distribution shift issues and are applicable in a wide range of scenarios. Extensive empirical analysis on both synthetic and real datasets are provided to illustrate that the proposed metrics are more coherent with gold standard metrics.},
  author = {Xu Zheng and Farhad Shirani and Tianchun Wang and Wei Cheng and Zhuomin Chen and Haifeng Chen and Hua Wei and Dongsheng Luo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=up6hr4hIQH},
  publisher = {OpenReview.net},
  title = {{Towards Robust Fidelity for Evaluating Explainability of Graph Neural Networks}},
  url = {https://openreview.net/forum?id=up6hr4hIQH},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024lmsyschat1m,
  abstract = {Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.},
  author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024lmsyschat1m.pdf:pdf},
  note = {DBLP last modified: 2025-02-11},
  pdf = {https://openreview.net/pdf?id=BOfDKxfwt0},
  publisher = {OpenReview.net},
  title = {{LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset}},
  url = {https://openreview.net/forum?id=BOfDKxfwt0},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024stabilizing,
  abstract = {Robotic systems that rely primarily on self-supervised learning have the potential to decrease the amount of human annotation and engineering effort required to learn control strategies. In the same way that prior robotic systems have leveraged self-supervised techniques from computer vision (CV) and natural language processing (NLP), our work builds on prior work showing that the reinforcement learning (RL) itself can be cast as a self-supervised problem: learning to reach any goal without human-specified rewards or labels. Despite the seeming appeal, little (if any) prior work has demonstrated how self-supervised RL methods can be practically deployed on robotic systems. By first studying a challenging simulated version of this task, we discover design decisions about architectures and hyperparameters that increase the success rate by 2√ó. These findings lay the groundwork for our main result: we demonstrate that a self-supervised RL algorithm based on contrastive learning can solve real-world, image-based robotic manipulation tasks, with tasks being specified by a single goal image provided after training.},
  author = {Chongyi Zheng and Benjamin Eysenbach and Homer Rich Walke and Patrick Yin and Kuan Fang and Ruslan Salakhutdinov and Sergey Levine},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024stabilizing.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=Xkf2EBj4w3},
  publisher = {OpenReview.net},
  title = {{Stabilizing Contrastive RL: Techniques for Robotic Goal Reaching from Offline Data}},
  url = {https://openreview.net/forum?id=Xkf2EBj4w3},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024federated,
  abstract = {In this paper, we consider federated reinforcement learning for tabular episodic Markov Decision Processes (MDP) where, under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. While linear speedup in the number of agents has been achieved for some metrics, such as convergence rate and sample complexity, in similar settings, it is unclear whether it is possible to design a model-free algorithm to achieve linear regret speedup with low communication cost. We propose two federated Q-Learning algorithms termed as FedQ-Hoeffding and FedQ-Bernstein, respectively, and show that the corresponding total regrets achieve a linear speedup compared with their single-agent counterparts, while the communication cost scales logarithmically in the total number of time steps T. Those results rely on an event-triggered synchronization mechanism between the agents and the server, a novel step size selection when the server aggregates the local estimates of the state-action values to form the global estimates, and a set of new concentration inequalities to bound the sum of non-martingale differences. This is the first work showing that linear regret speedup and logarithmic communication cost can be achieved by model-free algorithms in federated reinforcement learning.},
  author = {Zhong Zheng and Fengyu Gao and Lingzhou Xue and Jing Yang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024federated.pdf:pdf},
  note = {DBLP last modified: 2025-04-12},
  pdf = {https://openreview.net/pdf?id=fe6ANBxcKM},
  publisher = {OpenReview.net},
  title = {{Federated Q-Learning: Linear Regret Speedup with Low Communication Cost}},
  url = {https://openreview.net/forum?id=fe6ANBxcKM},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024steveeye,
  abstract = {Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to "a blindfolded text-based game." Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model to address this limitation. Steve-Eye integrates the LLM with a visual encoder to process visual-text inputs and generate multimodal feedback. We adopt a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, enabling our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge understanding, and skill prediction and planning. Extensive experiments demonstrate the effectiveness of our approach in enabling embodied agents to operate with enhanced visual perception in open-world environments.},
  author = {Sipeng Zheng and Jiazheng Liu and Yicheng Feng and Zongqing Lu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024steveeye.pdf:pdf},
  note = {DBLP last modified: 2025-04-09},
  pdf = {https://openreview.net/pdf?id=NltzxpG0nz},
  publisher = {OpenReview.net},
  title = {{Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds}},
  url = {https://openreview.net/forum?id=NltzxpG0nz},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024safe,
  abstract = {Safe offline RL is a promising way to bypass risky online interactions towards safe policy learning. Most existing methods only enforce soft constraints, i.e., constraining safety violations in expectation below thresholds predetermined. This can lead to potentially unsafe outcomes, thus unacceptable in safety-critical scenarios. An alternative is to enforce the hard constraint of zero violation. However, this can be challenging in offline setting, as it needs to strike the right balance among three highly intricate and correlated aspects: safety constraint satisfaction, reward maximization, and behavior regularization imposed by offline datasets. Interestingly, we discover that via reachability analysis of safe-control theory, the hard safety constraint can be equivalently translated to identifying the largest feasible region given the offline dataset. This seamlessly converts the original trilogy problem to a feasibility-dependent objective, i.e., maximizing reward value within the feasible region while minimizing safety risks in the infeasible region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline RL), which allows safety constraint adherence, reward maximization, and offline policy learning to be realized via three decoupled processes, while offering strong safety performance and stability. In FISOR, the optimal policy for the translated optimization problem can be derived in a special form of weighted behavior cloning. Thus, we propose a novel energy-guided diffusion model that does not require training a complicated time-dependent classifier to extract the policy, greatly simplifying the training.},
  author = {Yinan Zheng and Jianxiong Li and Dongjie Yu and Yujie Yang and Shengbo Eben Li and Xianyuan Zhan and Jingjing Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024safe.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=j5JvZCaDM0},
  publisher = {OpenReview.net},
  title = {{Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion Model}},
  url = {https://openreview.net/forum?id=j5JvZCaDM0},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024take,
  abstract = {We present STEP-BACK PROMPTING, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of STEP-BACK PROMPTING with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.},
  author = {Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and Ed H. Chi and Quoc V. Le and Denny Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024take.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3bq3jsvcQ1},
  publisher = {OpenReview.net},
  title = {{Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models}},
  url = {https://openreview.net/forum?id=3bq3jsvcQ1},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024intriguing,
  abstract = {Data attribution seeks to trace model outputs back to training data. With the recent development of diffusion models, data attribution has become a desired module to properly assign valuations for high-quality or copyrighted training samples, ensuring that data contributors are fairly compensated or credited. Several theoretically motivated methods have been proposed to implement data attribution, in an effort to improve the trade-off between computational scalability and effectiveness. In this work, we conduct extensive experiments and ablation studies on attributing diffusion models, specifically focusing on DDPMs trained on CIFAR-10 and CelebA, as well as a Stable Diffusion model LoRA-finetuned on ArtBench. Intriguingly, we report counter-intuitive observations that theoretically unjustified design choices for attribution empirically outperform previous baselines by a large margin, in terms of both linear datamodeling score and counterfactual evaluation. Our work presents a significantly more efficient approach for attributing diffusion models, while the unexpected findings suggest that at least in non-convex settings, constructions guided by theoretical assumptions may lead to inferior attribution performance.},
  author = {Xiaosen Zheng and Tianyu Pang and Chao Du and Jing Jiang and Min Lin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zheng2024intriguing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=vKViCoKGcB},
  publisher = {OpenReview.net},
  title = {{Intriguing Properties of Data Attribution on Diffusion Models}},
  url = {https://openreview.net/forum?id=vKViCoKGcB},
  venue = {ICLR 2024},
  year = {2024}
}

@inproceedings{zheng2024contrastive,
  abstract = {Predicting and reasoning about the future lie at the heart of many time-series questions. For example, goal-conditioned reinforcement learning can be viewed as learning representations to predict which states are likely to be visited in the future. While prior methods have used contrastive predictive coding to model time series data, learning representations that encode long-term dependencies usually requires large amounts of data. In this paper, we introduce a temporal difference version of contrastive predictive coding that stitches together pieces of different time series data to decrease the amount of data required to learn predictions of future events. We apply this representation learning method to derive an off-policy algorithm for goal-conditioned RL. Experiments demonstrate that, compared with prior RL methods, ours achieves 2√ó median improvement in success rates and can better cope with stochastic environments. In tabular settings, we show that our method is about 20√ó more sample efficient than the successor representation and 1500√ó more sample efficient than the standard (Monte Carlo) version of contrastive predictive coding.},
  archiveprefix = {arXiv},
  author = {Chongyi Zheng and Ruslan Salakhutdinov and Benjamin Eysenbach},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2310.20141},
  file = {:/home/b/documents/inproceedings/zheng2024contrastive.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0akLDTFR9x},
  publisher = {OpenReview.net},
  title = {Contrastive Difference Predictive Coding},
  url = {https://openreview.net/forum?id=0akLDTFR9x},
  year = {2024}
}

@inproceedings{zheng2024improving,
  abstract = {The success of AI assistants based on language models (LLMs) hinges crucially on Reinforcement Learning from Human Feedback (RLHF), which enables the generation of responses more aligned with human preferences. As universal AI assistants, there's a growing expectation for them to perform consistently across various domains. However, previous work shows that Reinforcement Learning (RL) often exploits shortcuts to attain high rewards and overlooks challenging samples. This focus on quick reward gains undermines both the stability in training and the model's ability to generalize to new, unseen data. In this work, we propose a novel approach that can learn a consistent policy via RL across various data groups or domains. Given the challenges associated with acquiring group annotations, our method automatically classifies data into different groups, deliberately maximizing performance variance. Then, we optimize the policy to perform well on challenging groups. Lastly, leveraging the established groups, our approach adaptively adjusts the exploration space, allocating more learning capacity to more challenging data and preventing the model from over-optimizing on simpler data. Experimental results indicate that our approach significantly enhances training stability and model generalization.},
  archiveprefix = {arXiv},
  author = {Rui Zheng and Wei Shen and Yuan Hua and Wenbin Lai and Shihan Dou and Yuhao Zhou and Zhiheng Xi and Xiao Wang and Haoran Huang and Tao Gui and Qi Zhang and Xuanjing Huang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2310.11971},
  file = {:/home/b/documents/inproceedings/zheng2024improving.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=fwCoLe3TAX},
  publisher = {OpenReview.net},
  title = {Improving Generalization of Alignment with Human Preferences through Group Invariant Learning},
  url = {https://openreview.net/forum?id=fwCoLe3TAX},
  year = {2024}
}

@inproceedings{zheng2024online,
  abstract = {Evaluating the performance of a well-trained GNN model on real-world graphs is a pivotal step for reliable GNN online deployment and serving. Due to a lack of test node labels and unknown potential training-test graph data distribution shifts, conventional model evaluation encounters limitations in calculating performance metrics (e.g., test error) and measuring graph data-level discrepancies, particularly when the training graph used for developing GNNs remains unobserved during test time. In this paper, we study a new research problem, online GNN evaluation, which aims to provide valuable insights into the well-trained GNNs's ability to effectively generalize to real-world unlabeled graphs under the test-time graph distribution shifts. Concretely, we develop an effective learning behavior discrepancy score, dubbed LeBeD, to estimate the test-time generalization errors of well-trained GNN models. Through a novel GNN re-training strategy with a parameter-free optimality criterion, the proposed LeBeD comprehensively integrates learning behavior discrepancies from both node prediction and structure reconstruction perspectives. This enables the effective evaluation of the well-trained GNNs' ability to capture test node semantics and structural representations, making it an expressive metric for estimating the generalization error in online GNN evaluation. Extensive experiments on real-world test graphs under diverse graph distribution shifts could verify the effectiveness of the proposed method, revealing its strong correlation with ground-truth test errors on various well-trained GNN models.},
  archiveprefix = {arXiv},
  author = {Xin Zheng and Dongjin Song and Qingsong Wen and Bo Du and Shirui Pan},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2403.09953},
  file = {:/home/b/documents/inproceedings/zheng2024online.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=KbetDM33YG},
  publisher = {OpenReview.net},
  title = {Online {GNN} Evaluation Under Test-time Graph Distribution Shifts},
  url = {https://openreview.net/forum?id=KbetDM33YG},
  year = {2024}
}

@inproceedings{zheng2024parametric,
  abstract = {Modern techniques like contrastive learning have been effectively used in many areas, including computer vision, natural language processing, and graph-structured data. Creating positive examples that assist the model in learning robust and discriminative representations is a crucial stage in contrastive learning approaches. Usually, preset human intuition directs the selection of relevant data augmentations. Due to patterns that are easily recognized by humans, this rule of thumb works well in the vision and language domains. However, it is impractical to visually inspect the temporal structures in time series. The diversity of time series augmentations at both the dataset and instance levels makes it difficult to choose meaningful augmentations on the fly. Thus, although prevalent, contrastive learning with data augmentation has been less studied in the time series domain. In this study, we address this gap by analyzing time series data augmentation using information theory and summarizing the most commonly adopted augmentations in a unified format. We then propose a parametric augmentation method, AutoTCL, which can be adaptively employed to support time series representation learning.},
  archiveprefix = {arXiv},
  author = {Xu Zheng and Tianchun Wang and Wei Cheng and Aitian Ma and Haifeng Chen and Mo Sha and Dongsheng Luo},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2402.10434},
  file = {:/home/b/documents/inproceedings/zheng2024parametric.pdf:pdf},
  note = {DBLP last modified: 2024-08-15},
  pdf = {https://openreview.net/pdf?id=EIPLdFy3vp},
  publisher = {OpenReview.net},
  title = {Parametric Augmentation for Time Series Contrastive Learning},
  url = {https://openreview.net/forum?id=EIPLdFy3vp},
  year = {2024}
}

@inproceedings{zheng2024synapse,
  abstract = {Building agents with large language models (LLMs) for computer control is a burgeoning research area, where the agent receives computer states and performs actions to complete complex tasks. Previous computer agents have demonstrated the benefits of in-context learning (ICL); however, their performance is hindered by several issues. First, the limited context length of LLMs and complex computer states restrict the number of exemplars, as a single webpage can consume the entire context. Second, the exemplars in current methods, such as high-level plans and multi-choice questions, cannot represent complete trajectories, leading to suboptimal performance in long-horizon tasks. Third, existing computer agents rely on task-specific exemplars and overlook the similarity among tasks, resulting in poor generalization to novel tasks. To address these challenges, we introduce Synapse, a computer agent featuring three key components: i) state abstraction, which filters out task-irrelevant information from raw states, allowing more exemplars within the limited context, ii) trajectory-as-exemplar prompting, which prompts the LLM with complete trajectories of the abstracted states and actions to improve multi-step decision-making, and iii) exemplar memory, which stores the embeddings of exemplars and retrieves them via similarity search for generalization to novel tasks. We evaluate Synapse on MiniWoB++, a standard task suite, and Mind2Web, a real-world website benchmark. In MiniWoB++, Synapse achieves a 99.2% average success rate (a 10% relative improvement) across 64 tasks using demonstrations from only 48 tasks. Notably, Synapse is the first ICL method to solve the book-flight task in MiniWoB++. Synapse also exhibits a 56% relative improvement in average step success rate over the previous state-of-the-art prompting methods.},
  archiveprefix = {arXiv},
  author = {Longtao Zheng and Rundong Wang and Xinrun Wang and Bo An},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2306.07863},
  file = {:/home/b/documents/inproceedings/zheng2024synapse.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Pc8AU1aF5e},
  publisher = {OpenReview.net},
  title = {Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control},
  url = {https://openreview.net/forum?id=Pc8AU1aF5e},
  year = {2024}
}

@inproceedings{zheng2024learning,
  abstract = {Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. In this work, we introduce the LEGO brick, a fundamental network unit that seamlessly integrates two essential components: Local-feature Enrichment and Global-content Orchestration. LEGO bricks can be stacked to create a test-time reconfigurable diffusion backbone. We explore stacking bricks with various input sizes to construct the network, enabling selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. Experimental results demonstrate that LEGO bricks enhance training efficiency, expedite convergence, and facilitate variable-resolution image generation while maintaining strong generative performance. Moreover, LEGO significantly reduces sampling time compared to other methods, establishing it as a valuable enhancement for diffusion models.},
  archiveprefix = {arXiv},
  author = {Huangjie Zheng and Zhendong Wang and Jianbo Yuan and Guanghan Ning and Pengcheng He and Quanzeng You and Hongxia Yang and Mingyuan Zhou},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2310.06389},
  file = {:/home/b/documents/inproceedings/zheng2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=qmXedvwrT1},
  publisher = {OpenReview.net},
  title = {Learning Stackable and Skippable {LEGO} Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling},
  url = {https://openreview.net/forum?id=qmXedvwrT1},
  year = {2024}
}

@inproceedings{zheng2024enhancing,
  abstract = {Contrastive learning, while highly effective for a lot of tasks, shows limited improvement in ordinal regression. We find that the limitation comes from the predefined strong data augmentations employed in contrastive learning. Intuitively, for ordinal regression datasets, the discriminative information (ordinal content information) contained in instances is subtle. The strong augmentations can easily overshadow or diminish this ordinal content information. As a result, when contrastive learning is used to extract common features between weakly and strongly augmented images, the derived features often lack this essential ordinal content, rendering them less useful in training models for ordinal regression. To improve contrastive learning's utility for ordinal regression, we propose a novel augmentation method to replace the predefined strong augmentation based on the principle of minimal change. This approach preserves the ordinal structure of the data during the augmentation process, which is essential for tasks where the ordering of labels carries important semantic meaning.},
  author = {Jiyang Zheng and Yu Yao and Bo Han and Dadong Wang and Tongliang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  file = {:/home/b/documents/inproceedings/zheng2024enhancing.pdf:pdf},
  note = {DBLP last modified: 2025-02-10},
  pdf = {https://openreview.net/pdf?id=kx2XZlmgB1},
  publisher = {OpenReview.net},
  title = {Enhancing Contrastive Learning for Ordinal Regression via Ordinal Content Preserved Data Augmentation},
  url = {https://openreview.net/forum?id=kx2XZlmgB1},
  year = {2024}
}

@inproceedings{zheng2024knowledge,
  abstract = {As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R√©nyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD.},
  archiveprefix = {arXiv},
  author = {Kaixiang Zheng and En-Hui Yang},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2402.11148},
  file = {:/home/b/documents/inproceedings/zheng2024knowledge.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=MJ3K7uDGGl},
  publisher = {OpenReview.net},
  title = {Knowledge Distillation Based on Transformed Teacher Matching},
  url = {https://openreview.net/forum?id=MJ3K7uDGGl},
  year = {2024}
}

@inproceedings{zheng2024noisediffusion,
  abstract = {Image interpolation based on diffusion models is promising in creating fresh and interesting images. Advanced interpolation methods mainly focus on spherical linear interpolation, where images are encoded into the noise space and then interpolated for denoising to images. However, existing methods face challenges in effectively interpolating natural images (not generated by diffusion models), thereby restricting their practical applicability. Our experimental investigations reveal that these challenges stem from the invalidity of the encoding noise, which may no longer obey the expected noise distribution, e.g., a normal distribution. To address these challenges, we propose a novel approach to correct noise for image interpolation, NoiseDiffusion. Specifically, NoiseDiffusion approaches the invalid noise to the expected distribution by introducing subtle Gaussian noise and introduces a constraint to suppress noise with extreme values. In this context, promoting noise validity contributes to mitigating image artifacts, but the constraint and introduced exogenous noise typically lead to a reduction in signal-to-noise ratio, i.e., loss of original image information. Hence, NoiseDiffusion performs interpolation within the noisy image space and injects raw images into these noisy counterparts to address the challenge of information loss. Consequently, NoiseDiffusion enables us to interpolate natural images without causing artifacts or information loss, thus achieving the best interpolation results.},
  archiveprefix = {arXiv},
  author = {Pengfei Zheng and Yonggang Zhang and Zhen Fang and Tongliang Liu and Defu Lian and Bo Han},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2403.08840},
  file = {:/home/b/documents/inproceedings/zheng2024noisediffusion.pdf:pdf},
  note = {DBLP last modified: 2025-01-29},
  pdf = {https://openreview.net/pdf?id=6O3Q6AFUTu},
  publisher = {OpenReview.net},
  title = {{NoiseDiffusion}: Correcting Noise for Image Interpolation with Diffusion Models beyond Spherical Linear Interpolation},
  url = {https://openreview.net/forum?id=6O3Q6AFUTu},
  year = {2024}
}

@inproceedings{zhong2024convolution,
  abstract = {The Segment-Anything Model ({SAM}) stands as a foundational framework for image segmentation. While it exhibits remarkable zero-shot generalization in typical scenarios, its advantage diminishes when applied to specialized domains like medical imagery and remote sensing. To address this limitation, we introduce Conv-{LoRA}, a parameter-efficient fine-tuning approach that integrates convolutional parameters into Low-Rank Adaptation ({LoRA}). This method enhances the model's ability to capture local spatial relationships, which are crucial for segmentation tasks in specialized domains. Our approach demonstrates significant improvements over standard {LoRA} while maintaining parameter efficiency, making it particularly suitable for domain-specific applications where computational resources may be limited.},
  archiveprefix = {arXiv},
  author = {Zihan Zhong and Zhiqiang Tang and Tong He and Haoyang Fang and Chun Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  eprint = {2401.17868},
  file = {:/home/b/documents/inproceedings/zhong2024convolution.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ezscMer8L0},
  publisher = {OpenReview.net},
  title = {Convolution Meets {LoRA}: Parameter Efficient Finetuning for Segment Anything Model},
  url = {https://openreview.net/forum?id=ezscMer8L0},
  year = {2024}
}

@inproceedings{zhou2024lidarptq,
  abstract = {Due to highly constrained computing power and memory, deploying {3D} lidar-based detectors on edge devices equipped in autonomous vehicles and robots poses a crucial challenge. Being a convenient and straightforward model compression approach, Post-Training Quantization ({PTQ}) has been widely adopted in {2D} vision tasks. However, applying it directly to {3D} lidar-based tasks inevitably leads to performance degradation. As a remedy, we propose an effective {PTQ} method called {LiDAR-PTQ}, which is particularly curated for {3D} lidar detection (both {SPConv}-based and {SPConv}-free). Our {LiDAR-PTQ} features three main components: (1) a sparsity-based calibration method to determine the initialization of quantization parameters, (2) a Task-guided Global Positive Loss ({TGPL}) to reduce the disparity between the final predictions and the original model outputs, and (3) an adaptive rounding-to-nearest operation. Extensive experiments demonstrate that our {LiDAR-PTQ} can achieve state-of-the-art quantization performance when applied to {CenterPoint} (both Pillar-based and Voxel-based). To our knowledge, for the very first time in lidar-based {3D} detection tasks, the {PTQ} {INT8} model's accuracy is almost the same as the {FP32} model while enjoying {3√ó} inference speedup. Moreover, our {LiDAR-PTQ} is cost-effective being {30√ó} faster than the quantization-aware training method.},
  author = {Sifan Zhou and Liang Li and Xinyu Zhang and Bo Zhang and Shipeng Bai and Miao Sun and Ziyu Zhao and Xiaobo Lu and Xiangxiang Chu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024lidarptq.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=0d1gQI114C},
  publisher = {OpenReview.net},
  title = {{LiDAR-PTQ}: Post-Training Quantization for Point Cloud {3D} Object Detection},
  url = {https://openreview.net/forum?id=0d1gQI114C},
  year = {2024}
}

@inproceedings{zhou2024universalner,
  abstract = {Large language models ({LLMs}) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling {LLMs} into more cost-efficient models such as {Alpaca} and {Vicuna}. Yet such student models still trail the original {LLMs} by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition ({NER}) for case study, we show how {ChatGPT} can be distilled into much smaller {UniversalNER} models for open {NER}. For evaluation, we assemble the largest {NER} benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, {UniversalNER} attains remarkable {NER} accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as {Alpaca} and {Vicuna} by over 30 absolute {F1} points in average. With a tiny fraction of parameters, {UniversalNER} not only acquires {ChatGPT}'s capability in recognizing arbitrary entity types, but also outperforms its {NER} accuracy by 7--9 absolute {F1} points in average. Remarkably, {UniversalNER} even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as {InstructUIE}, which uses supervised {NER} examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and {UniversalNER} models to facilitate future research on targeted distillation.},
  author = {Wenxuan Zhou and Sheng Zhang and Yu Gu and Muhao Chen and Hoifung Poon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024universalner.pdf:pdf},
  note = {DBLP last modified: 2025-06-26},
  pdf = {https://openreview.net/pdf?id=r65xfUb76p},
  publisher = {OpenReview.net},
  title = {{UniversalNER}: Targeted Distillation from Large Language Models for Open Named Entity Recognition},
  url = {https://openreview.net/forum?id=r65xfUb76p},
  year = {2024}
}

@inproceedings{zhou2024exploring,
  abstract = {We investigate a challenging task of nighttime optical flow, which suffers from weakened texture and amplified noise. These degradations weaken discriminative visual features, thus causing invalid motion feature matching. Typically, existing methods employ domain adaptation to transfer knowledge from auxiliary domain to nighttime domain in either input visual space or output motion space. However, this direct adaptation is ineffective, since there exists a large domain gap due to the intrinsic heterogeneous nature of the feature representations between auxiliary and nighttime domains. To overcome this issue, we explore a common-latent space as the intermediate bridge to reinforce the feature alignment between auxiliary and nighttime domains. In this work, we exploit two auxiliary daytime and event domains, and propose a novel common appearance-boundary adaptation framework for nighttime optical flow.},
  author = {Hanyu Zhou and Yi Chang and Haoyue Liu and Wending Yan and Yuxing Duan and Zhiwei Shi and Luxin Yan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024exploring.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=776lhoaulC},
  publisher = {OpenReview.net},
  title = {Exploring the Common Appearance-Boundary Adaptation for Nighttime Optical Flow},
  url = {https://openreview.net/forum?id=776lhoaulC},
  year = {2024}
}

@inproceedings{zhou2024sotopia,
  abstract = {Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, {AI} systems' abilities in this realm remain elusive. We present {SOTOPIA}, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between {LLM}-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called {SOTOPIA-Eval}. With {SOTOPIA}, we find significant differences between these models in terms of their social intelligence, and we identify a subset of {SOTOPIA} scenarios, {SOTOPIA-hard}, that is generally challenging for all models. We find that on this subset, {GPT-4} achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate {SOTOPIA}'s promise as a general platform for research on evaluating and improving social intelligence in artificial agents.},
  author = {Xuhui Zhou and Hao Zhu and Leena Mathur and Ruohong Zhang and Haofei Yu and Zhengyang Qi and Louis-Philippe Morency and Yonatan Bisk and Daniel Fried and Graham Neubig and Maarten Sap},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024sotopia.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=mM7VurbA4r},
  publisher = {OpenReview.net},
  title = {{SOTOPIA}: Interactive Evaluation for Social Intelligence in Language Agents},
  url = {https://openreview.net/forum?id=mM7VurbA4r},
  year = {2024}
}

@inproceedings{zhou2024pareto,
  abstract = {Deep long-tailed recognition ({DTLR}) has attracted much attention due to its close touch with realistic scenarios. Recent advances have focused on re-balancing across various aspects, e.g., sampling strategy, loss re-weighting, logit adjustment, and input/parameter perturbation, to name a few. However, few studies have considered dynamic re-balancing to address intrinsic optimization conflicts. In this paper, we first empirically argue that the optimizations of mainstream {DLTR} methods are still dominated by some categories (e.g., major) due to a fixed re-balancing strategy. Thus, they fail to deal with gradient conflicts among categories, which naturally deduces the motivation for reaching {Pareto} optimal solutions. Unfortunately, a naive integration of multi-objective optimization ({MOO}) with {DLTR} methods is not applicable due to the gap between multi-task learning ({MTL}) and {DLTR}, and can in turn lead to class-specific feature degradation. Thus, we provide effective alternatives by decoupling {MOO}-based {MTL} from the temporal rather than structure perspective, and enhancing it via optimizing variability collapse loss motivated by the derived {MOO}-based {DLTR} generalization bound. Moreover, we resort to anticipating worst-case optimization with theoretical insights to further ensure convergence. We build a {Pareto} deep long-tailed recognition method termed {PLOT} upon the proposed {MOO} framework. Extensive evaluations demonstrate that our method not only generally improves mainstream pipelines, but also achieves an augmented version to realize state-of-the-art performance across multiple benchmarks.},
  author = {Zhipeng Zhou and Liu Liu and Peilin Zhao and Wei Gong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024pareto.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=b66P1u0k15},
  publisher = {OpenReview.net},
  title = {{Pareto} Deep Long-Tailed Recognition: A Conflict-Averse Solution},
  url = {https://openreview.net/forum?id=b66P1u0k15},
  year = {2024}
}

@inproceedings{zhou2024towards,
  abstract = {Answering counterfactual queries has important applications such as explainability, robustness, and fairness but is challenging when the causal variables are unobserved and the observations are non-linear mixtures of these latent variables, such as pixels in images. One approach is to recover the latent Structural Causal Model ({SCM}), which may be infeasible in practice due to requiring strong assumptions, e.g., linearity of the causal mechanisms or perfect atomic interventions. Meanwhile, more practical {ML}-based approaches using naive domain translation models to generate counterfactual samples lack theoretical grounding and may construct invalid counterfactuals. In this work, we strive to strike a balance between practicality and theoretical guarantees by analyzing a specific type of causal query called \emph{domain counterfactuals}, which hypothesizes what a sample would have looked like if it had been generated in a different domain (or environment). We show that recovering the latent {SCM} is unnecessary for estimating domain counterfactuals, thereby sidestepping some of the theoretic challenges. By assuming invertibility and sparsity of intervention, we prove domain counterfactual estimation error can be bounded by a data fit term and intervention sparsity term. Building upon our theoretical results, we develop a theoretically grounded practical algorithm that simplifies the modeling process to generative model estimation under autoregressive and shared parameter constraints that enforce intervention sparsity. Finally, we show an improvement in counterfactual estimation over baseline methods through extensive simulated and image-based experiments.},
  author = {Zeyu Zhou and Ruqi Bai and Sean Kulinski and Murat Kocaoglu and David I. Inouye},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024towards.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=v1VvCWJAL8},
  publisher = {OpenReview.net},
  title = {Towards Characterizing Domain Counterfactuals for Invertible Latent Causal Models},
  url = {https://openreview.net/forum?id=v1VvCWJAL8},
  year = {2024}
}

@inproceedings{zhou2024what,
  abstract = {Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when {Transformer} models can learn the true algorithm for solving a task. We study the scope of {Transformers}' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how {Transformers} can exhibit strong length generalization on a given task. Specifically, we leverage {RASP} -- a programming language designed for the computational model of a {Transformer} -- and introduce the {RASP-Generalization Conjecture}: {Transformers} tend to length generalize on a task if the task can be solved by a short {RASP} program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of {Transformers}.},
  author = {Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and Omid Saremi and Joshua M. Susskind and Samy Bengio and Preetum Nakkiran},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024what.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=AssIuHnmHX},
  publisher = {OpenReview.net},
  title = {What Algorithms can {Transformers} Learn? {A} Study in Length Generalization},
  url = {https://openreview.net/forum?id=AssIuHnmHX},
  year = {2024}
}

@inproceedings{zhou2024predictive,
  abstract = {Intelligent tutoring systems optimize the selection and timing of learning materials to enhance understanding and long-term retention. This requires estimates of both the learner's progress (``knowledge tracing''; {KT}), and the prerequisite structure of the learning domain (``knowledge mapping''). While recent deep learning models achieve high {KT} accuracy, they do so at the expense of the interpretability of psychologically-inspired models. In this work, we present a solution to this trade-off. {PSI-KT} is a hierarchical generative approach that explicitly models how both individual cognitive traits and the prerequisite structure of knowledge influence learning dynamics, thus achieving interpretability by design. Moreover, by using scalable {Bayesian} inference, {PSI-KT} targets the real-world need for efficient personalization even with a growing body of learners and interaction data. Evaluated on three datasets from online learning platforms, {PSI-KT} achieves superior multi-step predictive accuracy and scalable inference in continual-learning settings, all while providing interpretable representations of learner-specific traits and the prerequisite structure of knowledge that causally supports learning. In sum, predictive, scalable and interpretable knowledge tracing with solid knowledge mapping lays a key foundation for effective personalized learning to make education accessible to a broad, global audience.},
  author = {Hanqi Zhou and Robert Bamler and Charley M. Wu and √Ålvaro Tejero-Cantero},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024predictive.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=NgaLU2fP5D},
  publisher = {OpenReview.net},
  title = {Predictive, scalable and interpretable knowledge tracing on structured domains},
  url = {https://openreview.net/forum?id=NgaLU2fP5D},
  year = {2024}
}

@inproceedings{zhou2024hazard,
  abstract = {Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called {HAZARD}, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. {HAZARD} consists of three unexpected disaster scenarios, including fire, flood, and wind, and specifically supports the utilization of large language models ({LLMs}) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents' decision-making capabilities across various pipelines, including reinforcement learning ({RL}), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an {LLM}-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks.},
  author = {Qinhong Zhou and Sunli Chen and Yisong Wang and Haozhe Xu and Weihua Du and Hongxin Zhang and Yilun Du and Joshua B. Tenenbaum and Chuang Gan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024hazard.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=n6mLhaBahJ},
  publisher = {OpenReview.net},
  title = {{HAZARD} Challenge: Embodied Decision Making in Dynamically Changing Environments},
  url = {https://openreview.net/forum?id=n6mLhaBahJ},
  year = {2024}
}

@inproceedings{zhou2024decompopt,
  abstract = {Recently, {3D} generative models have shown promising performances in structure-based drug design by learning to generate ligands given target binding sites. However, only modeling the target-ligand distribution can hardly fulfill one of the main goals in drug discovery -- designing novel ligands with desired properties, e.g., high binding affinity, easily synthesizable, etc. This challenge becomes particularly pronounced when the target-ligand pairs used for training do not align with these desired properties. Moreover, most existing methods aim at solving de novo design task, while many generative scenarios requiring flexible controllability, such as {R}-group optimization and scaffold hopping, have received little attention. In this work, we propose {DecompOpt}, a structure-based molecular optimization method based on a controllable and decomposed diffusion model. {DecompOpt} presents a new generation paradigm which combines optimization with conditional diffusion models to achieve desired properties while adhering to the molecular grammar. Additionally, {DecompOpt} offers a unified framework covering both de novo design and controllable generation. To achieve so, ligands are decomposed into substructures which allows fine-grained control and local optimization. Experiments show that {DecompOpt} can efficiently generate molecules with improved properties than strong de novo baselines, and demonstrate great potential in controllable generation tasks.},
  author = {Xiangxin Zhou and Xiwei Cheng and Yuwei Yang and Yu Bao and Liang Wang and Quanquan Gu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024decompopt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Y3BbxvAQS9},
  publisher = {OpenReview.net},
  title = {{DecompOpt}: Controllable and Decomposed Diffusion Models for Structure-based Molecular Optimization},
  url = {https://openreview.net/forum?id=Y3BbxvAQS9},
  year = {2024}
}

@inproceedings{zhou2024analyzing,
  abstract = {Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23\% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top.},
  author = {Yiyang Zhou and Chenhang Cui and Jaehong Yoon and Linjun Zhang and Zhun Deng and Chelsea Finn and Mohit Bansal and Huaxiu Yao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024analyzing.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oZDJKTlOUe},
  publisher = {OpenReview.net},
  title = {Analyzing and Mitigating Object Hallucination in Large Vision-Language Models},
  url = {https://openreview.net/forum?id=oZDJKTlOUe},
  year = {2024}
}

@inproceedings{zhou2024clipmused,
  abstract = {The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. It also incorporates learnable subject-specific tokens that facilitates the aggregation of multi-subject data without a linear increase of parameters. Additionally, we employ representational similarity analysis (RSA) to guide token representation learning based on the topological relationship of visual stimuli in the representation space of CLIP, enabling full characterization of the relationship between neural responses of different subjects under different stimuli. Finally, token representations are used for multi-subject semantic decoding. Our proposed method outperforms single-subject decoding methods and achieves state-of-the-art performance among the existing multi-subject methods on two fMRI datasets. Visualization results provide insights into the effectiveness of our proposed method.},
  author = {Qiongyi Zhou and Changde Du and Shengpei Wang and Huiguang He},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024clipmused.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=lKxL5zkssv},
  publisher = {OpenReview.net},
  title = {CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding},
  url = {https://openreview.net/forum?id=lKxL5zkssv},
  year = {2024}
}

@inproceedings{zhou2024learning,
  abstract = {Bilevel programs (BPs) find a wide range of applications in fields such as energy, transportation, and machine learning. As compared to BPs with continuous (linear/convex) optimization problems in both levels, the BPs with discrete decision variables have received much less attention, largely due to the ensuing computational intractability and the incapability of gradient-based algorithms for handling discrete optimization formulations. In this paper, we develop deep learning techniques to address this challenge. Specifically, we consider a BP with binary tender, wherein the upper and lower levels are linked via binary variables. We train a neural network to approximate the optimal value of the lower-level problem, as a function of the binary tender. Then, we obtain a single-level reformulation of the BP through a mixed-integer representation of the value function. Furthermore, we conduct a comparative analysis between two types of neural networks: general neural networks and the novel input supermodular neural networks, studying their representational capacities.},
  author = {Bo Zhou and Ruiwei Jiang and Siqian Shen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PsDFgTosqb},
  publisher = {OpenReview.net},
  title = {Learning to Solve Bilevel Programs with Binary Tender},
  url = {https://openreview.net/forum?id=PsDFgTosqb},
  year = {2024}
}

@inproceedings{zhou2024zeromean,
  abstract = {Contrastive learning has emerged as a popular paradigm of self-supervised learning that learns representations by encouraging representations of positive pairs to be similar while representations of negative pairs to be far apart. The spectral contrastive loss, in synergy with the notion of positive-pair graphs, offers valuable theoretical insights into the empirical successes of contrastive learning. In this paper, we propose incorporating an additive factor into the term of spectral contrastive loss involving negative pairs. This simple modification can be equivalently viewed as introducing a regularization term that enforces the mean of representations to be zero, which thus is referred to as zero-mean regularization. It intuitively relaxes the orthogonality of representations between negative pairs and implicitly alleviates the adverse effect of wrong connections in the positive-pair graph, leading to better performance and robustness. To clarify this, we thoroughly investigate the role of zero-mean regularized spectral contrastive loss in both unsupervised and supervised scenarios with respect to theoretical analysis and quantitative evaluation. These results highlight the potential of zero-mean regularized spectral contrastive learning to be a promising approach in various tasks.},
  author = {Xiong Zhou and Xianming Liu and Feilong Zhang and Gang Wu and Deming Zhai and Junjun Jiang and Xiangyang Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024zeromean.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RZBy8oHTz4},
  publisher = {OpenReview.net},
  title = {Zero-Mean Regularized Spectral Contrastive Learning: Implicitly Mitigating Wrong Connections in Positive-Pair Graphs},
  url = {https://openreview.net/forum?id=RZBy8oHTz4},
  year = {2024}
}

@inproceedings{zhou2024denoising,
  abstract = {Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score.},
  author = {Linqi Zhou and Aaron Lou and Samar Khanna and Stefano Ermon},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024denoising.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=FKksTayvGo},
  publisher = {OpenReview.net},
  title = {Denoising Diffusion Bridge Models},
  url = {https://openreview.net/forum?id=FKksTayvGo},
  year = {2024}
}

@inproceedings{zhou2024distillspec,
  abstract = {Speculative decoding (SD) accelerates large language model inference by employing a faster draft model for generating multiple tokens, which are then verified in parallel by the larger target model, resulting in the text generated according to the target model distribution. However, identifying a compact draft model that is well-aligned with the target model is challenging. To tackle this issue, we propose DistillSpec that uses knowledge distillation to better align the draft model with the target model, before applying SD. DistillSpec makes two key design choices, which we demonstrate via systematic study to be crucial to improve the draft and target alignment: utilizing on-policy data generation from the draft model, and tailoring the divergence function to the task and decoding strategy. Notably, DistillSpec yields impressive 10-45\% speedups over standard SD on a range of standard benchmarks, using both greedy and non-greedy sampling.},
  author = {Yongchao Zhou and Kaifeng Lyu and Ankit Singh Rawat and Aditya Krishna Menon and Afshin Rostamizadeh and Sanjiv Kumar and Jean-Fran√ßois Kagy and Rishabh Agarwal},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024distillspec.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=rsY6J3ZaTF},
  publisher = {OpenReview.net},
  title = {DistillSpec: Improving Speculative Decoding via Knowledge Distillation},
  url = {https://openreview.net/forum?id=rsY6J3ZaTF},
  year = {2024}
}

@inproceedings{zhou2024retrievalbased,
  abstract = {Disentangled representation learning remains challenging as the underlying factors of variation in the data do not naturally exist. The inherent complexity of real-world data makes it unfeasible to exhaustively enumerate and encapsulate all its variations within a finite set of factors. However, it is worth noting that most real-world data have linguistic equivalents, typically in the form of textual descriptions. These linguistic counterparts can represent the data and effortlessly decomposed into distinct tokens. In light of this, we present Vocabulary Disentangled Retrieval (VDR), a retrieval-based framework that harnesses natural language as proxies of the underlying data variation to drive disentangled representation learning. Our approach employ a bi-encoder model to represent both data and natural language in a vocabulary space, enabling the model to distinguish dimensions that capture intrinsic characteristics within data through its natural language counterpart, thus facilitating disentanglement.},
  author = {Jiawei Zhou and Xiaoguang Li and Lifeng Shang and Xin Jiang and Qun Liu and Lei Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024retrievalbased.pdf:pdf},
  note = {DBLP last modified: 2024-08-19},
  pdf = {https://openreview.net/pdf?id=ZlQRiFmq7Y},
  publisher = {OpenReview.net},
  title = {Retrieval-based Disentangled Representation Learning with Natural Language Supervision},
  url = {https://openreview.net/forum?id=ZlQRiFmq7Y},
  year = {2024}
}

@inproceedings{zhou2024varianceenlarged,
  abstract = {Graph-based semi-supervised learning, particularly in the context of extremely sparse labeled data, often suffers from degenerate solutions where label functions tend to be nearly constant across unlabeled data. In this paper, we introduce Variance-enlarged Poisson Learning (VPL), a simple yet powerful framework tailored to alleviate the issues arising from the presence of degenerate solutions. VPL incorporates a variance-enlarged regularization term, which induces a Poisson equation specifically for unlabeled data. Furthermore, we broaden the scope of VPL to encompass graph neural networks, introducing Variance-enlarged Graph Poisson Networks (V-GPN) to facilitate improved label propagation. To achieve a deeper understanding of VPL's behavior, we conduct a comprehensive theoretical exploration in both discrete and variational cases. Our findings elucidate that VPL inherently amplifies the importance of connections within the same class while concurrently tempering those between different classes. We support our claims with extensive experiments, demonstrating the effectiveness of VPL and showcasing its superiority over existing methods.},
  author = {Xiong Zhou and Xianming Liu and Hao Yu and Jialiang Wang and Zeke Xie and Junjun Jiang and Xiangyang Ji},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024varianceenlarged.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=yeeVBMDAwy},
  publisher = {OpenReview.net},
  title = {Variance-enlarged Poisson Learning for Graph-based Semi-Supervised Learning with Extremely Sparse Labeled Data},
  url = {https://openreview.net/forum?id=yeeVBMDAwy},
  year = {2024}
}

@inproceedings{zhou2024anomalyclip,
  abstract = {Zero-shot anomaly detection (ZSAD) requires detection models trained using auxiliary data to detect anomalies without any training sample in a target dataset. It is a crucial task when training data is not accessible due to various concerns, e.g., data privacy, yet it is challenging since the models need to generalize to anomalies across different domains where the appearance of foreground objects, abnormal regions, and background features, such as defects/tumors on different products/organs, can vary significantly. Recently large pre-trained vision-language models (VLMs), such as CLIP, have demonstrated strong zero-shot recognition ability in various vision tasks, including anomaly detection. The key insight of AnomalyCLIP is to learn object-agnostic text prompts that capture generic normality and abnormality in an image regardless of its foreground objects. This allows our model to focus on the abnormal image regions rather than the object semantics, enabling generalized normality and abnormality recognition on diverse types of objects. Large-scale experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP achieves superior zero-shot performance of detecting and segmenting anomalies in datasets of highly diverse class semantics from various defect inspection and medical imaging domains.},
  author = {Qihang Zhou and Guansong Pang and Yu Tian and Shibo He and Jiming Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024anomalyclip.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=buC4E91xZE},
  publisher = {OpenReview.net},
  title = {AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection},
  url = {https://openreview.net/forum?id=buC4E91xZE},
  year = {2024}
}

@inproceedings{zhou2024offline,
  abstract = {Hybrid RL is the setting where an RL agent has access to both offline data and online data by interacting with the real-world environment. In this work, we propose a new hybrid RL algorithm that combines an on-policy actor-critic method with offline data. On-policy methods such as policy gradient and natural policy gradient (NPG) have shown to be more robust to model misspecification, though sometimes it may not be as sample efficient as methods that rely on off-policy learning. On the other hand, offline methods that depend on off-policy training often require strong assumptions in theory and are less stable to train in practice. Our new approach integrates a procedure of off-policy training on the offline data into an on-policy NPG framework. The paper shows that their approach can obtain a "best-of-both-worlds" result - it achieves the state-of-the-art theoretical guarantees of offline RL when offline RL-specific assumptions hold, while maintaining the theoretical guarantees of on-policy NPG regardless of the offline RL assumptions' validity. Experimentally, in challenging rich-observation environments, the approach outperforms a state-of-the-art hybrid RL baseline which only relies on off-policy policy optimization.},
  author = {Yifei Zhou and Ayush Sekhari and Yuda Song and Wen Sun},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024offline.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=RMgqvQGTwH},
  publisher = {OpenReview.net},
  title = {Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees},
  url = {https://openreview.net/forum?id=RMgqvQGTwH},
  year = {2024}
}

@inproceedings{zhou2024dont,
  abstract = {Large language models ({LLM}), such as Google's Minerva and {OpenAI}'s {GPT} families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. This paper leverages the fact that if the training corpus of {LLMs} contained sufficiently many examples of formal mathematics, they can be prompted to autoformalize informal mathematical statements into formal Isabelle code -- which provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We introduce a method called Don't Trust: Verify ({DTV}) that autoformalizates {LLM}-generated solutions and uses formal theorem proving to verify their correctness. {DTV} outperforms vanilla majority voting by more than 12\% on {GSM}8K.},
  author = {Jin Peng Zhou and Charles E. Staats and Wenda Li and Christian Szegedy and Kilian Q. Weinberger and Yuhuai Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/jinpz/dtv},
  file = {:/home/b/documents/inproceedings/zhou2024dont.pdf:pdf},
  keywords = {Mathematical reasoning, Autoformalization, Automated theorem proving, Quantitative reasoning},
  note = {Primary Area: neurosymbolic \& hybrid AI systems; DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=V5tdi14ple},
  publisher = {OpenReview.net},
  title = {Don't Trust: Verify -- Grounding {LLM} Quantitative Reasoning with Autoformalization},
  url = {https://openreview.net/forum?id=V5tdi14ple},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024agnostic,
  abstract = {We study the cost of overfitting in noisy kernel ridge regression ({KRR}), which we define as the ratio between the test error of the interpolating ridgeless model and the test error of the optimally-tuned model. We take an agnostic view in the following sense: we consider the cost as a function of sample size for any target function, even if the sample size is not large enough for consistency or the target is outside the {RKHS}. The analysis of the cost of overfitting is conducted under a Gaussian universality ansatz using recently derived (non-rigorous) risk estimates in terms of the task eigenstructure. Our analysis provides a more refined characterization of benign, tempered and catastrophic overfitting.},
  archiveprefix = {arXiv},
  author = {Lijia Zhou and James B. Simon and Gal Vardi and Nathan Srebro},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2306.13185},
  file = {:/home/b/documents/inproceedings/zhou2024agnostic.pdf:pdf},
  keywords = {kernel ridge regression, cost of overfitting, benign overfitting, tempered overfitting},
  note = {Primary Area: Learning Theory; DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=YrTI2Zu0dd},
  publisher = {OpenReview.net},
  title = {An Agnostic View on the Cost of Overfitting in ({K}ernel) {R}idge {R}egression},
  url = {https://openreview.net/forum?id=YrTI2Zu0dd},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024vulnerability,
  abstract = {This paper discloses that adversarially trained models are vulnerable to two-faced attacks, where slight perturbations in input features are crafted to make the model exhibit a false sense of robustness in the verification phase. This threat is significantly important as it can mislead evaluation of the adversarial robustness of models, which could cause unpredictable security issues when deploying substandard models in reality. The threat appears to be pervasive and tricky: many types of models suffer from this threat, and models with higher adversarial robustness tend to be more vulnerable. The paper provides the first attempt to formulate this threat, disclose its relationships with adversarial risk, and try to circumvent it via a simple countermeasure. These findings serve as a crucial reminder for practitioners to exercise caution in the verification phase, urging them to refrain from blindly trusting the exhibited adversarial robustness of models.},
  author = {Shengjie Zhou and Lue Tao and Yuzhou Cao and Tao Xiang and Bo An and Lei Feng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024vulnerability.pdf:pdf},
  keywords = {adversarial training, adversarial attacks, adversarial robustness, security evaluation},
  note = {Primary Area: alignment, safety, fairness, privacy, and societal considerations; DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=mXpNp8MMr5},
  publisher = {OpenReview.net},
  title = {On the Vulnerability of Adversarially Trained Models Against Two-faced Attacks},
  url = {https://openreview.net/forum?id=mXpNp8MMr5},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024refactor,
  abstract = {Human mathematicians are skilled at recognizing modular and reusable theorems that make complex mathematical reasoning more tractable. In this paper, we propose a method called theoREm-from-prooF extrACTOR ({REFACTOR}) for training neural networks to mimic this ability in formal mathematical theorem proving. On a set of unseen proofs, {REFACTOR} is able to extract 19.6\% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, {REFACTOR} extracted 16 new theorems. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. The prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently leveraging a diverse set of newly extracted theorems.},
  archiveprefix = {arXiv},
  author = {Jin Peng Zhou and Yuhuai Wu and Qiyang Li and Roger Baker Grosse},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/jinpz/refactor},
  eprint = {2402.17032},
  file = {:/home/b/documents/inproceedings/zhou2024refactor.pdf:pdf},
  keywords = {theorem proving, neural networks, mathematical reasoning, proof extraction},
  note = {Equal contribution: Jin Peng Zhou, Yuhuai Wu; DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=fgKjiVrm6u},
  publisher = {OpenReview.net},
  title = {{REFACTOR}: Learning to Extract Theorems from Proofs},
  url = {https://openreview.net/forum?id=fgKjiVrm6u},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024solving,
  abstract = {This paper introduces a novel and effective prompting method, explicit code-based self-verification ({CSV}), to further boost the mathematical reasoning potential of {GPT}-4 Code Interpreter. This method employs a zero-shot prompt on {GPT}-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. We found that {GPT}-4 Code Interpreter's success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. With {GPT}-4 Code Interpreter and {CSV}, we achieve an impressive zero-shot accuracy on {MATH} dataset (53.9\% ‚Üí 84.3\%).},
  archiveprefix = {arXiv},
  author = {Aojun Zhou and Ke Wang and Zimu Lu and Weikang Shi and Sichun Luo and Zipeng Qin and Shaoqing Lu and Anya Jia and Linqi Song and Mingjie Zhan and Hongsheng Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/mathllm/MathCoder},
  eprint = {2308.07921},
  file = {:/home/b/documents/inproceedings/zhou2024solving.pdf:pdf},
  keywords = {mathematical reasoning, code generation, self-verification, large language models},
  note = {Primary Area: applications to physical sciences; DBLP last modified: 2025-04-08},
  pdf = {https://openreview.net/pdf?id=c8McWs4Av0},
  publisher = {OpenReview.net},
  title = {Solving Challenging Math Word Problems Using {GPT}-4 Code Interpreter with Code-based Self-Verification},
  url = {https://openreview.net/forum?id=c8McWs4Av0},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024uni3d,
  abstract = {{Uni3D} is a 3D foundation model that explores unified 3D representation at scale. {Uni3D} uses a 2D initialized {ViT} that is end-to-end pretrained to align 3D point cloud features with image-text aligned features. Via this simple architecture and pretext task, {Uni3D} can leverage abundant 2D pretrained models as initialization and image-text aligned models as the target, unlocking the great potential of 2D model zoos and scaling-up strategies to the 3D world. The framework efficiently scales up to one billion parameters and sets new records on a broad range of 3D tasks, such as zero-shot classification, few-shot classification, open-world understanding and zero-shot part segmentation. The strong {Uni3D} representation enables applications such as 3D painting and retrieval in the wild.},
  archiveprefix = {arXiv},
  author = {Junsheng Zhou and Jinsheng Wang and Baorui Ma and Yu-Shen Liu and Tiejun Huang and Xinlong Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/baaivision/Uni3D},
  eprint = {2310.06773},
  file = {:/home/b/documents/inproceedings/zhou2024uni3d.pdf:pdf},
  keywords = {3D representation learning, foundation model, point cloud, vision transformer, multimodal learning},
  note = {Spotlight presentation; Primary Area: representation learning for computer vision; DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=wcaE4Dfgt8},
  publisher = {OpenReview.net},
  title = {{Uni3D}: Exploring Unified {3D} Representation at Scale},
  url = {https://openreview.net/forum?id=wcaE4Dfgt8},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024batch,
  abstract = {Prompting and in-context learning ({ICL}) have become efficient learning paradigms for large language models ({LLMs}), but {LLMs} suffer from prompt brittleness and various bias factors including formatting, choice verbalizers, and {ICL} examples, leading to unexpected performance degradation. We conduct a systematic analysis of existing calibration methods, providing a unified view and revealing failure cases, then propose Batch Calibration ({BC}), a simple yet intuitive method that mitigates bias from a batch of inputs and unifies various prior approaches. {BC} is zero-shot, self-adaptive (inference-only), and incurs negligible additional costs. We validate {BC} with {PaLM} 2 and {CLIP} models, demonstrating state-of-the-art performance over previous calibration baselines across more than 10 natural language understanding and image classification tasks.},
  archiveprefix = {arXiv},
  author = {Han Zhou and Xingchen Wan and Lev Proleev and Diana Mincu and Jilin Chen and Katherine A. Heller and Subhrajit Roy},
  booktitle = {The Twelfth International Conference on Learning Representations},
  eprint = {2309.17249},
  file = {:/home/b/documents/inproceedings/zhou2024batch.pdf:pdf},
  keywords = {calibration, in-context learning, prompt engineering, large language models, bias mitigation},
  note = {Primary Area: foundation or frontier models, including LLMs; DBLP last modified: 2024-10-06},
  pdf = {https://openreview.net/pdf?id=L3FHMoKZcS},
  publisher = {OpenReview.net},
  title = {Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering},
  url = {https://openreview.net/forum?id=L3FHMoKZcS},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024webarena,
  abstract = {{WebArena} is a highly realistic and reproducible environment for language-guided agents that focuses on agents performing tasks on the web. The environment features fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. The environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving, and includes a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The results demonstrate significant challenges in solving complex tasks: our best {GPT}-4-based agent only achieves an end-to-end task success rate of 14.41\%, significantly lower than human performance of 78.24\%.},
  archiveprefix = {arXiv},
  author = {Shuyan Zhou and Frank F. Xu and Hao Zhu and Xuhui Zhou and Robert Lo and Abishek Sridhar and Xianyi Cheng and Tianyue Ou and Yonatan Bisk and Daniel Fried and Uri Alon and Graham Neubig},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/web-arena-x/webarena},
  eprint = {2307.13854},
  file = {:/home/b/documents/inproceedings/zhou2024webarena.pdf:pdf},
  keywords = {autonomous agents, web environment, benchmark, language-guided agents, reinforcement learning},
  note = {Primary Area: datasets and benchmarks; DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=oKn9c6ytLx},
  publisher = {OpenReview.net},
  title = {{WebArena}: A Realistic Web Environment for Building Autonomous Agents},
  url = {https://openreview.net/forum?id=oKn9c6ytLx},
  venue = {ICLR},
  website = {https://webarena.dev/},
  year = {2024}
}

@inproceedings{zhou2024phylogfn,
  abstract = {Phylogenetics is a branch of computational biology that studies the evolutionary relationships among biological entities. Its long history and numerous applications notwithstanding, inference of phylogenetic trees from sequence data remains challenging: the high complexity of tree space poses a significant obstacle for the current combinatorial and probabilistic techniques. We adopt the framework of generative flow networks ({GFlowNets}) to tackle two core problems in phylogenetics: parsimony-based and Bayesian phylogenetic inference. Because {GFlowNets} are well-suited for sampling complex combinatorial structures, they are a natural choice for exploring and sampling from the multimodal posterior distribution over tree topologies and evolutionary distances. {PhyloGFN} produces diverse and high-quality evolutionary hypotheses on real benchmark datasets. {PhyloGFN} is competitive with prior works in marginal likelihood estimation and achieves a closer fit to the target distribution than state-of-the-art variational inference methods.},
  archiveprefix = {arXiv},
  author = {Ming-Yang Zhou and Zichao Yan and Elliot Layne and Nikolay Malkin and Dinghuai Zhang and Moksh Jain and Mathieu Blanchette and Yoshua Bengio},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/zmy1116/phylogfn},
  eprint = {2310.08774},
  file = {:/home/b/documents/inproceedings/zhou2024phylogfn.pdf:pdf},
  keywords = {phylogenetic inference, generative flow networks, computational biology, Bayesian inference, tree topology},
  note = {Primary Area: applications to physical sciences; DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hB7SlfEmze},
  publisher = {OpenReview.net},
  title = {{PhyloGFN}: Phylogenetic inference with generative flow networks},
  url = {https://openreview.net/forum?id=hB7SlfEmze},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024less,
  abstract = {To deduce new facts on a knowledge graph ({KG}), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole {KG} for prediction, which hinders their promise on large scale {KGs} and cannot be directly addressed by vanilla sampling methods. In this paper, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction. The design principle is that, instead of directly acting on the whole {KG}, the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph. We reveal that the non-parametric and computation-efficient heuristics Personalized {PageRank} ({PPR}) can effectively identify the potential answers and supporting evidence. With efficient subgraph-based prediction, we further introduce the automated searching of the optimal configurations in both data and model spaces. Empirically, we achieve promoted efficiency and leading performances on five large-scale benchmarks.},
  archiveprefix = {arXiv},
  author = {Zhanke Zhou and Yongqi Zhang and Jiangchao Yao and Quanming Yao and Bo Han},
  booktitle = {The Twelfth International Conference on Learning Representations},
  code = {https://github.com/tmlr-group/one-shot-subgraph},
  eprint = {2403.10231},
  file = {:/home/b/documents/inproceedings/zhou2024less.pdf:pdf},
  keywords = {knowledge graphs, link prediction, subgraph reasoning, scalability, personalized pagerank},
  note = {Primary Area: learning on graphs and other geometries; DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=QHROe7Mfcb},
  publisher = {OpenReview.net},
  title = {Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs},
  url = {https://openreview.net/forum?id=QHROe7Mfcb},
  venue = {ICLR},
  year = {2024}
}

@inproceedings{zhou2024free,
  abstract = {Off-policy dynamic programming ({DP}) techniques such as $Q$-learning have proven to be important in sequential decision-making problems. In the presence of function approximation, however, these techniques often diverge due to the absence of {Bellman} completeness in the function classes considered, a crucial condition for the success of {DP}-based methods. In this paper, we show how off-policy learning techniques based on return-conditioned supervised learning ({RCSL}) are able to circumvent these challenges of {Bellman} completeness. We prove there exists a natural environment in which if one uses two-layer multilayer perceptron as the function approximator, the layer width needs to grow linearly with the state space size to satisfy {Bellman} completeness while a constant layer width is enough for {RCSL}. These findings take a step towards explaining the superior empirical performance of {RCSL} methods compared to {DP}-based methods in environments with near-optimal datasets. Furthermore, in order to learn from sub-optimal datasets, we propose a simple framework called {MBRCSL}, granting {RCSL} methods the ability of dynamic programming to stitch together segments from distinct trajectories. {MBRCSL} leverages learned dynamics models and forward sampling to accomplish trajectory stitching while avoiding the need for {Bellman} completeness that plagues all dynamic programming algorithms.},
  address = {Vienna, Austria},
  author = {Zhaoyi Zhou and Chuning Zhu and Runlong Zhou and Qiwen Cui and Abhishek Gupta and Simon Shaolei Du},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhou2024free.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=7zY781bMDO},
  publisher = {OpenReview.net},
  title = {Free from {Bellman} Completeness: Trajectory Stitching via Model-based Return-conditioned Supervised Learning},
  url = {https://openreview.net/forum?id=7zY781bMDO},
  year = {2024}
}

@inproceedings{zhu2024pose,
  abstract = {Large Language Models ({LLMs}) are trained with a pre-defined context length, restricting their use in scenarios requiring long inputs. Previous efforts for adapting {LLMs} to a longer length usually requires fine-tuning with this target length (Full-length fine-tuning), suffering intensive training cost. To decouple train length from target length for efficient context window extension, we propose Positional Skip-wise ({PoSE}) training that smartly simulates long inputs using a fixed context window. This is achieved by first dividing the original context window into several chunks, then designing distinct skipping bias terms to manipulate the position indices of each chunk. These bias terms and the lengths of each chunk are altered for every training example, allowing the model to adapt to all positions within target length. Experimental results show that {PoSE} greatly reduces memory and time overhead compared with Full-length fine-tuning, with minimal impact on performance.},
  address = {Vienna, Austria},
  author = {Dawei Zhu and Nan Yang and Liang Wang and Yifan Song and Wenhao Wu and Furu Wei and Sujian Li},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024pose.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2025-02-11},
  pdf = {https://openreview.net/pdf?id=3Z1gxuAQrA},
  publisher = {OpenReview.net},
  title = {{PoSE}: Efficient Context Window Extension of {LLMs} via Positional Skip-wise Training},
  url = {https://openreview.net/forum?id=3Z1gxuAQrA},
  year = {2024}
}

@inproceedings{zhu2024enhancing,
  abstract = {Vision Transformers ({ViTs}) have been widely used in various domains. Similar to Convolutional Neural Networks ({CNNs}), {ViTs} are prone to the impacts of adversarial samples, raising security concerns in real-world applications. As one of the most effective black-box attack methods, transferable attacks can generate adversarial samples on surrogate models to directly attack the target model without accessing the parameters. However, due to the distinct internal structures of {ViTs} and {CNNs}, adversarial samples constructed by traditional transferable attack methods may not be applicable to {ViTs}. Therefore, it is imperative to propose more effective transferability attack methods to unveil latent vulnerabilities in {ViTs}. Existing methods have found that applying gradient regularization to extreme gradients across different functional regions in the transformer structure can enhance sample transferability. However, in practice, substantial gradient disparities exist even within the same functional region across different layers. Furthermore, we find that mild gradients therein are the main culprits behind reduced transferability. In this paper, we introduce a novel Gradient Normalization Scaling method for fine-grained gradient editing to enhance the transferability of adversarial attacks on {ViTs}. More importantly, we highlight that {ViTs}, unlike traditional {CNNs}, exhibit distinct attention regions in the frequency domain. Leveraging this insight, we delve into exploring the frequency domain to further enhance the algorithm's transferability.},
  address = {Vienna, Austria},
  author = {Zhiyu Zhu and Xinyi Wang and Zhibo Jin and Jiayu Zhang and Huaming Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024enhancing.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2025-04-16},
  pdf = {https://openreview.net/pdf?id=1BuWv9poWz},
  publisher = {OpenReview.net},
  title = {Enhancing Transferable Adversarial Attacks on Vision Transformers through Gradient Normalization Scaling and High-Frequency Adaptation},
  url = {https://openreview.net/forum?id=1BuWv9poWz},
  year = {2024}
}

@inproceedings{zhu2024quadratic,
  abstract = {While neural networks can be approximated by linear models as their width increases, certain properties of wide neural networks cannot be captured by linear models. In this work we show that recently proposed Neural Quadratic Models can exhibit the ``catapult phase'' that arises when training such models with large learning rates. We then empirically show that the behaviour of quadratic models parallels that of neural networks in generalization, especially in the catapult phase regime. This demonstrates that quadratic models can be an effective tool for analysis of neural networks beyond the standard lazy training regime.},
  address = {Vienna, Austria},
  author = {Libin Zhu and Chaoyue Liu and Adityanarayanan Radhakrishnan and Mikhail Belkin},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024quadratic.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=PvJnX3dwsD},
  publisher = {OpenReview.net},
  title = {Quadratic models for understanding catapult dynamics of neural networks},
  url = {https://openreview.net/forum?id=PvJnX3dwsD},
  year = {2024}
}

@inproceedings{zhu2024minigpt4,
  abstract = {{GPT-4} has demonstrated extraordinary multi-modal abilities, such as directly generating websites from handwritten text and identifying humorous elements within images. These features are rarely observed in previous vision-language models. However, the technical details behind {GPT-4} continue to remain undisclosed. We believe that the enhanced multi-modal generation capabilities of {GPT-4} stem from the utilization of sophisticated large language models ({LLM}). To examine this phenomenon, we present {MiniGPT-4}, which aligns a frozen visual encoder with a frozen {LLM}, Vicuna, using just one projection layer. Our findings reveal that {MiniGPT-4} possesses many capabilities similar to those exhibited by {GPT-4} like detailed image description generation and website creation from hand-written drafts. Furthermore, we also observe other emerging capabilities in {MiniGPT-4}, including writing stories and poems inspired by given images, teaching users how to cook based on food photos, and so on. In our experiment, we found that the model trained on short image caption pairs could produce unnatural language outputs (e.g., repetition and fragmentation). To address this problem, we curate a detailed image description dataset in the second stage to finetune the model, which consequently improves the model's generation reliability and overall usability.},
  address = {Vienna, Austria},
  author = {Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024minigpt4.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=1tZbq88f27},
  publisher = {OpenReview.net},
  title = {{MiniGPT-4}: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  url = {https://openreview.net/forum?id=1tZbq88f27},
  year = {2024}
}

@inproceedings{zhu2024dyval,
  abstract = {Large language models ({LLMs}) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of {LLMs}. In this paper, we introduce {DyVal}, a general and flexible protocol for dynamic evaluation of {LLMs}. Based on our framework, we build graph-informed {DyVal} by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. {DyVal} generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various {LLMs} ranging from Flan-T5-large to {GPT-3.5-Turbo} and {GPT-4}. Experiments show that {LLMs} perform worse in {DyVal}-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, {DyVal}-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of {LLMs} on existing benchmarks.},
  address = {Vienna, Austria},
  author = {Kaijie Zhu and Jiaao Chen and Jindong Wang and Neil Zhenqiang Gong and Diyi Yang and Xing Xie},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024dyval.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=gjfOL9z5Xr},
  publisher = {OpenReview.net},
  title = {{DyVal}: Dynamic Evaluation of Large Language Models for Reasoning Tasks},
  url = {https://openreview.net/forum?id=gjfOL9z5Xr},
  year = {2024}
}

@inproceedings{zhu2024attexplore,
  abstract = {Due to the real-world noise and human-added perturbations, attaining the trustworthiness of deep neural networks ({DNNs}) is a challenging task. Therefore, it becomes essential to offer explanations for the decisions made by these non-linear and complex parameterized models. Attribution methods are promising for this goal, yet its performance can be further improved. In this paper, for the first time, we present that the decision boundary exploration approaches of attribution are consistent with the process for transferable adversarial attacks. Utilizing this consistency, we introduce a novel attribution method via model parameter exploration. Furthermore, inspired by the capability of frequency exploration to investigate the model parameters, we provide enhanced explainability for {DNNs} by manipulating the input features based on frequency information to explore the decision boundaries of different models. Large-scale experiments demonstrate that our {AttEXplore} outperforms other state-of-the-art interpretability methods. Moreover, by employing other transferable attack techniques, {AttEXplore} can explore potential variations in attribution outcomes.},
  address = {Vienna, Austria},
  author = {Zhiyu Zhu and Huaming Chen and Jiayu Zhang and Xinyi Wang and Zhibo Jin and Jason Xue and Flora D. Salim},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024attexplore.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2025-04-16},
  pdf = {https://openreview.net/pdf?id=FsVxd9CIlb},
  publisher = {OpenReview.net},
  title = {{AttEXplore}: Attribution for Explanation with model parameters e{X}ploration},
  url = {https://openreview.net/forum?id=FsVxd9CIlb},
  year = {2024}
}

@inproceedings{zhu2024online,
  abstract = {Spiking neural networks ({SNNs}), attributed to the binary, event-driven nature of spikes, possess heightened biological plausibility and enhanced energy efficiency on neuromorphic hardware compared to analog neural networks ({ANNs}). Mainstream {SNN} training schemes apply backpropagation-through-time ({BPTT}) with surrogate gradients to replace the non-differentiable spike emitting process during backpropagation. While achieving competitive performance, the requirement for storing intermediate information at all time-steps incurs higher memory consumption and fails to fulfill the online property crucial to biological brains. Our work focuses on online training techniques, aiming for memory efficiency while preserving biological plausibility. The limitation of not having access to future information in early time steps in online training has constrained previous efforts to incorporate advantageous modules such as batch normalization. To address this problem, we propose Online Spiking Renormalization ({OSR}) to ensure consistent parameters between testing and training, and Online Threshold Stabilizer ({OTS}) to stabilize neuron firing rates across time steps. Furthermore, we design a novel online approach to compute the sample mean and variance over time for {OSR}. Experiments conducted on various datasets demonstrate the proposed method's superior performance among {SNN} online training algorithms.},
  address = {Vienna, Austria},
  author = {Yaoyu Zhu and Jianhao Ding and Tiejun Huang and Xiaodong Xie and Zhaofei Yu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024online.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=CIj1CVbkpr},
  publisher = {OpenReview.net},
  title = {Online Stabilization of Spiking Neural Networks},
  url = {https://openreview.net/forum?id=CIj1CVbkpr},
  year = {2024}
}

@inproceedings{zhu2024improved,
  abstract = {We revisit the problem of sparse linear regression in the local differential privacy ({LDP}) model. Existing research in the non-interactive and sequentially interactive local models has focused on obtaining the lower bounds for the case where the underlying parameter is 1-sparse, and extending such bounds to the more general $k$-sparse case has proven to be challenging. Moreover, it is unclear whether efficient non-interactive {LDP} ({NLDP}) algorithms exist. To address these issues, we first consider the problem in the $\epsilon$ non-interactive {LDP} model and provide a lower bound of $\Omega(\sqrt{dk \log d}/(\sqrt{n} \epsilon))$ on the $\ell_2$-norm estimation error for sub-Gaussian data, where $n$ is the sample size and $d$ is the dimension of the space. We then propose an innovative {NLDP} algorithm, the very first of its kind for the problem. As a remarkable outcome, this algorithm also yields a novel and highly efficient estimator as a valuable by-product. We further present a sequentially interactive {LDP} algorithm with an improved convergence rate. Finally, extensive experiments on both synthetic and real datasets confirm that our proposed algorithms outperform existing baselines.},
  address = {Vienna, Austria},
  author = {Liyang Zhu and Meng Ding and Vaneet Aggarwal and Jinhui Xu and Di Wang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024improved.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=cVUOnF7iVp},
  publisher = {OpenReview.net},
  title = {Improved Analysis of Sparse Linear Regression in Local Differential Privacy Model},
  url = {https://openreview.net/forum?id=cVUOnF7iVp},
  year = {2024}
}

@inproceedings{zhu2024representation,
  abstract = {We study the representation complexity of model-based and model-free reinforcement learning ({RL}) in the context of circuit complexity. We prove theoretically that there exists a broad class of {MDPs} such that their underlying transition and reward functions can be represented by constant depth circuits with polynomial size, while the optimal $Q$-function suffers an exponential circuit complexity in constant-depth circuits. The theory provides unique insights into why model-based algorithms usually enjoy better sample complexity than model-free algorithms from a novel representation complexity perspective: in some cases, the ground-truth rule (model) of the environment is simple to represent, while other quantities such as value functions are significantly more complex. We further validate our theoretical findings with experiments on various {RL} benchmarks that empirically demonstrate the representation complexity differences between model-based and model-free approaches.},
  address = {Vienna, Austria},
  author = {Hanlin Zhu and Baihe Huang and Stuart Russell},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024representation.pdf:pdf},
  month = {5},
  note = {{DBLP} last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=3K3s9qxSn7},
  publisher = {OpenReview.net},
  title = {On Representation Complexity of Model-based and Model-free Reinforcement Learning},
  url = {https://openreview.net/forum?id=3K3s9qxSn7},
  year = {2024}
}

@inproceedings{zhu2024languagebind,
  abstract = {The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N ‚â• 3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining and then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with 10 Million data with Video, Infrared, Depth, Audio and their corresponding Language. In our VIDAL-10M, all videos are from short video platforms with complete semantics rather than truncated segments from long videos, and all the video, depth, infrared, and audio modalities are aligned to their textual descriptions. LanguageBind has achieved superior performance on a wide range of 15 benchmarks covering video, audio, depth, and infrared. Moreover, multiple experiments have provided evidence for the effectiveness of LanguageBind in achieving indirect alignment and complementarity among diverse modalities.},
  author = {Bin Zhu and Bin Lin and Munan Ning and Yang Yan and Jiaxi Cui and Hongfa Wang and Yatian Pang and Wenhao Jiang and Junwu Zhang and Zongwei Li and Caiwan Zhang and Zhifeng Li and Wei Liu and Li Yuan},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024languagebind.pdf:pdf},
  note = {DBLP last modified: 2025-05-29},
  pdf = {https://openreview.net/pdf?id=QmZKc7UZCy},
  publisher = {OpenReview.net},
  title = {{LanguageBind}: Extending Video-Language Pretraining to {N}-modality by Language-based Semantic Alignment},
  url = {https://openreview.net/forum?id=QmZKc7UZCy},
  year = {2024}
}

@inproceedings{zhu2024unmasking,
  abstract = {Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless \& Red Team, PKU BeaverTails \& SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification.},
  author = {Zhaowei Zhu and Jialu Wang and Hao Cheng and Yang Liu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024unmasking.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=6bcAD6g688},
  publisher = {OpenReview.net},
  title = {Unmasking and Improving Data Credibility: {A} Study with Datasets for Training Harmless Language Models},
  url = {https://openreview.net/forum?id=6bcAD6g688},
  year = {2024}
}

@inproceedings{zhu2024learning,
  abstract = {Training energy-based models (EBMs) on high-dimensional data can be both challenging and time-consuming, and there exists a noticeable gap in sample quality between EBMs and other generative frameworks like GANs and diffusion models. To close this gap, inspired by the recent efforts of learning EBMs by maximizing diffusion recovery likelihood (DRL), we propose cooperative diffusion recovery likelihood (CDRL), an effective approach to tractably learn and sample from a series of EBMs defined on increasingly noisy versions of a dataset, paired with an initializer model for each EBM. At each noise level, the two models are jointly estimated within a cooperative training framework: samples from the initializer serve as starting points that are refined by a few MCMC sampling steps from the EBM.},
  author = {Yaxuan Zhu and Jianwen Xie and Ying Nian Wu and Ruiqi Gao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024learning.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=AyzkDpuqcl},
  publisher = {OpenReview.net},
  title = {Learning Energy-Based Models by Cooperative Diffusion Recovery Likelihood},
  url = {https://openreview.net/forum?id=AyzkDpuqcl},
  year = {2024}
}

@inproceedings{zhu2024hifa,
  abstract = {The advancements in automatic text-to-3D generation have been remarkable. Most existing methods use pre-trained text-to-image diffusion models to optimize 3D representations like Neural Radiance Fields (NeRFs) via latent-space denoising score matching. Yet, these methods often result in artifacts and inconsistencies across different views due to their suboptimal optimization approaches and limited understanding of 3D geometry. Moreover, the inherent constraints of NeRFs in rendering crisp geometry and stable textures usually lead to a two-stage optimization to attain high-resolution details. This work proposes holistic sampling and smoothing approaches to achieve high-quality text-to-3D generation, all in a single-stage optimization.},
  author = {Junzhe Zhu and Peiye Zhuang and Sanmi Koyejo},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024hifa.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=IZMPWmcS3H},
  publisher = {OpenReview.net},
  title = {{HIFA}: High-fidelity Text-to-{3D} Generation with Advanced Diffusion Guidance},
  url = {https://openreview.net/forum?id=IZMPWmcS3H},
  year = {2024}
}

@inproceedings{zhu2024vdc,
  abstract = {The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vulnerable and unreliable. We find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning. It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency.},
  author = {Zihao Zhu and Mingda Zhang and Shaokui Wei and Bingzhe Wu and Baoyuan Wu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhu2024vdc.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=ygxTuVz9eU},
  publisher = {OpenReview.net},
  title = {{VDC}: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models},
  url = {https://openreview.net/forum?id=ygxTuVz9eU},
  year = {2024}
}

@inproceedings{zhuang2024toolchain,
  abstract = {Large language models (LLMs) have demonstrated powerful decision-making and planning capabilities in solving complicated real-world problems. LLM-based autonomous agents can interact with diverse tools (e.g., functional APIs) and generate solution plans that execute a series of API function calls in a step-by-step manner. The multitude of candidate API function calls significantly expands the action space, amplifying the critical need for efficient action space navigation. However, existing methods either struggle with unidirectional exploration in expansive action spaces, trapped into a locally optimal solution, or suffer from exhaustively traversing all potential actions, causing inefficient navigation. To address these issues, we propose ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents. It formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan. By incorporating the A* search algorithm with task-specific cost function design, it efficiently prunes high-cost branches that may involve incorrect actions, identifying the most low-cost valid path as the solution.},
  author = {Yuchen Zhuang and Xiang Chen and Tong Yu and Saayan Mitra and Victor S. Bursztyn and Ryan A. Rossi and Somdeb Sarkhel and Chao Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhuang2024toolchain.pdf:pdf},
  note = {DBLP last modified: 2024-10-15},
  pdf = {https://openreview.net/pdf?id=B6pQxqUcT8},
  publisher = {OpenReview.net},
  title = {{ToolChain*}: Efficient Action Space Navigation in Large Language Models with {A*} Search},
  url = {https://openreview.net/forum?id=B6pQxqUcT8},
  year = {2024}
}

@inproceedings{zhuang2024statistically,
  abstract = {K-means clustering is a widely used machine learning method for identifying patterns in large datasets. Recently, semidefinite programming (SDP) relaxations have been proposed for solving the K-means optimization problem, which enjoy strong statistical optimality guarantees. However, the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. In contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm widely used by machine learning practitioners, but it lacks a solid statistical underpinning and theoretical guarantees. In this paper, we consider an NMF-like algorithm that solves a nonnegative low-rank restriction of the SDP-relaxed K-means formulation using a nonconvex Burer-Monteiro factorization approach. The resulting algorithm is as simple and scalable as state-of-the-art NMF algorithms while also enjoying the same strong statistical optimality guarantees.},
  author = {Yubo Zhuang and Xiaohui Chen and Yun Yang and Richard Y. Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhuang2024statistically.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=v7ZPwoHU1j},
  publisher = {OpenReview.net},
  title = {Statistically Optimal {K}-means Clustering via Nonnegative Low-rank Semidefinite Programming},
  url = {https://openreview.net/forum?id=v7ZPwoHU1j},
  year = {2024}
}

@inproceedings{zhuang2024fedwon,
  abstract = {Federated learning (FL) enhances data privacy with collaborative in-situ training on decentralized clients. Nevertheless, FL encounters challenges due to non-independent and identically distributed (non-i.i.d) data, leading to potential performance degradation and hindered convergence. While prior studies predominantly addressed the issue of skewed label distribution, our research addresses a crucial yet frequently overlooked problem known as multi-domain FL. In this scenario, clients' data originate from diverse domains with distinct feature distributions, instead of label distributions. To address the multi-domain problem in FL, we propose a novel method called Federated Learning Without Normalizations (FedWon). The approach draws inspiration from the observation that batch normalization (BN) faces challenges in effectively modeling the statistics of multiple domains, while existing normalization techniques possess their own limitations. FedWon eliminates the normalization layers in FL and reparameterizes convolution layers with scaled weight standardization.},
  author = {Weiming Zhuang and Lingjuan Lyu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhuang2024fedwon.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=hAYHmV1gM8},
  publisher = {OpenReview.net},
  title = {{FedWon}: Triumphing Multi-domain Federated Learning Without Normalization},
  url = {https://openreview.net/forum?id=hAYHmV1gM8},
  year = {2024}
}

@inproceedings{zhuang2024backdoor,
  abstract = {Federated learning (FL) has been widely deployed to enable machine learning training on sensitive data across distributed devices. However, the decentralized learning paradigm and heterogeneity of FL further extend the attack surface for backdoor attacks. In this work, we identify the concept of backdoor-critical (BC) layers - a small subset of layers that dominate the model's backdoor behaviors. Based on this insight, we design an efficient backdoor attack for federated learning that focuses on poisoning BC layers. We propose a novel approach to identify BC layers and demonstrate that targeting these layers with only 10% malicious clients can successfully perform backdoor attacks even under state-of-the-art defenses.},
  author = {Haomin Zhuang and Mingxian Yu and Hao Wang 0022 and Yang Hua 0001 and Jian Li 0008 and Xu Yuan 0001},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhuang2024backdoor.pdf:pdf},
  note = {DBLP last modified: 2025-02-12},
  pdf = {https://openreview.net/pdf?id=AJBGSVSTT2},
  publisher = {OpenReview.net},
  title = {Backdoor Federated Learning by Poisoning Backdoor-Critical Layers},
  url = {https://openreview.net/forum?id=AJBGSVSTT2},
  year = {2024}
}

@inproceedings{zhuang2024gradual,
  abstract = {Domain shift degrades classification models when deployed on new data distributions. Conventional unsupervised domain adaptation (UDA) aims to learn features that bridge labeled source and unlabeled target domains. In this work, we propose a gradual domain adaptation method that dynamically generates intermediate domains and gradually updates the classifier via gradient flow. Our approach leverages the continuous nature of gradient flow to create a smooth transition between source and target domains, enabling more effective adaptation. We demonstrate that this gradual adaptation strategy achieves superior performance compared to direct domain adaptation methods across multiple benchmarks.},
  author = {Zhan Zhuang and Yu Zhang and Ying Wei},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhuang2024gradual.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=iTTZFKrlGV},
  publisher = {OpenReview.net},
  title = {Gradual Domain Adaptation via Gradient Flow},
  url = {https://openreview.net/forum?id=iTTZFKrlGV},
  year = {2024}
}

@inproceedings{zhuo2024partitioning,
  abstract = {Label imbalance and homophily-heterophily mixture are the fundamental problems encountered when applying Graph Neural Networks ({GNN}s) to Graph Fraud Detection ({GFD}) tasks. Existing {GNN}-based {GFD} models are designed to augment graph structure to accommodate the inductive bias of {GNN}s towards homophily, by excluding heterophilic neighbors during message passing. In our work, we argue that the key to applying {GNN}s for {GFD} is not to exclude but to distinguish neighbors with different labels. Grounded in this perspective, we introduce Partitioning Message Passing ({PMP}), an intuitive yet effective message passing paradigm expressly crafted for {GFD}. Specifically, in the neighbor aggregation stage of {PMP}, neighbors with different classes are aggregated with distinct node-specific aggregation functions. By this means, the center node can adaptively adjust the information aggregated from its heterophilic and homophilic neighbors, thus avoiding the model gradient being dominated by benign nodes which occupy the majority of the population. We theoretically establish a connection between the spatial formulation of {PMP} and spectral analysis to characterize that {PMP} operates an adaptive node-specific spectral graph filter, which demonstrates the capability of {PMP} to handle heterophily-homophily mixed graphs. Extensive experimental results show that {PMP} can significantly boost the performance on {GFD} tasks.},
  author = {Wei Zhuo and Zemin Liu and Bryan Hooi and Bingsheng He and Guang Tan and Rizal Fathony and Jia Chen},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zhuo2024partitioning.pdf:pdf},
  note = {DBLP last modified: 2025-01-09},
  pdf = {https://openreview.net/pdf?id=tEgrUrUuwA},
  publisher = {OpenReview.net},
  title = {Partitioning Message Passing for Graph Fraud Detection},
  url = {https://openreview.net/forum?id=tEgrUrUuwA},
  year = {2024}
}

@inproceedings{zimmer2024sparse,
  abstract = {Neural networks can be significantly compressed by pruning, yielding sparse models with reduced storage and computational demands while preserving predictive performance. Model soups (Wortsman et al., 2022) enhance generalization and out-of-distribution ({OOD}) performance by averaging the parameters of multiple models into a single one, without increasing inference time. However, achieving both sparsity and parameter averaging is challenging as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. This work addresses these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning ({IMP}) with varied hyperparameter configurations such as batch ordering or weight decay yields models suitable for averaging, sharing identical sparse connectivity by design. Averaging these models significantly enhances generalization and {OOD} performance over their individual counterparts. Building on this, we introduce Sparse Model Soups ({SMS}), a novel method for merging sparse models by initiating each prune-retrain cycle with the averaged model from the previous phase. {SMS} preserves sparsity, exploits sparse network benefits, is modular and fully parallelizable, and substantially improves {IMP}'s performance. We further demonstrate that {SMS} can be adapted to enhance state-of-the-art pruning-during-training approaches.},
  author = {Max Zimmer and Christoph Spiegel and Sebastian Pokutta},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zimmer2024sparse.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=xx0ITyHp3u},
  publisher = {OpenReview.net},
  title = {Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging},
  url = {https://openreview.net/forum?id=xx0ITyHp3u},
  year = {2024}
}

@inproceedings{ziv2024masked,
  abstract = {We introduce {MAGNeT}, a masked generative sequence modeling method that operates directly over several streams of audio tokens. Unlike prior work, {MAGNeT} is comprised of a single-stage, non-autoregressive transformer. During training, we predict spans of masked tokens obtained from a masking scheduler, while during inference we gradually construct the output sequence using several decoding steps. To further enhance the quality of the generated audio, we introduce a novel rescoring method in which, we leverage an external pre-trained model to rescore and rank predictions from {MAGNeT}, which will be then used for later decoding steps. Lastly, we explore a hybrid version of {MAGNeT}, in which we fuse between autoregressive and non-autoregressive models to generate the first few seconds in an autoregressive manner. We demonstrate the efficiency of {MAGNeT} for the task of text-to-music and text-to-audio generation and conduct an extensive empirical evaluation, considering both objective metrics and human studies. The proposed approach is comparable to the evaluated baselines, while being significantly faster (7x faster than the autoregressive baseline). Through ablation studies and analysis, we shed light on the importance of each of the components comprising {MAGNeT}, together with pointing to the trade-offs between autoregressive and non-autoregressive modeling, considering latency, throughput, and generation quality.},
  author = {Alon Ziv and Itai Gat and Gal {Le Lan} and Tal Remez and Felix Kreuk and Jade Copet and Alexandre D√©fossez and Gabriel Synnaeve and Yossi Adi},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/ziv2024masked.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=Ny8NiVfi95},
  publisher = {OpenReview.net},
  title = {Masked Audio Generation using a Single Non-Autoregressive Transformer},
  url = {https://openreview.net/forum?id=Ny8NiVfi95},
  year = {2024}
}

@inproceedings{zollo2024prompt,
  abstract = {The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical question summarization, and code generation highlight how such a framework can foster responsible deployment by reducing the risk of the worst outcomes.},
  author = {Thomas P. Zollo and Todd Morrill and Zhun Deng and Jake C. Snell and Toniann Pitassi and Richard S. Zemel},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zollo2024prompt.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=5tGGWOijvq},
  publisher = {OpenReview.net},
  title = {Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models},
  url = {https://openreview.net/forum?id=5tGGWOijvq},
  year = {2024}
}

@inproceedings{zou2024vflair,
  abstract = {Vertical Federated Learning ({VFL}) has emerged as a collaborative training paradigm that allows participants with different features of the same group of users to accomplish cooperative training without exposing their raw data or model parameters. {VFL} has gained significant attention for its research potential and real-world applications in recent years, but still faces substantial challenges, such as in defending various kinds of data inference and backdoor attacks. Moreover, most of existing {VFL} projects are industry-facing and not easily used for keeping track of the current research progress. To address this need, we present an extensible and lightweight {VFL} framework {VFLAIR}, which supports {VFL} training with a variety of models, datasets and protocols, along with standardized modules for comprehensive evaluations of attacks and defense strategies. We also benchmark 11 attacks and 8 defenses performance under different communication and model partition settings and draw concrete insights and recommendations on the choice of defense strategies for different practical {VFL} deployment scenarios.},
  author = {Tianyuan Zou and Zixuan Gu and Yu He and Hideaki Takahashi and Yang Liu and Ya-Qin Zhang},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zou2024vflair.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=sqRgz88TM3},
  publisher = {OpenReview.net},
  title = {{VFLAIR}: A Research Library and Benchmark for Vertical Federated Learning},
  url = {https://openreview.net/forum?id=sqRgz88TM3},
  year = {2024}
}

@inproceedings{zou2024towards,
  abstract = {Generalizing to out-of-distribution ({OOD}) data or unseen domain, termed {OOD} generalization, still lacks appropriate theoretical guarantees. Canonical {OOD} bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences {OOD} generalization. To bridge this gap between optimization and {OOD} generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by ``robustness'' in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better {OOD} guarantees for robust algorithms. It also provides a theoretical backing for ``flat minima leads to better {OOD} generalization''. We propose a sharpness-based {OOD} generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees.},
  author = {Yingtian Zou and Kenji Kawaguchi and Yingnan Liu and Jiashuo Liu and Mong-Li Lee and Wynne Hsu},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zou2024towards.pdf:pdf},
  note = {DBLP last modified: 2025-05-14},
  pdf = {https://openreview.net/pdf?id=tPEwSYPtAC},
  publisher = {OpenReview.net},
  title = {Towards Robust Out-of-Distribution Generalization Bounds via Sharpness},
  url = {https://openreview.net/forum?id=tPEwSYPtAC},
  year = {2024}
}

@inproceedings{zou2024multilevel,
  abstract = {The fast growing capabilities of large-scale deep learning models, such as {BERT}, {GPT} and {ViT}, are revolutionizing the landscape of {NLP}, {CV} and many other domains. Training such models, however, poses an unprecedented demand for computing power, which incurs exponentially increasing energy cost and carbon dioxide emissions. It is thus critical to develop efficient training solutions to reduce the training costs. Motivated by a set of key observations of inter- and intra-layer similarities among feature maps and attentions that can be identified from typical training processes, we propose a multi-level framework for training acceleration. Specifically, the framework is based on three basic operators, Coalescing, De-coalescing and Interpolation, which can be orchestrated to build a multi-level training framework that reduces training cost while maintaining model performance.},
  author = {Longwei Zou and Han Zhang and Yangdong Deng},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zou2024multilevel.pdf:pdf},
  note = {DBLP last modified: 2024-08-07},
  pdf = {https://openreview.net/pdf?id=BI1N3lTWtn},
  publisher = {OpenReview.net},
  title = {A Multi-Level Framework for Accelerating Training Transformer Models},
  url = {https://openreview.net/forum?id=BI1N3lTWtn},
  year = {2024}
}

@inproceedings{zuo2024interventional,
  abstract = {Fair machine learning aims to prevent discrimination against individuals or sub-populations based on sensitive attributes such as gender and race. In recent years, causal inference methods have been increasingly used in fair machine learning to measure unfairness by causal effects. However, current methods assume that the true causal graph is given, which is often not true in real-world applications. To address this limitation, this paper proposes a framework for achieving causal fairness based on the notion of interventions when the true causal graph is partially known. The proposed approach involves modeling fair prediction using a Partially Directed Acyclic Graph ({PDAG}), specifically, a class of causal {DAG}s that can be learned from observational data combined with domain knowledge. The {PDAG} is used to measure causal fairness, and a constrained optimization problem is formulated to balance between fairness and accuracy. Results on both simulated and real-world datasets demonstrate the effectiveness of this method.},
  author = {Aoqi Zuo and Yiqing Li 0002 and Susan Wei and Mingming Gong},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zuo2024interventional.pdf:pdf},
  note = {DBLP last modified: 2025-05-28},
  pdf = {https://openreview.net/pdf?id=SKulT2VX9p},
  publisher = {OpenReview.net},
  title = {Interventional Fairness on Partially Known Causal Graphs: A Constrained Optimization Approach},
  url = {https://openreview.net/forum?id=SKulT2VX9p},
  year = {2024}
}

@inproceedings{zweig2024symmetric,
  abstract = {Few neural architectures lend themselves to provable learning with gradient based methods. One popular model is the single-index model, in which labels are produced by composing an unknown linear projection with a possibly unknown scalar link function. Learning this model with {SGD} is relatively well-understood, whereby the so-called information exponent of the link function governs a polynomial sample complexity rate. However, extending this analysis to deeper or more complicated architectures remains challenging. In this work, we consider single index learning in the setting of symmetric neural networks. Under analytic assumptions on the activation and maximum degree assumptions on the link function, we prove that gradient flow recovers the hidden planted direction, represented as a finitely supported vector in the feature space of power sum polynomials. We characterize a notion of information exponent adapted to our setting that controls the efficiency of learning.},
  author = {Aaron Zweig and Joan Bruna},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/zweig2024symmetric.pdf:pdf},
  note = {DBLP last modified: 2024-08-26},
  pdf = {https://openreview.net/pdf?id=e1vqloonRy},
  publisher = {OpenReview.net},
  title = {Symmetric Single Index Learning},
  url = {https://openreview.net/forum?id=e1vqloonRy},
  year = {2024}
}

@inproceedings{dascoli2024ode,
  abstract = {We introduce {ODE}Former, the first transformer able to infer multidimensional ordinary differential equation ({ODE}) systems in symbolic form from the observation of a single solution trajectory. We perform extensive evaluations on two datasets: (i) the existing '{S}trogatz' dataset featuring two-dimensional systems; (ii) {ODE}Bench, a collection of one- to four-dimensional systems that we carefully curated from the literature to provide a more holistic benchmark. {ODE}Former consistently outperforms existing methods while displaying substantially improved robustness to noisy and irregularly sampled observations, as well as faster inference.},
  author = {Stphane d'Ascoli and Sren Becker and Philippe Schwaller and Alexander Mathis and Niki Kilbertus},
  booktitle = {The Twelfth International Conference on Learning Representations},
  file = {:/home/b/documents/inproceedings/dascoli2024ode.pdf:pdf},
  note = {DBLP last modified: 2024-07-29},
  pdf = {https://openreview.net/pdf?id=TzoHLiGVMo},
  publisher = {OpenReview.net},
  title = {{ODE}Former: Symbolic Regression of Dynamical Systems with Transformers},
  url = {https://openreview.net/forum?id=TzoHLiGVMo},
  year = {2024}
}
