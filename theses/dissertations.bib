@phdthesis{ariosto2025,
  address = {Varese, Italy},
  author = {Sebastiano Ariosto},
  department = {Dipartimento di Scienza e Alta Tecnologia},
  file = {:/home/b/documents/phdthesis/ariosto2025.pdf:pdf},
  month = {January},
  note = {Available as arXiv preprint arXiv:2501.19281},
  openalex = {W4407091726},
  pdf = {https://arxiv.org/pdf/2501.19281.pdf},
  school = {Università degli Studi dell'Insubria},
  title = {Statistical Physics of Deep Neural Networks: Generalization Capability, Beyond the Infinite Width, and Feature Learning},
  type = {PhD thesis},
  url = {https://arxiv.org/abs/2501.19281},
  year = {2025}
}

@phdthesis{cui2024,
  address = {Lausanne, Switzerland},
  advisor = {Lenka Zdeborová},
  author = {Hugo Chao Cui},
  doi = {10.5075/epfl-thesis-10948},
  file = {:/home/b/documents/phdthesis/cui2024.pdf:pdf},
  isbn = {9782832213609},
  keywords = {statistical physics, machine learning, high-dimensional learning, feature extraction, neural networks, kernel methods, Bayesian learning},
  note = {EPFL Physics doctoral thesis award 1st prize; EPFL best 8% PhD distinction in Physics},
  pages = {xvi + 190},
  pdf = {https://infoscience.epfl.ch/record/311765/files/EPFL_TH10948.pdf},
  school = {École Polytechnique Fédérale de Lausanne},
  title = {Topics in statistical physics of high-dimensional machine learning},
  type = {PhD thesis},
  url = {https://infoscience.epfl.ch/handle/20.500.14299/208803},
  year = {2024}
}

@phdthesis{zavatoneveth2023,
  abstract = {This thesis collects a few of my essays towards understanding representation learning and generalization in neural networks. I focus on the model setting of Bayesian learning and inference, where the problem of deep learning is naturally viewed through the lens of statistical mechanics.},
  address = {Cambridge, MA, USA},
  author = {Jacob Andreas Zavatone-Veth},
  file = {:/home/b/documents/phdthesis/zavatoneveth2023.pdf:pdf},
  keywords = {Deep learning, Random matrices, Theoretical neuroscience, Theoretical physics, Neurosciences, Statistical physics},
  month = {April},
  note = {PhD thesis defended April 4, 2024. Recipient of 2024 American Physical Society Dissertation Award in Statistical and Nonlinear Physics. Permanent URI: https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37378715},
  pdf = {https://dash.harvard.edu/bitstreams/3c4d5429-39e9-4bb7-90ee-17dd110a3499/download},
  school = {Harvard University Graduate School of Arts and Sciences},
  title = {Statistical mechanics of Bayesian inference and learning in neural networks},
  url = {https://dash.harvard.edu/entities/publication/081c6cc0-6ae2-4066-8618-bd19ebc24293},
  year = {2024}
}

@phdthesis{pezzicoli2024,
  address = {Paris, France},
  author = {Francesco Saverio Pezzicoli},
  file = {:/home/b/documents/phdthesis/pezzicoli2024.pdf:pdf},
  hal = {tel-04910839},
  halversion = {v1},
  keywords = {Statistical Physics, Machine Learning, Replica Theory, Class Imbalance, Graph Neural Networks, SE(3)-Equivariant, Dynamical Heterogeneities, Glassy Liquids},
  month = {January},
  nnt = {2024UPASG115},
  note = {Supervised by Guillaume Charpiat and François Pascal Landes},
  openalex = {W4406880525},
  pdf = {https://theses.hal.science/tel-04910839v1/file/142018_PEZZICOLI_2024_archivage.pdf},
  school = {Université Paris-Saclay},
  subject = {Machine Learning [cs.LG]},
  title = {Statistical Physics - Machine Learning Interplay: From Addressing Class Imbalance with Replica Theory to Predicting Dynamical Heterogeneities with SE(3)-Equivariant Graph Neural Networks},
  type = {PhD thesis},
  url = {https://theses.hal.science/tel-04910839v1},
  year = {2024}
}

@phdthesis{marion2023,
  address = {Paris, France},
  author = {Pierre Marion},
  file = {:/home/b/documents/phdthesis/marion2023.pdf:pdf},
  hal = {tel-04453458},
  keywords = {deep learning, neural networks, generalization bounds, optimization, continuous-time models, residual networks, transformers},
  month = {November},
  note = {NNT: 2023SORUS517},
  openalex = {W4392673599},
  pdf = {https://theses.hal.science/tel-04453458v1/file/MARION_Pierre_these_2023.pdf},
  school = {Sorbonne Université},
  title = {Mathematics of deep learning: generalization, optimization, continuous-time models},
  type = {PhD thesis},
  url = {https://theses.hal.science/tel-04453458},
  year = {2023}
}

@phdthesis{baskerville2023,
  abstract = {Neural network models are one of the most successful approaches to machine learning, yet the theoretical understanding of neural networks trails significantly behind their practical success. Random matrix theory provides a rich framework of tools with which aspects of neural network phenomenology can be explored theoretically. This thesis establishes significant extensions of prior work using random matrix theory to understand and describe the loss surfaces of large neural networks, particularly generalising to different architectures.},
  archiveprefix = {arXiv},
  author = {Nicholas P. Baskerville},
  doi = {10.48550/arXiv.2306.02108},
  eprint = {2306.02108},
  file = {:/home/b/documents/phdthesis/baskerville2023.pdf:pdf},
  keywords = {random matrix theory, neural networks, loss surfaces, mathematical physics, machine learning},
  month = {June},
  note = {arXiv:2306.02108},
  pages = {320},
  primaryclass = {math-ph},
  school = {University of Bristol},
  supervisor = {J. Najnudel and F. Mezzadri and J. P. Keating},
  title = {Random matrix theory and the loss surfaces of neural networks},
  type = {PhD thesis},
  url = {https://arxiv.org/abs/2306.02108},
  year = {2023}
}

@phdthesis{zhong2023,
  address = {Cambridge, MA},
  advisor = {Haim Sompolinsky and Mehran Kardar},
  author = {Weishun Zhong},
  department = {Department of Physics},
  file = {:/home/b/documents/phdthesis/zhong2023.pdf:pdf},
  month = {June},
  note = {Also available as arXiv:2308.01538},
  openalex = {W4385964745},
  pdf = {https://arxiv.org/pdf/2308.01538.pdf},
  school = {Massachusetts Institute of Technology},
  title = {Non-equilibrium physics: from spin glasses to machine and neural learning},
  type = {Ph.D. thesis},
  url = {https://hdl.handle.net/1721.1/152568},
  year = {2023}
}

@phdthesis{mignacco2022,
  abstract = {This thesis explores the mechanisms underlying learning in artificial neural networks through the prism of statistical physics. It addresses both static properties of learning problems and the dynamics of learning algorithms, examining prototype classification, multi-class classification, stochastic gradient descent dynamics, and the role of stochasticity in non-convex optimization.},
  address = {Paris, France},
  advisor = {Lenka Zdeborová and Pierfrancesco Urbani},
  author = {Francesca Mignacco},
  file = {:/home/b/documents/phdthesis/mignacco2022.pdf:pdf},
  hal = {tel-03848811},
  halversion = {v1},
  keywords = {Artificial neural networks, Disordered systems, Dynamics of learning, Stochastic gradient descent},
  note = {NNT: 2022UPASP074},
  pdf = {https://theses.hal.science/tel-03848811v1/file/115507_MIGNACCO_2022_archivage.pdf},
  school = {Université Paris-Saclay},
  title = {Statistical physics insights on the dynamics and generalisation of artificial neural networks},
  type = {PhD thesis},
  url = {https://theses.hal.science/tel-03848811v1},
  year = {2022}
}

@phdthesis{canatar2022,
  abstract = {A theoretical exploration of generalization in machine learning models, focusing on kernel regression and wide neural networks using statistical mechanics' replica theory. The work develops an analytical theory applicable to any kernel and data distribution, identifying inductive bias towards simple functions and phase transitions in generalization performance.},
  address = {Cambridge, MA, USA},
  advisor = {Cengiz Pehlevan},
  author = {Abdulkadir Canatar},
  file = {:/home/b/documents/phdthesis/canatar2022.pdf:pdf},
  keywords = {Generalization Error, Kernel Regression, Replica Theory, Artificial intelligence, Statistical physics},
  month = {September},
  school = {Harvard University},
  title = {Statistical Mechanics of Generalization in Kernel Regression and Wide Neural Networks},
  type = {Ph.D. dissertation},
  url = {https://dash.harvard.edu/handle/1/37373673},
  year = {2022}
}

@phdthesis{loureiro2018,
  abstract = {This thesis investigates holographic dualities as tools for studying universal properties of strongly coupled field theories, with focus on theories without translational symmetry through three new approaches. Covers phenomenological holographic models achieving momentum relaxation, holographic theories that explicitly break translational symmetry, and studies of spatially varying random Maxwell potentials driving dual field theory to non-trivial infra-red fixed points with emerging scale invariance.},
  address = {Cambridge, UK},
  advisor = {Antonio M. Garcia-Garcia},
  author = {Bruno Loureiro},
  department = {Department of Applied Mathematics and Theoretical Physics},
  doi = {10.17863/CAM.25246},
  file = {:/home/b/documents/phdthesis/loureiro2018.pdf:pdf},
  keywords = {holographic duality, AdS/CFT correspondence, disorder, momentum relaxation, SYK model, condensed matter theory},
  month = {April},
  note = {Thesis focuses on applications of holographic principle to strongly coupled condensed matter systems with particular focus on disordered systems},
  openalex = {W2887505930},
  pages = {1--180},
  school = {University of Cambridge},
  title = {Disorder in holographic field theories: inhomogeneous geometries, momentum relaxation and SYK models},
  type = {PhD thesis},
  url = {https://www.repository.cam.ac.uk/handle/1810/277911},
  year = {2019}
}

@phdthesis{chung2017,
  address = {Cambridge, MA},
  advisor = {Haim Sompolinsky and Ryan P. Adams},
  author = {SueYeon Chung},
  file = {:/home/b/documents/phdthesis/chung2017.pdf:pdf},
  keywords = {Replica Theory, Perceptron, Support Vector Machines, Object Manifolds, Perceptual Invariance, Object Recognition, Statistical Mechanics},
  month = {May},
  note = {Graduate School of Arts and Sciences},
  pdf = {https://dash.harvard.edu/bitstream/handle/1/41141361/CHUNG-DISSERTATION-2017.pdf?sequence=1&isAllowed=y},
  school = {Harvard University},
  title = {Statistical Mechanics of Neural Processing of Object Manifolds},
  type = {Ph.D. dissertation},
  url = {http://nrs.harvard.edu/urn-3:HUL.InstRepos:41141361},
  year = {2017}
}

@phdthesis{saxe2015,
  address = {Stanford, CA, USA},
  advisor = {James L. McClelland and Andrew Y. Ng and Surya Ganguli and Christoph E. Schreiner},
  author = {Andrew Michael Saxe},
  department = {Department of Electrical Engineering},
  file = {:/home/b/documents/phdthesis/saxe2015.pdf:pdf},
  month = {June},
  note = {Robert J. Glushko Dissertation Prize recipient},
  pdf = {https://stacks.stanford.edu/file/druid:nv482qj2831/Thesis-augmented.pdf},
  school = {Stanford University},
  title = {Deep Linear Neural Networks: A Theory of Learning in the Brain and Mind},
  type = {Ph.D. thesis},
  url = {https://purl.stanford.edu/nv482qj2831},
  year = {2015}
}

@phdthesis{zdeborova2008,
  abstract = {This thesis addresses the question: How to recognize if an NP-complete constraint satisfaction problem is typically hard and what are the main reasons for this? The research adopts approaches from the statistical physics of disordered systems, in particular the cavity method developed originally to describe glassy systems.},
  address = {Paris, France and Prague, Czech Republic},
  arxiv = {0806.4112},
  author = {Lenka Zdeborová},
  file = {:/home/b/documents/phdthesis/zdeborova2008.pdf:pdf},
  hal_id = {tel-00294232},
  keywords = {optimization problems, computational complexity, cavity method, spin glasses, phase transitions},
  month = {May},
  nnt = {2008PA112080},
  note = {Joint PhD degree (cotutelle)},
  pdf = {https://theses.hal.science/tel-00294232/file/lenicka.pdf},
  school = {Université Paris-Sud and Univerzita Karlova v Praze},
  supervisor = {Marc Mézard and Václav Janiš},
  title = {Statistical Physics of Hard Optimization Problems},
  type = {PhD thesis},
  url = {https://theses.hal.science/tel-00294232},
  year = {2008}
}

@phdthesis{whyte1995,
  abstract = {This thesis investigates five problems in the statistical mechanics of neural networks. The first three problems involve attractor neural networks that optimize particular cost functions for storage of static memories as attractors of the neural dynamics. Research areas include effects of replica symmetry breaking, noise-optimal networks, perceptron algorithm performance, sequence storage and processing, and impact of correlations on network behavior.},
  author = {William John Whyte},
  file = {:/home/b/documents/phdthesis/whyte1995.pdf:pdf},
  keywords = {Neural networks, Statistical mechanics, Attractor networks, Replica symmetry breaking},
  note = {EThOS thesis ID: uk.bl.ethos.297261},
  school = {University of Oxford},
  title = {Statistical mechanics of neural networks},
  type = {DPhil thesis},
  url = {https://ora.ox.ac.uk/objects/uuid:e17f9b27-58ac-41ad-8722-cfab75139d9a},
  year = {1995}
}

@phdthesis{dunmur1994,
  abstract = {Examines the performance of a simple neural network model learning a rule from noisy examples using methods of statistical mechanics. The free energy for the model is defined and order parameters that capture the statistical behaviour of the system are evaluated analytically. A weight decay term is used to regularise the effect of the noise added to the examples.},
  author = {Alan P. Dunmur},
  file = {:/home/b/documents/phdthesis/dunmur1994.pdf:pdf},
  keywords = {statistical mechanics, neural networks, regularisation, generalisation, weight decay, noisy data},
  note = {Available from Edinburgh Research Archive; PDF download possible.},
  openalex = {W2302528735},
  school = {University of Edinburgh},
  title = {Statistical mechanics, generalisation and regularisation of neural network models},
  type = {PhD thesis},
  url = {https://era.ed.ac.uk/handle/1842/13743},
  year = {1994}
}

@phdthesis{feynman1942principle,
  address = {Princeton, NJ, USA},
  advisor = {John Archibald Wheeler},
  author = {Richard Phillips Feynman},
  file = {:/home/b/documents/phdthesis/feynman1942principle.pdf:pdf},
  keywords = {quantum mechanics, path integrals, least action principle, Wheeler-Feynman absorber theory},
  month = {May},
  note = {Foundational work on path integral formulation of quantum mechanics. Later republished by World Scientific (2005) as ``Feynman's Thesis: A New Approach to Quantum Theory''},
  school = {Princeton University},
  title = {The Principle of Least Action in Quantum Mechanics},
  type = {PhD thesis},
  year = {1942}
}
